function dd(e,t){for(var o=0;o<t.length;o++){const n=t[o];if(typeof n!="string"&&!Array.isArray(n)){for(const i in n)if(i!=="default"&&!(i in e)){const a=Object.getOwnPropertyDescriptor(n,i);a&&Object.defineProperty(e,i,a.get?a:{enumerable:!0,get:()=>n[i]})}}}return Object.freeze(Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}))}(function(){const t=document.createElement("link").relList;if(t&&t.supports&&t.supports("modulepreload"))return;for(const i of document.querySelectorAll('link[rel="modulepreload"]'))n(i);new MutationObserver(i=>{for(const a of i)if(a.type==="childList")for(const r of a.addedNodes)r.tagName==="LINK"&&r.rel==="modulepreload"&&n(r)}).observe(document,{childList:!0,subtree:!0});function o(i){const a={};return i.integrity&&(a.integrity=i.integrity),i.referrerPolicy&&(a.referrerPolicy=i.referrerPolicy),i.crossOrigin==="use-credentials"?a.credentials="include":i.crossOrigin==="anonymous"?a.credentials="omit":a.credentials="same-origin",a}function n(i){if(i.ep)return;i.ep=!0;const a=o(i);fetch(i.href,a)}})();function hd(e){return e&&e.__esModule&&Object.prototype.hasOwnProperty.call(e,"default")?e.default:e}var El={exports:{}},Ai={},Bl={exports:{}},R={};/**
 * @license React
 * react.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var mn=Symbol.for("react.element"),pd=Symbol.for("react.portal"),gd=Symbol.for("react.fragment"),md=Symbol.for("react.strict_mode"),fd=Symbol.for("react.profiler"),yd=Symbol.for("react.provider"),vd=Symbol.for("react.context"),bd=Symbol.for("react.forward_ref"),wd=Symbol.for("react.suspense"),Cd=Symbol.for("react.memo"),Sd=Symbol.for("react.lazy"),cs=Symbol.iterator;function Ad(e){return e===null||typeof e!="object"?null:(e=cs&&e[cs]||e["@@iterator"],typeof e=="function"?e:null)}var Gl={isMounted:function(){return!1},enqueueForceUpdate:function(){},enqueueReplaceState:function(){},enqueueSetState:function(){}},ql=Object.assign,Ml={};function So(e,t,o){this.props=e,this.context=t,this.refs=Ml,this.updater=o||Gl}So.prototype.isReactComponent={};So.prototype.setState=function(e,t){if(typeof e!="object"&&typeof e!="function"&&e!=null)throw Error("setState(...): takes an object of state variables to update or a function which returns an object of state variables.");this.updater.enqueueSetState(this,e,t,"setState")};So.prototype.forceUpdate=function(e){this.updater.enqueueForceUpdate(this,e,"forceUpdate")};function Ll(){}Ll.prototype=So.prototype;function dr(e,t,o){this.props=e,this.context=t,this.refs=Ml,this.updater=o||Gl}var hr=dr.prototype=new Ll;hr.constructor=dr;ql(hr,So.prototype);hr.isPureReactComponent=!0;var us=Array.isArray,zl=Object.prototype.hasOwnProperty,pr={current:null},Rl={key:!0,ref:!0,__self:!0,__source:!0};function jl(e,t,o){var n,i={},a=null,r=null;if(t!=null)for(n in t.ref!==void 0&&(r=t.ref),t.key!==void 0&&(a=""+t.key),t)zl.call(t,n)&&!Rl.hasOwnProperty(n)&&(i[n]=t[n]);var s=arguments.length-2;if(s===1)i.children=o;else if(1<s){for(var l=Array(s),c=0;c<s;c++)l[c]=arguments[c+2];i.children=l}if(e&&e.defaultProps)for(n in s=e.defaultProps,s)i[n]===void 0&&(i[n]=s[n]);return{$$typeof:mn,type:e,key:a,ref:r,props:i,_owner:pr.current}}function kd(e,t){return{$$typeof:mn,type:e.type,key:t,ref:e.ref,props:e.props,_owner:e._owner}}function gr(e){return typeof e=="object"&&e!==null&&e.$$typeof===mn}function xd(e){var t={"=":"=0",":":"=2"};return"$"+e.replace(/[=:]/g,function(o){return t[o]})}var ds=/\/+/g;function Oi(e,t){return typeof e=="object"&&e!==null&&e.key!=null?xd(""+e.key):t.toString(36)}function Un(e,t,o,n,i){var a=typeof e;(a==="undefined"||a==="boolean")&&(e=null);var r=!1;if(e===null)r=!0;else switch(a){case"string":case"number":r=!0;break;case"object":switch(e.$$typeof){case mn:case pd:r=!0}}if(r)return r=e,i=i(r),e=n===""?"."+Oi(r,0):n,us(i)?(o="",e!=null&&(o=e.replace(ds,"$&/")+"/"),Un(i,t,o,"",function(c){return c})):i!=null&&(gr(i)&&(i=kd(i,o+(!i.key||r&&r.key===i.key?"":(""+i.key).replace(ds,"$&/")+"/")+e)),t.push(i)),1;if(r=0,n=n===""?".":n+":",us(e))for(var s=0;s<e.length;s++){a=e[s];var l=n+Oi(a,s);r+=Un(a,t,o,l,i)}else if(l=Ad(e),typeof l=="function")for(e=l.call(e),s=0;!(a=e.next()).done;)a=a.value,l=n+Oi(a,s++),r+=Un(a,t,o,l,i);else if(a==="object")throw t=String(e),Error("Objects are not valid as a React child (found: "+(t==="[object Object]"?"object with keys {"+Object.keys(e).join(", ")+"}":t)+"). If you meant to render a collection of children, use an array instead.");return r}function Cn(e,t,o){if(e==null)return e;var n=[],i=0;return Un(e,n,"","",function(a){return t.call(o,a,i++)}),n}function Pd(e){if(e._status===-1){var t=e._result;t=t(),t.then(function(o){(e._status===0||e._status===-1)&&(e._status=1,e._result=o)},function(o){(e._status===0||e._status===-1)&&(e._status=2,e._result=o)}),e._status===-1&&(e._status=0,e._result=t)}if(e._status===1)return e._result.default;throw e._result}var ve={current:null},Yn={transition:null},Td={ReactCurrentDispatcher:ve,ReactCurrentBatchConfig:Yn,ReactCurrentOwner:pr};function Ul(){throw Error("act(...) is not supported in production builds of React.")}R.Children={map:Cn,forEach:function(e,t,o){Cn(e,function(){t.apply(this,arguments)},o)},count:function(e){var t=0;return Cn(e,function(){t++}),t},toArray:function(e){return Cn(e,function(t){return t})||[]},only:function(e){if(!gr(e))throw Error("React.Children.only expected to receive a single React element child.");return e}};R.Component=So;R.Fragment=gd;R.Profiler=fd;R.PureComponent=dr;R.StrictMode=md;R.Suspense=wd;R.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=Td;R.act=Ul;R.cloneElement=function(e,t,o){if(e==null)throw Error("React.cloneElement(...): The argument must be a React element, but you passed "+e+".");var n=ql({},e.props),i=e.key,a=e.ref,r=e._owner;if(t!=null){if(t.ref!==void 0&&(a=t.ref,r=pr.current),t.key!==void 0&&(i=""+t.key),e.type&&e.type.defaultProps)var s=e.type.defaultProps;for(l in t)zl.call(t,l)&&!Rl.hasOwnProperty(l)&&(n[l]=t[l]===void 0&&s!==void 0?s[l]:t[l])}var l=arguments.length-2;if(l===1)n.children=o;else if(1<l){s=Array(l);for(var c=0;c<l;c++)s[c]=arguments[c+2];n.children=s}return{$$typeof:mn,type:e.type,key:i,ref:a,props:n,_owner:r}};R.createContext=function(e){return e={$$typeof:vd,_currentValue:e,_currentValue2:e,_threadCount:0,Provider:null,Consumer:null,_defaultValue:null,_globalName:null},e.Provider={$$typeof:yd,_context:e},e.Consumer=e};R.createElement=jl;R.createFactory=function(e){var t=jl.bind(null,e);return t.type=e,t};R.createRef=function(){return{current:null}};R.forwardRef=function(e){return{$$typeof:bd,render:e}};R.isValidElement=gr;R.lazy=function(e){return{$$typeof:Sd,_payload:{_status:-1,_result:e},_init:Pd}};R.memo=function(e,t){return{$$typeof:Cd,type:e,compare:t===void 0?null:t}};R.startTransition=function(e){var t=Yn.transition;Yn.transition={};try{e()}finally{Yn.transition=t}};R.unstable_act=Ul;R.useCallback=function(e,t){return ve.current.useCallback(e,t)};R.useContext=function(e){return ve.current.useContext(e)};R.useDebugValue=function(){};R.useDeferredValue=function(e){return ve.current.useDeferredValue(e)};R.useEffect=function(e,t){return ve.current.useEffect(e,t)};R.useId=function(){return ve.current.useId()};R.useImperativeHandle=function(e,t,o){return ve.current.useImperativeHandle(e,t,o)};R.useInsertionEffect=function(e,t){return ve.current.useInsertionEffect(e,t)};R.useLayoutEffect=function(e,t){return ve.current.useLayoutEffect(e,t)};R.useMemo=function(e,t){return ve.current.useMemo(e,t)};R.useReducer=function(e,t,o){return ve.current.useReducer(e,t,o)};R.useRef=function(e){return ve.current.useRef(e)};R.useState=function(e){return ve.current.useState(e)};R.useSyncExternalStore=function(e,t,o){return ve.current.useSyncExternalStore(e,t,o)};R.useTransition=function(){return ve.current.useTransition()};R.version="18.3.1";Bl.exports=R;var A=Bl.exports;const Yl=hd(A),Dd=dd({__proto__:null,default:Yl},[A]);/**
 * @license React
 * react-jsx-runtime.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var Id=A,Ed=Symbol.for("react.element"),Bd=Symbol.for("react.fragment"),Gd=Object.prototype.hasOwnProperty,qd=Id.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED.ReactCurrentOwner,Md={key:!0,ref:!0,__self:!0,__source:!0};function Nl(e,t,o){var n,i={},a=null,r=null;o!==void 0&&(a=""+o),t.key!==void 0&&(a=""+t.key),t.ref!==void 0&&(r=t.ref);for(n in t)Gd.call(t,n)&&!Md.hasOwnProperty(n)&&(i[n]=t[n]);if(e&&e.defaultProps)for(n in t=e.defaultProps,t)i[n]===void 0&&(i[n]=t[n]);return{$$typeof:Ed,type:e,key:a,ref:r,props:i,_owner:qd.current}}Ai.Fragment=Bd;Ai.jsx=Nl;Ai.jsxs=Nl;El.exports=Ai;var g=El.exports,ma={},Ol={exports:{}},Be={},Wl={exports:{}},Hl={};/**
 * @license React
 * scheduler.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */(function(e){function t(C,G){var D=C.length;C.push(G);e:for(;0<D;){var z=D-1>>>1,M=C[z];if(0<i(M,G))C[z]=G,C[D]=M,D=z;else break e}}function o(C){return C.length===0?null:C[0]}function n(C){if(C.length===0)return null;var G=C[0],D=C.pop();if(D!==G){C[0]=D;e:for(var z=0,M=C.length,O=M>>>1;z<O;){var re=2*(z+1)-1,Z=C[re],Pe=re+1,se=C[Pe];if(0>i(Z,D))Pe<M&&0>i(se,Z)?(C[z]=se,C[Pe]=D,z=Pe):(C[z]=Z,C[re]=D,z=re);else if(Pe<M&&0>i(se,D))C[z]=se,C[Pe]=D,z=Pe;else break e}}return G}function i(C,G){var D=C.sortIndex-G.sortIndex;return D!==0?D:C.id-G.id}if(typeof performance=="object"&&typeof performance.now=="function"){var a=performance;e.unstable_now=function(){return a.now()}}else{var r=Date,s=r.now();e.unstable_now=function(){return r.now()-s}}var l=[],c=[],h=1,m=null,f=3,v=!1,w=!1,S=!1,T=typeof setTimeout=="function"?setTimeout:null,d=typeof clearTimeout=="function"?clearTimeout:null,u=typeof setImmediate<"u"?setImmediate:null;typeof navigator<"u"&&navigator.scheduling!==void 0&&navigator.scheduling.isInputPending!==void 0&&navigator.scheduling.isInputPending.bind(navigator.scheduling);function p(C){for(var G=o(c);G!==null;){if(G.callback===null)n(c);else if(G.startTime<=C)n(c),G.sortIndex=G.expirationTime,t(l,G);else break;G=o(c)}}function y(C){if(S=!1,p(C),!w)if(o(l)!==null)w=!0,L(b);else{var G=o(c);G!==null&&we(y,G.startTime-C)}}function b(C,G){w=!1,S&&(S=!1,d(I),I=-1),v=!0;var D=f;try{for(p(G),m=o(l);m!==null&&(!(m.expirationTime>G)||C&&!te());){var z=m.callback;if(typeof z=="function"){m.callback=null,f=m.priorityLevel;var M=z(m.expirationTime<=G);G=e.unstable_now(),typeof M=="function"?m.callback=M:m===o(l)&&n(l),p(G)}else n(l);m=o(l)}if(m!==null)var O=!0;else{var re=o(c);re!==null&&we(y,re.startTime-G),O=!1}return O}finally{m=null,f=D,v=!1}}var k=!1,P=null,I=-1,q=5,E=-1;function te(){return!(e.unstable_now()-E<q)}function Xe(){if(P!==null){var C=e.unstable_now();E=C;var G=!0;try{G=P(!0,C)}finally{G?Y():(k=!1,P=null)}}else k=!1}var Y;if(typeof u=="function")Y=function(){u(Xe)};else if(typeof MessageChannel<"u"){var de=new MessageChannel,Ue=de.port2;de.port1.onmessage=Xe,Y=function(){Ue.postMessage(null)}}else Y=function(){T(Xe,0)};function L(C){P=C,k||(k=!0,Y())}function we(C,G){I=T(function(){C(e.unstable_now())},G)}e.unstable_IdlePriority=5,e.unstable_ImmediatePriority=1,e.unstable_LowPriority=4,e.unstable_NormalPriority=3,e.unstable_Profiling=null,e.unstable_UserBlockingPriority=2,e.unstable_cancelCallback=function(C){C.callback=null},e.unstable_continueExecution=function(){w||v||(w=!0,L(b))},e.unstable_forceFrameRate=function(C){0>C||125<C?console.error("forceFrameRate takes a positive int between 0 and 125, forcing frame rates higher than 125 fps is not supported"):q=0<C?Math.floor(1e3/C):5},e.unstable_getCurrentPriorityLevel=function(){return f},e.unstable_getFirstCallbackNode=function(){return o(l)},e.unstable_next=function(C){switch(f){case 1:case 2:case 3:var G=3;break;default:G=f}var D=f;f=G;try{return C()}finally{f=D}},e.unstable_pauseExecution=function(){},e.unstable_requestPaint=function(){},e.unstable_runWithPriority=function(C,G){switch(C){case 1:case 2:case 3:case 4:case 5:break;default:C=3}var D=f;f=C;try{return G()}finally{f=D}},e.unstable_scheduleCallback=function(C,G,D){var z=e.unstable_now();switch(typeof D=="object"&&D!==null?(D=D.delay,D=typeof D=="number"&&0<D?z+D:z):D=z,C){case 1:var M=-1;break;case 2:M=250;break;case 5:M=1073741823;break;case 4:M=1e4;break;default:M=5e3}return M=D+M,C={id:h++,callback:G,priorityLevel:C,startTime:D,expirationTime:M,sortIndex:-1},D>z?(C.sortIndex=D,t(c,C),o(l)===null&&C===o(c)&&(S?(d(I),I=-1):S=!0,we(y,D-z))):(C.sortIndex=M,t(l,C),w||v||(w=!0,L(b))),C},e.unstable_shouldYield=te,e.unstable_wrapCallback=function(C){var G=f;return function(){var D=f;f=G;try{return C.apply(this,arguments)}finally{f=D}}}})(Hl);Wl.exports=Hl;var Ld=Wl.exports;/**
 * @license React
 * react-dom.production.min.js
 *
 * Copyright (c) Facebook, Inc. and its affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var zd=A,Ee=Ld;function x(e){for(var t="https://reactjs.org/docs/error-decoder.html?invariant="+e,o=1;o<arguments.length;o++)t+="&args[]="+encodeURIComponent(arguments[o]);return"Minified React error #"+e+"; visit "+t+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}var Vl=new Set,_o={};function Vt(e,t){mo(e,t),mo(e+"Capture",t)}function mo(e,t){for(_o[e]=t,e=0;e<t.length;e++)Vl.add(t[e])}var it=!(typeof window>"u"||typeof window.document>"u"||typeof window.document.createElement>"u"),fa=Object.prototype.hasOwnProperty,Rd=/^[:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD][:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD\-.0-9\u00B7\u0300-\u036F\u203F-\u2040]*$/,hs={},ps={};function jd(e){return fa.call(ps,e)?!0:fa.call(hs,e)?!1:Rd.test(e)?ps[e]=!0:(hs[e]=!0,!1)}function Ud(e,t,o,n){if(o!==null&&o.type===0)return!1;switch(typeof t){case"function":case"symbol":return!0;case"boolean":return n?!1:o!==null?!o.acceptsBooleans:(e=e.toLowerCase().slice(0,5),e!=="data-"&&e!=="aria-");default:return!1}}function Yd(e,t,o,n){if(t===null||typeof t>"u"||Ud(e,t,o,n))return!0;if(n)return!1;if(o!==null)switch(o.type){case 3:return!t;case 4:return t===!1;case 5:return isNaN(t);case 6:return isNaN(t)||1>t}return!1}function be(e,t,o,n,i,a,r){this.acceptsBooleans=t===2||t===3||t===4,this.attributeName=n,this.attributeNamespace=i,this.mustUseProperty=o,this.propertyName=e,this.type=t,this.sanitizeURL=a,this.removeEmptyString=r}var ue={};"children dangerouslySetInnerHTML defaultValue defaultChecked innerHTML suppressContentEditableWarning suppressHydrationWarning style".split(" ").forEach(function(e){ue[e]=new be(e,0,!1,e,null,!1,!1)});[["acceptCharset","accept-charset"],["className","class"],["htmlFor","for"],["httpEquiv","http-equiv"]].forEach(function(e){var t=e[0];ue[t]=new be(t,1,!1,e[1],null,!1,!1)});["contentEditable","draggable","spellCheck","value"].forEach(function(e){ue[e]=new be(e,2,!1,e.toLowerCase(),null,!1,!1)});["autoReverse","externalResourcesRequired","focusable","preserveAlpha"].forEach(function(e){ue[e]=new be(e,2,!1,e,null,!1,!1)});"allowFullScreen async autoFocus autoPlay controls default defer disabled disablePictureInPicture disableRemotePlayback formNoValidate hidden loop noModule noValidate open playsInline readOnly required reversed scoped seamless itemScope".split(" ").forEach(function(e){ue[e]=new be(e,3,!1,e.toLowerCase(),null,!1,!1)});["checked","multiple","muted","selected"].forEach(function(e){ue[e]=new be(e,3,!0,e,null,!1,!1)});["capture","download"].forEach(function(e){ue[e]=new be(e,4,!1,e,null,!1,!1)});["cols","rows","size","span"].forEach(function(e){ue[e]=new be(e,6,!1,e,null,!1,!1)});["rowSpan","start"].forEach(function(e){ue[e]=new be(e,5,!1,e.toLowerCase(),null,!1,!1)});var mr=/[\-:]([a-z])/g;function fr(e){return e[1].toUpperCase()}"accent-height alignment-baseline arabic-form baseline-shift cap-height clip-path clip-rule color-interpolation color-interpolation-filters color-profile color-rendering dominant-baseline enable-background fill-opacity fill-rule flood-color flood-opacity font-family font-size font-size-adjust font-stretch font-style font-variant font-weight glyph-name glyph-orientation-horizontal glyph-orientation-vertical horiz-adv-x horiz-origin-x image-rendering letter-spacing lighting-color marker-end marker-mid marker-start overline-position overline-thickness paint-order panose-1 pointer-events rendering-intent shape-rendering stop-color stop-opacity strikethrough-position strikethrough-thickness stroke-dasharray stroke-dashoffset stroke-linecap stroke-linejoin stroke-miterlimit stroke-opacity stroke-width text-anchor text-decoration text-rendering underline-position underline-thickness unicode-bidi unicode-range units-per-em v-alphabetic v-hanging v-ideographic v-mathematical vector-effect vert-adv-y vert-origin-x vert-origin-y word-spacing writing-mode xmlns:xlink x-height".split(" ").forEach(function(e){var t=e.replace(mr,fr);ue[t]=new be(t,1,!1,e,null,!1,!1)});"xlink:actuate xlink:arcrole xlink:role xlink:show xlink:title xlink:type".split(" ").forEach(function(e){var t=e.replace(mr,fr);ue[t]=new be(t,1,!1,e,"http://www.w3.org/1999/xlink",!1,!1)});["xml:base","xml:lang","xml:space"].forEach(function(e){var t=e.replace(mr,fr);ue[t]=new be(t,1,!1,e,"http://www.w3.org/XML/1998/namespace",!1,!1)});["tabIndex","crossOrigin"].forEach(function(e){ue[e]=new be(e,1,!1,e.toLowerCase(),null,!1,!1)});ue.xlinkHref=new be("xlinkHref",1,!1,"xlink:href","http://www.w3.org/1999/xlink",!0,!1);["src","href","action","formAction"].forEach(function(e){ue[e]=new be(e,1,!1,e.toLowerCase(),null,!0,!0)});function yr(e,t,o,n){var i=ue.hasOwnProperty(t)?ue[t]:null;(i!==null?i.type!==0:n||!(2<t.length)||t[0]!=="o"&&t[0]!=="O"||t[1]!=="n"&&t[1]!=="N")&&(Yd(t,o,i,n)&&(o=null),n||i===null?jd(t)&&(o===null?e.removeAttribute(t):e.setAttribute(t,""+o)):i.mustUseProperty?e[i.propertyName]=o===null?i.type===3?!1:"":o:(t=i.attributeName,n=i.attributeNamespace,o===null?e.removeAttribute(t):(i=i.type,o=i===3||i===4&&o===!0?"":""+o,n?e.setAttributeNS(n,t,o):e.setAttribute(t,o))))}var lt=zd.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED,Sn=Symbol.for("react.element"),$t=Symbol.for("react.portal"),Jt=Symbol.for("react.fragment"),vr=Symbol.for("react.strict_mode"),ya=Symbol.for("react.profiler"),Kl=Symbol.for("react.provider"),Ql=Symbol.for("react.context"),br=Symbol.for("react.forward_ref"),va=Symbol.for("react.suspense"),ba=Symbol.for("react.suspense_list"),wr=Symbol.for("react.memo"),dt=Symbol.for("react.lazy"),Fl=Symbol.for("react.offscreen"),gs=Symbol.iterator;function Do(e){return e===null||typeof e!="object"?null:(e=gs&&e[gs]||e["@@iterator"],typeof e=="function"?e:null)}var F=Object.assign,Wi;function zo(e){if(Wi===void 0)try{throw Error()}catch(o){var t=o.stack.trim().match(/\n( *(at )?)/);Wi=t&&t[1]||""}return`
`+Wi+e}var Hi=!1;function Vi(e,t){if(!e||Hi)return"";Hi=!0;var o=Error.prepareStackTrace;Error.prepareStackTrace=void 0;try{if(t)if(t=function(){throw Error()},Object.defineProperty(t.prototype,"props",{set:function(){throw Error()}}),typeof Reflect=="object"&&Reflect.construct){try{Reflect.construct(t,[])}catch(c){var n=c}Reflect.construct(e,[],t)}else{try{t.call()}catch(c){n=c}e.call(t.prototype)}else{try{throw Error()}catch(c){n=c}e()}}catch(c){if(c&&n&&typeof c.stack=="string"){for(var i=c.stack.split(`
`),a=n.stack.split(`
`),r=i.length-1,s=a.length-1;1<=r&&0<=s&&i[r]!==a[s];)s--;for(;1<=r&&0<=s;r--,s--)if(i[r]!==a[s]){if(r!==1||s!==1)do if(r--,s--,0>s||i[r]!==a[s]){var l=`
`+i[r].replace(" at new "," at ");return e.displayName&&l.includes("<anonymous>")&&(l=l.replace("<anonymous>",e.displayName)),l}while(1<=r&&0<=s);break}}}finally{Hi=!1,Error.prepareStackTrace=o}return(e=e?e.displayName||e.name:"")?zo(e):""}function Nd(e){switch(e.tag){case 5:return zo(e.type);case 16:return zo("Lazy");case 13:return zo("Suspense");case 19:return zo("SuspenseList");case 0:case 2:case 15:return e=Vi(e.type,!1),e;case 11:return e=Vi(e.type.render,!1),e;case 1:return e=Vi(e.type,!0),e;default:return""}}function wa(e){if(e==null)return null;if(typeof e=="function")return e.displayName||e.name||null;if(typeof e=="string")return e;switch(e){case Jt:return"Fragment";case $t:return"Portal";case ya:return"Profiler";case vr:return"StrictMode";case va:return"Suspense";case ba:return"SuspenseList"}if(typeof e=="object")switch(e.$$typeof){case Ql:return(e.displayName||"Context")+".Consumer";case Kl:return(e._context.displayName||"Context")+".Provider";case br:var t=e.render;return e=e.displayName,e||(e=t.displayName||t.name||"",e=e!==""?"ForwardRef("+e+")":"ForwardRef"),e;case wr:return t=e.displayName||null,t!==null?t:wa(e.type)||"Memo";case dt:t=e._payload,e=e._init;try{return wa(e(t))}catch{}}return null}function Od(e){var t=e.type;switch(e.tag){case 24:return"Cache";case 9:return(t.displayName||"Context")+".Consumer";case 10:return(t._context.displayName||"Context")+".Provider";case 18:return"DehydratedFragment";case 11:return e=t.render,e=e.displayName||e.name||"",t.displayName||(e!==""?"ForwardRef("+e+")":"ForwardRef");case 7:return"Fragment";case 5:return t;case 4:return"Portal";case 3:return"Root";case 6:return"Text";case 16:return wa(t);case 8:return t===vr?"StrictMode":"Mode";case 22:return"Offscreen";case 12:return"Profiler";case 21:return"Scope";case 13:return"Suspense";case 19:return"SuspenseList";case 25:return"TracingMarker";case 1:case 0:case 17:case 2:case 14:case 15:if(typeof t=="function")return t.displayName||t.name||null;if(typeof t=="string")return t}return null}function Tt(e){switch(typeof e){case"boolean":case"number":case"string":case"undefined":return e;case"object":return e;default:return""}}function _l(e){var t=e.type;return(e=e.nodeName)&&e.toLowerCase()==="input"&&(t==="checkbox"||t==="radio")}function Wd(e){var t=_l(e)?"checked":"value",o=Object.getOwnPropertyDescriptor(e.constructor.prototype,t),n=""+e[t];if(!e.hasOwnProperty(t)&&typeof o<"u"&&typeof o.get=="function"&&typeof o.set=="function"){var i=o.get,a=o.set;return Object.defineProperty(e,t,{configurable:!0,get:function(){return i.call(this)},set:function(r){n=""+r,a.call(this,r)}}),Object.defineProperty(e,t,{enumerable:o.enumerable}),{getValue:function(){return n},setValue:function(r){n=""+r},stopTracking:function(){e._valueTracker=null,delete e[t]}}}}function An(e){e._valueTracker||(e._valueTracker=Wd(e))}function $l(e){if(!e)return!1;var t=e._valueTracker;if(!t)return!0;var o=t.getValue(),n="";return e&&(n=_l(e)?e.checked?"true":"false":e.value),e=n,e!==o?(t.setValue(e),!0):!1}function Jn(e){if(e=e||(typeof document<"u"?document:void 0),typeof e>"u")return null;try{return e.activeElement||e.body}catch{return e.body}}function Ca(e,t){var o=t.checked;return F({},t,{defaultChecked:void 0,defaultValue:void 0,value:void 0,checked:o??e._wrapperState.initialChecked})}function ms(e,t){var o=t.defaultValue==null?"":t.defaultValue,n=t.checked!=null?t.checked:t.defaultChecked;o=Tt(t.value!=null?t.value:o),e._wrapperState={initialChecked:n,initialValue:o,controlled:t.type==="checkbox"||t.type==="radio"?t.checked!=null:t.value!=null}}function Jl(e,t){t=t.checked,t!=null&&yr(e,"checked",t,!1)}function Sa(e,t){Jl(e,t);var o=Tt(t.value),n=t.type;if(o!=null)n==="number"?(o===0&&e.value===""||e.value!=o)&&(e.value=""+o):e.value!==""+o&&(e.value=""+o);else if(n==="submit"||n==="reset"){e.removeAttribute("value");return}t.hasOwnProperty("value")?Aa(e,t.type,o):t.hasOwnProperty("defaultValue")&&Aa(e,t.type,Tt(t.defaultValue)),t.checked==null&&t.defaultChecked!=null&&(e.defaultChecked=!!t.defaultChecked)}function fs(e,t,o){if(t.hasOwnProperty("value")||t.hasOwnProperty("defaultValue")){var n=t.type;if(!(n!=="submit"&&n!=="reset"||t.value!==void 0&&t.value!==null))return;t=""+e._wrapperState.initialValue,o||t===e.value||(e.value=t),e.defaultValue=t}o=e.name,o!==""&&(e.name=""),e.defaultChecked=!!e._wrapperState.initialChecked,o!==""&&(e.name=o)}function Aa(e,t,o){(t!=="number"||Jn(e.ownerDocument)!==e)&&(o==null?e.defaultValue=""+e._wrapperState.initialValue:e.defaultValue!==""+o&&(e.defaultValue=""+o))}var Ro=Array.isArray;function lo(e,t,o,n){if(e=e.options,t){t={};for(var i=0;i<o.length;i++)t["$"+o[i]]=!0;for(o=0;o<e.length;o++)i=t.hasOwnProperty("$"+e[o].value),e[o].selected!==i&&(e[o].selected=i),i&&n&&(e[o].defaultSelected=!0)}else{for(o=""+Tt(o),t=null,i=0;i<e.length;i++){if(e[i].value===o){e[i].selected=!0,n&&(e[i].defaultSelected=!0);return}t!==null||e[i].disabled||(t=e[i])}t!==null&&(t.selected=!0)}}function ka(e,t){if(t.dangerouslySetInnerHTML!=null)throw Error(x(91));return F({},t,{value:void 0,defaultValue:void 0,children:""+e._wrapperState.initialValue})}function ys(e,t){var o=t.value;if(o==null){if(o=t.children,t=t.defaultValue,o!=null){if(t!=null)throw Error(x(92));if(Ro(o)){if(1<o.length)throw Error(x(93));o=o[0]}t=o}t==null&&(t=""),o=t}e._wrapperState={initialValue:Tt(o)}}function Xl(e,t){var o=Tt(t.value),n=Tt(t.defaultValue);o!=null&&(o=""+o,o!==e.value&&(e.value=o),t.defaultValue==null&&e.defaultValue!==o&&(e.defaultValue=o)),n!=null&&(e.defaultValue=""+n)}function vs(e){var t=e.textContent;t===e._wrapperState.initialValue&&t!==""&&t!==null&&(e.value=t)}function Zl(e){switch(e){case"svg":return"http://www.w3.org/2000/svg";case"math":return"http://www.w3.org/1998/Math/MathML";default:return"http://www.w3.org/1999/xhtml"}}function xa(e,t){return e==null||e==="http://www.w3.org/1999/xhtml"?Zl(t):e==="http://www.w3.org/2000/svg"&&t==="foreignObject"?"http://www.w3.org/1999/xhtml":e}var kn,ec=function(e){return typeof MSApp<"u"&&MSApp.execUnsafeLocalFunction?function(t,o,n,i){MSApp.execUnsafeLocalFunction(function(){return e(t,o,n,i)})}:e}(function(e,t){if(e.namespaceURI!=="http://www.w3.org/2000/svg"||"innerHTML"in e)e.innerHTML=t;else{for(kn=kn||document.createElement("div"),kn.innerHTML="<svg>"+t.valueOf().toString()+"</svg>",t=kn.firstChild;e.firstChild;)e.removeChild(e.firstChild);for(;t.firstChild;)e.appendChild(t.firstChild)}});function $o(e,t){if(t){var o=e.firstChild;if(o&&o===e.lastChild&&o.nodeType===3){o.nodeValue=t;return}}e.textContent=t}var Yo={animationIterationCount:!0,aspectRatio:!0,borderImageOutset:!0,borderImageSlice:!0,borderImageWidth:!0,boxFlex:!0,boxFlexGroup:!0,boxOrdinalGroup:!0,columnCount:!0,columns:!0,flex:!0,flexGrow:!0,flexPositive:!0,flexShrink:!0,flexNegative:!0,flexOrder:!0,gridArea:!0,gridRow:!0,gridRowEnd:!0,gridRowSpan:!0,gridRowStart:!0,gridColumn:!0,gridColumnEnd:!0,gridColumnSpan:!0,gridColumnStart:!0,fontWeight:!0,lineClamp:!0,lineHeight:!0,opacity:!0,order:!0,orphans:!0,tabSize:!0,widows:!0,zIndex:!0,zoom:!0,fillOpacity:!0,floodOpacity:!0,stopOpacity:!0,strokeDasharray:!0,strokeDashoffset:!0,strokeMiterlimit:!0,strokeOpacity:!0,strokeWidth:!0},Hd=["Webkit","ms","Moz","O"];Object.keys(Yo).forEach(function(e){Hd.forEach(function(t){t=t+e.charAt(0).toUpperCase()+e.substring(1),Yo[t]=Yo[e]})});function tc(e,t,o){return t==null||typeof t=="boolean"||t===""?"":o||typeof t!="number"||t===0||Yo.hasOwnProperty(e)&&Yo[e]?(""+t).trim():t+"px"}function oc(e,t){e=e.style;for(var o in t)if(t.hasOwnProperty(o)){var n=o.indexOf("--")===0,i=tc(o,t[o],n);o==="float"&&(o="cssFloat"),n?e.setProperty(o,i):e[o]=i}}var Vd=F({menuitem:!0},{area:!0,base:!0,br:!0,col:!0,embed:!0,hr:!0,img:!0,input:!0,keygen:!0,link:!0,meta:!0,param:!0,source:!0,track:!0,wbr:!0});function Pa(e,t){if(t){if(Vd[e]&&(t.children!=null||t.dangerouslySetInnerHTML!=null))throw Error(x(137,e));if(t.dangerouslySetInnerHTML!=null){if(t.children!=null)throw Error(x(60));if(typeof t.dangerouslySetInnerHTML!="object"||!("__html"in t.dangerouslySetInnerHTML))throw Error(x(61))}if(t.style!=null&&typeof t.style!="object")throw Error(x(62))}}function Ta(e,t){if(e.indexOf("-")===-1)return typeof t.is=="string";switch(e){case"annotation-xml":case"color-profile":case"font-face":case"font-face-src":case"font-face-uri":case"font-face-format":case"font-face-name":case"missing-glyph":return!1;default:return!0}}var Da=null;function Cr(e){return e=e.target||e.srcElement||window,e.correspondingUseElement&&(e=e.correspondingUseElement),e.nodeType===3?e.parentNode:e}var Ia=null,co=null,uo=null;function bs(e){if(e=vn(e)){if(typeof Ia!="function")throw Error(x(280));var t=e.stateNode;t&&(t=Di(t),Ia(e.stateNode,e.type,t))}}function nc(e){co?uo?uo.push(e):uo=[e]:co=e}function ic(){if(co){var e=co,t=uo;if(uo=co=null,bs(e),t)for(e=0;e<t.length;e++)bs(t[e])}}function ac(e,t){return e(t)}function rc(){}var Ki=!1;function sc(e,t,o){if(Ki)return e(t,o);Ki=!0;try{return ac(e,t,o)}finally{Ki=!1,(co!==null||uo!==null)&&(rc(),ic())}}function Jo(e,t){var o=e.stateNode;if(o===null)return null;var n=Di(o);if(n===null)return null;o=n[t];e:switch(t){case"onClick":case"onClickCapture":case"onDoubleClick":case"onDoubleClickCapture":case"onMouseDown":case"onMouseDownCapture":case"onMouseMove":case"onMouseMoveCapture":case"onMouseUp":case"onMouseUpCapture":case"onMouseEnter":(n=!n.disabled)||(e=e.type,n=!(e==="button"||e==="input"||e==="select"||e==="textarea")),e=!n;break e;default:e=!1}if(e)return null;if(o&&typeof o!="function")throw Error(x(231,t,typeof o));return o}var Ea=!1;if(it)try{var Io={};Object.defineProperty(Io,"passive",{get:function(){Ea=!0}}),window.addEventListener("test",Io,Io),window.removeEventListener("test",Io,Io)}catch{Ea=!1}function Kd(e,t,o,n,i,a,r,s,l){var c=Array.prototype.slice.call(arguments,3);try{t.apply(o,c)}catch(h){this.onError(h)}}var No=!1,Xn=null,Zn=!1,Ba=null,Qd={onError:function(e){No=!0,Xn=e}};function Fd(e,t,o,n,i,a,r,s,l){No=!1,Xn=null,Kd.apply(Qd,arguments)}function _d(e,t,o,n,i,a,r,s,l){if(Fd.apply(this,arguments),No){if(No){var c=Xn;No=!1,Xn=null}else throw Error(x(198));Zn||(Zn=!0,Ba=c)}}function Kt(e){var t=e,o=e;if(e.alternate)for(;t.return;)t=t.return;else{e=t;do t=e,t.flags&4098&&(o=t.return),e=t.return;while(e)}return t.tag===3?o:null}function lc(e){if(e.tag===13){var t=e.memoizedState;if(t===null&&(e=e.alternate,e!==null&&(t=e.memoizedState)),t!==null)return t.dehydrated}return null}function ws(e){if(Kt(e)!==e)throw Error(x(188))}function $d(e){var t=e.alternate;if(!t){if(t=Kt(e),t===null)throw Error(x(188));return t!==e?null:e}for(var o=e,n=t;;){var i=o.return;if(i===null)break;var a=i.alternate;if(a===null){if(n=i.return,n!==null){o=n;continue}break}if(i.child===a.child){for(a=i.child;a;){if(a===o)return ws(i),e;if(a===n)return ws(i),t;a=a.sibling}throw Error(x(188))}if(o.return!==n.return)o=i,n=a;else{for(var r=!1,s=i.child;s;){if(s===o){r=!0,o=i,n=a;break}if(s===n){r=!0,n=i,o=a;break}s=s.sibling}if(!r){for(s=a.child;s;){if(s===o){r=!0,o=a,n=i;break}if(s===n){r=!0,n=a,o=i;break}s=s.sibling}if(!r)throw Error(x(189))}}if(o.alternate!==n)throw Error(x(190))}if(o.tag!==3)throw Error(x(188));return o.stateNode.current===o?e:t}function cc(e){return e=$d(e),e!==null?uc(e):null}function uc(e){if(e.tag===5||e.tag===6)return e;for(e=e.child;e!==null;){var t=uc(e);if(t!==null)return t;e=e.sibling}return null}var dc=Ee.unstable_scheduleCallback,Cs=Ee.unstable_cancelCallback,Jd=Ee.unstable_shouldYield,Xd=Ee.unstable_requestPaint,J=Ee.unstable_now,Zd=Ee.unstable_getCurrentPriorityLevel,Sr=Ee.unstable_ImmediatePriority,hc=Ee.unstable_UserBlockingPriority,ei=Ee.unstable_NormalPriority,eh=Ee.unstable_LowPriority,pc=Ee.unstable_IdlePriority,ki=null,$e=null;function th(e){if($e&&typeof $e.onCommitFiberRoot=="function")try{$e.onCommitFiberRoot(ki,e,void 0,(e.current.flags&128)===128)}catch{}}var He=Math.clz32?Math.clz32:ih,oh=Math.log,nh=Math.LN2;function ih(e){return e>>>=0,e===0?32:31-(oh(e)/nh|0)|0}var xn=64,Pn=4194304;function jo(e){switch(e&-e){case 1:return 1;case 2:return 2;case 4:return 4;case 8:return 8;case 16:return 16;case 32:return 32;case 64:case 128:case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:return e&4194240;case 4194304:case 8388608:case 16777216:case 33554432:case 67108864:return e&130023424;case 134217728:return 134217728;case 268435456:return 268435456;case 536870912:return 536870912;case 1073741824:return 1073741824;default:return e}}function ti(e,t){var o=e.pendingLanes;if(o===0)return 0;var n=0,i=e.suspendedLanes,a=e.pingedLanes,r=o&268435455;if(r!==0){var s=r&~i;s!==0?n=jo(s):(a&=r,a!==0&&(n=jo(a)))}else r=o&~i,r!==0?n=jo(r):a!==0&&(n=jo(a));if(n===0)return 0;if(t!==0&&t!==n&&!(t&i)&&(i=n&-n,a=t&-t,i>=a||i===16&&(a&4194240)!==0))return t;if(n&4&&(n|=o&16),t=e.entangledLanes,t!==0)for(e=e.entanglements,t&=n;0<t;)o=31-He(t),i=1<<o,n|=e[o],t&=~i;return n}function ah(e,t){switch(e){case 1:case 2:case 4:return t+250;case 8:case 16:case 32:case 64:case 128:case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:return t+5e3;case 4194304:case 8388608:case 16777216:case 33554432:case 67108864:return-1;case 134217728:case 268435456:case 536870912:case 1073741824:return-1;default:return-1}}function rh(e,t){for(var o=e.suspendedLanes,n=e.pingedLanes,i=e.expirationTimes,a=e.pendingLanes;0<a;){var r=31-He(a),s=1<<r,l=i[r];l===-1?(!(s&o)||s&n)&&(i[r]=ah(s,t)):l<=t&&(e.expiredLanes|=s),a&=~s}}function Ga(e){return e=e.pendingLanes&-1073741825,e!==0?e:e&1073741824?1073741824:0}function gc(){var e=xn;return xn<<=1,!(xn&4194240)&&(xn=64),e}function Qi(e){for(var t=[],o=0;31>o;o++)t.push(e);return t}function fn(e,t,o){e.pendingLanes|=t,t!==536870912&&(e.suspendedLanes=0,e.pingedLanes=0),e=e.eventTimes,t=31-He(t),e[t]=o}function sh(e,t){var o=e.pendingLanes&~t;e.pendingLanes=t,e.suspendedLanes=0,e.pingedLanes=0,e.expiredLanes&=t,e.mutableReadLanes&=t,e.entangledLanes&=t,t=e.entanglements;var n=e.eventTimes;for(e=e.expirationTimes;0<o;){var i=31-He(o),a=1<<i;t[i]=0,n[i]=-1,e[i]=-1,o&=~a}}function Ar(e,t){var o=e.entangledLanes|=t;for(e=e.entanglements;o;){var n=31-He(o),i=1<<n;i&t|e[n]&t&&(e[n]|=t),o&=~i}}var U=0;function mc(e){return e&=-e,1<e?4<e?e&268435455?16:536870912:4:1}var fc,kr,yc,vc,bc,qa=!1,Tn=[],vt=null,bt=null,wt=null,Xo=new Map,Zo=new Map,pt=[],lh="mousedown mouseup touchcancel touchend touchstart auxclick dblclick pointercancel pointerdown pointerup dragend dragstart drop compositionend compositionstart keydown keypress keyup input textInput copy cut paste click change contextmenu reset submit".split(" ");function Ss(e,t){switch(e){case"focusin":case"focusout":vt=null;break;case"dragenter":case"dragleave":bt=null;break;case"mouseover":case"mouseout":wt=null;break;case"pointerover":case"pointerout":Xo.delete(t.pointerId);break;case"gotpointercapture":case"lostpointercapture":Zo.delete(t.pointerId)}}function Eo(e,t,o,n,i,a){return e===null||e.nativeEvent!==a?(e={blockedOn:t,domEventName:o,eventSystemFlags:n,nativeEvent:a,targetContainers:[i]},t!==null&&(t=vn(t),t!==null&&kr(t)),e):(e.eventSystemFlags|=n,t=e.targetContainers,i!==null&&t.indexOf(i)===-1&&t.push(i),e)}function ch(e,t,o,n,i){switch(t){case"focusin":return vt=Eo(vt,e,t,o,n,i),!0;case"dragenter":return bt=Eo(bt,e,t,o,n,i),!0;case"mouseover":return wt=Eo(wt,e,t,o,n,i),!0;case"pointerover":var a=i.pointerId;return Xo.set(a,Eo(Xo.get(a)||null,e,t,o,n,i)),!0;case"gotpointercapture":return a=i.pointerId,Zo.set(a,Eo(Zo.get(a)||null,e,t,o,n,i)),!0}return!1}function wc(e){var t=Lt(e.target);if(t!==null){var o=Kt(t);if(o!==null){if(t=o.tag,t===13){if(t=lc(o),t!==null){e.blockedOn=t,bc(e.priority,function(){yc(o)});return}}else if(t===3&&o.stateNode.current.memoizedState.isDehydrated){e.blockedOn=o.tag===3?o.stateNode.containerInfo:null;return}}}e.blockedOn=null}function Nn(e){if(e.blockedOn!==null)return!1;for(var t=e.targetContainers;0<t.length;){var o=Ma(e.domEventName,e.eventSystemFlags,t[0],e.nativeEvent);if(o===null){o=e.nativeEvent;var n=new o.constructor(o.type,o);Da=n,o.target.dispatchEvent(n),Da=null}else return t=vn(o),t!==null&&kr(t),e.blockedOn=o,!1;t.shift()}return!0}function As(e,t,o){Nn(e)&&o.delete(t)}function uh(){qa=!1,vt!==null&&Nn(vt)&&(vt=null),bt!==null&&Nn(bt)&&(bt=null),wt!==null&&Nn(wt)&&(wt=null),Xo.forEach(As),Zo.forEach(As)}function Bo(e,t){e.blockedOn===t&&(e.blockedOn=null,qa||(qa=!0,Ee.unstable_scheduleCallback(Ee.unstable_NormalPriority,uh)))}function en(e){function t(i){return Bo(i,e)}if(0<Tn.length){Bo(Tn[0],e);for(var o=1;o<Tn.length;o++){var n=Tn[o];n.blockedOn===e&&(n.blockedOn=null)}}for(vt!==null&&Bo(vt,e),bt!==null&&Bo(bt,e),wt!==null&&Bo(wt,e),Xo.forEach(t),Zo.forEach(t),o=0;o<pt.length;o++)n=pt[o],n.blockedOn===e&&(n.blockedOn=null);for(;0<pt.length&&(o=pt[0],o.blockedOn===null);)wc(o),o.blockedOn===null&&pt.shift()}var ho=lt.ReactCurrentBatchConfig,oi=!0;function dh(e,t,o,n){var i=U,a=ho.transition;ho.transition=null;try{U=1,xr(e,t,o,n)}finally{U=i,ho.transition=a}}function hh(e,t,o,n){var i=U,a=ho.transition;ho.transition=null;try{U=4,xr(e,t,o,n)}finally{U=i,ho.transition=a}}function xr(e,t,o,n){if(oi){var i=Ma(e,t,o,n);if(i===null)na(e,t,n,ni,o),Ss(e,n);else if(ch(i,e,t,o,n))n.stopPropagation();else if(Ss(e,n),t&4&&-1<lh.indexOf(e)){for(;i!==null;){var a=vn(i);if(a!==null&&fc(a),a=Ma(e,t,o,n),a===null&&na(e,t,n,ni,o),a===i)break;i=a}i!==null&&n.stopPropagation()}else na(e,t,n,null,o)}}var ni=null;function Ma(e,t,o,n){if(ni=null,e=Cr(n),e=Lt(e),e!==null)if(t=Kt(e),t===null)e=null;else if(o=t.tag,o===13){if(e=lc(t),e!==null)return e;e=null}else if(o===3){if(t.stateNode.current.memoizedState.isDehydrated)return t.tag===3?t.stateNode.containerInfo:null;e=null}else t!==e&&(e=null);return ni=e,null}function Cc(e){switch(e){case"cancel":case"click":case"close":case"contextmenu":case"copy":case"cut":case"auxclick":case"dblclick":case"dragend":case"dragstart":case"drop":case"focusin":case"focusout":case"input":case"invalid":case"keydown":case"keypress":case"keyup":case"mousedown":case"mouseup":case"paste":case"pause":case"play":case"pointercancel":case"pointerdown":case"pointerup":case"ratechange":case"reset":case"resize":case"seeked":case"submit":case"touchcancel":case"touchend":case"touchstart":case"volumechange":case"change":case"selectionchange":case"textInput":case"compositionstart":case"compositionend":case"compositionupdate":case"beforeblur":case"afterblur":case"beforeinput":case"blur":case"fullscreenchange":case"focus":case"hashchange":case"popstate":case"select":case"selectstart":return 1;case"drag":case"dragenter":case"dragexit":case"dragleave":case"dragover":case"mousemove":case"mouseout":case"mouseover":case"pointermove":case"pointerout":case"pointerover":case"scroll":case"toggle":case"touchmove":case"wheel":case"mouseenter":case"mouseleave":case"pointerenter":case"pointerleave":return 4;case"message":switch(Zd()){case Sr:return 1;case hc:return 4;case ei:case eh:return 16;case pc:return 536870912;default:return 16}default:return 16}}var mt=null,Pr=null,On=null;function Sc(){if(On)return On;var e,t=Pr,o=t.length,n,i="value"in mt?mt.value:mt.textContent,a=i.length;for(e=0;e<o&&t[e]===i[e];e++);var r=o-e;for(n=1;n<=r&&t[o-n]===i[a-n];n++);return On=i.slice(e,1<n?1-n:void 0)}function Wn(e){var t=e.keyCode;return"charCode"in e?(e=e.charCode,e===0&&t===13&&(e=13)):e=t,e===10&&(e=13),32<=e||e===13?e:0}function Dn(){return!0}function ks(){return!1}function Ge(e){function t(o,n,i,a,r){this._reactName=o,this._targetInst=i,this.type=n,this.nativeEvent=a,this.target=r,this.currentTarget=null;for(var s in e)e.hasOwnProperty(s)&&(o=e[s],this[s]=o?o(a):a[s]);return this.isDefaultPrevented=(a.defaultPrevented!=null?a.defaultPrevented:a.returnValue===!1)?Dn:ks,this.isPropagationStopped=ks,this}return F(t.prototype,{preventDefault:function(){this.defaultPrevented=!0;var o=this.nativeEvent;o&&(o.preventDefault?o.preventDefault():typeof o.returnValue!="unknown"&&(o.returnValue=!1),this.isDefaultPrevented=Dn)},stopPropagation:function(){var o=this.nativeEvent;o&&(o.stopPropagation?o.stopPropagation():typeof o.cancelBubble!="unknown"&&(o.cancelBubble=!0),this.isPropagationStopped=Dn)},persist:function(){},isPersistent:Dn}),t}var Ao={eventPhase:0,bubbles:0,cancelable:0,timeStamp:function(e){return e.timeStamp||Date.now()},defaultPrevented:0,isTrusted:0},Tr=Ge(Ao),yn=F({},Ao,{view:0,detail:0}),ph=Ge(yn),Fi,_i,Go,xi=F({},yn,{screenX:0,screenY:0,clientX:0,clientY:0,pageX:0,pageY:0,ctrlKey:0,shiftKey:0,altKey:0,metaKey:0,getModifierState:Dr,button:0,buttons:0,relatedTarget:function(e){return e.relatedTarget===void 0?e.fromElement===e.srcElement?e.toElement:e.fromElement:e.relatedTarget},movementX:function(e){return"movementX"in e?e.movementX:(e!==Go&&(Go&&e.type==="mousemove"?(Fi=e.screenX-Go.screenX,_i=e.screenY-Go.screenY):_i=Fi=0,Go=e),Fi)},movementY:function(e){return"movementY"in e?e.movementY:_i}}),xs=Ge(xi),gh=F({},xi,{dataTransfer:0}),mh=Ge(gh),fh=F({},yn,{relatedTarget:0}),$i=Ge(fh),yh=F({},Ao,{animationName:0,elapsedTime:0,pseudoElement:0}),vh=Ge(yh),bh=F({},Ao,{clipboardData:function(e){return"clipboardData"in e?e.clipboardData:window.clipboardData}}),wh=Ge(bh),Ch=F({},Ao,{data:0}),Ps=Ge(Ch),Sh={Esc:"Escape",Spacebar:" ",Left:"ArrowLeft",Up:"ArrowUp",Right:"ArrowRight",Down:"ArrowDown",Del:"Delete",Win:"OS",Menu:"ContextMenu",Apps:"ContextMenu",Scroll:"ScrollLock",MozPrintableKey:"Unidentified"},Ah={8:"Backspace",9:"Tab",12:"Clear",13:"Enter",16:"Shift",17:"Control",18:"Alt",19:"Pause",20:"CapsLock",27:"Escape",32:" ",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"ArrowLeft",38:"ArrowUp",39:"ArrowRight",40:"ArrowDown",45:"Insert",46:"Delete",112:"F1",113:"F2",114:"F3",115:"F4",116:"F5",117:"F6",118:"F7",119:"F8",120:"F9",121:"F10",122:"F11",123:"F12",144:"NumLock",145:"ScrollLock",224:"Meta"},kh={Alt:"altKey",Control:"ctrlKey",Meta:"metaKey",Shift:"shiftKey"};function xh(e){var t=this.nativeEvent;return t.getModifierState?t.getModifierState(e):(e=kh[e])?!!t[e]:!1}function Dr(){return xh}var Ph=F({},yn,{key:function(e){if(e.key){var t=Sh[e.key]||e.key;if(t!=="Unidentified")return t}return e.type==="keypress"?(e=Wn(e),e===13?"Enter":String.fromCharCode(e)):e.type==="keydown"||e.type==="keyup"?Ah[e.keyCode]||"Unidentified":""},code:0,location:0,ctrlKey:0,shiftKey:0,altKey:0,metaKey:0,repeat:0,locale:0,getModifierState:Dr,charCode:function(e){return e.type==="keypress"?Wn(e):0},keyCode:function(e){return e.type==="keydown"||e.type==="keyup"?e.keyCode:0},which:function(e){return e.type==="keypress"?Wn(e):e.type==="keydown"||e.type==="keyup"?e.keyCode:0}}),Th=Ge(Ph),Dh=F({},xi,{pointerId:0,width:0,height:0,pressure:0,tangentialPressure:0,tiltX:0,tiltY:0,twist:0,pointerType:0,isPrimary:0}),Ts=Ge(Dh),Ih=F({},yn,{touches:0,targetTouches:0,changedTouches:0,altKey:0,metaKey:0,ctrlKey:0,shiftKey:0,getModifierState:Dr}),Eh=Ge(Ih),Bh=F({},Ao,{propertyName:0,elapsedTime:0,pseudoElement:0}),Gh=Ge(Bh),qh=F({},xi,{deltaX:function(e){return"deltaX"in e?e.deltaX:"wheelDeltaX"in e?-e.wheelDeltaX:0},deltaY:function(e){return"deltaY"in e?e.deltaY:"wheelDeltaY"in e?-e.wheelDeltaY:"wheelDelta"in e?-e.wheelDelta:0},deltaZ:0,deltaMode:0}),Mh=Ge(qh),Lh=[9,13,27,32],Ir=it&&"CompositionEvent"in window,Oo=null;it&&"documentMode"in document&&(Oo=document.documentMode);var zh=it&&"TextEvent"in window&&!Oo,Ac=it&&(!Ir||Oo&&8<Oo&&11>=Oo),Ds=" ",Is=!1;function kc(e,t){switch(e){case"keyup":return Lh.indexOf(t.keyCode)!==-1;case"keydown":return t.keyCode!==229;case"keypress":case"mousedown":case"focusout":return!0;default:return!1}}function xc(e){return e=e.detail,typeof e=="object"&&"data"in e?e.data:null}var Xt=!1;function Rh(e,t){switch(e){case"compositionend":return xc(t);case"keypress":return t.which!==32?null:(Is=!0,Ds);case"textInput":return e=t.data,e===Ds&&Is?null:e;default:return null}}function jh(e,t){if(Xt)return e==="compositionend"||!Ir&&kc(e,t)?(e=Sc(),On=Pr=mt=null,Xt=!1,e):null;switch(e){case"paste":return null;case"keypress":if(!(t.ctrlKey||t.altKey||t.metaKey)||t.ctrlKey&&t.altKey){if(t.char&&1<t.char.length)return t.char;if(t.which)return String.fromCharCode(t.which)}return null;case"compositionend":return Ac&&t.locale!=="ko"?null:t.data;default:return null}}var Uh={color:!0,date:!0,datetime:!0,"datetime-local":!0,email:!0,month:!0,number:!0,password:!0,range:!0,search:!0,tel:!0,text:!0,time:!0,url:!0,week:!0};function Es(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return t==="input"?!!Uh[e.type]:t==="textarea"}function Pc(e,t,o,n){nc(n),t=ii(t,"onChange"),0<t.length&&(o=new Tr("onChange","change",null,o,n),e.push({event:o,listeners:t}))}var Wo=null,tn=null;function Yh(e){Rc(e,0)}function Pi(e){var t=to(e);if($l(t))return e}function Nh(e,t){if(e==="change")return t}var Tc=!1;if(it){var Ji;if(it){var Xi="oninput"in document;if(!Xi){var Bs=document.createElement("div");Bs.setAttribute("oninput","return;"),Xi=typeof Bs.oninput=="function"}Ji=Xi}else Ji=!1;Tc=Ji&&(!document.documentMode||9<document.documentMode)}function Gs(){Wo&&(Wo.detachEvent("onpropertychange",Dc),tn=Wo=null)}function Dc(e){if(e.propertyName==="value"&&Pi(tn)){var t=[];Pc(t,tn,e,Cr(e)),sc(Yh,t)}}function Oh(e,t,o){e==="focusin"?(Gs(),Wo=t,tn=o,Wo.attachEvent("onpropertychange",Dc)):e==="focusout"&&Gs()}function Wh(e){if(e==="selectionchange"||e==="keyup"||e==="keydown")return Pi(tn)}function Hh(e,t){if(e==="click")return Pi(t)}function Vh(e,t){if(e==="input"||e==="change")return Pi(t)}function Kh(e,t){return e===t&&(e!==0||1/e===1/t)||e!==e&&t!==t}var Ke=typeof Object.is=="function"?Object.is:Kh;function on(e,t){if(Ke(e,t))return!0;if(typeof e!="object"||e===null||typeof t!="object"||t===null)return!1;var o=Object.keys(e),n=Object.keys(t);if(o.length!==n.length)return!1;for(n=0;n<o.length;n++){var i=o[n];if(!fa.call(t,i)||!Ke(e[i],t[i]))return!1}return!0}function qs(e){for(;e&&e.firstChild;)e=e.firstChild;return e}function Ms(e,t){var o=qs(e);e=0;for(var n;o;){if(o.nodeType===3){if(n=e+o.textContent.length,e<=t&&n>=t)return{node:o,offset:t-e};e=n}e:{for(;o;){if(o.nextSibling){o=o.nextSibling;break e}o=o.parentNode}o=void 0}o=qs(o)}}function Ic(e,t){return e&&t?e===t?!0:e&&e.nodeType===3?!1:t&&t.nodeType===3?Ic(e,t.parentNode):"contains"in e?e.contains(t):e.compareDocumentPosition?!!(e.compareDocumentPosition(t)&16):!1:!1}function Ec(){for(var e=window,t=Jn();t instanceof e.HTMLIFrameElement;){try{var o=typeof t.contentWindow.location.href=="string"}catch{o=!1}if(o)e=t.contentWindow;else break;t=Jn(e.document)}return t}function Er(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return t&&(t==="input"&&(e.type==="text"||e.type==="search"||e.type==="tel"||e.type==="url"||e.type==="password")||t==="textarea"||e.contentEditable==="true")}function Qh(e){var t=Ec(),o=e.focusedElem,n=e.selectionRange;if(t!==o&&o&&o.ownerDocument&&Ic(o.ownerDocument.documentElement,o)){if(n!==null&&Er(o)){if(t=n.start,e=n.end,e===void 0&&(e=t),"selectionStart"in o)o.selectionStart=t,o.selectionEnd=Math.min(e,o.value.length);else if(e=(t=o.ownerDocument||document)&&t.defaultView||window,e.getSelection){e=e.getSelection();var i=o.textContent.length,a=Math.min(n.start,i);n=n.end===void 0?a:Math.min(n.end,i),!e.extend&&a>n&&(i=n,n=a,a=i),i=Ms(o,a);var r=Ms(o,n);i&&r&&(e.rangeCount!==1||e.anchorNode!==i.node||e.anchorOffset!==i.offset||e.focusNode!==r.node||e.focusOffset!==r.offset)&&(t=t.createRange(),t.setStart(i.node,i.offset),e.removeAllRanges(),a>n?(e.addRange(t),e.extend(r.node,r.offset)):(t.setEnd(r.node,r.offset),e.addRange(t)))}}for(t=[],e=o;e=e.parentNode;)e.nodeType===1&&t.push({element:e,left:e.scrollLeft,top:e.scrollTop});for(typeof o.focus=="function"&&o.focus(),o=0;o<t.length;o++)e=t[o],e.element.scrollLeft=e.left,e.element.scrollTop=e.top}}var Fh=it&&"documentMode"in document&&11>=document.documentMode,Zt=null,La=null,Ho=null,za=!1;function Ls(e,t,o){var n=o.window===o?o.document:o.nodeType===9?o:o.ownerDocument;za||Zt==null||Zt!==Jn(n)||(n=Zt,"selectionStart"in n&&Er(n)?n={start:n.selectionStart,end:n.selectionEnd}:(n=(n.ownerDocument&&n.ownerDocument.defaultView||window).getSelection(),n={anchorNode:n.anchorNode,anchorOffset:n.anchorOffset,focusNode:n.focusNode,focusOffset:n.focusOffset}),Ho&&on(Ho,n)||(Ho=n,n=ii(La,"onSelect"),0<n.length&&(t=new Tr("onSelect","select",null,t,o),e.push({event:t,listeners:n}),t.target=Zt)))}function In(e,t){var o={};return o[e.toLowerCase()]=t.toLowerCase(),o["Webkit"+e]="webkit"+t,o["Moz"+e]="moz"+t,o}var eo={animationend:In("Animation","AnimationEnd"),animationiteration:In("Animation","AnimationIteration"),animationstart:In("Animation","AnimationStart"),transitionend:In("Transition","TransitionEnd")},Zi={},Bc={};it&&(Bc=document.createElement("div").style,"AnimationEvent"in window||(delete eo.animationend.animation,delete eo.animationiteration.animation,delete eo.animationstart.animation),"TransitionEvent"in window||delete eo.transitionend.transition);function Ti(e){if(Zi[e])return Zi[e];if(!eo[e])return e;var t=eo[e],o;for(o in t)if(t.hasOwnProperty(o)&&o in Bc)return Zi[e]=t[o];return e}var Gc=Ti("animationend"),qc=Ti("animationiteration"),Mc=Ti("animationstart"),Lc=Ti("transitionend"),zc=new Map,zs="abort auxClick cancel canPlay canPlayThrough click close contextMenu copy cut drag dragEnd dragEnter dragExit dragLeave dragOver dragStart drop durationChange emptied encrypted ended error gotPointerCapture input invalid keyDown keyPress keyUp load loadedData loadedMetadata loadStart lostPointerCapture mouseDown mouseMove mouseOut mouseOver mouseUp paste pause play playing pointerCancel pointerDown pointerMove pointerOut pointerOver pointerUp progress rateChange reset resize seeked seeking stalled submit suspend timeUpdate touchCancel touchEnd touchStart volumeChange scroll toggle touchMove waiting wheel".split(" ");function It(e,t){zc.set(e,t),Vt(t,[e])}for(var ea=0;ea<zs.length;ea++){var ta=zs[ea],_h=ta.toLowerCase(),$h=ta[0].toUpperCase()+ta.slice(1);It(_h,"on"+$h)}It(Gc,"onAnimationEnd");It(qc,"onAnimationIteration");It(Mc,"onAnimationStart");It("dblclick","onDoubleClick");It("focusin","onFocus");It("focusout","onBlur");It(Lc,"onTransitionEnd");mo("onMouseEnter",["mouseout","mouseover"]);mo("onMouseLeave",["mouseout","mouseover"]);mo("onPointerEnter",["pointerout","pointerover"]);mo("onPointerLeave",["pointerout","pointerover"]);Vt("onChange","change click focusin focusout input keydown keyup selectionchange".split(" "));Vt("onSelect","focusout contextmenu dragend focusin keydown keyup mousedown mouseup selectionchange".split(" "));Vt("onBeforeInput",["compositionend","keypress","textInput","paste"]);Vt("onCompositionEnd","compositionend focusout keydown keypress keyup mousedown".split(" "));Vt("onCompositionStart","compositionstart focusout keydown keypress keyup mousedown".split(" "));Vt("onCompositionUpdate","compositionupdate focusout keydown keypress keyup mousedown".split(" "));var Uo="abort canplay canplaythrough durationchange emptied encrypted ended error loadeddata loadedmetadata loadstart pause play playing progress ratechange resize seeked seeking stalled suspend timeupdate volumechange waiting".split(" "),Jh=new Set("cancel close invalid load scroll toggle".split(" ").concat(Uo));function Rs(e,t,o){var n=e.type||"unknown-event";e.currentTarget=o,_d(n,t,void 0,e),e.currentTarget=null}function Rc(e,t){t=(t&4)!==0;for(var o=0;o<e.length;o++){var n=e[o],i=n.event;n=n.listeners;e:{var a=void 0;if(t)for(var r=n.length-1;0<=r;r--){var s=n[r],l=s.instance,c=s.currentTarget;if(s=s.listener,l!==a&&i.isPropagationStopped())break e;Rs(i,s,c),a=l}else for(r=0;r<n.length;r++){if(s=n[r],l=s.instance,c=s.currentTarget,s=s.listener,l!==a&&i.isPropagationStopped())break e;Rs(i,s,c),a=l}}}if(Zn)throw e=Ba,Zn=!1,Ba=null,e}function W(e,t){var o=t[Na];o===void 0&&(o=t[Na]=new Set);var n=e+"__bubble";o.has(n)||(jc(t,e,2,!1),o.add(n))}function oa(e,t,o){var n=0;t&&(n|=4),jc(o,e,n,t)}var En="_reactListening"+Math.random().toString(36).slice(2);function nn(e){if(!e[En]){e[En]=!0,Vl.forEach(function(o){o!=="selectionchange"&&(Jh.has(o)||oa(o,!1,e),oa(o,!0,e))});var t=e.nodeType===9?e:e.ownerDocument;t===null||t[En]||(t[En]=!0,oa("selectionchange",!1,t))}}function jc(e,t,o,n){switch(Cc(t)){case 1:var i=dh;break;case 4:i=hh;break;default:i=xr}o=i.bind(null,t,o,e),i=void 0,!Ea||t!=="touchstart"&&t!=="touchmove"&&t!=="wheel"||(i=!0),n?i!==void 0?e.addEventListener(t,o,{capture:!0,passive:i}):e.addEventListener(t,o,!0):i!==void 0?e.addEventListener(t,o,{passive:i}):e.addEventListener(t,o,!1)}function na(e,t,o,n,i){var a=n;if(!(t&1)&&!(t&2)&&n!==null)e:for(;;){if(n===null)return;var r=n.tag;if(r===3||r===4){var s=n.stateNode.containerInfo;if(s===i||s.nodeType===8&&s.parentNode===i)break;if(r===4)for(r=n.return;r!==null;){var l=r.tag;if((l===3||l===4)&&(l=r.stateNode.containerInfo,l===i||l.nodeType===8&&l.parentNode===i))return;r=r.return}for(;s!==null;){if(r=Lt(s),r===null)return;if(l=r.tag,l===5||l===6){n=a=r;continue e}s=s.parentNode}}n=n.return}sc(function(){var c=a,h=Cr(o),m=[];e:{var f=zc.get(e);if(f!==void 0){var v=Tr,w=e;switch(e){case"keypress":if(Wn(o)===0)break e;case"keydown":case"keyup":v=Th;break;case"focusin":w="focus",v=$i;break;case"focusout":w="blur",v=$i;break;case"beforeblur":case"afterblur":v=$i;break;case"click":if(o.button===2)break e;case"auxclick":case"dblclick":case"mousedown":case"mousemove":case"mouseup":case"mouseout":case"mouseover":case"contextmenu":v=xs;break;case"drag":case"dragend":case"dragenter":case"dragexit":case"dragleave":case"dragover":case"dragstart":case"drop":v=mh;break;case"touchcancel":case"touchend":case"touchmove":case"touchstart":v=Eh;break;case Gc:case qc:case Mc:v=vh;break;case Lc:v=Gh;break;case"scroll":v=ph;break;case"wheel":v=Mh;break;case"copy":case"cut":case"paste":v=wh;break;case"gotpointercapture":case"lostpointercapture":case"pointercancel":case"pointerdown":case"pointermove":case"pointerout":case"pointerover":case"pointerup":v=Ts}var S=(t&4)!==0,T=!S&&e==="scroll",d=S?f!==null?f+"Capture":null:f;S=[];for(var u=c,p;u!==null;){p=u;var y=p.stateNode;if(p.tag===5&&y!==null&&(p=y,d!==null&&(y=Jo(u,d),y!=null&&S.push(an(u,y,p)))),T)break;u=u.return}0<S.length&&(f=new v(f,w,null,o,h),m.push({event:f,listeners:S}))}}if(!(t&7)){e:{if(f=e==="mouseover"||e==="pointerover",v=e==="mouseout"||e==="pointerout",f&&o!==Da&&(w=o.relatedTarget||o.fromElement)&&(Lt(w)||w[at]))break e;if((v||f)&&(f=h.window===h?h:(f=h.ownerDocument)?f.defaultView||f.parentWindow:window,v?(w=o.relatedTarget||o.toElement,v=c,w=w?Lt(w):null,w!==null&&(T=Kt(w),w!==T||w.tag!==5&&w.tag!==6)&&(w=null)):(v=null,w=c),v!==w)){if(S=xs,y="onMouseLeave",d="onMouseEnter",u="mouse",(e==="pointerout"||e==="pointerover")&&(S=Ts,y="onPointerLeave",d="onPointerEnter",u="pointer"),T=v==null?f:to(v),p=w==null?f:to(w),f=new S(y,u+"leave",v,o,h),f.target=T,f.relatedTarget=p,y=null,Lt(h)===c&&(S=new S(d,u+"enter",w,o,h),S.target=p,S.relatedTarget=T,y=S),T=y,v&&w)t:{for(S=v,d=w,u=0,p=S;p;p=Qt(p))u++;for(p=0,y=d;y;y=Qt(y))p++;for(;0<u-p;)S=Qt(S),u--;for(;0<p-u;)d=Qt(d),p--;for(;u--;){if(S===d||d!==null&&S===d.alternate)break t;S=Qt(S),d=Qt(d)}S=null}else S=null;v!==null&&js(m,f,v,S,!1),w!==null&&T!==null&&js(m,T,w,S,!0)}}e:{if(f=c?to(c):window,v=f.nodeName&&f.nodeName.toLowerCase(),v==="select"||v==="input"&&f.type==="file")var b=Nh;else if(Es(f))if(Tc)b=Vh;else{b=Wh;var k=Oh}else(v=f.nodeName)&&v.toLowerCase()==="input"&&(f.type==="checkbox"||f.type==="radio")&&(b=Hh);if(b&&(b=b(e,c))){Pc(m,b,o,h);break e}k&&k(e,f,c),e==="focusout"&&(k=f._wrapperState)&&k.controlled&&f.type==="number"&&Aa(f,"number",f.value)}switch(k=c?to(c):window,e){case"focusin":(Es(k)||k.contentEditable==="true")&&(Zt=k,La=c,Ho=null);break;case"focusout":Ho=La=Zt=null;break;case"mousedown":za=!0;break;case"contextmenu":case"mouseup":case"dragend":za=!1,Ls(m,o,h);break;case"selectionchange":if(Fh)break;case"keydown":case"keyup":Ls(m,o,h)}var P;if(Ir)e:{switch(e){case"compositionstart":var I="onCompositionStart";break e;case"compositionend":I="onCompositionEnd";break e;case"compositionupdate":I="onCompositionUpdate";break e}I=void 0}else Xt?kc(e,o)&&(I="onCompositionEnd"):e==="keydown"&&o.keyCode===229&&(I="onCompositionStart");I&&(Ac&&o.locale!=="ko"&&(Xt||I!=="onCompositionStart"?I==="onCompositionEnd"&&Xt&&(P=Sc()):(mt=h,Pr="value"in mt?mt.value:mt.textContent,Xt=!0)),k=ii(c,I),0<k.length&&(I=new Ps(I,e,null,o,h),m.push({event:I,listeners:k}),P?I.data=P:(P=xc(o),P!==null&&(I.data=P)))),(P=zh?Rh(e,o):jh(e,o))&&(c=ii(c,"onBeforeInput"),0<c.length&&(h=new Ps("onBeforeInput","beforeinput",null,o,h),m.push({event:h,listeners:c}),h.data=P))}Rc(m,t)})}function an(e,t,o){return{instance:e,listener:t,currentTarget:o}}function ii(e,t){for(var o=t+"Capture",n=[];e!==null;){var i=e,a=i.stateNode;i.tag===5&&a!==null&&(i=a,a=Jo(e,o),a!=null&&n.unshift(an(e,a,i)),a=Jo(e,t),a!=null&&n.push(an(e,a,i))),e=e.return}return n}function Qt(e){if(e===null)return null;do e=e.return;while(e&&e.tag!==5);return e||null}function js(e,t,o,n,i){for(var a=t._reactName,r=[];o!==null&&o!==n;){var s=o,l=s.alternate,c=s.stateNode;if(l!==null&&l===n)break;s.tag===5&&c!==null&&(s=c,i?(l=Jo(o,a),l!=null&&r.unshift(an(o,l,s))):i||(l=Jo(o,a),l!=null&&r.push(an(o,l,s)))),o=o.return}r.length!==0&&e.push({event:t,listeners:r})}var Xh=/\r\n?/g,Zh=/\u0000|\uFFFD/g;function Us(e){return(typeof e=="string"?e:""+e).replace(Xh,`
`).replace(Zh,"")}function Bn(e,t,o){if(t=Us(t),Us(e)!==t&&o)throw Error(x(425))}function ai(){}var Ra=null,ja=null;function Ua(e,t){return e==="textarea"||e==="noscript"||typeof t.children=="string"||typeof t.children=="number"||typeof t.dangerouslySetInnerHTML=="object"&&t.dangerouslySetInnerHTML!==null&&t.dangerouslySetInnerHTML.__html!=null}var Ya=typeof setTimeout=="function"?setTimeout:void 0,ep=typeof clearTimeout=="function"?clearTimeout:void 0,Ys=typeof Promise=="function"?Promise:void 0,tp=typeof queueMicrotask=="function"?queueMicrotask:typeof Ys<"u"?function(e){return Ys.resolve(null).then(e).catch(op)}:Ya;function op(e){setTimeout(function(){throw e})}function ia(e,t){var o=t,n=0;do{var i=o.nextSibling;if(e.removeChild(o),i&&i.nodeType===8)if(o=i.data,o==="/$"){if(n===0){e.removeChild(i),en(t);return}n--}else o!=="$"&&o!=="$?"&&o!=="$!"||n++;o=i}while(o);en(t)}function Ct(e){for(;e!=null;e=e.nextSibling){var t=e.nodeType;if(t===1||t===3)break;if(t===8){if(t=e.data,t==="$"||t==="$!"||t==="$?")break;if(t==="/$")return null}}return e}function Ns(e){e=e.previousSibling;for(var t=0;e;){if(e.nodeType===8){var o=e.data;if(o==="$"||o==="$!"||o==="$?"){if(t===0)return e;t--}else o==="/$"&&t++}e=e.previousSibling}return null}var ko=Math.random().toString(36).slice(2),_e="__reactFiber$"+ko,rn="__reactProps$"+ko,at="__reactContainer$"+ko,Na="__reactEvents$"+ko,np="__reactListeners$"+ko,ip="__reactHandles$"+ko;function Lt(e){var t=e[_e];if(t)return t;for(var o=e.parentNode;o;){if(t=o[at]||o[_e]){if(o=t.alternate,t.child!==null||o!==null&&o.child!==null)for(e=Ns(e);e!==null;){if(o=e[_e])return o;e=Ns(e)}return t}e=o,o=e.parentNode}return null}function vn(e){return e=e[_e]||e[at],!e||e.tag!==5&&e.tag!==6&&e.tag!==13&&e.tag!==3?null:e}function to(e){if(e.tag===5||e.tag===6)return e.stateNode;throw Error(x(33))}function Di(e){return e[rn]||null}var Oa=[],oo=-1;function Et(e){return{current:e}}function H(e){0>oo||(e.current=Oa[oo],Oa[oo]=null,oo--)}function N(e,t){oo++,Oa[oo]=e.current,e.current=t}var Dt={},me=Et(Dt),Ae=Et(!1),Yt=Dt;function fo(e,t){var o=e.type.contextTypes;if(!o)return Dt;var n=e.stateNode;if(n&&n.__reactInternalMemoizedUnmaskedChildContext===t)return n.__reactInternalMemoizedMaskedChildContext;var i={},a;for(a in o)i[a]=t[a];return n&&(e=e.stateNode,e.__reactInternalMemoizedUnmaskedChildContext=t,e.__reactInternalMemoizedMaskedChildContext=i),i}function ke(e){return e=e.childContextTypes,e!=null}function ri(){H(Ae),H(me)}function Os(e,t,o){if(me.current!==Dt)throw Error(x(168));N(me,t),N(Ae,o)}function Uc(e,t,o){var n=e.stateNode;if(t=t.childContextTypes,typeof n.getChildContext!="function")return o;n=n.getChildContext();for(var i in n)if(!(i in t))throw Error(x(108,Od(e)||"Unknown",i));return F({},o,n)}function si(e){return e=(e=e.stateNode)&&e.__reactInternalMemoizedMergedChildContext||Dt,Yt=me.current,N(me,e),N(Ae,Ae.current),!0}function Ws(e,t,o){var n=e.stateNode;if(!n)throw Error(x(169));o?(e=Uc(e,t,Yt),n.__reactInternalMemoizedMergedChildContext=e,H(Ae),H(me),N(me,e)):H(Ae),N(Ae,o)}var et=null,Ii=!1,aa=!1;function Yc(e){et===null?et=[e]:et.push(e)}function ap(e){Ii=!0,Yc(e)}function Bt(){if(!aa&&et!==null){aa=!0;var e=0,t=U;try{var o=et;for(U=1;e<o.length;e++){var n=o[e];do n=n(!0);while(n!==null)}et=null,Ii=!1}catch(i){throw et!==null&&(et=et.slice(e+1)),dc(Sr,Bt),i}finally{U=t,aa=!1}}return null}var no=[],io=0,li=null,ci=0,qe=[],Me=0,Nt=null,tt=1,ot="";function qt(e,t){no[io++]=ci,no[io++]=li,li=e,ci=t}function Nc(e,t,o){qe[Me++]=tt,qe[Me++]=ot,qe[Me++]=Nt,Nt=e;var n=tt;e=ot;var i=32-He(n)-1;n&=~(1<<i),o+=1;var a=32-He(t)+i;if(30<a){var r=i-i%5;a=(n&(1<<r)-1).toString(32),n>>=r,i-=r,tt=1<<32-He(t)+i|o<<i|n,ot=a+e}else tt=1<<a|o<<i|n,ot=e}function Br(e){e.return!==null&&(qt(e,1),Nc(e,1,0))}function Gr(e){for(;e===li;)li=no[--io],no[io]=null,ci=no[--io],no[io]=null;for(;e===Nt;)Nt=qe[--Me],qe[Me]=null,ot=qe[--Me],qe[Me]=null,tt=qe[--Me],qe[Me]=null}var Ie=null,De=null,V=!1,We=null;function Oc(e,t){var o=Le(5,null,null,0);o.elementType="DELETED",o.stateNode=t,o.return=e,t=e.deletions,t===null?(e.deletions=[o],e.flags|=16):t.push(o)}function Hs(e,t){switch(e.tag){case 5:var o=e.type;return t=t.nodeType!==1||o.toLowerCase()!==t.nodeName.toLowerCase()?null:t,t!==null?(e.stateNode=t,Ie=e,De=Ct(t.firstChild),!0):!1;case 6:return t=e.pendingProps===""||t.nodeType!==3?null:t,t!==null?(e.stateNode=t,Ie=e,De=null,!0):!1;case 13:return t=t.nodeType!==8?null:t,t!==null?(o=Nt!==null?{id:tt,overflow:ot}:null,e.memoizedState={dehydrated:t,treeContext:o,retryLane:1073741824},o=Le(18,null,null,0),o.stateNode=t,o.return=e,e.child=o,Ie=e,De=null,!0):!1;default:return!1}}function Wa(e){return(e.mode&1)!==0&&(e.flags&128)===0}function Ha(e){if(V){var t=De;if(t){var o=t;if(!Hs(e,t)){if(Wa(e))throw Error(x(418));t=Ct(o.nextSibling);var n=Ie;t&&Hs(e,t)?Oc(n,o):(e.flags=e.flags&-4097|2,V=!1,Ie=e)}}else{if(Wa(e))throw Error(x(418));e.flags=e.flags&-4097|2,V=!1,Ie=e}}}function Vs(e){for(e=e.return;e!==null&&e.tag!==5&&e.tag!==3&&e.tag!==13;)e=e.return;Ie=e}function Gn(e){if(e!==Ie)return!1;if(!V)return Vs(e),V=!0,!1;var t;if((t=e.tag!==3)&&!(t=e.tag!==5)&&(t=e.type,t=t!=="head"&&t!=="body"&&!Ua(e.type,e.memoizedProps)),t&&(t=De)){if(Wa(e))throw Wc(),Error(x(418));for(;t;)Oc(e,t),t=Ct(t.nextSibling)}if(Vs(e),e.tag===13){if(e=e.memoizedState,e=e!==null?e.dehydrated:null,!e)throw Error(x(317));e:{for(e=e.nextSibling,t=0;e;){if(e.nodeType===8){var o=e.data;if(o==="/$"){if(t===0){De=Ct(e.nextSibling);break e}t--}else o!=="$"&&o!=="$!"&&o!=="$?"||t++}e=e.nextSibling}De=null}}else De=Ie?Ct(e.stateNode.nextSibling):null;return!0}function Wc(){for(var e=De;e;)e=Ct(e.nextSibling)}function yo(){De=Ie=null,V=!1}function qr(e){We===null?We=[e]:We.push(e)}var rp=lt.ReactCurrentBatchConfig;function qo(e,t,o){if(e=o.ref,e!==null&&typeof e!="function"&&typeof e!="object"){if(o._owner){if(o=o._owner,o){if(o.tag!==1)throw Error(x(309));var n=o.stateNode}if(!n)throw Error(x(147,e));var i=n,a=""+e;return t!==null&&t.ref!==null&&typeof t.ref=="function"&&t.ref._stringRef===a?t.ref:(t=function(r){var s=i.refs;r===null?delete s[a]:s[a]=r},t._stringRef=a,t)}if(typeof e!="string")throw Error(x(284));if(!o._owner)throw Error(x(290,e))}return e}function qn(e,t){throw e=Object.prototype.toString.call(t),Error(x(31,e==="[object Object]"?"object with keys {"+Object.keys(t).join(", ")+"}":e))}function Ks(e){var t=e._init;return t(e._payload)}function Hc(e){function t(d,u){if(e){var p=d.deletions;p===null?(d.deletions=[u],d.flags|=16):p.push(u)}}function o(d,u){if(!e)return null;for(;u!==null;)t(d,u),u=u.sibling;return null}function n(d,u){for(d=new Map;u!==null;)u.key!==null?d.set(u.key,u):d.set(u.index,u),u=u.sibling;return d}function i(d,u){return d=xt(d,u),d.index=0,d.sibling=null,d}function a(d,u,p){return d.index=p,e?(p=d.alternate,p!==null?(p=p.index,p<u?(d.flags|=2,u):p):(d.flags|=2,u)):(d.flags|=1048576,u)}function r(d){return e&&d.alternate===null&&(d.flags|=2),d}function s(d,u,p,y){return u===null||u.tag!==6?(u=ha(p,d.mode,y),u.return=d,u):(u=i(u,p),u.return=d,u)}function l(d,u,p,y){var b=p.type;return b===Jt?h(d,u,p.props.children,y,p.key):u!==null&&(u.elementType===b||typeof b=="object"&&b!==null&&b.$$typeof===dt&&Ks(b)===u.type)?(y=i(u,p.props),y.ref=qo(d,u,p),y.return=d,y):(y=$n(p.type,p.key,p.props,null,d.mode,y),y.ref=qo(d,u,p),y.return=d,y)}function c(d,u,p,y){return u===null||u.tag!==4||u.stateNode.containerInfo!==p.containerInfo||u.stateNode.implementation!==p.implementation?(u=pa(p,d.mode,y),u.return=d,u):(u=i(u,p.children||[]),u.return=d,u)}function h(d,u,p,y,b){return u===null||u.tag!==7?(u=Ut(p,d.mode,y,b),u.return=d,u):(u=i(u,p),u.return=d,u)}function m(d,u,p){if(typeof u=="string"&&u!==""||typeof u=="number")return u=ha(""+u,d.mode,p),u.return=d,u;if(typeof u=="object"&&u!==null){switch(u.$$typeof){case Sn:return p=$n(u.type,u.key,u.props,null,d.mode,p),p.ref=qo(d,null,u),p.return=d,p;case $t:return u=pa(u,d.mode,p),u.return=d,u;case dt:var y=u._init;return m(d,y(u._payload),p)}if(Ro(u)||Do(u))return u=Ut(u,d.mode,p,null),u.return=d,u;qn(d,u)}return null}function f(d,u,p,y){var b=u!==null?u.key:null;if(typeof p=="string"&&p!==""||typeof p=="number")return b!==null?null:s(d,u,""+p,y);if(typeof p=="object"&&p!==null){switch(p.$$typeof){case Sn:return p.key===b?l(d,u,p,y):null;case $t:return p.key===b?c(d,u,p,y):null;case dt:return b=p._init,f(d,u,b(p._payload),y)}if(Ro(p)||Do(p))return b!==null?null:h(d,u,p,y,null);qn(d,p)}return null}function v(d,u,p,y,b){if(typeof y=="string"&&y!==""||typeof y=="number")return d=d.get(p)||null,s(u,d,""+y,b);if(typeof y=="object"&&y!==null){switch(y.$$typeof){case Sn:return d=d.get(y.key===null?p:y.key)||null,l(u,d,y,b);case $t:return d=d.get(y.key===null?p:y.key)||null,c(u,d,y,b);case dt:var k=y._init;return v(d,u,p,k(y._payload),b)}if(Ro(y)||Do(y))return d=d.get(p)||null,h(u,d,y,b,null);qn(u,y)}return null}function w(d,u,p,y){for(var b=null,k=null,P=u,I=u=0,q=null;P!==null&&I<p.length;I++){P.index>I?(q=P,P=null):q=P.sibling;var E=f(d,P,p[I],y);if(E===null){P===null&&(P=q);break}e&&P&&E.alternate===null&&t(d,P),u=a(E,u,I),k===null?b=E:k.sibling=E,k=E,P=q}if(I===p.length)return o(d,P),V&&qt(d,I),b;if(P===null){for(;I<p.length;I++)P=m(d,p[I],y),P!==null&&(u=a(P,u,I),k===null?b=P:k.sibling=P,k=P);return V&&qt(d,I),b}for(P=n(d,P);I<p.length;I++)q=v(P,d,I,p[I],y),q!==null&&(e&&q.alternate!==null&&P.delete(q.key===null?I:q.key),u=a(q,u,I),k===null?b=q:k.sibling=q,k=q);return e&&P.forEach(function(te){return t(d,te)}),V&&qt(d,I),b}function S(d,u,p,y){var b=Do(p);if(typeof b!="function")throw Error(x(150));if(p=b.call(p),p==null)throw Error(x(151));for(var k=b=null,P=u,I=u=0,q=null,E=p.next();P!==null&&!E.done;I++,E=p.next()){P.index>I?(q=P,P=null):q=P.sibling;var te=f(d,P,E.value,y);if(te===null){P===null&&(P=q);break}e&&P&&te.alternate===null&&t(d,P),u=a(te,u,I),k===null?b=te:k.sibling=te,k=te,P=q}if(E.done)return o(d,P),V&&qt(d,I),b;if(P===null){for(;!E.done;I++,E=p.next())E=m(d,E.value,y),E!==null&&(u=a(E,u,I),k===null?b=E:k.sibling=E,k=E);return V&&qt(d,I),b}for(P=n(d,P);!E.done;I++,E=p.next())E=v(P,d,I,E.value,y),E!==null&&(e&&E.alternate!==null&&P.delete(E.key===null?I:E.key),u=a(E,u,I),k===null?b=E:k.sibling=E,k=E);return e&&P.forEach(function(Xe){return t(d,Xe)}),V&&qt(d,I),b}function T(d,u,p,y){if(typeof p=="object"&&p!==null&&p.type===Jt&&p.key===null&&(p=p.props.children),typeof p=="object"&&p!==null){switch(p.$$typeof){case Sn:e:{for(var b=p.key,k=u;k!==null;){if(k.key===b){if(b=p.type,b===Jt){if(k.tag===7){o(d,k.sibling),u=i(k,p.props.children),u.return=d,d=u;break e}}else if(k.elementType===b||typeof b=="object"&&b!==null&&b.$$typeof===dt&&Ks(b)===k.type){o(d,k.sibling),u=i(k,p.props),u.ref=qo(d,k,p),u.return=d,d=u;break e}o(d,k);break}else t(d,k);k=k.sibling}p.type===Jt?(u=Ut(p.props.children,d.mode,y,p.key),u.return=d,d=u):(y=$n(p.type,p.key,p.props,null,d.mode,y),y.ref=qo(d,u,p),y.return=d,d=y)}return r(d);case $t:e:{for(k=p.key;u!==null;){if(u.key===k)if(u.tag===4&&u.stateNode.containerInfo===p.containerInfo&&u.stateNode.implementation===p.implementation){o(d,u.sibling),u=i(u,p.children||[]),u.return=d,d=u;break e}else{o(d,u);break}else t(d,u);u=u.sibling}u=pa(p,d.mode,y),u.return=d,d=u}return r(d);case dt:return k=p._init,T(d,u,k(p._payload),y)}if(Ro(p))return w(d,u,p,y);if(Do(p))return S(d,u,p,y);qn(d,p)}return typeof p=="string"&&p!==""||typeof p=="number"?(p=""+p,u!==null&&u.tag===6?(o(d,u.sibling),u=i(u,p),u.return=d,d=u):(o(d,u),u=ha(p,d.mode,y),u.return=d,d=u),r(d)):o(d,u)}return T}var vo=Hc(!0),Vc=Hc(!1),ui=Et(null),di=null,ao=null,Mr=null;function Lr(){Mr=ao=di=null}function zr(e){var t=ui.current;H(ui),e._currentValue=t}function Va(e,t,o){for(;e!==null;){var n=e.alternate;if((e.childLanes&t)!==t?(e.childLanes|=t,n!==null&&(n.childLanes|=t)):n!==null&&(n.childLanes&t)!==t&&(n.childLanes|=t),e===o)break;e=e.return}}function po(e,t){di=e,Mr=ao=null,e=e.dependencies,e!==null&&e.firstContext!==null&&(e.lanes&t&&(Se=!0),e.firstContext=null)}function Re(e){var t=e._currentValue;if(Mr!==e)if(e={context:e,memoizedValue:t,next:null},ao===null){if(di===null)throw Error(x(308));ao=e,di.dependencies={lanes:0,firstContext:e}}else ao=ao.next=e;return t}var zt=null;function Rr(e){zt===null?zt=[e]:zt.push(e)}function Kc(e,t,o,n){var i=t.interleaved;return i===null?(o.next=o,Rr(t)):(o.next=i.next,i.next=o),t.interleaved=o,rt(e,n)}function rt(e,t){e.lanes|=t;var o=e.alternate;for(o!==null&&(o.lanes|=t),o=e,e=e.return;e!==null;)e.childLanes|=t,o=e.alternate,o!==null&&(o.childLanes|=t),o=e,e=e.return;return o.tag===3?o.stateNode:null}var ht=!1;function jr(e){e.updateQueue={baseState:e.memoizedState,firstBaseUpdate:null,lastBaseUpdate:null,shared:{pending:null,interleaved:null,lanes:0},effects:null}}function Qc(e,t){e=e.updateQueue,t.updateQueue===e&&(t.updateQueue={baseState:e.baseState,firstBaseUpdate:e.firstBaseUpdate,lastBaseUpdate:e.lastBaseUpdate,shared:e.shared,effects:e.effects})}function nt(e,t){return{eventTime:e,lane:t,tag:0,payload:null,callback:null,next:null}}function St(e,t,o){var n=e.updateQueue;if(n===null)return null;if(n=n.shared,j&2){var i=n.pending;return i===null?t.next=t:(t.next=i.next,i.next=t),n.pending=t,rt(e,o)}return i=n.interleaved,i===null?(t.next=t,Rr(n)):(t.next=i.next,i.next=t),n.interleaved=t,rt(e,o)}function Hn(e,t,o){if(t=t.updateQueue,t!==null&&(t=t.shared,(o&4194240)!==0)){var n=t.lanes;n&=e.pendingLanes,o|=n,t.lanes=o,Ar(e,o)}}function Qs(e,t){var o=e.updateQueue,n=e.alternate;if(n!==null&&(n=n.updateQueue,o===n)){var i=null,a=null;if(o=o.firstBaseUpdate,o!==null){do{var r={eventTime:o.eventTime,lane:o.lane,tag:o.tag,payload:o.payload,callback:o.callback,next:null};a===null?i=a=r:a=a.next=r,o=o.next}while(o!==null);a===null?i=a=t:a=a.next=t}else i=a=t;o={baseState:n.baseState,firstBaseUpdate:i,lastBaseUpdate:a,shared:n.shared,effects:n.effects},e.updateQueue=o;return}e=o.lastBaseUpdate,e===null?o.firstBaseUpdate=t:e.next=t,o.lastBaseUpdate=t}function hi(e,t,o,n){var i=e.updateQueue;ht=!1;var a=i.firstBaseUpdate,r=i.lastBaseUpdate,s=i.shared.pending;if(s!==null){i.shared.pending=null;var l=s,c=l.next;l.next=null,r===null?a=c:r.next=c,r=l;var h=e.alternate;h!==null&&(h=h.updateQueue,s=h.lastBaseUpdate,s!==r&&(s===null?h.firstBaseUpdate=c:s.next=c,h.lastBaseUpdate=l))}if(a!==null){var m=i.baseState;r=0,h=c=l=null,s=a;do{var f=s.lane,v=s.eventTime;if((n&f)===f){h!==null&&(h=h.next={eventTime:v,lane:0,tag:s.tag,payload:s.payload,callback:s.callback,next:null});e:{var w=e,S=s;switch(f=t,v=o,S.tag){case 1:if(w=S.payload,typeof w=="function"){m=w.call(v,m,f);break e}m=w;break e;case 3:w.flags=w.flags&-65537|128;case 0:if(w=S.payload,f=typeof w=="function"?w.call(v,m,f):w,f==null)break e;m=F({},m,f);break e;case 2:ht=!0}}s.callback!==null&&s.lane!==0&&(e.flags|=64,f=i.effects,f===null?i.effects=[s]:f.push(s))}else v={eventTime:v,lane:f,tag:s.tag,payload:s.payload,callback:s.callback,next:null},h===null?(c=h=v,l=m):h=h.next=v,r|=f;if(s=s.next,s===null){if(s=i.shared.pending,s===null)break;f=s,s=f.next,f.next=null,i.lastBaseUpdate=f,i.shared.pending=null}}while(!0);if(h===null&&(l=m),i.baseState=l,i.firstBaseUpdate=c,i.lastBaseUpdate=h,t=i.shared.interleaved,t!==null){i=t;do r|=i.lane,i=i.next;while(i!==t)}else a===null&&(i.shared.lanes=0);Wt|=r,e.lanes=r,e.memoizedState=m}}function Fs(e,t,o){if(e=t.effects,t.effects=null,e!==null)for(t=0;t<e.length;t++){var n=e[t],i=n.callback;if(i!==null){if(n.callback=null,n=o,typeof i!="function")throw Error(x(191,i));i.call(n)}}}var bn={},Je=Et(bn),sn=Et(bn),ln=Et(bn);function Rt(e){if(e===bn)throw Error(x(174));return e}function Ur(e,t){switch(N(ln,t),N(sn,e),N(Je,bn),e=t.nodeType,e){case 9:case 11:t=(t=t.documentElement)?t.namespaceURI:xa(null,"");break;default:e=e===8?t.parentNode:t,t=e.namespaceURI||null,e=e.tagName,t=xa(t,e)}H(Je),N(Je,t)}function bo(){H(Je),H(sn),H(ln)}function Fc(e){Rt(ln.current);var t=Rt(Je.current),o=xa(t,e.type);t!==o&&(N(sn,e),N(Je,o))}function Yr(e){sn.current===e&&(H(Je),H(sn))}var K=Et(0);function pi(e){for(var t=e;t!==null;){if(t.tag===13){var o=t.memoizedState;if(o!==null&&(o=o.dehydrated,o===null||o.data==="$?"||o.data==="$!"))return t}else if(t.tag===19&&t.memoizedProps.revealOrder!==void 0){if(t.flags&128)return t}else if(t.child!==null){t.child.return=t,t=t.child;continue}if(t===e)break;for(;t.sibling===null;){if(t.return===null||t.return===e)return null;t=t.return}t.sibling.return=t.return,t=t.sibling}return null}var ra=[];function Nr(){for(var e=0;e<ra.length;e++)ra[e]._workInProgressVersionPrimary=null;ra.length=0}var Vn=lt.ReactCurrentDispatcher,sa=lt.ReactCurrentBatchConfig,Ot=0,Q=null,oe=null,ie=null,gi=!1,Vo=!1,cn=0,sp=0;function he(){throw Error(x(321))}function Or(e,t){if(t===null)return!1;for(var o=0;o<t.length&&o<e.length;o++)if(!Ke(e[o],t[o]))return!1;return!0}function Wr(e,t,o,n,i,a){if(Ot=a,Q=t,t.memoizedState=null,t.updateQueue=null,t.lanes=0,Vn.current=e===null||e.memoizedState===null?dp:hp,e=o(n,i),Vo){a=0;do{if(Vo=!1,cn=0,25<=a)throw Error(x(301));a+=1,ie=oe=null,t.updateQueue=null,Vn.current=pp,e=o(n,i)}while(Vo)}if(Vn.current=mi,t=oe!==null&&oe.next!==null,Ot=0,ie=oe=Q=null,gi=!1,t)throw Error(x(300));return e}function Hr(){var e=cn!==0;return cn=0,e}function Fe(){var e={memoizedState:null,baseState:null,baseQueue:null,queue:null,next:null};return ie===null?Q.memoizedState=ie=e:ie=ie.next=e,ie}function je(){if(oe===null){var e=Q.alternate;e=e!==null?e.memoizedState:null}else e=oe.next;var t=ie===null?Q.memoizedState:ie.next;if(t!==null)ie=t,oe=e;else{if(e===null)throw Error(x(310));oe=e,e={memoizedState:oe.memoizedState,baseState:oe.baseState,baseQueue:oe.baseQueue,queue:oe.queue,next:null},ie===null?Q.memoizedState=ie=e:ie=ie.next=e}return ie}function un(e,t){return typeof t=="function"?t(e):t}function la(e){var t=je(),o=t.queue;if(o===null)throw Error(x(311));o.lastRenderedReducer=e;var n=oe,i=n.baseQueue,a=o.pending;if(a!==null){if(i!==null){var r=i.next;i.next=a.next,a.next=r}n.baseQueue=i=a,o.pending=null}if(i!==null){a=i.next,n=n.baseState;var s=r=null,l=null,c=a;do{var h=c.lane;if((Ot&h)===h)l!==null&&(l=l.next={lane:0,action:c.action,hasEagerState:c.hasEagerState,eagerState:c.eagerState,next:null}),n=c.hasEagerState?c.eagerState:e(n,c.action);else{var m={lane:h,action:c.action,hasEagerState:c.hasEagerState,eagerState:c.eagerState,next:null};l===null?(s=l=m,r=n):l=l.next=m,Q.lanes|=h,Wt|=h}c=c.next}while(c!==null&&c!==a);l===null?r=n:l.next=s,Ke(n,t.memoizedState)||(Se=!0),t.memoizedState=n,t.baseState=r,t.baseQueue=l,o.lastRenderedState=n}if(e=o.interleaved,e!==null){i=e;do a=i.lane,Q.lanes|=a,Wt|=a,i=i.next;while(i!==e)}else i===null&&(o.lanes=0);return[t.memoizedState,o.dispatch]}function ca(e){var t=je(),o=t.queue;if(o===null)throw Error(x(311));o.lastRenderedReducer=e;var n=o.dispatch,i=o.pending,a=t.memoizedState;if(i!==null){o.pending=null;var r=i=i.next;do a=e(a,r.action),r=r.next;while(r!==i);Ke(a,t.memoizedState)||(Se=!0),t.memoizedState=a,t.baseQueue===null&&(t.baseState=a),o.lastRenderedState=a}return[a,n]}function _c(){}function $c(e,t){var o=Q,n=je(),i=t(),a=!Ke(n.memoizedState,i);if(a&&(n.memoizedState=i,Se=!0),n=n.queue,Vr(Zc.bind(null,o,n,e),[e]),n.getSnapshot!==t||a||ie!==null&&ie.memoizedState.tag&1){if(o.flags|=2048,dn(9,Xc.bind(null,o,n,i,t),void 0,null),ae===null)throw Error(x(349));Ot&30||Jc(o,t,i)}return i}function Jc(e,t,o){e.flags|=16384,e={getSnapshot:t,value:o},t=Q.updateQueue,t===null?(t={lastEffect:null,stores:null},Q.updateQueue=t,t.stores=[e]):(o=t.stores,o===null?t.stores=[e]:o.push(e))}function Xc(e,t,o,n){t.value=o,t.getSnapshot=n,eu(t)&&tu(e)}function Zc(e,t,o){return o(function(){eu(t)&&tu(e)})}function eu(e){var t=e.getSnapshot;e=e.value;try{var o=t();return!Ke(e,o)}catch{return!0}}function tu(e){var t=rt(e,1);t!==null&&Ve(t,e,1,-1)}function _s(e){var t=Fe();return typeof e=="function"&&(e=e()),t.memoizedState=t.baseState=e,e={pending:null,interleaved:null,lanes:0,dispatch:null,lastRenderedReducer:un,lastRenderedState:e},t.queue=e,e=e.dispatch=up.bind(null,Q,e),[t.memoizedState,e]}function dn(e,t,o,n){return e={tag:e,create:t,destroy:o,deps:n,next:null},t=Q.updateQueue,t===null?(t={lastEffect:null,stores:null},Q.updateQueue=t,t.lastEffect=e.next=e):(o=t.lastEffect,o===null?t.lastEffect=e.next=e:(n=o.next,o.next=e,e.next=n,t.lastEffect=e)),e}function ou(){return je().memoizedState}function Kn(e,t,o,n){var i=Fe();Q.flags|=e,i.memoizedState=dn(1|t,o,void 0,n===void 0?null:n)}function Ei(e,t,o,n){var i=je();n=n===void 0?null:n;var a=void 0;if(oe!==null){var r=oe.memoizedState;if(a=r.destroy,n!==null&&Or(n,r.deps)){i.memoizedState=dn(t,o,a,n);return}}Q.flags|=e,i.memoizedState=dn(1|t,o,a,n)}function $s(e,t){return Kn(8390656,8,e,t)}function Vr(e,t){return Ei(2048,8,e,t)}function nu(e,t){return Ei(4,2,e,t)}function iu(e,t){return Ei(4,4,e,t)}function au(e,t){if(typeof t=="function")return e=e(),t(e),function(){t(null)};if(t!=null)return e=e(),t.current=e,function(){t.current=null}}function ru(e,t,o){return o=o!=null?o.concat([e]):null,Ei(4,4,au.bind(null,t,e),o)}function Kr(){}function su(e,t){var o=je();t=t===void 0?null:t;var n=o.memoizedState;return n!==null&&t!==null&&Or(t,n[1])?n[0]:(o.memoizedState=[e,t],e)}function lu(e,t){var o=je();t=t===void 0?null:t;var n=o.memoizedState;return n!==null&&t!==null&&Or(t,n[1])?n[0]:(e=e(),o.memoizedState=[e,t],e)}function cu(e,t,o){return Ot&21?(Ke(o,t)||(o=gc(),Q.lanes|=o,Wt|=o,e.baseState=!0),t):(e.baseState&&(e.baseState=!1,Se=!0),e.memoizedState=o)}function lp(e,t){var o=U;U=o!==0&&4>o?o:4,e(!0);var n=sa.transition;sa.transition={};try{e(!1),t()}finally{U=o,sa.transition=n}}function uu(){return je().memoizedState}function cp(e,t,o){var n=kt(e);if(o={lane:n,action:o,hasEagerState:!1,eagerState:null,next:null},du(e))hu(t,o);else if(o=Kc(e,t,o,n),o!==null){var i=ye();Ve(o,e,n,i),pu(o,t,n)}}function up(e,t,o){var n=kt(e),i={lane:n,action:o,hasEagerState:!1,eagerState:null,next:null};if(du(e))hu(t,i);else{var a=e.alternate;if(e.lanes===0&&(a===null||a.lanes===0)&&(a=t.lastRenderedReducer,a!==null))try{var r=t.lastRenderedState,s=a(r,o);if(i.hasEagerState=!0,i.eagerState=s,Ke(s,r)){var l=t.interleaved;l===null?(i.next=i,Rr(t)):(i.next=l.next,l.next=i),t.interleaved=i;return}}catch{}finally{}o=Kc(e,t,i,n),o!==null&&(i=ye(),Ve(o,e,n,i),pu(o,t,n))}}function du(e){var t=e.alternate;return e===Q||t!==null&&t===Q}function hu(e,t){Vo=gi=!0;var o=e.pending;o===null?t.next=t:(t.next=o.next,o.next=t),e.pending=t}function pu(e,t,o){if(o&4194240){var n=t.lanes;n&=e.pendingLanes,o|=n,t.lanes=o,Ar(e,o)}}var mi={readContext:Re,useCallback:he,useContext:he,useEffect:he,useImperativeHandle:he,useInsertionEffect:he,useLayoutEffect:he,useMemo:he,useReducer:he,useRef:he,useState:he,useDebugValue:he,useDeferredValue:he,useTransition:he,useMutableSource:he,useSyncExternalStore:he,useId:he,unstable_isNewReconciler:!1},dp={readContext:Re,useCallback:function(e,t){return Fe().memoizedState=[e,t===void 0?null:t],e},useContext:Re,useEffect:$s,useImperativeHandle:function(e,t,o){return o=o!=null?o.concat([e]):null,Kn(4194308,4,au.bind(null,t,e),o)},useLayoutEffect:function(e,t){return Kn(4194308,4,e,t)},useInsertionEffect:function(e,t){return Kn(4,2,e,t)},useMemo:function(e,t){var o=Fe();return t=t===void 0?null:t,e=e(),o.memoizedState=[e,t],e},useReducer:function(e,t,o){var n=Fe();return t=o!==void 0?o(t):t,n.memoizedState=n.baseState=t,e={pending:null,interleaved:null,lanes:0,dispatch:null,lastRenderedReducer:e,lastRenderedState:t},n.queue=e,e=e.dispatch=cp.bind(null,Q,e),[n.memoizedState,e]},useRef:function(e){var t=Fe();return e={current:e},t.memoizedState=e},useState:_s,useDebugValue:Kr,useDeferredValue:function(e){return Fe().memoizedState=e},useTransition:function(){var e=_s(!1),t=e[0];return e=lp.bind(null,e[1]),Fe().memoizedState=e,[t,e]},useMutableSource:function(){},useSyncExternalStore:function(e,t,o){var n=Q,i=Fe();if(V){if(o===void 0)throw Error(x(407));o=o()}else{if(o=t(),ae===null)throw Error(x(349));Ot&30||Jc(n,t,o)}i.memoizedState=o;var a={value:o,getSnapshot:t};return i.queue=a,$s(Zc.bind(null,n,a,e),[e]),n.flags|=2048,dn(9,Xc.bind(null,n,a,o,t),void 0,null),o},useId:function(){var e=Fe(),t=ae.identifierPrefix;if(V){var o=ot,n=tt;o=(n&~(1<<32-He(n)-1)).toString(32)+o,t=":"+t+"R"+o,o=cn++,0<o&&(t+="H"+o.toString(32)),t+=":"}else o=sp++,t=":"+t+"r"+o.toString(32)+":";return e.memoizedState=t},unstable_isNewReconciler:!1},hp={readContext:Re,useCallback:su,useContext:Re,useEffect:Vr,useImperativeHandle:ru,useInsertionEffect:nu,useLayoutEffect:iu,useMemo:lu,useReducer:la,useRef:ou,useState:function(){return la(un)},useDebugValue:Kr,useDeferredValue:function(e){var t=je();return cu(t,oe.memoizedState,e)},useTransition:function(){var e=la(un)[0],t=je().memoizedState;return[e,t]},useMutableSource:_c,useSyncExternalStore:$c,useId:uu,unstable_isNewReconciler:!1},pp={readContext:Re,useCallback:su,useContext:Re,useEffect:Vr,useImperativeHandle:ru,useInsertionEffect:nu,useLayoutEffect:iu,useMemo:lu,useReducer:ca,useRef:ou,useState:function(){return ca(un)},useDebugValue:Kr,useDeferredValue:function(e){var t=je();return oe===null?t.memoizedState=e:cu(t,oe.memoizedState,e)},useTransition:function(){var e=ca(un)[0],t=je().memoizedState;return[e,t]},useMutableSource:_c,useSyncExternalStore:$c,useId:uu,unstable_isNewReconciler:!1};function Ne(e,t){if(e&&e.defaultProps){t=F({},t),e=e.defaultProps;for(var o in e)t[o]===void 0&&(t[o]=e[o]);return t}return t}function Ka(e,t,o,n){t=e.memoizedState,o=o(n,t),o=o==null?t:F({},t,o),e.memoizedState=o,e.lanes===0&&(e.updateQueue.baseState=o)}var Bi={isMounted:function(e){return(e=e._reactInternals)?Kt(e)===e:!1},enqueueSetState:function(e,t,o){e=e._reactInternals;var n=ye(),i=kt(e),a=nt(n,i);a.payload=t,o!=null&&(a.callback=o),t=St(e,a,i),t!==null&&(Ve(t,e,i,n),Hn(t,e,i))},enqueueReplaceState:function(e,t,o){e=e._reactInternals;var n=ye(),i=kt(e),a=nt(n,i);a.tag=1,a.payload=t,o!=null&&(a.callback=o),t=St(e,a,i),t!==null&&(Ve(t,e,i,n),Hn(t,e,i))},enqueueForceUpdate:function(e,t){e=e._reactInternals;var o=ye(),n=kt(e),i=nt(o,n);i.tag=2,t!=null&&(i.callback=t),t=St(e,i,n),t!==null&&(Ve(t,e,n,o),Hn(t,e,n))}};function Js(e,t,o,n,i,a,r){return e=e.stateNode,typeof e.shouldComponentUpdate=="function"?e.shouldComponentUpdate(n,a,r):t.prototype&&t.prototype.isPureReactComponent?!on(o,n)||!on(i,a):!0}function gu(e,t,o){var n=!1,i=Dt,a=t.contextType;return typeof a=="object"&&a!==null?a=Re(a):(i=ke(t)?Yt:me.current,n=t.contextTypes,a=(n=n!=null)?fo(e,i):Dt),t=new t(o,a),e.memoizedState=t.state!==null&&t.state!==void 0?t.state:null,t.updater=Bi,e.stateNode=t,t._reactInternals=e,n&&(e=e.stateNode,e.__reactInternalMemoizedUnmaskedChildContext=i,e.__reactInternalMemoizedMaskedChildContext=a),t}function Xs(e,t,o,n){e=t.state,typeof t.componentWillReceiveProps=="function"&&t.componentWillReceiveProps(o,n),typeof t.UNSAFE_componentWillReceiveProps=="function"&&t.UNSAFE_componentWillReceiveProps(o,n),t.state!==e&&Bi.enqueueReplaceState(t,t.state,null)}function Qa(e,t,o,n){var i=e.stateNode;i.props=o,i.state=e.memoizedState,i.refs={},jr(e);var a=t.contextType;typeof a=="object"&&a!==null?i.context=Re(a):(a=ke(t)?Yt:me.current,i.context=fo(e,a)),i.state=e.memoizedState,a=t.getDerivedStateFromProps,typeof a=="function"&&(Ka(e,t,a,o),i.state=e.memoizedState),typeof t.getDerivedStateFromProps=="function"||typeof i.getSnapshotBeforeUpdate=="function"||typeof i.UNSAFE_componentWillMount!="function"&&typeof i.componentWillMount!="function"||(t=i.state,typeof i.componentWillMount=="function"&&i.componentWillMount(),typeof i.UNSAFE_componentWillMount=="function"&&i.UNSAFE_componentWillMount(),t!==i.state&&Bi.enqueueReplaceState(i,i.state,null),hi(e,o,i,n),i.state=e.memoizedState),typeof i.componentDidMount=="function"&&(e.flags|=4194308)}function wo(e,t){try{var o="",n=t;do o+=Nd(n),n=n.return;while(n);var i=o}catch(a){i=`
Error generating stack: `+a.message+`
`+a.stack}return{value:e,source:t,stack:i,digest:null}}function ua(e,t,o){return{value:e,source:null,stack:o??null,digest:t??null}}function Fa(e,t){try{console.error(t.value)}catch(o){setTimeout(function(){throw o})}}var gp=typeof WeakMap=="function"?WeakMap:Map;function mu(e,t,o){o=nt(-1,o),o.tag=3,o.payload={element:null};var n=t.value;return o.callback=function(){yi||(yi=!0,ir=n),Fa(e,t)},o}function fu(e,t,o){o=nt(-1,o),o.tag=3;var n=e.type.getDerivedStateFromError;if(typeof n=="function"){var i=t.value;o.payload=function(){return n(i)},o.callback=function(){Fa(e,t)}}var a=e.stateNode;return a!==null&&typeof a.componentDidCatch=="function"&&(o.callback=function(){Fa(e,t),typeof n!="function"&&(At===null?At=new Set([this]):At.add(this));var r=t.stack;this.componentDidCatch(t.value,{componentStack:r!==null?r:""})}),o}function Zs(e,t,o){var n=e.pingCache;if(n===null){n=e.pingCache=new gp;var i=new Set;n.set(t,i)}else i=n.get(t),i===void 0&&(i=new Set,n.set(t,i));i.has(o)||(i.add(o),e=Dp.bind(null,e,t,o),t.then(e,e))}function el(e){do{var t;if((t=e.tag===13)&&(t=e.memoizedState,t=t!==null?t.dehydrated!==null:!0),t)return e;e=e.return}while(e!==null);return null}function tl(e,t,o,n,i){return e.mode&1?(e.flags|=65536,e.lanes=i,e):(e===t?e.flags|=65536:(e.flags|=128,o.flags|=131072,o.flags&=-52805,o.tag===1&&(o.alternate===null?o.tag=17:(t=nt(-1,1),t.tag=2,St(o,t,1))),o.lanes|=1),e)}var mp=lt.ReactCurrentOwner,Se=!1;function fe(e,t,o,n){t.child=e===null?Vc(t,null,o,n):vo(t,e.child,o,n)}function ol(e,t,o,n,i){o=o.render;var a=t.ref;return po(t,i),n=Wr(e,t,o,n,a,i),o=Hr(),e!==null&&!Se?(t.updateQueue=e.updateQueue,t.flags&=-2053,e.lanes&=~i,st(e,t,i)):(V&&o&&Br(t),t.flags|=1,fe(e,t,n,i),t.child)}function nl(e,t,o,n,i){if(e===null){var a=o.type;return typeof a=="function"&&!es(a)&&a.defaultProps===void 0&&o.compare===null&&o.defaultProps===void 0?(t.tag=15,t.type=a,yu(e,t,a,n,i)):(e=$n(o.type,null,n,t,t.mode,i),e.ref=t.ref,e.return=t,t.child=e)}if(a=e.child,!(e.lanes&i)){var r=a.memoizedProps;if(o=o.compare,o=o!==null?o:on,o(r,n)&&e.ref===t.ref)return st(e,t,i)}return t.flags|=1,e=xt(a,n),e.ref=t.ref,e.return=t,t.child=e}function yu(e,t,o,n,i){if(e!==null){var a=e.memoizedProps;if(on(a,n)&&e.ref===t.ref)if(Se=!1,t.pendingProps=n=a,(e.lanes&i)!==0)e.flags&131072&&(Se=!0);else return t.lanes=e.lanes,st(e,t,i)}return _a(e,t,o,n,i)}function vu(e,t,o){var n=t.pendingProps,i=n.children,a=e!==null?e.memoizedState:null;if(n.mode==="hidden")if(!(t.mode&1))t.memoizedState={baseLanes:0,cachePool:null,transitions:null},N(so,Te),Te|=o;else{if(!(o&1073741824))return e=a!==null?a.baseLanes|o:o,t.lanes=t.childLanes=1073741824,t.memoizedState={baseLanes:e,cachePool:null,transitions:null},t.updateQueue=null,N(so,Te),Te|=e,null;t.memoizedState={baseLanes:0,cachePool:null,transitions:null},n=a!==null?a.baseLanes:o,N(so,Te),Te|=n}else a!==null?(n=a.baseLanes|o,t.memoizedState=null):n=o,N(so,Te),Te|=n;return fe(e,t,i,o),t.child}function bu(e,t){var o=t.ref;(e===null&&o!==null||e!==null&&e.ref!==o)&&(t.flags|=512,t.flags|=2097152)}function _a(e,t,o,n,i){var a=ke(o)?Yt:me.current;return a=fo(t,a),po(t,i),o=Wr(e,t,o,n,a,i),n=Hr(),e!==null&&!Se?(t.updateQueue=e.updateQueue,t.flags&=-2053,e.lanes&=~i,st(e,t,i)):(V&&n&&Br(t),t.flags|=1,fe(e,t,o,i),t.child)}function il(e,t,o,n,i){if(ke(o)){var a=!0;si(t)}else a=!1;if(po(t,i),t.stateNode===null)Qn(e,t),gu(t,o,n),Qa(t,o,n,i),n=!0;else if(e===null){var r=t.stateNode,s=t.memoizedProps;r.props=s;var l=r.context,c=o.contextType;typeof c=="object"&&c!==null?c=Re(c):(c=ke(o)?Yt:me.current,c=fo(t,c));var h=o.getDerivedStateFromProps,m=typeof h=="function"||typeof r.getSnapshotBeforeUpdate=="function";m||typeof r.UNSAFE_componentWillReceiveProps!="function"&&typeof r.componentWillReceiveProps!="function"||(s!==n||l!==c)&&Xs(t,r,n,c),ht=!1;var f=t.memoizedState;r.state=f,hi(t,n,r,i),l=t.memoizedState,s!==n||f!==l||Ae.current||ht?(typeof h=="function"&&(Ka(t,o,h,n),l=t.memoizedState),(s=ht||Js(t,o,s,n,f,l,c))?(m||typeof r.UNSAFE_componentWillMount!="function"&&typeof r.componentWillMount!="function"||(typeof r.componentWillMount=="function"&&r.componentWillMount(),typeof r.UNSAFE_componentWillMount=="function"&&r.UNSAFE_componentWillMount()),typeof r.componentDidMount=="function"&&(t.flags|=4194308)):(typeof r.componentDidMount=="function"&&(t.flags|=4194308),t.memoizedProps=n,t.memoizedState=l),r.props=n,r.state=l,r.context=c,n=s):(typeof r.componentDidMount=="function"&&(t.flags|=4194308),n=!1)}else{r=t.stateNode,Qc(e,t),s=t.memoizedProps,c=t.type===t.elementType?s:Ne(t.type,s),r.props=c,m=t.pendingProps,f=r.context,l=o.contextType,typeof l=="object"&&l!==null?l=Re(l):(l=ke(o)?Yt:me.current,l=fo(t,l));var v=o.getDerivedStateFromProps;(h=typeof v=="function"||typeof r.getSnapshotBeforeUpdate=="function")||typeof r.UNSAFE_componentWillReceiveProps!="function"&&typeof r.componentWillReceiveProps!="function"||(s!==m||f!==l)&&Xs(t,r,n,l),ht=!1,f=t.memoizedState,r.state=f,hi(t,n,r,i);var w=t.memoizedState;s!==m||f!==w||Ae.current||ht?(typeof v=="function"&&(Ka(t,o,v,n),w=t.memoizedState),(c=ht||Js(t,o,c,n,f,w,l)||!1)?(h||typeof r.UNSAFE_componentWillUpdate!="function"&&typeof r.componentWillUpdate!="function"||(typeof r.componentWillUpdate=="function"&&r.componentWillUpdate(n,w,l),typeof r.UNSAFE_componentWillUpdate=="function"&&r.UNSAFE_componentWillUpdate(n,w,l)),typeof r.componentDidUpdate=="function"&&(t.flags|=4),typeof r.getSnapshotBeforeUpdate=="function"&&(t.flags|=1024)):(typeof r.componentDidUpdate!="function"||s===e.memoizedProps&&f===e.memoizedState||(t.flags|=4),typeof r.getSnapshotBeforeUpdate!="function"||s===e.memoizedProps&&f===e.memoizedState||(t.flags|=1024),t.memoizedProps=n,t.memoizedState=w),r.props=n,r.state=w,r.context=l,n=c):(typeof r.componentDidUpdate!="function"||s===e.memoizedProps&&f===e.memoizedState||(t.flags|=4),typeof r.getSnapshotBeforeUpdate!="function"||s===e.memoizedProps&&f===e.memoizedState||(t.flags|=1024),n=!1)}return $a(e,t,o,n,a,i)}function $a(e,t,o,n,i,a){bu(e,t);var r=(t.flags&128)!==0;if(!n&&!r)return i&&Ws(t,o,!1),st(e,t,a);n=t.stateNode,mp.current=t;var s=r&&typeof o.getDerivedStateFromError!="function"?null:n.render();return t.flags|=1,e!==null&&r?(t.child=vo(t,e.child,null,a),t.child=vo(t,null,s,a)):fe(e,t,s,a),t.memoizedState=n.state,i&&Ws(t,o,!0),t.child}function wu(e){var t=e.stateNode;t.pendingContext?Os(e,t.pendingContext,t.pendingContext!==t.context):t.context&&Os(e,t.context,!1),Ur(e,t.containerInfo)}function al(e,t,o,n,i){return yo(),qr(i),t.flags|=256,fe(e,t,o,n),t.child}var Ja={dehydrated:null,treeContext:null,retryLane:0};function Xa(e){return{baseLanes:e,cachePool:null,transitions:null}}function Cu(e,t,o){var n=t.pendingProps,i=K.current,a=!1,r=(t.flags&128)!==0,s;if((s=r)||(s=e!==null&&e.memoizedState===null?!1:(i&2)!==0),s?(a=!0,t.flags&=-129):(e===null||e.memoizedState!==null)&&(i|=1),N(K,i&1),e===null)return Ha(t),e=t.memoizedState,e!==null&&(e=e.dehydrated,e!==null)?(t.mode&1?e.data==="$!"?t.lanes=8:t.lanes=1073741824:t.lanes=1,null):(r=n.children,e=n.fallback,a?(n=t.mode,a=t.child,r={mode:"hidden",children:r},!(n&1)&&a!==null?(a.childLanes=0,a.pendingProps=r):a=Mi(r,n,0,null),e=Ut(e,n,o,null),a.return=t,e.return=t,a.sibling=e,t.child=a,t.child.memoizedState=Xa(o),t.memoizedState=Ja,e):Qr(t,r));if(i=e.memoizedState,i!==null&&(s=i.dehydrated,s!==null))return fp(e,t,r,n,s,i,o);if(a){a=n.fallback,r=t.mode,i=e.child,s=i.sibling;var l={mode:"hidden",children:n.children};return!(r&1)&&t.child!==i?(n=t.child,n.childLanes=0,n.pendingProps=l,t.deletions=null):(n=xt(i,l),n.subtreeFlags=i.subtreeFlags&14680064),s!==null?a=xt(s,a):(a=Ut(a,r,o,null),a.flags|=2),a.return=t,n.return=t,n.sibling=a,t.child=n,n=a,a=t.child,r=e.child.memoizedState,r=r===null?Xa(o):{baseLanes:r.baseLanes|o,cachePool:null,transitions:r.transitions},a.memoizedState=r,a.childLanes=e.childLanes&~o,t.memoizedState=Ja,n}return a=e.child,e=a.sibling,n=xt(a,{mode:"visible",children:n.children}),!(t.mode&1)&&(n.lanes=o),n.return=t,n.sibling=null,e!==null&&(o=t.deletions,o===null?(t.deletions=[e],t.flags|=16):o.push(e)),t.child=n,t.memoizedState=null,n}function Qr(e,t){return t=Mi({mode:"visible",children:t},e.mode,0,null),t.return=e,e.child=t}function Mn(e,t,o,n){return n!==null&&qr(n),vo(t,e.child,null,o),e=Qr(t,t.pendingProps.children),e.flags|=2,t.memoizedState=null,e}function fp(e,t,o,n,i,a,r){if(o)return t.flags&256?(t.flags&=-257,n=ua(Error(x(422))),Mn(e,t,r,n)):t.memoizedState!==null?(t.child=e.child,t.flags|=128,null):(a=n.fallback,i=t.mode,n=Mi({mode:"visible",children:n.children},i,0,null),a=Ut(a,i,r,null),a.flags|=2,n.return=t,a.return=t,n.sibling=a,t.child=n,t.mode&1&&vo(t,e.child,null,r),t.child.memoizedState=Xa(r),t.memoizedState=Ja,a);if(!(t.mode&1))return Mn(e,t,r,null);if(i.data==="$!"){if(n=i.nextSibling&&i.nextSibling.dataset,n)var s=n.dgst;return n=s,a=Error(x(419)),n=ua(a,n,void 0),Mn(e,t,r,n)}if(s=(r&e.childLanes)!==0,Se||s){if(n=ae,n!==null){switch(r&-r){case 4:i=2;break;case 16:i=8;break;case 64:case 128:case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:case 4194304:case 8388608:case 16777216:case 33554432:case 67108864:i=32;break;case 536870912:i=268435456;break;default:i=0}i=i&(n.suspendedLanes|r)?0:i,i!==0&&i!==a.retryLane&&(a.retryLane=i,rt(e,i),Ve(n,e,i,-1))}return Zr(),n=ua(Error(x(421))),Mn(e,t,r,n)}return i.data==="$?"?(t.flags|=128,t.child=e.child,t=Ip.bind(null,e),i._reactRetry=t,null):(e=a.treeContext,De=Ct(i.nextSibling),Ie=t,V=!0,We=null,e!==null&&(qe[Me++]=tt,qe[Me++]=ot,qe[Me++]=Nt,tt=e.id,ot=e.overflow,Nt=t),t=Qr(t,n.children),t.flags|=4096,t)}function rl(e,t,o){e.lanes|=t;var n=e.alternate;n!==null&&(n.lanes|=t),Va(e.return,t,o)}function da(e,t,o,n,i){var a=e.memoizedState;a===null?e.memoizedState={isBackwards:t,rendering:null,renderingStartTime:0,last:n,tail:o,tailMode:i}:(a.isBackwards=t,a.rendering=null,a.renderingStartTime=0,a.last=n,a.tail=o,a.tailMode=i)}function Su(e,t,o){var n=t.pendingProps,i=n.revealOrder,a=n.tail;if(fe(e,t,n.children,o),n=K.current,n&2)n=n&1|2,t.flags|=128;else{if(e!==null&&e.flags&128)e:for(e=t.child;e!==null;){if(e.tag===13)e.memoizedState!==null&&rl(e,o,t);else if(e.tag===19)rl(e,o,t);else if(e.child!==null){e.child.return=e,e=e.child;continue}if(e===t)break e;for(;e.sibling===null;){if(e.return===null||e.return===t)break e;e=e.return}e.sibling.return=e.return,e=e.sibling}n&=1}if(N(K,n),!(t.mode&1))t.memoizedState=null;else switch(i){case"forwards":for(o=t.child,i=null;o!==null;)e=o.alternate,e!==null&&pi(e)===null&&(i=o),o=o.sibling;o=i,o===null?(i=t.child,t.child=null):(i=o.sibling,o.sibling=null),da(t,!1,i,o,a);break;case"backwards":for(o=null,i=t.child,t.child=null;i!==null;){if(e=i.alternate,e!==null&&pi(e)===null){t.child=i;break}e=i.sibling,i.sibling=o,o=i,i=e}da(t,!0,o,null,a);break;case"together":da(t,!1,null,null,void 0);break;default:t.memoizedState=null}return t.child}function Qn(e,t){!(t.mode&1)&&e!==null&&(e.alternate=null,t.alternate=null,t.flags|=2)}function st(e,t,o){if(e!==null&&(t.dependencies=e.dependencies),Wt|=t.lanes,!(o&t.childLanes))return null;if(e!==null&&t.child!==e.child)throw Error(x(153));if(t.child!==null){for(e=t.child,o=xt(e,e.pendingProps),t.child=o,o.return=t;e.sibling!==null;)e=e.sibling,o=o.sibling=xt(e,e.pendingProps),o.return=t;o.sibling=null}return t.child}function yp(e,t,o){switch(t.tag){case 3:wu(t),yo();break;case 5:Fc(t);break;case 1:ke(t.type)&&si(t);break;case 4:Ur(t,t.stateNode.containerInfo);break;case 10:var n=t.type._context,i=t.memoizedProps.value;N(ui,n._currentValue),n._currentValue=i;break;case 13:if(n=t.memoizedState,n!==null)return n.dehydrated!==null?(N(K,K.current&1),t.flags|=128,null):o&t.child.childLanes?Cu(e,t,o):(N(K,K.current&1),e=st(e,t,o),e!==null?e.sibling:null);N(K,K.current&1);break;case 19:if(n=(o&t.childLanes)!==0,e.flags&128){if(n)return Su(e,t,o);t.flags|=128}if(i=t.memoizedState,i!==null&&(i.rendering=null,i.tail=null,i.lastEffect=null),N(K,K.current),n)break;return null;case 22:case 23:return t.lanes=0,vu(e,t,o)}return st(e,t,o)}var Au,Za,ku,xu;Au=function(e,t){for(var o=t.child;o!==null;){if(o.tag===5||o.tag===6)e.appendChild(o.stateNode);else if(o.tag!==4&&o.child!==null){o.child.return=o,o=o.child;continue}if(o===t)break;for(;o.sibling===null;){if(o.return===null||o.return===t)return;o=o.return}o.sibling.return=o.return,o=o.sibling}};Za=function(){};ku=function(e,t,o,n){var i=e.memoizedProps;if(i!==n){e=t.stateNode,Rt(Je.current);var a=null;switch(o){case"input":i=Ca(e,i),n=Ca(e,n),a=[];break;case"select":i=F({},i,{value:void 0}),n=F({},n,{value:void 0}),a=[];break;case"textarea":i=ka(e,i),n=ka(e,n),a=[];break;default:typeof i.onClick!="function"&&typeof n.onClick=="function"&&(e.onclick=ai)}Pa(o,n);var r;o=null;for(c in i)if(!n.hasOwnProperty(c)&&i.hasOwnProperty(c)&&i[c]!=null)if(c==="style"){var s=i[c];for(r in s)s.hasOwnProperty(r)&&(o||(o={}),o[r]="")}else c!=="dangerouslySetInnerHTML"&&c!=="children"&&c!=="suppressContentEditableWarning"&&c!=="suppressHydrationWarning"&&c!=="autoFocus"&&(_o.hasOwnProperty(c)?a||(a=[]):(a=a||[]).push(c,null));for(c in n){var l=n[c];if(s=i!=null?i[c]:void 0,n.hasOwnProperty(c)&&l!==s&&(l!=null||s!=null))if(c==="style")if(s){for(r in s)!s.hasOwnProperty(r)||l&&l.hasOwnProperty(r)||(o||(o={}),o[r]="");for(r in l)l.hasOwnProperty(r)&&s[r]!==l[r]&&(o||(o={}),o[r]=l[r])}else o||(a||(a=[]),a.push(c,o)),o=l;else c==="dangerouslySetInnerHTML"?(l=l?l.__html:void 0,s=s?s.__html:void 0,l!=null&&s!==l&&(a=a||[]).push(c,l)):c==="children"?typeof l!="string"&&typeof l!="number"||(a=a||[]).push(c,""+l):c!=="suppressContentEditableWarning"&&c!=="suppressHydrationWarning"&&(_o.hasOwnProperty(c)?(l!=null&&c==="onScroll"&&W("scroll",e),a||s===l||(a=[])):(a=a||[]).push(c,l))}o&&(a=a||[]).push("style",o);var c=a;(t.updateQueue=c)&&(t.flags|=4)}};xu=function(e,t,o,n){o!==n&&(t.flags|=4)};function Mo(e,t){if(!V)switch(e.tailMode){case"hidden":t=e.tail;for(var o=null;t!==null;)t.alternate!==null&&(o=t),t=t.sibling;o===null?e.tail=null:o.sibling=null;break;case"collapsed":o=e.tail;for(var n=null;o!==null;)o.alternate!==null&&(n=o),o=o.sibling;n===null?t||e.tail===null?e.tail=null:e.tail.sibling=null:n.sibling=null}}function pe(e){var t=e.alternate!==null&&e.alternate.child===e.child,o=0,n=0;if(t)for(var i=e.child;i!==null;)o|=i.lanes|i.childLanes,n|=i.subtreeFlags&14680064,n|=i.flags&14680064,i.return=e,i=i.sibling;else for(i=e.child;i!==null;)o|=i.lanes|i.childLanes,n|=i.subtreeFlags,n|=i.flags,i.return=e,i=i.sibling;return e.subtreeFlags|=n,e.childLanes=o,t}function vp(e,t,o){var n=t.pendingProps;switch(Gr(t),t.tag){case 2:case 16:case 15:case 0:case 11:case 7:case 8:case 12:case 9:case 14:return pe(t),null;case 1:return ke(t.type)&&ri(),pe(t),null;case 3:return n=t.stateNode,bo(),H(Ae),H(me),Nr(),n.pendingContext&&(n.context=n.pendingContext,n.pendingContext=null),(e===null||e.child===null)&&(Gn(t)?t.flags|=4:e===null||e.memoizedState.isDehydrated&&!(t.flags&256)||(t.flags|=1024,We!==null&&(sr(We),We=null))),Za(e,t),pe(t),null;case 5:Yr(t);var i=Rt(ln.current);if(o=t.type,e!==null&&t.stateNode!=null)ku(e,t,o,n,i),e.ref!==t.ref&&(t.flags|=512,t.flags|=2097152);else{if(!n){if(t.stateNode===null)throw Error(x(166));return pe(t),null}if(e=Rt(Je.current),Gn(t)){n=t.stateNode,o=t.type;var a=t.memoizedProps;switch(n[_e]=t,n[rn]=a,e=(t.mode&1)!==0,o){case"dialog":W("cancel",n),W("close",n);break;case"iframe":case"object":case"embed":W("load",n);break;case"video":case"audio":for(i=0;i<Uo.length;i++)W(Uo[i],n);break;case"source":W("error",n);break;case"img":case"image":case"link":W("error",n),W("load",n);break;case"details":W("toggle",n);break;case"input":ms(n,a),W("invalid",n);break;case"select":n._wrapperState={wasMultiple:!!a.multiple},W("invalid",n);break;case"textarea":ys(n,a),W("invalid",n)}Pa(o,a),i=null;for(var r in a)if(a.hasOwnProperty(r)){var s=a[r];r==="children"?typeof s=="string"?n.textContent!==s&&(a.suppressHydrationWarning!==!0&&Bn(n.textContent,s,e),i=["children",s]):typeof s=="number"&&n.textContent!==""+s&&(a.suppressHydrationWarning!==!0&&Bn(n.textContent,s,e),i=["children",""+s]):_o.hasOwnProperty(r)&&s!=null&&r==="onScroll"&&W("scroll",n)}switch(o){case"input":An(n),fs(n,a,!0);break;case"textarea":An(n),vs(n);break;case"select":case"option":break;default:typeof a.onClick=="function"&&(n.onclick=ai)}n=i,t.updateQueue=n,n!==null&&(t.flags|=4)}else{r=i.nodeType===9?i:i.ownerDocument,e==="http://www.w3.org/1999/xhtml"&&(e=Zl(o)),e==="http://www.w3.org/1999/xhtml"?o==="script"?(e=r.createElement("div"),e.innerHTML="<script><\/script>",e=e.removeChild(e.firstChild)):typeof n.is=="string"?e=r.createElement(o,{is:n.is}):(e=r.createElement(o),o==="select"&&(r=e,n.multiple?r.multiple=!0:n.size&&(r.size=n.size))):e=r.createElementNS(e,o),e[_e]=t,e[rn]=n,Au(e,t,!1,!1),t.stateNode=e;e:{switch(r=Ta(o,n),o){case"dialog":W("cancel",e),W("close",e),i=n;break;case"iframe":case"object":case"embed":W("load",e),i=n;break;case"video":case"audio":for(i=0;i<Uo.length;i++)W(Uo[i],e);i=n;break;case"source":W("error",e),i=n;break;case"img":case"image":case"link":W("error",e),W("load",e),i=n;break;case"details":W("toggle",e),i=n;break;case"input":ms(e,n),i=Ca(e,n),W("invalid",e);break;case"option":i=n;break;case"select":e._wrapperState={wasMultiple:!!n.multiple},i=F({},n,{value:void 0}),W("invalid",e);break;case"textarea":ys(e,n),i=ka(e,n),W("invalid",e);break;default:i=n}Pa(o,i),s=i;for(a in s)if(s.hasOwnProperty(a)){var l=s[a];a==="style"?oc(e,l):a==="dangerouslySetInnerHTML"?(l=l?l.__html:void 0,l!=null&&ec(e,l)):a==="children"?typeof l=="string"?(o!=="textarea"||l!=="")&&$o(e,l):typeof l=="number"&&$o(e,""+l):a!=="suppressContentEditableWarning"&&a!=="suppressHydrationWarning"&&a!=="autoFocus"&&(_o.hasOwnProperty(a)?l!=null&&a==="onScroll"&&W("scroll",e):l!=null&&yr(e,a,l,r))}switch(o){case"input":An(e),fs(e,n,!1);break;case"textarea":An(e),vs(e);break;case"option":n.value!=null&&e.setAttribute("value",""+Tt(n.value));break;case"select":e.multiple=!!n.multiple,a=n.value,a!=null?lo(e,!!n.multiple,a,!1):n.defaultValue!=null&&lo(e,!!n.multiple,n.defaultValue,!0);break;default:typeof i.onClick=="function"&&(e.onclick=ai)}switch(o){case"button":case"input":case"select":case"textarea":n=!!n.autoFocus;break e;case"img":n=!0;break e;default:n=!1}}n&&(t.flags|=4)}t.ref!==null&&(t.flags|=512,t.flags|=2097152)}return pe(t),null;case 6:if(e&&t.stateNode!=null)xu(e,t,e.memoizedProps,n);else{if(typeof n!="string"&&t.stateNode===null)throw Error(x(166));if(o=Rt(ln.current),Rt(Je.current),Gn(t)){if(n=t.stateNode,o=t.memoizedProps,n[_e]=t,(a=n.nodeValue!==o)&&(e=Ie,e!==null))switch(e.tag){case 3:Bn(n.nodeValue,o,(e.mode&1)!==0);break;case 5:e.memoizedProps.suppressHydrationWarning!==!0&&Bn(n.nodeValue,o,(e.mode&1)!==0)}a&&(t.flags|=4)}else n=(o.nodeType===9?o:o.ownerDocument).createTextNode(n),n[_e]=t,t.stateNode=n}return pe(t),null;case 13:if(H(K),n=t.memoizedState,e===null||e.memoizedState!==null&&e.memoizedState.dehydrated!==null){if(V&&De!==null&&t.mode&1&&!(t.flags&128))Wc(),yo(),t.flags|=98560,a=!1;else if(a=Gn(t),n!==null&&n.dehydrated!==null){if(e===null){if(!a)throw Error(x(318));if(a=t.memoizedState,a=a!==null?a.dehydrated:null,!a)throw Error(x(317));a[_e]=t}else yo(),!(t.flags&128)&&(t.memoizedState=null),t.flags|=4;pe(t),a=!1}else We!==null&&(sr(We),We=null),a=!0;if(!a)return t.flags&65536?t:null}return t.flags&128?(t.lanes=o,t):(n=n!==null,n!==(e!==null&&e.memoizedState!==null)&&n&&(t.child.flags|=8192,t.mode&1&&(e===null||K.current&1?ne===0&&(ne=3):Zr())),t.updateQueue!==null&&(t.flags|=4),pe(t),null);case 4:return bo(),Za(e,t),e===null&&nn(t.stateNode.containerInfo),pe(t),null;case 10:return zr(t.type._context),pe(t),null;case 17:return ke(t.type)&&ri(),pe(t),null;case 19:if(H(K),a=t.memoizedState,a===null)return pe(t),null;if(n=(t.flags&128)!==0,r=a.rendering,r===null)if(n)Mo(a,!1);else{if(ne!==0||e!==null&&e.flags&128)for(e=t.child;e!==null;){if(r=pi(e),r!==null){for(t.flags|=128,Mo(a,!1),n=r.updateQueue,n!==null&&(t.updateQueue=n,t.flags|=4),t.subtreeFlags=0,n=o,o=t.child;o!==null;)a=o,e=n,a.flags&=14680066,r=a.alternate,r===null?(a.childLanes=0,a.lanes=e,a.child=null,a.subtreeFlags=0,a.memoizedProps=null,a.memoizedState=null,a.updateQueue=null,a.dependencies=null,a.stateNode=null):(a.childLanes=r.childLanes,a.lanes=r.lanes,a.child=r.child,a.subtreeFlags=0,a.deletions=null,a.memoizedProps=r.memoizedProps,a.memoizedState=r.memoizedState,a.updateQueue=r.updateQueue,a.type=r.type,e=r.dependencies,a.dependencies=e===null?null:{lanes:e.lanes,firstContext:e.firstContext}),o=o.sibling;return N(K,K.current&1|2),t.child}e=e.sibling}a.tail!==null&&J()>Co&&(t.flags|=128,n=!0,Mo(a,!1),t.lanes=4194304)}else{if(!n)if(e=pi(r),e!==null){if(t.flags|=128,n=!0,o=e.updateQueue,o!==null&&(t.updateQueue=o,t.flags|=4),Mo(a,!0),a.tail===null&&a.tailMode==="hidden"&&!r.alternate&&!V)return pe(t),null}else 2*J()-a.renderingStartTime>Co&&o!==1073741824&&(t.flags|=128,n=!0,Mo(a,!1),t.lanes=4194304);a.isBackwards?(r.sibling=t.child,t.child=r):(o=a.last,o!==null?o.sibling=r:t.child=r,a.last=r)}return a.tail!==null?(t=a.tail,a.rendering=t,a.tail=t.sibling,a.renderingStartTime=J(),t.sibling=null,o=K.current,N(K,n?o&1|2:o&1),t):(pe(t),null);case 22:case 23:return Xr(),n=t.memoizedState!==null,e!==null&&e.memoizedState!==null!==n&&(t.flags|=8192),n&&t.mode&1?Te&1073741824&&(pe(t),t.subtreeFlags&6&&(t.flags|=8192)):pe(t),null;case 24:return null;case 25:return null}throw Error(x(156,t.tag))}function bp(e,t){switch(Gr(t),t.tag){case 1:return ke(t.type)&&ri(),e=t.flags,e&65536?(t.flags=e&-65537|128,t):null;case 3:return bo(),H(Ae),H(me),Nr(),e=t.flags,e&65536&&!(e&128)?(t.flags=e&-65537|128,t):null;case 5:return Yr(t),null;case 13:if(H(K),e=t.memoizedState,e!==null&&e.dehydrated!==null){if(t.alternate===null)throw Error(x(340));yo()}return e=t.flags,e&65536?(t.flags=e&-65537|128,t):null;case 19:return H(K),null;case 4:return bo(),null;case 10:return zr(t.type._context),null;case 22:case 23:return Xr(),null;case 24:return null;default:return null}}var Ln=!1,ge=!1,wp=typeof WeakSet=="function"?WeakSet:Set,B=null;function ro(e,t){var o=e.ref;if(o!==null)if(typeof o=="function")try{o(null)}catch(n){_(e,t,n)}else o.current=null}function er(e,t,o){try{o()}catch(n){_(e,t,n)}}var sl=!1;function Cp(e,t){if(Ra=oi,e=Ec(),Er(e)){if("selectionStart"in e)var o={start:e.selectionStart,end:e.selectionEnd};else e:{o=(o=e.ownerDocument)&&o.defaultView||window;var n=o.getSelection&&o.getSelection();if(n&&n.rangeCount!==0){o=n.anchorNode;var i=n.anchorOffset,a=n.focusNode;n=n.focusOffset;try{o.nodeType,a.nodeType}catch{o=null;break e}var r=0,s=-1,l=-1,c=0,h=0,m=e,f=null;t:for(;;){for(var v;m!==o||i!==0&&m.nodeType!==3||(s=r+i),m!==a||n!==0&&m.nodeType!==3||(l=r+n),m.nodeType===3&&(r+=m.nodeValue.length),(v=m.firstChild)!==null;)f=m,m=v;for(;;){if(m===e)break t;if(f===o&&++c===i&&(s=r),f===a&&++h===n&&(l=r),(v=m.nextSibling)!==null)break;m=f,f=m.parentNode}m=v}o=s===-1||l===-1?null:{start:s,end:l}}else o=null}o=o||{start:0,end:0}}else o=null;for(ja={focusedElem:e,selectionRange:o},oi=!1,B=t;B!==null;)if(t=B,e=t.child,(t.subtreeFlags&1028)!==0&&e!==null)e.return=t,B=e;else for(;B!==null;){t=B;try{var w=t.alternate;if(t.flags&1024)switch(t.tag){case 0:case 11:case 15:break;case 1:if(w!==null){var S=w.memoizedProps,T=w.memoizedState,d=t.stateNode,u=d.getSnapshotBeforeUpdate(t.elementType===t.type?S:Ne(t.type,S),T);d.__reactInternalSnapshotBeforeUpdate=u}break;case 3:var p=t.stateNode.containerInfo;p.nodeType===1?p.textContent="":p.nodeType===9&&p.documentElement&&p.removeChild(p.documentElement);break;case 5:case 6:case 4:case 17:break;default:throw Error(x(163))}}catch(y){_(t,t.return,y)}if(e=t.sibling,e!==null){e.return=t.return,B=e;break}B=t.return}return w=sl,sl=!1,w}function Ko(e,t,o){var n=t.updateQueue;if(n=n!==null?n.lastEffect:null,n!==null){var i=n=n.next;do{if((i.tag&e)===e){var a=i.destroy;i.destroy=void 0,a!==void 0&&er(t,o,a)}i=i.next}while(i!==n)}}function Gi(e,t){if(t=t.updateQueue,t=t!==null?t.lastEffect:null,t!==null){var o=t=t.next;do{if((o.tag&e)===e){var n=o.create;o.destroy=n()}o=o.next}while(o!==t)}}function tr(e){var t=e.ref;if(t!==null){var o=e.stateNode;switch(e.tag){case 5:e=o;break;default:e=o}typeof t=="function"?t(e):t.current=e}}function Pu(e){var t=e.alternate;t!==null&&(e.alternate=null,Pu(t)),e.child=null,e.deletions=null,e.sibling=null,e.tag===5&&(t=e.stateNode,t!==null&&(delete t[_e],delete t[rn],delete t[Na],delete t[np],delete t[ip])),e.stateNode=null,e.return=null,e.dependencies=null,e.memoizedProps=null,e.memoizedState=null,e.pendingProps=null,e.stateNode=null,e.updateQueue=null}function Tu(e){return e.tag===5||e.tag===3||e.tag===4}function ll(e){e:for(;;){for(;e.sibling===null;){if(e.return===null||Tu(e.return))return null;e=e.return}for(e.sibling.return=e.return,e=e.sibling;e.tag!==5&&e.tag!==6&&e.tag!==18;){if(e.flags&2||e.child===null||e.tag===4)continue e;e.child.return=e,e=e.child}if(!(e.flags&2))return e.stateNode}}function or(e,t,o){var n=e.tag;if(n===5||n===6)e=e.stateNode,t?o.nodeType===8?o.parentNode.insertBefore(e,t):o.insertBefore(e,t):(o.nodeType===8?(t=o.parentNode,t.insertBefore(e,o)):(t=o,t.appendChild(e)),o=o._reactRootContainer,o!=null||t.onclick!==null||(t.onclick=ai));else if(n!==4&&(e=e.child,e!==null))for(or(e,t,o),e=e.sibling;e!==null;)or(e,t,o),e=e.sibling}function nr(e,t,o){var n=e.tag;if(n===5||n===6)e=e.stateNode,t?o.insertBefore(e,t):o.appendChild(e);else if(n!==4&&(e=e.child,e!==null))for(nr(e,t,o),e=e.sibling;e!==null;)nr(e,t,o),e=e.sibling}var le=null,Oe=!1;function ut(e,t,o){for(o=o.child;o!==null;)Du(e,t,o),o=o.sibling}function Du(e,t,o){if($e&&typeof $e.onCommitFiberUnmount=="function")try{$e.onCommitFiberUnmount(ki,o)}catch{}switch(o.tag){case 5:ge||ro(o,t);case 6:var n=le,i=Oe;le=null,ut(e,t,o),le=n,Oe=i,le!==null&&(Oe?(e=le,o=o.stateNode,e.nodeType===8?e.parentNode.removeChild(o):e.removeChild(o)):le.removeChild(o.stateNode));break;case 18:le!==null&&(Oe?(e=le,o=o.stateNode,e.nodeType===8?ia(e.parentNode,o):e.nodeType===1&&ia(e,o),en(e)):ia(le,o.stateNode));break;case 4:n=le,i=Oe,le=o.stateNode.containerInfo,Oe=!0,ut(e,t,o),le=n,Oe=i;break;case 0:case 11:case 14:case 15:if(!ge&&(n=o.updateQueue,n!==null&&(n=n.lastEffect,n!==null))){i=n=n.next;do{var a=i,r=a.destroy;a=a.tag,r!==void 0&&(a&2||a&4)&&er(o,t,r),i=i.next}while(i!==n)}ut(e,t,o);break;case 1:if(!ge&&(ro(o,t),n=o.stateNode,typeof n.componentWillUnmount=="function"))try{n.props=o.memoizedProps,n.state=o.memoizedState,n.componentWillUnmount()}catch(s){_(o,t,s)}ut(e,t,o);break;case 21:ut(e,t,o);break;case 22:o.mode&1?(ge=(n=ge)||o.memoizedState!==null,ut(e,t,o),ge=n):ut(e,t,o);break;default:ut(e,t,o)}}function cl(e){var t=e.updateQueue;if(t!==null){e.updateQueue=null;var o=e.stateNode;o===null&&(o=e.stateNode=new wp),t.forEach(function(n){var i=Ep.bind(null,e,n);o.has(n)||(o.add(n),n.then(i,i))})}}function Ye(e,t){var o=t.deletions;if(o!==null)for(var n=0;n<o.length;n++){var i=o[n];try{var a=e,r=t,s=r;e:for(;s!==null;){switch(s.tag){case 5:le=s.stateNode,Oe=!1;break e;case 3:le=s.stateNode.containerInfo,Oe=!0;break e;case 4:le=s.stateNode.containerInfo,Oe=!0;break e}s=s.return}if(le===null)throw Error(x(160));Du(a,r,i),le=null,Oe=!1;var l=i.alternate;l!==null&&(l.return=null),i.return=null}catch(c){_(i,t,c)}}if(t.subtreeFlags&12854)for(t=t.child;t!==null;)Iu(t,e),t=t.sibling}function Iu(e,t){var o=e.alternate,n=e.flags;switch(e.tag){case 0:case 11:case 14:case 15:if(Ye(t,e),Qe(e),n&4){try{Ko(3,e,e.return),Gi(3,e)}catch(S){_(e,e.return,S)}try{Ko(5,e,e.return)}catch(S){_(e,e.return,S)}}break;case 1:Ye(t,e),Qe(e),n&512&&o!==null&&ro(o,o.return);break;case 5:if(Ye(t,e),Qe(e),n&512&&o!==null&&ro(o,o.return),e.flags&32){var i=e.stateNode;try{$o(i,"")}catch(S){_(e,e.return,S)}}if(n&4&&(i=e.stateNode,i!=null)){var a=e.memoizedProps,r=o!==null?o.memoizedProps:a,s=e.type,l=e.updateQueue;if(e.updateQueue=null,l!==null)try{s==="input"&&a.type==="radio"&&a.name!=null&&Jl(i,a),Ta(s,r);var c=Ta(s,a);for(r=0;r<l.length;r+=2){var h=l[r],m=l[r+1];h==="style"?oc(i,m):h==="dangerouslySetInnerHTML"?ec(i,m):h==="children"?$o(i,m):yr(i,h,m,c)}switch(s){case"input":Sa(i,a);break;case"textarea":Xl(i,a);break;case"select":var f=i._wrapperState.wasMultiple;i._wrapperState.wasMultiple=!!a.multiple;var v=a.value;v!=null?lo(i,!!a.multiple,v,!1):f!==!!a.multiple&&(a.defaultValue!=null?lo(i,!!a.multiple,a.defaultValue,!0):lo(i,!!a.multiple,a.multiple?[]:"",!1))}i[rn]=a}catch(S){_(e,e.return,S)}}break;case 6:if(Ye(t,e),Qe(e),n&4){if(e.stateNode===null)throw Error(x(162));i=e.stateNode,a=e.memoizedProps;try{i.nodeValue=a}catch(S){_(e,e.return,S)}}break;case 3:if(Ye(t,e),Qe(e),n&4&&o!==null&&o.memoizedState.isDehydrated)try{en(t.containerInfo)}catch(S){_(e,e.return,S)}break;case 4:Ye(t,e),Qe(e);break;case 13:Ye(t,e),Qe(e),i=e.child,i.flags&8192&&(a=i.memoizedState!==null,i.stateNode.isHidden=a,!a||i.alternate!==null&&i.alternate.memoizedState!==null||($r=J())),n&4&&cl(e);break;case 22:if(h=o!==null&&o.memoizedState!==null,e.mode&1?(ge=(c=ge)||h,Ye(t,e),ge=c):Ye(t,e),Qe(e),n&8192){if(c=e.memoizedState!==null,(e.stateNode.isHidden=c)&&!h&&e.mode&1)for(B=e,h=e.child;h!==null;){for(m=B=h;B!==null;){switch(f=B,v=f.child,f.tag){case 0:case 11:case 14:case 15:Ko(4,f,f.return);break;case 1:ro(f,f.return);var w=f.stateNode;if(typeof w.componentWillUnmount=="function"){n=f,o=f.return;try{t=n,w.props=t.memoizedProps,w.state=t.memoizedState,w.componentWillUnmount()}catch(S){_(n,o,S)}}break;case 5:ro(f,f.return);break;case 22:if(f.memoizedState!==null){dl(m);continue}}v!==null?(v.return=f,B=v):dl(m)}h=h.sibling}e:for(h=null,m=e;;){if(m.tag===5){if(h===null){h=m;try{i=m.stateNode,c?(a=i.style,typeof a.setProperty=="function"?a.setProperty("display","none","important"):a.display="none"):(s=m.stateNode,l=m.memoizedProps.style,r=l!=null&&l.hasOwnProperty("display")?l.display:null,s.style.display=tc("display",r))}catch(S){_(e,e.return,S)}}}else if(m.tag===6){if(h===null)try{m.stateNode.nodeValue=c?"":m.memoizedProps}catch(S){_(e,e.return,S)}}else if((m.tag!==22&&m.tag!==23||m.memoizedState===null||m===e)&&m.child!==null){m.child.return=m,m=m.child;continue}if(m===e)break e;for(;m.sibling===null;){if(m.return===null||m.return===e)break e;h===m&&(h=null),m=m.return}h===m&&(h=null),m.sibling.return=m.return,m=m.sibling}}break;case 19:Ye(t,e),Qe(e),n&4&&cl(e);break;case 21:break;default:Ye(t,e),Qe(e)}}function Qe(e){var t=e.flags;if(t&2){try{e:{for(var o=e.return;o!==null;){if(Tu(o)){var n=o;break e}o=o.return}throw Error(x(160))}switch(n.tag){case 5:var i=n.stateNode;n.flags&32&&($o(i,""),n.flags&=-33);var a=ll(e);nr(e,a,i);break;case 3:case 4:var r=n.stateNode.containerInfo,s=ll(e);or(e,s,r);break;default:throw Error(x(161))}}catch(l){_(e,e.return,l)}e.flags&=-3}t&4096&&(e.flags&=-4097)}function Sp(e,t,o){B=e,Eu(e)}function Eu(e,t,o){for(var n=(e.mode&1)!==0;B!==null;){var i=B,a=i.child;if(i.tag===22&&n){var r=i.memoizedState!==null||Ln;if(!r){var s=i.alternate,l=s!==null&&s.memoizedState!==null||ge;s=Ln;var c=ge;if(Ln=r,(ge=l)&&!c)for(B=i;B!==null;)r=B,l=r.child,r.tag===22&&r.memoizedState!==null?hl(i):l!==null?(l.return=r,B=l):hl(i);for(;a!==null;)B=a,Eu(a),a=a.sibling;B=i,Ln=s,ge=c}ul(e)}else i.subtreeFlags&8772&&a!==null?(a.return=i,B=a):ul(e)}}function ul(e){for(;B!==null;){var t=B;if(t.flags&8772){var o=t.alternate;try{if(t.flags&8772)switch(t.tag){case 0:case 11:case 15:ge||Gi(5,t);break;case 1:var n=t.stateNode;if(t.flags&4&&!ge)if(o===null)n.componentDidMount();else{var i=t.elementType===t.type?o.memoizedProps:Ne(t.type,o.memoizedProps);n.componentDidUpdate(i,o.memoizedState,n.__reactInternalSnapshotBeforeUpdate)}var a=t.updateQueue;a!==null&&Fs(t,a,n);break;case 3:var r=t.updateQueue;if(r!==null){if(o=null,t.child!==null)switch(t.child.tag){case 5:o=t.child.stateNode;break;case 1:o=t.child.stateNode}Fs(t,r,o)}break;case 5:var s=t.stateNode;if(o===null&&t.flags&4){o=s;var l=t.memoizedProps;switch(t.type){case"button":case"input":case"select":case"textarea":l.autoFocus&&o.focus();break;case"img":l.src&&(o.src=l.src)}}break;case 6:break;case 4:break;case 12:break;case 13:if(t.memoizedState===null){var c=t.alternate;if(c!==null){var h=c.memoizedState;if(h!==null){var m=h.dehydrated;m!==null&&en(m)}}}break;case 19:case 17:case 21:case 22:case 23:case 25:break;default:throw Error(x(163))}ge||t.flags&512&&tr(t)}catch(f){_(t,t.return,f)}}if(t===e){B=null;break}if(o=t.sibling,o!==null){o.return=t.return,B=o;break}B=t.return}}function dl(e){for(;B!==null;){var t=B;if(t===e){B=null;break}var o=t.sibling;if(o!==null){o.return=t.return,B=o;break}B=t.return}}function hl(e){for(;B!==null;){var t=B;try{switch(t.tag){case 0:case 11:case 15:var o=t.return;try{Gi(4,t)}catch(l){_(t,o,l)}break;case 1:var n=t.stateNode;if(typeof n.componentDidMount=="function"){var i=t.return;try{n.componentDidMount()}catch(l){_(t,i,l)}}var a=t.return;try{tr(t)}catch(l){_(t,a,l)}break;case 5:var r=t.return;try{tr(t)}catch(l){_(t,r,l)}}}catch(l){_(t,t.return,l)}if(t===e){B=null;break}var s=t.sibling;if(s!==null){s.return=t.return,B=s;break}B=t.return}}var Ap=Math.ceil,fi=lt.ReactCurrentDispatcher,Fr=lt.ReactCurrentOwner,ze=lt.ReactCurrentBatchConfig,j=0,ae=null,ee=null,ce=0,Te=0,so=Et(0),ne=0,hn=null,Wt=0,qi=0,_r=0,Qo=null,Ce=null,$r=0,Co=1/0,Ze=null,yi=!1,ir=null,At=null,zn=!1,ft=null,vi=0,Fo=0,ar=null,Fn=-1,_n=0;function ye(){return j&6?J():Fn!==-1?Fn:Fn=J()}function kt(e){return e.mode&1?j&2&&ce!==0?ce&-ce:rp.transition!==null?(_n===0&&(_n=gc()),_n):(e=U,e!==0||(e=window.event,e=e===void 0?16:Cc(e.type)),e):1}function Ve(e,t,o,n){if(50<Fo)throw Fo=0,ar=null,Error(x(185));fn(e,o,n),(!(j&2)||e!==ae)&&(e===ae&&(!(j&2)&&(qi|=o),ne===4&&gt(e,ce)),xe(e,n),o===1&&j===0&&!(t.mode&1)&&(Co=J()+500,Ii&&Bt()))}function xe(e,t){var o=e.callbackNode;rh(e,t);var n=ti(e,e===ae?ce:0);if(n===0)o!==null&&Cs(o),e.callbackNode=null,e.callbackPriority=0;else if(t=n&-n,e.callbackPriority!==t){if(o!=null&&Cs(o),t===1)e.tag===0?ap(pl.bind(null,e)):Yc(pl.bind(null,e)),tp(function(){!(j&6)&&Bt()}),o=null;else{switch(mc(n)){case 1:o=Sr;break;case 4:o=hc;break;case 16:o=ei;break;case 536870912:o=pc;break;default:o=ei}o=ju(o,Bu.bind(null,e))}e.callbackPriority=t,e.callbackNode=o}}function Bu(e,t){if(Fn=-1,_n=0,j&6)throw Error(x(327));var o=e.callbackNode;if(go()&&e.callbackNode!==o)return null;var n=ti(e,e===ae?ce:0);if(n===0)return null;if(n&30||n&e.expiredLanes||t)t=bi(e,n);else{t=n;var i=j;j|=2;var a=qu();(ae!==e||ce!==t)&&(Ze=null,Co=J()+500,jt(e,t));do try{Pp();break}catch(s){Gu(e,s)}while(!0);Lr(),fi.current=a,j=i,ee!==null?t=0:(ae=null,ce=0,t=ne)}if(t!==0){if(t===2&&(i=Ga(e),i!==0&&(n=i,t=rr(e,i))),t===1)throw o=hn,jt(e,0),gt(e,n),xe(e,J()),o;if(t===6)gt(e,n);else{if(i=e.current.alternate,!(n&30)&&!kp(i)&&(t=bi(e,n),t===2&&(a=Ga(e),a!==0&&(n=a,t=rr(e,a))),t===1))throw o=hn,jt(e,0),gt(e,n),xe(e,J()),o;switch(e.finishedWork=i,e.finishedLanes=n,t){case 0:case 1:throw Error(x(345));case 2:Mt(e,Ce,Ze);break;case 3:if(gt(e,n),(n&130023424)===n&&(t=$r+500-J(),10<t)){if(ti(e,0)!==0)break;if(i=e.suspendedLanes,(i&n)!==n){ye(),e.pingedLanes|=e.suspendedLanes&i;break}e.timeoutHandle=Ya(Mt.bind(null,e,Ce,Ze),t);break}Mt(e,Ce,Ze);break;case 4:if(gt(e,n),(n&4194240)===n)break;for(t=e.eventTimes,i=-1;0<n;){var r=31-He(n);a=1<<r,r=t[r],r>i&&(i=r),n&=~a}if(n=i,n=J()-n,n=(120>n?120:480>n?480:1080>n?1080:1920>n?1920:3e3>n?3e3:4320>n?4320:1960*Ap(n/1960))-n,10<n){e.timeoutHandle=Ya(Mt.bind(null,e,Ce,Ze),n);break}Mt(e,Ce,Ze);break;case 5:Mt(e,Ce,Ze);break;default:throw Error(x(329))}}}return xe(e,J()),e.callbackNode===o?Bu.bind(null,e):null}function rr(e,t){var o=Qo;return e.current.memoizedState.isDehydrated&&(jt(e,t).flags|=256),e=bi(e,t),e!==2&&(t=Ce,Ce=o,t!==null&&sr(t)),e}function sr(e){Ce===null?Ce=e:Ce.push.apply(Ce,e)}function kp(e){for(var t=e;;){if(t.flags&16384){var o=t.updateQueue;if(o!==null&&(o=o.stores,o!==null))for(var n=0;n<o.length;n++){var i=o[n],a=i.getSnapshot;i=i.value;try{if(!Ke(a(),i))return!1}catch{return!1}}}if(o=t.child,t.subtreeFlags&16384&&o!==null)o.return=t,t=o;else{if(t===e)break;for(;t.sibling===null;){if(t.return===null||t.return===e)return!0;t=t.return}t.sibling.return=t.return,t=t.sibling}}return!0}function gt(e,t){for(t&=~_r,t&=~qi,e.suspendedLanes|=t,e.pingedLanes&=~t,e=e.expirationTimes;0<t;){var o=31-He(t),n=1<<o;e[o]=-1,t&=~n}}function pl(e){if(j&6)throw Error(x(327));go();var t=ti(e,0);if(!(t&1))return xe(e,J()),null;var o=bi(e,t);if(e.tag!==0&&o===2){var n=Ga(e);n!==0&&(t=n,o=rr(e,n))}if(o===1)throw o=hn,jt(e,0),gt(e,t),xe(e,J()),o;if(o===6)throw Error(x(345));return e.finishedWork=e.current.alternate,e.finishedLanes=t,Mt(e,Ce,Ze),xe(e,J()),null}function Jr(e,t){var o=j;j|=1;try{return e(t)}finally{j=o,j===0&&(Co=J()+500,Ii&&Bt())}}function Ht(e){ft!==null&&ft.tag===0&&!(j&6)&&go();var t=j;j|=1;var o=ze.transition,n=U;try{if(ze.transition=null,U=1,e)return e()}finally{U=n,ze.transition=o,j=t,!(j&6)&&Bt()}}function Xr(){Te=so.current,H(so)}function jt(e,t){e.finishedWork=null,e.finishedLanes=0;var o=e.timeoutHandle;if(o!==-1&&(e.timeoutHandle=-1,ep(o)),ee!==null)for(o=ee.return;o!==null;){var n=o;switch(Gr(n),n.tag){case 1:n=n.type.childContextTypes,n!=null&&ri();break;case 3:bo(),H(Ae),H(me),Nr();break;case 5:Yr(n);break;case 4:bo();break;case 13:H(K);break;case 19:H(K);break;case 10:zr(n.type._context);break;case 22:case 23:Xr()}o=o.return}if(ae=e,ee=e=xt(e.current,null),ce=Te=t,ne=0,hn=null,_r=qi=Wt=0,Ce=Qo=null,zt!==null){for(t=0;t<zt.length;t++)if(o=zt[t],n=o.interleaved,n!==null){o.interleaved=null;var i=n.next,a=o.pending;if(a!==null){var r=a.next;a.next=i,n.next=r}o.pending=n}zt=null}return e}function Gu(e,t){do{var o=ee;try{if(Lr(),Vn.current=mi,gi){for(var n=Q.memoizedState;n!==null;){var i=n.queue;i!==null&&(i.pending=null),n=n.next}gi=!1}if(Ot=0,ie=oe=Q=null,Vo=!1,cn=0,Fr.current=null,o===null||o.return===null){ne=1,hn=t,ee=null;break}e:{var a=e,r=o.return,s=o,l=t;if(t=ce,s.flags|=32768,l!==null&&typeof l=="object"&&typeof l.then=="function"){var c=l,h=s,m=h.tag;if(!(h.mode&1)&&(m===0||m===11||m===15)){var f=h.alternate;f?(h.updateQueue=f.updateQueue,h.memoizedState=f.memoizedState,h.lanes=f.lanes):(h.updateQueue=null,h.memoizedState=null)}var v=el(r);if(v!==null){v.flags&=-257,tl(v,r,s,a,t),v.mode&1&&Zs(a,c,t),t=v,l=c;var w=t.updateQueue;if(w===null){var S=new Set;S.add(l),t.updateQueue=S}else w.add(l);break e}else{if(!(t&1)){Zs(a,c,t),Zr();break e}l=Error(x(426))}}else if(V&&s.mode&1){var T=el(r);if(T!==null){!(T.flags&65536)&&(T.flags|=256),tl(T,r,s,a,t),qr(wo(l,s));break e}}a=l=wo(l,s),ne!==4&&(ne=2),Qo===null?Qo=[a]:Qo.push(a),a=r;do{switch(a.tag){case 3:a.flags|=65536,t&=-t,a.lanes|=t;var d=mu(a,l,t);Qs(a,d);break e;case 1:s=l;var u=a.type,p=a.stateNode;if(!(a.flags&128)&&(typeof u.getDerivedStateFromError=="function"||p!==null&&typeof p.componentDidCatch=="function"&&(At===null||!At.has(p)))){a.flags|=65536,t&=-t,a.lanes|=t;var y=fu(a,s,t);Qs(a,y);break e}}a=a.return}while(a!==null)}Lu(o)}catch(b){t=b,ee===o&&o!==null&&(ee=o=o.return);continue}break}while(!0)}function qu(){var e=fi.current;return fi.current=mi,e===null?mi:e}function Zr(){(ne===0||ne===3||ne===2)&&(ne=4),ae===null||!(Wt&268435455)&&!(qi&268435455)||gt(ae,ce)}function bi(e,t){var o=j;j|=2;var n=qu();(ae!==e||ce!==t)&&(Ze=null,jt(e,t));do try{xp();break}catch(i){Gu(e,i)}while(!0);if(Lr(),j=o,fi.current=n,ee!==null)throw Error(x(261));return ae=null,ce=0,ne}function xp(){for(;ee!==null;)Mu(ee)}function Pp(){for(;ee!==null&&!Jd();)Mu(ee)}function Mu(e){var t=Ru(e.alternate,e,Te);e.memoizedProps=e.pendingProps,t===null?Lu(e):ee=t,Fr.current=null}function Lu(e){var t=e;do{var o=t.alternate;if(e=t.return,t.flags&32768){if(o=bp(o,t),o!==null){o.flags&=32767,ee=o;return}if(e!==null)e.flags|=32768,e.subtreeFlags=0,e.deletions=null;else{ne=6,ee=null;return}}else if(o=vp(o,t,Te),o!==null){ee=o;return}if(t=t.sibling,t!==null){ee=t;return}ee=t=e}while(t!==null);ne===0&&(ne=5)}function Mt(e,t,o){var n=U,i=ze.transition;try{ze.transition=null,U=1,Tp(e,t,o,n)}finally{ze.transition=i,U=n}return null}function Tp(e,t,o,n){do go();while(ft!==null);if(j&6)throw Error(x(327));o=e.finishedWork;var i=e.finishedLanes;if(o===null)return null;if(e.finishedWork=null,e.finishedLanes=0,o===e.current)throw Error(x(177));e.callbackNode=null,e.callbackPriority=0;var a=o.lanes|o.childLanes;if(sh(e,a),e===ae&&(ee=ae=null,ce=0),!(o.subtreeFlags&2064)&&!(o.flags&2064)||zn||(zn=!0,ju(ei,function(){return go(),null})),a=(o.flags&15990)!==0,o.subtreeFlags&15990||a){a=ze.transition,ze.transition=null;var r=U;U=1;var s=j;j|=4,Fr.current=null,Cp(e,o),Iu(o,e),Qh(ja),oi=!!Ra,ja=Ra=null,e.current=o,Sp(o),Xd(),j=s,U=r,ze.transition=a}else e.current=o;if(zn&&(zn=!1,ft=e,vi=i),a=e.pendingLanes,a===0&&(At=null),th(o.stateNode),xe(e,J()),t!==null)for(n=e.onRecoverableError,o=0;o<t.length;o++)i=t[o],n(i.value,{componentStack:i.stack,digest:i.digest});if(yi)throw yi=!1,e=ir,ir=null,e;return vi&1&&e.tag!==0&&go(),a=e.pendingLanes,a&1?e===ar?Fo++:(Fo=0,ar=e):Fo=0,Bt(),null}function go(){if(ft!==null){var e=mc(vi),t=ze.transition,o=U;try{if(ze.transition=null,U=16>e?16:e,ft===null)var n=!1;else{if(e=ft,ft=null,vi=0,j&6)throw Error(x(331));var i=j;for(j|=4,B=e.current;B!==null;){var a=B,r=a.child;if(B.flags&16){var s=a.deletions;if(s!==null){for(var l=0;l<s.length;l++){var c=s[l];for(B=c;B!==null;){var h=B;switch(h.tag){case 0:case 11:case 15:Ko(8,h,a)}var m=h.child;if(m!==null)m.return=h,B=m;else for(;B!==null;){h=B;var f=h.sibling,v=h.return;if(Pu(h),h===c){B=null;break}if(f!==null){f.return=v,B=f;break}B=v}}}var w=a.alternate;if(w!==null){var S=w.child;if(S!==null){w.child=null;do{var T=S.sibling;S.sibling=null,S=T}while(S!==null)}}B=a}}if(a.subtreeFlags&2064&&r!==null)r.return=a,B=r;else e:for(;B!==null;){if(a=B,a.flags&2048)switch(a.tag){case 0:case 11:case 15:Ko(9,a,a.return)}var d=a.sibling;if(d!==null){d.return=a.return,B=d;break e}B=a.return}}var u=e.current;for(B=u;B!==null;){r=B;var p=r.child;if(r.subtreeFlags&2064&&p!==null)p.return=r,B=p;else e:for(r=u;B!==null;){if(s=B,s.flags&2048)try{switch(s.tag){case 0:case 11:case 15:Gi(9,s)}}catch(b){_(s,s.return,b)}if(s===r){B=null;break e}var y=s.sibling;if(y!==null){y.return=s.return,B=y;break e}B=s.return}}if(j=i,Bt(),$e&&typeof $e.onPostCommitFiberRoot=="function")try{$e.onPostCommitFiberRoot(ki,e)}catch{}n=!0}return n}finally{U=o,ze.transition=t}}return!1}function gl(e,t,o){t=wo(o,t),t=mu(e,t,1),e=St(e,t,1),t=ye(),e!==null&&(fn(e,1,t),xe(e,t))}function _(e,t,o){if(e.tag===3)gl(e,e,o);else for(;t!==null;){if(t.tag===3){gl(t,e,o);break}else if(t.tag===1){var n=t.stateNode;if(typeof t.type.getDerivedStateFromError=="function"||typeof n.componentDidCatch=="function"&&(At===null||!At.has(n))){e=wo(o,e),e=fu(t,e,1),t=St(t,e,1),e=ye(),t!==null&&(fn(t,1,e),xe(t,e));break}}t=t.return}}function Dp(e,t,o){var n=e.pingCache;n!==null&&n.delete(t),t=ye(),e.pingedLanes|=e.suspendedLanes&o,ae===e&&(ce&o)===o&&(ne===4||ne===3&&(ce&130023424)===ce&&500>J()-$r?jt(e,0):_r|=o),xe(e,t)}function zu(e,t){t===0&&(e.mode&1?(t=Pn,Pn<<=1,!(Pn&130023424)&&(Pn=4194304)):t=1);var o=ye();e=rt(e,t),e!==null&&(fn(e,t,o),xe(e,o))}function Ip(e){var t=e.memoizedState,o=0;t!==null&&(o=t.retryLane),zu(e,o)}function Ep(e,t){var o=0;switch(e.tag){case 13:var n=e.stateNode,i=e.memoizedState;i!==null&&(o=i.retryLane);break;case 19:n=e.stateNode;break;default:throw Error(x(314))}n!==null&&n.delete(t),zu(e,o)}var Ru;Ru=function(e,t,o){if(e!==null)if(e.memoizedProps!==t.pendingProps||Ae.current)Se=!0;else{if(!(e.lanes&o)&&!(t.flags&128))return Se=!1,yp(e,t,o);Se=!!(e.flags&131072)}else Se=!1,V&&t.flags&1048576&&Nc(t,ci,t.index);switch(t.lanes=0,t.tag){case 2:var n=t.type;Qn(e,t),e=t.pendingProps;var i=fo(t,me.current);po(t,o),i=Wr(null,t,n,e,i,o);var a=Hr();return t.flags|=1,typeof i=="object"&&i!==null&&typeof i.render=="function"&&i.$$typeof===void 0?(t.tag=1,t.memoizedState=null,t.updateQueue=null,ke(n)?(a=!0,si(t)):a=!1,t.memoizedState=i.state!==null&&i.state!==void 0?i.state:null,jr(t),i.updater=Bi,t.stateNode=i,i._reactInternals=t,Qa(t,n,e,o),t=$a(null,t,n,!0,a,o)):(t.tag=0,V&&a&&Br(t),fe(null,t,i,o),t=t.child),t;case 16:n=t.elementType;e:{switch(Qn(e,t),e=t.pendingProps,i=n._init,n=i(n._payload),t.type=n,i=t.tag=Gp(n),e=Ne(n,e),i){case 0:t=_a(null,t,n,e,o);break e;case 1:t=il(null,t,n,e,o);break e;case 11:t=ol(null,t,n,e,o);break e;case 14:t=nl(null,t,n,Ne(n.type,e),o);break e}throw Error(x(306,n,""))}return t;case 0:return n=t.type,i=t.pendingProps,i=t.elementType===n?i:Ne(n,i),_a(e,t,n,i,o);case 1:return n=t.type,i=t.pendingProps,i=t.elementType===n?i:Ne(n,i),il(e,t,n,i,o);case 3:e:{if(wu(t),e===null)throw Error(x(387));n=t.pendingProps,a=t.memoizedState,i=a.element,Qc(e,t),hi(t,n,null,o);var r=t.memoizedState;if(n=r.element,a.isDehydrated)if(a={element:n,isDehydrated:!1,cache:r.cache,pendingSuspenseBoundaries:r.pendingSuspenseBoundaries,transitions:r.transitions},t.updateQueue.baseState=a,t.memoizedState=a,t.flags&256){i=wo(Error(x(423)),t),t=al(e,t,n,o,i);break e}else if(n!==i){i=wo(Error(x(424)),t),t=al(e,t,n,o,i);break e}else for(De=Ct(t.stateNode.containerInfo.firstChild),Ie=t,V=!0,We=null,o=Vc(t,null,n,o),t.child=o;o;)o.flags=o.flags&-3|4096,o=o.sibling;else{if(yo(),n===i){t=st(e,t,o);break e}fe(e,t,n,o)}t=t.child}return t;case 5:return Fc(t),e===null&&Ha(t),n=t.type,i=t.pendingProps,a=e!==null?e.memoizedProps:null,r=i.children,Ua(n,i)?r=null:a!==null&&Ua(n,a)&&(t.flags|=32),bu(e,t),fe(e,t,r,o),t.child;case 6:return e===null&&Ha(t),null;case 13:return Cu(e,t,o);case 4:return Ur(t,t.stateNode.containerInfo),n=t.pendingProps,e===null?t.child=vo(t,null,n,o):fe(e,t,n,o),t.child;case 11:return n=t.type,i=t.pendingProps,i=t.elementType===n?i:Ne(n,i),ol(e,t,n,i,o);case 7:return fe(e,t,t.pendingProps,o),t.child;case 8:return fe(e,t,t.pendingProps.children,o),t.child;case 12:return fe(e,t,t.pendingProps.children,o),t.child;case 10:e:{if(n=t.type._context,i=t.pendingProps,a=t.memoizedProps,r=i.value,N(ui,n._currentValue),n._currentValue=r,a!==null)if(Ke(a.value,r)){if(a.children===i.children&&!Ae.current){t=st(e,t,o);break e}}else for(a=t.child,a!==null&&(a.return=t);a!==null;){var s=a.dependencies;if(s!==null){r=a.child;for(var l=s.firstContext;l!==null;){if(l.context===n){if(a.tag===1){l=nt(-1,o&-o),l.tag=2;var c=a.updateQueue;if(c!==null){c=c.shared;var h=c.pending;h===null?l.next=l:(l.next=h.next,h.next=l),c.pending=l}}a.lanes|=o,l=a.alternate,l!==null&&(l.lanes|=o),Va(a.return,o,t),s.lanes|=o;break}l=l.next}}else if(a.tag===10)r=a.type===t.type?null:a.child;else if(a.tag===18){if(r=a.return,r===null)throw Error(x(341));r.lanes|=o,s=r.alternate,s!==null&&(s.lanes|=o),Va(r,o,t),r=a.sibling}else r=a.child;if(r!==null)r.return=a;else for(r=a;r!==null;){if(r===t){r=null;break}if(a=r.sibling,a!==null){a.return=r.return,r=a;break}r=r.return}a=r}fe(e,t,i.children,o),t=t.child}return t;case 9:return i=t.type,n=t.pendingProps.children,po(t,o),i=Re(i),n=n(i),t.flags|=1,fe(e,t,n,o),t.child;case 14:return n=t.type,i=Ne(n,t.pendingProps),i=Ne(n.type,i),nl(e,t,n,i,o);case 15:return yu(e,t,t.type,t.pendingProps,o);case 17:return n=t.type,i=t.pendingProps,i=t.elementType===n?i:Ne(n,i),Qn(e,t),t.tag=1,ke(n)?(e=!0,si(t)):e=!1,po(t,o),gu(t,n,i),Qa(t,n,i,o),$a(null,t,n,!0,e,o);case 19:return Su(e,t,o);case 22:return vu(e,t,o)}throw Error(x(156,t.tag))};function ju(e,t){return dc(e,t)}function Bp(e,t,o,n){this.tag=e,this.key=o,this.sibling=this.child=this.return=this.stateNode=this.type=this.elementType=null,this.index=0,this.ref=null,this.pendingProps=t,this.dependencies=this.memoizedState=this.updateQueue=this.memoizedProps=null,this.mode=n,this.subtreeFlags=this.flags=0,this.deletions=null,this.childLanes=this.lanes=0,this.alternate=null}function Le(e,t,o,n){return new Bp(e,t,o,n)}function es(e){return e=e.prototype,!(!e||!e.isReactComponent)}function Gp(e){if(typeof e=="function")return es(e)?1:0;if(e!=null){if(e=e.$$typeof,e===br)return 11;if(e===wr)return 14}return 2}function xt(e,t){var o=e.alternate;return o===null?(o=Le(e.tag,t,e.key,e.mode),o.elementType=e.elementType,o.type=e.type,o.stateNode=e.stateNode,o.alternate=e,e.alternate=o):(o.pendingProps=t,o.type=e.type,o.flags=0,o.subtreeFlags=0,o.deletions=null),o.flags=e.flags&14680064,o.childLanes=e.childLanes,o.lanes=e.lanes,o.child=e.child,o.memoizedProps=e.memoizedProps,o.memoizedState=e.memoizedState,o.updateQueue=e.updateQueue,t=e.dependencies,o.dependencies=t===null?null:{lanes:t.lanes,firstContext:t.firstContext},o.sibling=e.sibling,o.index=e.index,o.ref=e.ref,o}function $n(e,t,o,n,i,a){var r=2;if(n=e,typeof e=="function")es(e)&&(r=1);else if(typeof e=="string")r=5;else e:switch(e){case Jt:return Ut(o.children,i,a,t);case vr:r=8,i|=8;break;case ya:return e=Le(12,o,t,i|2),e.elementType=ya,e.lanes=a,e;case va:return e=Le(13,o,t,i),e.elementType=va,e.lanes=a,e;case ba:return e=Le(19,o,t,i),e.elementType=ba,e.lanes=a,e;case Fl:return Mi(o,i,a,t);default:if(typeof e=="object"&&e!==null)switch(e.$$typeof){case Kl:r=10;break e;case Ql:r=9;break e;case br:r=11;break e;case wr:r=14;break e;case dt:r=16,n=null;break e}throw Error(x(130,e==null?e:typeof e,""))}return t=Le(r,o,t,i),t.elementType=e,t.type=n,t.lanes=a,t}function Ut(e,t,o,n){return e=Le(7,e,n,t),e.lanes=o,e}function Mi(e,t,o,n){return e=Le(22,e,n,t),e.elementType=Fl,e.lanes=o,e.stateNode={isHidden:!1},e}function ha(e,t,o){return e=Le(6,e,null,t),e.lanes=o,e}function pa(e,t,o){return t=Le(4,e.children!==null?e.children:[],e.key,t),t.lanes=o,t.stateNode={containerInfo:e.containerInfo,pendingChildren:null,implementation:e.implementation},t}function qp(e,t,o,n,i){this.tag=t,this.containerInfo=e,this.finishedWork=this.pingCache=this.current=this.pendingChildren=null,this.timeoutHandle=-1,this.callbackNode=this.pendingContext=this.context=null,this.callbackPriority=0,this.eventTimes=Qi(0),this.expirationTimes=Qi(-1),this.entangledLanes=this.finishedLanes=this.mutableReadLanes=this.expiredLanes=this.pingedLanes=this.suspendedLanes=this.pendingLanes=0,this.entanglements=Qi(0),this.identifierPrefix=n,this.onRecoverableError=i,this.mutableSourceEagerHydrationData=null}function ts(e,t,o,n,i,a,r,s,l){return e=new qp(e,t,o,s,l),t===1?(t=1,a===!0&&(t|=8)):t=0,a=Le(3,null,null,t),e.current=a,a.stateNode=e,a.memoizedState={element:n,isDehydrated:o,cache:null,transitions:null,pendingSuspenseBoundaries:null},jr(a),e}function Mp(e,t,o){var n=3<arguments.length&&arguments[3]!==void 0?arguments[3]:null;return{$$typeof:$t,key:n==null?null:""+n,children:e,containerInfo:t,implementation:o}}function Uu(e){if(!e)return Dt;e=e._reactInternals;e:{if(Kt(e)!==e||e.tag!==1)throw Error(x(170));var t=e;do{switch(t.tag){case 3:t=t.stateNode.context;break e;case 1:if(ke(t.type)){t=t.stateNode.__reactInternalMemoizedMergedChildContext;break e}}t=t.return}while(t!==null);throw Error(x(171))}if(e.tag===1){var o=e.type;if(ke(o))return Uc(e,o,t)}return t}function Yu(e,t,o,n,i,a,r,s,l){return e=ts(o,n,!0,e,i,a,r,s,l),e.context=Uu(null),o=e.current,n=ye(),i=kt(o),a=nt(n,i),a.callback=t??null,St(o,a,i),e.current.lanes=i,fn(e,i,n),xe(e,n),e}function Li(e,t,o,n){var i=t.current,a=ye(),r=kt(i);return o=Uu(o),t.context===null?t.context=o:t.pendingContext=o,t=nt(a,r),t.payload={element:e},n=n===void 0?null:n,n!==null&&(t.callback=n),e=St(i,t,r),e!==null&&(Ve(e,i,r,a),Hn(e,i,r)),r}function wi(e){if(e=e.current,!e.child)return null;switch(e.child.tag){case 5:return e.child.stateNode;default:return e.child.stateNode}}function ml(e,t){if(e=e.memoizedState,e!==null&&e.dehydrated!==null){var o=e.retryLane;e.retryLane=o!==0&&o<t?o:t}}function os(e,t){ml(e,t),(e=e.alternate)&&ml(e,t)}function Lp(){return null}var Nu=typeof reportError=="function"?reportError:function(e){console.error(e)};function ns(e){this._internalRoot=e}zi.prototype.render=ns.prototype.render=function(e){var t=this._internalRoot;if(t===null)throw Error(x(409));Li(e,t,null,null)};zi.prototype.unmount=ns.prototype.unmount=function(){var e=this._internalRoot;if(e!==null){this._internalRoot=null;var t=e.containerInfo;Ht(function(){Li(null,e,null,null)}),t[at]=null}};function zi(e){this._internalRoot=e}zi.prototype.unstable_scheduleHydration=function(e){if(e){var t=vc();e={blockedOn:null,target:e,priority:t};for(var o=0;o<pt.length&&t!==0&&t<pt[o].priority;o++);pt.splice(o,0,e),o===0&&wc(e)}};function is(e){return!(!e||e.nodeType!==1&&e.nodeType!==9&&e.nodeType!==11)}function Ri(e){return!(!e||e.nodeType!==1&&e.nodeType!==9&&e.nodeType!==11&&(e.nodeType!==8||e.nodeValue!==" react-mount-point-unstable "))}function fl(){}function zp(e,t,o,n,i){if(i){if(typeof n=="function"){var a=n;n=function(){var c=wi(r);a.call(c)}}var r=Yu(t,n,e,0,null,!1,!1,"",fl);return e._reactRootContainer=r,e[at]=r.current,nn(e.nodeType===8?e.parentNode:e),Ht(),r}for(;i=e.lastChild;)e.removeChild(i);if(typeof n=="function"){var s=n;n=function(){var c=wi(l);s.call(c)}}var l=ts(e,0,!1,null,null,!1,!1,"",fl);return e._reactRootContainer=l,e[at]=l.current,nn(e.nodeType===8?e.parentNode:e),Ht(function(){Li(t,l,o,n)}),l}function ji(e,t,o,n,i){var a=o._reactRootContainer;if(a){var r=a;if(typeof i=="function"){var s=i;i=function(){var l=wi(r);s.call(l)}}Li(t,r,e,i)}else r=zp(o,t,e,i,n);return wi(r)}fc=function(e){switch(e.tag){case 3:var t=e.stateNode;if(t.current.memoizedState.isDehydrated){var o=jo(t.pendingLanes);o!==0&&(Ar(t,o|1),xe(t,J()),!(j&6)&&(Co=J()+500,Bt()))}break;case 13:Ht(function(){var n=rt(e,1);if(n!==null){var i=ye();Ve(n,e,1,i)}}),os(e,1)}};kr=function(e){if(e.tag===13){var t=rt(e,134217728);if(t!==null){var o=ye();Ve(t,e,134217728,o)}os(e,134217728)}};yc=function(e){if(e.tag===13){var t=kt(e),o=rt(e,t);if(o!==null){var n=ye();Ve(o,e,t,n)}os(e,t)}};vc=function(){return U};bc=function(e,t){var o=U;try{return U=e,t()}finally{U=o}};Ia=function(e,t,o){switch(t){case"input":if(Sa(e,o),t=o.name,o.type==="radio"&&t!=null){for(o=e;o.parentNode;)o=o.parentNode;for(o=o.querySelectorAll("input[name="+JSON.stringify(""+t)+'][type="radio"]'),t=0;t<o.length;t++){var n=o[t];if(n!==e&&n.form===e.form){var i=Di(n);if(!i)throw Error(x(90));$l(n),Sa(n,i)}}}break;case"textarea":Xl(e,o);break;case"select":t=o.value,t!=null&&lo(e,!!o.multiple,t,!1)}};ac=Jr;rc=Ht;var Rp={usingClientEntryPoint:!1,Events:[vn,to,Di,nc,ic,Jr]},Lo={findFiberByHostInstance:Lt,bundleType:0,version:"18.3.1",rendererPackageName:"react-dom"},jp={bundleType:Lo.bundleType,version:Lo.version,rendererPackageName:Lo.rendererPackageName,rendererConfig:Lo.rendererConfig,overrideHookState:null,overrideHookStateDeletePath:null,overrideHookStateRenamePath:null,overrideProps:null,overridePropsDeletePath:null,overridePropsRenamePath:null,setErrorHandler:null,setSuspenseHandler:null,scheduleUpdate:null,currentDispatcherRef:lt.ReactCurrentDispatcher,findHostInstanceByFiber:function(e){return e=cc(e),e===null?null:e.stateNode},findFiberByHostInstance:Lo.findFiberByHostInstance||Lp,findHostInstancesForRefresh:null,scheduleRefresh:null,scheduleRoot:null,setRefreshHandler:null,getCurrentFiber:null,reconcilerVersion:"18.3.1-next-f1338f8080-20240426"};if(typeof __REACT_DEVTOOLS_GLOBAL_HOOK__<"u"){var Rn=__REACT_DEVTOOLS_GLOBAL_HOOK__;if(!Rn.isDisabled&&Rn.supportsFiber)try{ki=Rn.inject(jp),$e=Rn}catch{}}Be.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=Rp;Be.createPortal=function(e,t){var o=2<arguments.length&&arguments[2]!==void 0?arguments[2]:null;if(!is(t))throw Error(x(200));return Mp(e,t,null,o)};Be.createRoot=function(e,t){if(!is(e))throw Error(x(299));var o=!1,n="",i=Nu;return t!=null&&(t.unstable_strictMode===!0&&(o=!0),t.identifierPrefix!==void 0&&(n=t.identifierPrefix),t.onRecoverableError!==void 0&&(i=t.onRecoverableError)),t=ts(e,1,!1,null,null,o,!1,n,i),e[at]=t.current,nn(e.nodeType===8?e.parentNode:e),new ns(t)};Be.findDOMNode=function(e){if(e==null)return null;if(e.nodeType===1)return e;var t=e._reactInternals;if(t===void 0)throw typeof e.render=="function"?Error(x(188)):(e=Object.keys(e).join(","),Error(x(268,e)));return e=cc(t),e=e===null?null:e.stateNode,e};Be.flushSync=function(e){return Ht(e)};Be.hydrate=function(e,t,o){if(!Ri(t))throw Error(x(200));return ji(null,e,t,!0,o)};Be.hydrateRoot=function(e,t,o){if(!is(e))throw Error(x(405));var n=o!=null&&o.hydratedSources||null,i=!1,a="",r=Nu;if(o!=null&&(o.unstable_strictMode===!0&&(i=!0),o.identifierPrefix!==void 0&&(a=o.identifierPrefix),o.onRecoverableError!==void 0&&(r=o.onRecoverableError)),t=Yu(t,null,e,1,o??null,i,!1,a,r),e[at]=t.current,nn(e),n)for(e=0;e<n.length;e++)o=n[e],i=o._getVersion,i=i(o._source),t.mutableSourceEagerHydrationData==null?t.mutableSourceEagerHydrationData=[o,i]:t.mutableSourceEagerHydrationData.push(o,i);return new zi(t)};Be.render=function(e,t,o){if(!Ri(t))throw Error(x(200));return ji(null,e,t,!1,o)};Be.unmountComponentAtNode=function(e){if(!Ri(e))throw Error(x(40));return e._reactRootContainer?(Ht(function(){ji(null,null,e,!1,function(){e._reactRootContainer=null,e[at]=null})}),!0):!1};Be.unstable_batchedUpdates=Jr;Be.unstable_renderSubtreeIntoContainer=function(e,t,o,n){if(!Ri(o))throw Error(x(200));if(e==null||e._reactInternals===void 0)throw Error(x(38));return ji(e,t,o,!1,n)};Be.version="18.3.1-next-f1338f8080-20240426";function Ou(){if(!(typeof __REACT_DEVTOOLS_GLOBAL_HOOK__>"u"||typeof __REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE!="function"))try{__REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE(Ou)}catch(e){console.error(e)}}Ou(),Ol.exports=Be;var Up=Ol.exports,yl=Up;ma.createRoot=yl.createRoot,ma.hydrateRoot=yl.hydrateRoot;/**
 * @remix-run/router v1.23.0
 *
 * Copyright (c) Remix Software Inc.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE.md file in the root directory of this source tree.
 *
 * @license MIT
 */function pn(){return pn=Object.assign?Object.assign.bind():function(e){for(var t=1;t<arguments.length;t++){var o=arguments[t];for(var n in o)Object.prototype.hasOwnProperty.call(o,n)&&(e[n]=o[n])}return e},pn.apply(this,arguments)}var yt;(function(e){e.Pop="POP",e.Push="PUSH",e.Replace="REPLACE"})(yt||(yt={}));const vl="popstate";function Yp(e){e===void 0&&(e={});function t(n,i){let{pathname:a,search:r,hash:s}=n.location;return lr("",{pathname:a,search:r,hash:s},i.state&&i.state.usr||null,i.state&&i.state.key||"default")}function o(n,i){return typeof i=="string"?i:Ci(i)}return Op(t,o,null,e)}function X(e,t){if(e===!1||e===null||typeof e>"u")throw new Error(t)}function Wu(e,t){if(!e){typeof console<"u"&&console.warn(t);try{throw new Error(t)}catch{}}}function Np(){return Math.random().toString(36).substr(2,8)}function bl(e,t){return{usr:e.state,key:e.key,idx:t}}function lr(e,t,o,n){return o===void 0&&(o=null),pn({pathname:typeof e=="string"?e:e.pathname,search:"",hash:""},typeof t=="string"?xo(t):t,{state:o,key:t&&t.key||n||Np()})}function Ci(e){let{pathname:t="/",search:o="",hash:n=""}=e;return o&&o!=="?"&&(t+=o.charAt(0)==="?"?o:"?"+o),n&&n!=="#"&&(t+=n.charAt(0)==="#"?n:"#"+n),t}function xo(e){let t={};if(e){let o=e.indexOf("#");o>=0&&(t.hash=e.substr(o),e=e.substr(0,o));let n=e.indexOf("?");n>=0&&(t.search=e.substr(n),e=e.substr(0,n)),e&&(t.pathname=e)}return t}function Op(e,t,o,n){n===void 0&&(n={});let{window:i=document.defaultView,v5Compat:a=!1}=n,r=i.history,s=yt.Pop,l=null,c=h();c==null&&(c=0,r.replaceState(pn({},r.state,{idx:c}),""));function h(){return(r.state||{idx:null}).idx}function m(){s=yt.Pop;let T=h(),d=T==null?null:T-c;c=T,l&&l({action:s,location:S.location,delta:d})}function f(T,d){s=yt.Push;let u=lr(S.location,T,d);c=h()+1;let p=bl(u,c),y=S.createHref(u);try{r.pushState(p,"",y)}catch(b){if(b instanceof DOMException&&b.name==="DataCloneError")throw b;i.location.assign(y)}a&&l&&l({action:s,location:S.location,delta:1})}function v(T,d){s=yt.Replace;let u=lr(S.location,T,d);c=h();let p=bl(u,c),y=S.createHref(u);r.replaceState(p,"",y),a&&l&&l({action:s,location:S.location,delta:0})}function w(T){let d=i.location.origin!=="null"?i.location.origin:i.location.href,u=typeof T=="string"?T:Ci(T);return u=u.replace(/ $/,"%20"),X(d,"No window.location.(origin|href) available to create URL for href: "+u),new URL(u,d)}let S={get action(){return s},get location(){return e(i,r)},listen(T){if(l)throw new Error("A history only accepts one active listener");return i.addEventListener(vl,m),l=T,()=>{i.removeEventListener(vl,m),l=null}},createHref(T){return t(i,T)},createURL:w,encodeLocation(T){let d=w(T);return{pathname:d.pathname,search:d.search,hash:d.hash}},push:f,replace:v,go(T){return r.go(T)}};return S}var wl;(function(e){e.data="data",e.deferred="deferred",e.redirect="redirect",e.error="error"})(wl||(wl={}));function Wp(e,t,o){return o===void 0&&(o="/"),Hp(e,t,o)}function Hp(e,t,o,n){let i=typeof t=="string"?xo(t):t,a=as(i.pathname||"/",o);if(a==null)return null;let r=Hu(e);Vp(r);let s=null;for(let l=0;s==null&&l<r.length;++l){let c=ng(a);s=eg(r[l],c)}return s}function Hu(e,t,o,n){t===void 0&&(t=[]),o===void 0&&(o=[]),n===void 0&&(n="");let i=(a,r,s)=>{let l={relativePath:s===void 0?a.path||"":s,caseSensitive:a.caseSensitive===!0,childrenIndex:r,route:a};l.relativePath.startsWith("/")&&(X(l.relativePath.startsWith(n),'Absolute route path "'+l.relativePath+'" nested under path '+('"'+n+'" is not valid. An absolute child route path ')+"must start with the combined path of all its parent routes."),l.relativePath=l.relativePath.slice(n.length));let c=Pt([n,l.relativePath]),h=o.concat(l);a.children&&a.children.length>0&&(X(a.index!==!0,"Index routes must not have child routes. Please remove "+('all child routes from route path "'+c+'".')),Hu(a.children,t,h,c)),!(a.path==null&&!a.index)&&t.push({path:c,score:Xp(c,a.index),routesMeta:h})};return e.forEach((a,r)=>{var s;if(a.path===""||!((s=a.path)!=null&&s.includes("?")))i(a,r);else for(let l of Vu(a.path))i(a,r,l)}),t}function Vu(e){let t=e.split("/");if(t.length===0)return[];let[o,...n]=t,i=o.endsWith("?"),a=o.replace(/\?$/,"");if(n.length===0)return i?[a,""]:[a];let r=Vu(n.join("/")),s=[];return s.push(...r.map(l=>l===""?a:[a,l].join("/"))),i&&s.push(...r),s.map(l=>e.startsWith("/")&&l===""?"/":l)}function Vp(e){e.sort((t,o)=>t.score!==o.score?o.score-t.score:Zp(t.routesMeta.map(n=>n.childrenIndex),o.routesMeta.map(n=>n.childrenIndex)))}const Kp=/^:[\w-]+$/,Qp=3,Fp=2,_p=1,$p=10,Jp=-2,Cl=e=>e==="*";function Xp(e,t){let o=e.split("/"),n=o.length;return o.some(Cl)&&(n+=Jp),t&&(n+=Fp),o.filter(i=>!Cl(i)).reduce((i,a)=>i+(Kp.test(a)?Qp:a===""?_p:$p),n)}function Zp(e,t){return e.length===t.length&&e.slice(0,-1).every((n,i)=>n===t[i])?e[e.length-1]-t[t.length-1]:0}function eg(e,t,o){let{routesMeta:n}=e,i={},a="/",r=[];for(let s=0;s<n.length;++s){let l=n[s],c=s===n.length-1,h=a==="/"?t:t.slice(a.length)||"/",m=tg({path:l.relativePath,caseSensitive:l.caseSensitive,end:c},h),f=l.route;if(!m)return null;Object.assign(i,m.params),r.push({params:i,pathname:Pt([a,m.pathname]),pathnameBase:sg(Pt([a,m.pathnameBase])),route:f}),m.pathnameBase!=="/"&&(a=Pt([a,m.pathnameBase]))}return r}function tg(e,t){typeof e=="string"&&(e={path:e,caseSensitive:!1,end:!0});let[o,n]=og(e.path,e.caseSensitive,e.end),i=t.match(o);if(!i)return null;let a=i[0],r=a.replace(/(.)\/+$/,"$1"),s=i.slice(1);return{params:n.reduce((c,h,m)=>{let{paramName:f,isOptional:v}=h;if(f==="*"){let S=s[m]||"";r=a.slice(0,a.length-S.length).replace(/(.)\/+$/,"$1")}const w=s[m];return v&&!w?c[f]=void 0:c[f]=(w||"").replace(/%2F/g,"/"),c},{}),pathname:a,pathnameBase:r,pattern:e}}function og(e,t,o){t===void 0&&(t=!1),o===void 0&&(o=!0),Wu(e==="*"||!e.endsWith("*")||e.endsWith("/*"),'Route path "'+e+'" will be treated as if it were '+('"'+e.replace(/\*$/,"/*")+'" because the `*` character must ')+"always follow a `/` in the pattern. To get rid of this warning, "+('please change the route path to "'+e.replace(/\*$/,"/*")+'".'));let n=[],i="^"+e.replace(/\/*\*?$/,"").replace(/^\/*/,"/").replace(/[\\.*+^${}|()[\]]/g,"\\$&").replace(/\/:([\w-]+)(\?)?/g,(r,s,l)=>(n.push({paramName:s,isOptional:l!=null}),l?"/?([^\\/]+)?":"/([^\\/]+)"));return e.endsWith("*")?(n.push({paramName:"*"}),i+=e==="*"||e==="/*"?"(.*)$":"(?:\\/(.+)|\\/*)$"):o?i+="\\/*$":e!==""&&e!=="/"&&(i+="(?:(?=\\/|$))"),[new RegExp(i,t?void 0:"i"),n]}function ng(e){try{return e.split("/").map(t=>decodeURIComponent(t).replace(/\//g,"%2F")).join("/")}catch(t){return Wu(!1,'The URL path "'+e+'" could not be decoded because it is is a malformed URL segment. This is probably due to a bad percent '+("encoding ("+t+").")),e}}function as(e,t){if(t==="/")return e;if(!e.toLowerCase().startsWith(t.toLowerCase()))return null;let o=t.endsWith("/")?t.length-1:t.length,n=e.charAt(o);return n&&n!=="/"?null:e.slice(o)||"/"}function ig(e,t){t===void 0&&(t="/");let{pathname:o,search:n="",hash:i=""}=typeof e=="string"?xo(e):e;return{pathname:o?o.startsWith("/")?o:ag(o,t):t,search:lg(n),hash:cg(i)}}function ag(e,t){let o=t.replace(/\/+$/,"").split("/");return e.split("/").forEach(i=>{i===".."?o.length>1&&o.pop():i!=="."&&o.push(i)}),o.length>1?o.join("/"):"/"}function ga(e,t,o,n){return"Cannot include a '"+e+"' character in a manually specified "+("`to."+t+"` field ["+JSON.stringify(n)+"].  Please separate it out to the ")+("`to."+o+"` field. Alternatively you may provide the full path as ")+'a string in <Link to="..."> and the router will parse it for you.'}function rg(e){return e.filter((t,o)=>o===0||t.route.path&&t.route.path.length>0)}function rs(e,t){let o=rg(e);return t?o.map((n,i)=>i===o.length-1?n.pathname:n.pathnameBase):o.map(n=>n.pathnameBase)}function ss(e,t,o,n){n===void 0&&(n=!1);let i;typeof e=="string"?i=xo(e):(i=pn({},e),X(!i.pathname||!i.pathname.includes("?"),ga("?","pathname","search",i)),X(!i.pathname||!i.pathname.includes("#"),ga("#","pathname","hash",i)),X(!i.search||!i.search.includes("#"),ga("#","search","hash",i)));let a=e===""||i.pathname==="",r=a?"/":i.pathname,s;if(r==null)s=o;else{let m=t.length-1;if(!n&&r.startsWith("..")){let f=r.split("/");for(;f[0]==="..";)f.shift(),m-=1;i.pathname=f.join("/")}s=m>=0?t[m]:"/"}let l=ig(i,s),c=r&&r!=="/"&&r.endsWith("/"),h=(a||r===".")&&o.endsWith("/");return!l.pathname.endsWith("/")&&(c||h)&&(l.pathname+="/"),l}const Pt=e=>e.join("/").replace(/\/\/+/g,"/"),sg=e=>e.replace(/\/+$/,"").replace(/^\/*/,"/"),lg=e=>!e||e==="?"?"":e.startsWith("?")?e:"?"+e,cg=e=>!e||e==="#"?"":e.startsWith("#")?e:"#"+e;function ug(e){return e!=null&&typeof e.status=="number"&&typeof e.statusText=="string"&&typeof e.internal=="boolean"&&"data"in e}const Ku=["post","put","patch","delete"];new Set(Ku);const dg=["get",...Ku];new Set(dg);/**
 * React Router v6.30.0
 *
 * Copyright (c) Remix Software Inc.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE.md file in the root directory of this source tree.
 *
 * @license MIT
 */function gn(){return gn=Object.assign?Object.assign.bind():function(e){for(var t=1;t<arguments.length;t++){var o=arguments[t];for(var n in o)Object.prototype.hasOwnProperty.call(o,n)&&(e[n]=o[n])}return e},gn.apply(this,arguments)}const ls=A.createContext(null),hg=A.createContext(null),Gt=A.createContext(null),Ui=A.createContext(null),ct=A.createContext({outlet:null,matches:[],isDataRoute:!1}),Qu=A.createContext(null);function pg(e,t){let{relative:o}=t===void 0?{}:t;Po()||X(!1);let{basename:n,navigator:i}=A.useContext(Gt),{hash:a,pathname:r,search:s}=_u(e,{relative:o}),l=r;return n!=="/"&&(l=r==="/"?n:Pt([n,r])),i.createHref({pathname:l,search:s,hash:a})}function Po(){return A.useContext(Ui)!=null}function To(){return Po()||X(!1),A.useContext(Ui).location}function Fu(e){A.useContext(Gt).static||A.useLayoutEffect(e)}function Yi(){let{isDataRoute:e}=A.useContext(ct);return e?Tg():gg()}function gg(){Po()||X(!1);let e=A.useContext(ls),{basename:t,future:o,navigator:n}=A.useContext(Gt),{matches:i}=A.useContext(ct),{pathname:a}=To(),r=JSON.stringify(rs(i,o.v7_relativeSplatPath)),s=A.useRef(!1);return Fu(()=>{s.current=!0}),A.useCallback(function(c,h){if(h===void 0&&(h={}),!s.current)return;if(typeof c=="number"){n.go(c);return}let m=ss(c,JSON.parse(r),a,h.relative==="path");e==null&&t!=="/"&&(m.pathname=m.pathname==="/"?t:Pt([t,m.pathname])),(h.replace?n.replace:n.push)(m,h.state,h)},[t,n,r,a,e])}function mg(){let{matches:e}=A.useContext(ct),t=e[e.length-1];return t?t.params:{}}function _u(e,t){let{relative:o}=t===void 0?{}:t,{future:n}=A.useContext(Gt),{matches:i}=A.useContext(ct),{pathname:a}=To(),r=JSON.stringify(rs(i,n.v7_relativeSplatPath));return A.useMemo(()=>ss(e,JSON.parse(r),a,o==="path"),[e,r,a,o])}function fg(e,t){return yg(e,t)}function yg(e,t,o,n){Po()||X(!1);let{navigator:i,static:a}=A.useContext(Gt),{matches:r}=A.useContext(ct),s=r[r.length-1],l=s?s.params:{};s&&s.pathname;let c=s?s.pathnameBase:"/";s&&s.route;let h=To(),m;if(t){var f;let d=typeof t=="string"?xo(t):t;c==="/"||(f=d.pathname)!=null&&f.startsWith(c)||X(!1),m=d}else m=h;let v=m.pathname||"/",w=v;if(c!=="/"){let d=c.replace(/^\//,"").split("/");w="/"+v.replace(/^\//,"").split("/").slice(d.length).join("/")}let S=Wp(e,{pathname:w}),T=Sg(S&&S.map(d=>Object.assign({},d,{params:Object.assign({},l,d.params),pathname:Pt([c,i.encodeLocation?i.encodeLocation(d.pathname).pathname:d.pathname]),pathnameBase:d.pathnameBase==="/"?c:Pt([c,i.encodeLocation?i.encodeLocation(d.pathnameBase).pathname:d.pathnameBase])})),r,o,n);return t&&T?A.createElement(Ui.Provider,{value:{location:gn({pathname:"/",search:"",hash:"",state:null,key:"default"},m),navigationType:yt.Pop}},T):T}function vg(){let e=Pg(),t=ug(e)?e.status+" "+e.statusText:e instanceof Error?e.message:JSON.stringify(e),o=e instanceof Error?e.stack:null,i={padding:"0.5rem",backgroundColor:"rgba(200,200,200, 0.5)"};return A.createElement(A.Fragment,null,A.createElement("h2",null,"Unexpected Application Error!"),A.createElement("h3",{style:{fontStyle:"italic"}},t),o?A.createElement("pre",{style:i},o):null,null)}const bg=A.createElement(vg,null);class wg extends A.Component{constructor(t){super(t),this.state={location:t.location,revalidation:t.revalidation,error:t.error}}static getDerivedStateFromError(t){return{error:t}}static getDerivedStateFromProps(t,o){return o.location!==t.location||o.revalidation!=="idle"&&t.revalidation==="idle"?{error:t.error,location:t.location,revalidation:t.revalidation}:{error:t.error!==void 0?t.error:o.error,location:o.location,revalidation:t.revalidation||o.revalidation}}componentDidCatch(t,o){console.error("React Router caught the following error during render",t,o)}render(){return this.state.error!==void 0?A.createElement(ct.Provider,{value:this.props.routeContext},A.createElement(Qu.Provider,{value:this.state.error,children:this.props.component})):this.props.children}}function Cg(e){let{routeContext:t,match:o,children:n}=e,i=A.useContext(ls);return i&&i.static&&i.staticContext&&(o.route.errorElement||o.route.ErrorBoundary)&&(i.staticContext._deepestRenderedBoundaryId=o.route.id),A.createElement(ct.Provider,{value:t},n)}function Sg(e,t,o,n){var i;if(t===void 0&&(t=[]),o===void 0&&(o=null),n===void 0&&(n=null),e==null){var a;if(!o)return null;if(o.errors)e=o.matches;else if((a=n)!=null&&a.v7_partialHydration&&t.length===0&&!o.initialized&&o.matches.length>0)e=o.matches;else return null}let r=e,s=(i=o)==null?void 0:i.errors;if(s!=null){let h=r.findIndex(m=>m.route.id&&(s==null?void 0:s[m.route.id])!==void 0);h>=0||X(!1),r=r.slice(0,Math.min(r.length,h+1))}let l=!1,c=-1;if(o&&n&&n.v7_partialHydration)for(let h=0;h<r.length;h++){let m=r[h];if((m.route.HydrateFallback||m.route.hydrateFallbackElement)&&(c=h),m.route.id){let{loaderData:f,errors:v}=o,w=m.route.loader&&f[m.route.id]===void 0&&(!v||v[m.route.id]===void 0);if(m.route.lazy||w){l=!0,c>=0?r=r.slice(0,c+1):r=[r[0]];break}}}return r.reduceRight((h,m,f)=>{let v,w=!1,S=null,T=null;o&&(v=s&&m.route.id?s[m.route.id]:void 0,S=m.route.errorElement||bg,l&&(c<0&&f===0?(Dg("route-fallback"),w=!0,T=null):c===f&&(w=!0,T=m.route.hydrateFallbackElement||null)));let d=t.concat(r.slice(0,f+1)),u=()=>{let p;return v?p=S:w?p=T:m.route.Component?p=A.createElement(m.route.Component,null):m.route.element?p=m.route.element:p=h,A.createElement(Cg,{match:m,routeContext:{outlet:h,matches:d,isDataRoute:o!=null},children:p})};return o&&(m.route.ErrorBoundary||m.route.errorElement||f===0)?A.createElement(wg,{location:o.location,revalidation:o.revalidation,component:S,error:v,children:u(),routeContext:{outlet:null,matches:d,isDataRoute:!0}}):u()},null)}var $u=function(e){return e.UseBlocker="useBlocker",e.UseRevalidator="useRevalidator",e.UseNavigateStable="useNavigate",e}($u||{}),Ju=function(e){return e.UseBlocker="useBlocker",e.UseLoaderData="useLoaderData",e.UseActionData="useActionData",e.UseRouteError="useRouteError",e.UseNavigation="useNavigation",e.UseRouteLoaderData="useRouteLoaderData",e.UseMatches="useMatches",e.UseRevalidator="useRevalidator",e.UseNavigateStable="useNavigate",e.UseRouteId="useRouteId",e}(Ju||{});function Ag(e){let t=A.useContext(ls);return t||X(!1),t}function kg(e){let t=A.useContext(hg);return t||X(!1),t}function xg(e){let t=A.useContext(ct);return t||X(!1),t}function Xu(e){let t=xg(),o=t.matches[t.matches.length-1];return o.route.id||X(!1),o.route.id}function Pg(){var e;let t=A.useContext(Qu),o=kg(),n=Xu();return t!==void 0?t:(e=o.errors)==null?void 0:e[n]}function Tg(){let{router:e}=Ag($u.UseNavigateStable),t=Xu(Ju.UseNavigateStable),o=A.useRef(!1);return Fu(()=>{o.current=!0}),A.useCallback(function(i,a){a===void 0&&(a={}),o.current&&(typeof i=="number"?e.navigate(i):e.navigate(i,gn({fromRouteId:t},a)))},[e,t])}const Sl={};function Dg(e,t,o){Sl[e]||(Sl[e]=!0)}function Ig(e,t){e==null||e.v7_startTransition,e==null||e.v7_relativeSplatPath}function Zu(e){let{to:t,replace:o,state:n,relative:i}=e;Po()||X(!1);let{future:a,static:r}=A.useContext(Gt),{matches:s}=A.useContext(ct),{pathname:l}=To(),c=Yi(),h=ss(t,rs(s,a.v7_relativeSplatPath),l,i==="path"),m=JSON.stringify(h);return A.useEffect(()=>c(JSON.parse(m),{replace:o,state:n,relative:i}),[c,m,i,o,n]),null}function _t(e){X(!1)}function Eg(e){let{basename:t="/",children:o=null,location:n,navigationType:i=yt.Pop,navigator:a,static:r=!1,future:s}=e;Po()&&X(!1);let l=t.replace(/^\/*/,"/"),c=A.useMemo(()=>({basename:l,navigator:a,static:r,future:gn({v7_relativeSplatPath:!1},s)}),[l,s,a,r]);typeof n=="string"&&(n=xo(n));let{pathname:h="/",search:m="",hash:f="",state:v=null,key:w="default"}=n,S=A.useMemo(()=>{let T=as(h,l);return T==null?null:{location:{pathname:T,search:m,hash:f,state:v,key:w},navigationType:i}},[l,h,m,f,v,w,i]);return S==null?null:A.createElement(Gt.Provider,{value:c},A.createElement(Ui.Provider,{children:o,value:S}))}function Bg(e){let{children:t,location:o}=e;return fg(cr(t),o)}new Promise(()=>{});function cr(e,t){t===void 0&&(t=[]);let o=[];return A.Children.forEach(e,(n,i)=>{if(!A.isValidElement(n))return;let a=[...t,i];if(n.type===A.Fragment){o.push.apply(o,cr(n.props.children,a));return}n.type!==_t&&X(!1),!n.props.index||!n.props.children||X(!1);let r={id:n.props.id||a.join("-"),caseSensitive:n.props.caseSensitive,element:n.props.element,Component:n.props.Component,index:n.props.index,path:n.props.path,loader:n.props.loader,action:n.props.action,errorElement:n.props.errorElement,ErrorBoundary:n.props.ErrorBoundary,hasErrorBoundary:n.props.ErrorBoundary!=null||n.props.errorElement!=null,shouldRevalidate:n.props.shouldRevalidate,handle:n.props.handle,lazy:n.props.lazy};n.props.children&&(r.children=cr(n.props.children,a)),o.push(r)}),o}/**
 * React Router DOM v6.30.0
 *
 * Copyright (c) Remix Software Inc.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE.md file in the root directory of this source tree.
 *
 * @license MIT
 */function ur(){return ur=Object.assign?Object.assign.bind():function(e){for(var t=1;t<arguments.length;t++){var o=arguments[t];for(var n in o)Object.prototype.hasOwnProperty.call(o,n)&&(e[n]=o[n])}return e},ur.apply(this,arguments)}function Gg(e,t){if(e==null)return{};var o={},n=Object.keys(e),i,a;for(a=0;a<n.length;a++)i=n[a],!(t.indexOf(i)>=0)&&(o[i]=e[i]);return o}function qg(e){return!!(e.metaKey||e.altKey||e.ctrlKey||e.shiftKey)}function Mg(e,t){return e.button===0&&(!t||t==="_self")&&!qg(e)}const Lg=["onClick","relative","reloadDocument","replace","state","target","to","preventScrollReset","viewTransition"],zg="6";try{window.__reactRouterVersion=zg}catch{}const Rg="startTransition",Al=Dd[Rg];function jg(e){let{basename:t,children:o,future:n,window:i}=e,a=A.useRef();a.current==null&&(a.current=Yp({window:i,v5Compat:!0}));let r=a.current,[s,l]=A.useState({action:r.action,location:r.location}),{v7_startTransition:c}=n||{},h=A.useCallback(m=>{c&&Al?Al(()=>l(m)):l(m)},[l,c]);return A.useLayoutEffect(()=>r.listen(h),[r,h]),A.useEffect(()=>Ig(n),[n]),A.createElement(Eg,{basename:t,children:o,location:s.location,navigationType:s.action,navigator:r,future:n})}const Ug=typeof window<"u"&&typeof window.document<"u"&&typeof window.document.createElement<"u",Yg=/^(?:[a-z][a-z0-9+.-]*:|\/\/)/i,kl=A.forwardRef(function(t,o){let{onClick:n,relative:i,reloadDocument:a,replace:r,state:s,target:l,to:c,preventScrollReset:h,viewTransition:m}=t,f=Gg(t,Lg),{basename:v}=A.useContext(Gt),w,S=!1;if(typeof c=="string"&&Yg.test(c)&&(w=c,Ug))try{let p=new URL(window.location.href),y=c.startsWith("//")?new URL(p.protocol+c):new URL(c),b=as(y.pathname,v);y.origin===p.origin&&b!=null?c=b+y.search+y.hash:S=!0}catch{}let T=pg(c,{relative:i}),d=Ng(c,{replace:r,state:s,target:l,preventScrollReset:h,relative:i,viewTransition:m});function u(p){n&&n(p),p.defaultPrevented||d(p)}return A.createElement("a",ur({},f,{href:w||T,onClick:S||a?n:u,ref:o,target:l}))});var xl;(function(e){e.UseScrollRestoration="useScrollRestoration",e.UseSubmit="useSubmit",e.UseSubmitFetcher="useSubmitFetcher",e.UseFetcher="useFetcher",e.useViewTransitionState="useViewTransitionState"})(xl||(xl={}));var Pl;(function(e){e.UseFetcher="useFetcher",e.UseFetchers="useFetchers",e.UseScrollRestoration="useScrollRestoration"})(Pl||(Pl={}));function Ng(e,t){let{target:o,replace:n,state:i,preventScrollReset:a,relative:r,viewTransition:s}=t===void 0?{}:t,l=Yi(),c=To(),h=_u(e,{relative:r});return A.useCallback(m=>{if(Mg(m,o)){m.preventDefault();let f=n!==void 0?n:Ci(c)===Ci(h);l(e,{replace:f,state:i,preventScrollReset:a,relative:r,viewTransition:s})}},[c,l,h,n,i,o,e,a,r,s])}const $=({children:e,className:t="",...o})=>g.jsx("button",{className:`basic-button ${t}`,...o,children:e}),Tl=({percentage:e,threshold:t=80,className:o=""})=>{const n=e>=t,r=`ml-2 text-xs font-bold px-2 py-0.5 rounded-full whitespace-nowrap ${n?"bg-green-100 text-green-700":"bg-red-100 text-red-700"} ${o}`.trim(),s=typeof e=="number"&&!isNaN(e)?`${e.toFixed(0)}%`:"N/A";return g.jsxs("span",{className:r,children:[n?"Passed":"Failed"," (",s,")"]})},Og=({courses:e,onSelectQuiz:t,inProgressQuizzes:o={},completedQuizHistory:n={},getQuizStateKey:i,onRefreshStates:a,passThreshold:r=80})=>{const[s,l]=A.useState(()=>{if(typeof window<"u")try{const b=localStorage.getItem("selectedCourseId_v2");if(b&&e.find(k=>k.id===b))return b}catch(b){console.error("Failed to read selected course ID from localStorage:",b)}return e.length>0?e[0].id:null});A.useEffect(()=>{console.log("QuizSelection mounted, calling onRefreshStates."),a()},[]);const c=A.useMemo(()=>s?e.find(b=>b.id===s):null,[s,e]),h=A.useMemo(()=>{if(!(c!=null&&c.quizItems))return{};const b={};return c.quizItems.forEach(k=>{k&&k.topic&&(b[k.topic]=(b[k.topic]||0)+1)}),b},[c]),m=A.useMemo(()=>{var b;return((b=c==null?void 0:c.quizItems)==null?void 0:b.length)??0},[c]),f=A.useMemo(()=>{var P;if(!c)return[];const b=[...new Set(((P=c.quizItems)==null?void 0:P.map(I=>I.topic).filter(Boolean))||[])],k=Array.isArray(c.topics)?c.topics:[];return[...new Set([...k,...b])].sort()},[c]),v=b=>{if(l(b),typeof window<"u")try{localStorage.setItem("selectedCourseId_v2",b)}catch(k){console.error("Failed to save selected course ID to localStorage:",k)}},w=b=>{s&&t(s,b)},S=()=>{if(l(null),typeof window<"u")try{localStorage.removeItem("selectedCourseId_v2")}catch(b){console.error("Failed to remove selected course ID from localStorage:",b)}},T=()=>{console.log("Refresh button clicked, calling onRefreshStates."),a()};if(!s||!c)return g.jsxs("div",{className:"quiz-selection-container max-w-2xl mx-auto p-4",children:[g.jsx("h2",{className:"text-2xl font-bold text-center mb-6",children:"Select a Course"}),g.jsx("div",{className:"mb-4 text-right",children:g.jsx($,{onClick:T,variant:"outline",size:"sm",children:"Refresh States"})}),g.jsxs("ul",{className:"space-y-3",children:[e.map(b=>g.jsxs("li",{className:"selection-item justify-between items-center p-4 border bg-white rounded-md shadow-sm flex",children:[g.jsx("span",{className:"selection-item-label font-medium text-slate-700 flex-grow",children:b.title}),g.jsx($,{onClick:()=>v(b.id),children:"Select"})]},b.id)),e.length===0&&g.jsx("li",{className:"text-center text-slate-500",children:"No courses available."})]})]});const u=i({courseId:s,topicId:null,type:"full"}),p=u?o[u]:null,y=u?n[u]:null;return g.jsxs("div",{className:"quiz-selection-container max-w-3xl mx-auto p-4",children:[g.jsx("div",{className:"flex justify-between items-start mb-4",children:g.jsxs("div",{children:[g.jsx("h2",{className:"text-2xl font-bold mb-1",children:c.title}),g.jsx($,{onClick:S,variant:"link",size:"sm",className:"text-blue-600 hover:text-blue-800 p-0",children:"Change Course"})," ",g.jsx($,{onClick:T,variant:"outline",size:"sm",children:"Refresh States"})]})}),g.jsxs("div",{className:"selection-item justify-between items-center p-4 border bg-white rounded-md shadow-sm mb-4 flex flex-wrap gap-y-2",children:[g.jsxs("span",{className:"selection-item-label font-medium text-slate-700 flex-grow basis-full sm:basis-auto mb-2 sm:mb-0",children:["Full Quiz (",m," Questions)"]}),g.jsxs("div",{className:"selection-item-actions flex items-center gap-2 flex-wrap justify-end basis-full sm:basis-auto",children:[p?g.jsxs("span",{className:"progress-indicator text-xs font-semibold text-amber-600 mr-2 whitespace-nowrap",children:["(In Progress - Q# ",p.currentIndex+1,")"]}):y?g.jsxs("div",{className:"flex items-center",children:[g.jsxs("span",{className:"progress-indicator text-xs font-semibold mr-1 whitespace-nowrap",children:["(Last: ",y.score,"/",y.totalQuestions,")"]}),typeof y.percentage=="number"&&g.jsx(Tl,{percentage:y.percentage,threshold:r})]}):null,p?g.jsx($,{onClick:()=>w(null),size:"sm",children:"Resume"}):y?g.jsx($,{onClick:()=>w(null),size:"sm",variant:"secondary",children:"Retake"}):g.jsx($,{onClick:()=>w(null),size:"sm",children:"Start"})]})]}),f.length>0&&g.jsxs(g.Fragment,{children:[g.jsx("h3",{className:"text-lg font-semibold text-center my-4",children:"Or Select a Topic:"}),g.jsx("ul",{className:"topic-list space-y-3",children:f.map(b=>{if(!b)return null;const P=i({courseId:s,topicId:b,type:"topic"}),I=h[b]||0,q=P?o[P]:null,E=P?n[P]:null;return I===0?null:g.jsxs("li",{className:"selection-item justify-between items-center p-4 border bg-white rounded-md shadow-sm flex flex-wrap gap-y-2",children:[g.jsxs("span",{className:"selection-item-label font-medium text-slate-700 flex-grow basis-full sm:basis-auto mb-2 sm:mb-0",children:[b," (",I," Questions)"]}),g.jsxs("div",{className:"selection-item-actions flex items-center gap-2 flex-wrap justify-end basis-full sm:basis-auto",children:[q?g.jsxs("span",{className:"progress-indicator text-xs font-semibold text-amber-600 mr-2 whitespace-nowrap",children:["(In Progress - Q# ",q.currentIndex+1,")"]}):E?g.jsxs("div",{className:"flex items-center",children:[g.jsxs("span",{className:"progress-indicator text-xs font-semibold mr-1 whitespace-nowrap",children:["(Last: ",E.score,"/",E.totalQuestions,")"]}),typeof E.percentage=="number"&&g.jsx(Tl,{percentage:E.percentage,threshold:r})]}):null,q?g.jsx($,{onClick:()=>w(b),size:"sm",children:"Resume"}):E?g.jsx($,{onClick:()=>w(b),size:"sm",variant:"secondary",children:"Retake"}):g.jsx($,{onClick:()=>w(b),size:"sm",children:"Start"})]})]},b)})})]}),f.length===0&&g.jsx("p",{className:"text-center text-slate-500 mt-4",children:"No specific topics found for this course."})]})},ed="Google Cloud Architect Practice Questions",td={CS001:{caseStudyId:"CS001",caseStudyText:"<h1>Cymbal Direct</h1><h2>Company overview</h2><p>Cymbal Direct is an online direct-to-consumer Chicago-based footwear and apparel retailer founded in 2008 and acquired by Cymbal Group in 2010. Cymbal Direct is a fair trade and B Corp certified, sustainability-focused company that works with cotton farmers to reinvest in their communities, a fact which appeals to Cymbal Direct's younger target market demographic.</p><h2>Solution concept</h2><ol><li>The beta Delivery by Drone initiative enables licensed drone pilots to team up with Cymbal Direct to deliver shoes and sandals to customers via drone. DBD allows customers to place their orders and then get their shoes delivered in an expedited amount of time. The drones stream real-time video to their pilots, as well as their coordinates, so that customers can see the location of their shoes on a map.</li><li>Cymbal Direct wants to release official APIs for partners. APIs will be published in a controllable, versionable way, with the ability to track, secure and monetize.</li><li>A social integration service initiative which highlights images hashtagged with Cymbal Direct's products using machine learning to ensure images are appropriate. The social media highlighting service is currently proof-of-concept. Built by a developer in their own time after hours as an experiment, the service garnered a lot of excitement and interest, especially from the marketing team. During one of the internal demos, however, inappropriate images were included in the product gallery.</li></ol><h2>Existing technical environment</h2><p>Delivery by Drone is an experiment by the supply chain and logistics team. Their core customer-facing application does order processing, showing the current status and location of their delivery. The drones connect via the cellular network. The drones use the drone API to receive commands and send real-time information and video about their location and status. The existing technical environment includes: A website frontend and pilot and truck management systems run on Kubernetes. Positional data for drone and truck location kept in MongoDB database clusters. Drones connected to virtual machines using a stateful connection, streaming video via RTMP to the pilots and sending commands from the pilots to the drones.</p><p>Purchase & Product APIs were developed over time as the business was being built. They were initially only intended to be used in-house, and not exposed to 3rd parties and partners. Many of the APIs are simply built into monolithic apps, and were not designed for partner integration, lacking functionality such as versioning. The majority of the APIs run on Ubuntu Linux VMs, and scaling has been somewhat difficult because of the use of virtual machines and monolithic architecture. APIs do not have a built-in mechanism for supporting multiple accounts and granting access is very limited as a result.</p><p>The social media highlighting service currently runs on a single virtual machine (SuSE linux, MySQL DB, Redis, Python), and while it does work, it has some performance and scalability issues.</p><h2>Business requirements</h2><ul><li>Easily scale to handle additional demand when needed and expand to more test markets.</li><li>Streamline development for application modernization and new features/products.</li><li>Ensure that developers spend as much time on core business functionality as possible, and not have to worry about scalability wherever possible.</li><li>Let partners order directly via API.</li><li>Deploy a production version of the social media highlighting service and ensure no inappropriate content.</li></ul><h2>Technical requirements</h2><ul><li>Move to managed services wherever possible.</li><li>Ensure that developers can deploy container-based workloads to testing and production environments in a highly scalable environment.</li><li>Standardize on containers where possible, but also allow for existing virtualization infrastructure to run as-is without a re-write, so it can be slowly refactored over time.</li><li>Securely allow partner integration.</li><li>Stream IoT data from drones.</li></ul><h2>Executive statement</h2><p>Cymbal Direct has three areas of strategic focus: improving customer experience, leveraging analytics, and improving digital marketing. Cymbal Direct has experienced rapid growth and has had trouble meeting demand. The organization wants to implement solutions that will help scale services and personalize customer experiences. Cymbal Direct wants to be able to dynamically surge delivery during peak periods. Cymbal Direct also wants to be able to facilitate large scale B2B orders and better predict customer demand and trends. The organization wants to ensure the security of its B2B partners' business plans and make it easier for those partners to integrate with Cymbal Direct's APIs to submit orders and specify customizations. Cymbal Direct also wants to integrate social media and marketing applications into its platform. They would like to be able to highlight posts on social media platforms which feature Cymbal Direct products directly on their product pages, but are concerned about the possibility of having unsavory content shown to users accidentally.</p>"},cs_dress4win_v1:{caseStudyId:"cs_dress4win_v1",caseStudyText:"<h1>Dress4Win</h1><h2>Company overview</h2><p>Dress4Win is a web-based company that helps their users organize and manage their personal wardrobe using a web app and mobile application. The company also cultivates an active social network that connects their users with designers and retailers. They monetize their services through advertising, ecommerce, referrals, and a freemium app model. The application has grown from a few servers in the founder's garage to several hundred servers and appliances in a colocated data center. However, the capacity of their infrastructure is now insufficient for the application's rapid growth. Because of this growth and the company's desire to innovate faster, Dress4Win is committing to a full migration to a public cloud.</p><h2>Solution concept</h2><p>For the first phase of their migration to the cloud, Dress4Win is moving their development and test environments. They are also building a disaster recovery site, because their current infrastructure is at a single location. They are not sure which components of their architecture they can migrate as is and which components they need to change before migrating them.</p><h2>Existing Technical Environment</h2><p>The Dress4Win application is served out of a single data center location. All servers run Ubuntu LTS v16.04.</p><p><strong>Databases:</strong> MySQL. One server for user data, inventory, static data (MySQL 5.7, 8 core CPUs, 128 GB of RAM, 2x 5 TB HDD (RAID 1)).</p><p><strong>Compute:</strong> 40 web application servers providing micro-services based APIs and static content (Tomcat - Java, Nginx, Four core CPUs, 32 GB of RAM). 20 Apache Hadoop/Spark servers (Data analysis, Real-time trending calculations, Eight core CPUs, 128 GB of RAM, 4x 5 TB HDD (RAID 1)). Three RabbitMQ servers for messaging, social notifications, and events (Eight core CPUs, 32GB of RAM). Miscellaneous servers (Jenkins, monitoring, bastion hosts, security scanners, Eight core CPUs, 32GB of RAM).</p><p><strong>Storage appliances:</strong> iSCSI for VM hosts. Fibre channel SAN - MySQL databases (1 PB total storage; 400 TB available). NAS - image storage, logs, backups (100 TB total storage; 35 TB available).</p><h2>Business Requirements</h2><ul><li>Build a reliable and reproducible environment with scaled parity of production.</li><li>Improve security by defining and adhering to a set of security and identity and access management (IAM) best practices for cloud.</li><li>Improve business agility and speed of innovation through rapid provisioning of new resources.</li><li>Analyze and optimize architecture for performance in the cloud.</li></ul><h2>Technical requirements</h2><ul><li>Easily create non-production environments in the cloud.</li><li>Implement an automation framework for provisioning resources in cloud.</li><li>Implement a continuous deployment process for deploying applications to the on-premises data center or cloud.</li><li>Support failover of the production environment to cloud during an emergency.</li><li>Encrypt data on the wire and at rest.</li><li>Support multiple private connections between the production data center and cloud environment.</li></ul><h2>Executive statement</h2><p>Our investors are concerned about our ability to scale and contain costs with our current infrastructure. They are also concerned that a competitor could use a public cloud platform to offset their up-front investment and free them to focus on developing better features. Our traffic patterns are highest in the mornings and weekend evenings; during other times, 80% of our capacity is sitting idle. Our capital expenditure is now exceeding our quarterly projections. Migrating to the cloud will likely cause an initial increase in spending, but we expect to fully transition before our next hardware refresh cycle. Our total cost of ownership (TCO) analysis over the next five years for a public cloud strategy achieves a cost reduction between 30% and 50% over our current model.</p>"},cs_ehr_healthcare:{caseStudyId:"cs_ehr_healthcare",caseStudyText:"<h1>EHR Healthcare</h1><h2>Company overview</h2><p>EHR Healthcare is a leading provider of electronic health record software to the medical industry. EHR Healthcare provides their software as a service to multi-national medical offices, hospitals, and insurance providers.</p><h2>Solution concept</h2><p>Due to rapid changes in the healthcare and insurance industry, EHR Healthcare's business has been growing exponentially year over year. They need to be able to scale their environment, adapt their disaster recovery plan, and roll out new continuous deployment capabilities to update their software at a fast pace. Google Cloud has been chosen to replace their current colocation facilities.</p><h2>Existing technical environment</h2><p>EHR's software is currently hosted in multiple colocation facilities. The lease on one of the data centers is about to expire. Customer-facing applications are web-based, and many have recently been containerized to run on a group of Kubernetes clusters. Data is stored in a mixture of relational and NoSQL databases (MySQL, MS SQL Server, Redis, and MongoDB).</p><p>EHR is hosting several legacy file- and API-based integrations with insurance providers on-premises. These systems are scheduled to be replaced over the next several years. There is no plan to upgrade or move these systems at the current time. Users are managed via Microsoft Active Directory. Monitoring is currently being done via various open source tools. Alerts are sent via email and are often ignored.</p><h2>Business requirements</h2><ul><li>On-board new insurance providers as quickly as possible.</li><li>Provide a minimum 99.9% availability for all customer-facing systems.</li><li>Provide centralized visibility and proactive action on system performance and usage.</li><li>Increase ability to provide insights into healthcare trends.</li><li>Reduce latency to all customers.</li><li>Maintain regulatory compliance.</li><li>Decrease infrastructure administration costs.</li><li>Make predictions and generate reports on industry trends based on provider data.</li></ul><h2>Technical requirements</h2><ul><li>Maintain legacy interfaces to insurance providers with connectivity to both on-premises systems and cloud providers.</li><li>Provide a consistent way to manage customer-facing applications that are container-based.</li><li>Provide a secure and high-performance connection between on-premises systems and Google Cloud.</li><li>Provide consistent logging, log retention, monitoring, and alerting capabilities.</li><li>Maintain and manage multiple container-based environments.</li><li>Dynamically scale and provision new environments.</li><li>Create interfaces to ingest and process data from new providers.</li></ul><h2>Executive statement</h2><p>Our on-premises strategy has worked for years but has required a major investment of time and money in training our team on distinctly different systems, managing similar but separate environments, and responding to outages. Many of these outages have been a result of misconfigured systems, inadequate capacity to manage spikes in traffic, and inconsistent monitoring practices. We want to use Google Cloud to leverage a scalable, resilient platform that can span multiple environments seamlessly and provide a consistent and stable user experience that positions us for future growth.</p>"},cs_hrl:{caseStudyId:"cs_hrl",caseStudyText:"<h1>Helicopter Racing League</h1><h2>Company overview</h2><p>Helicopter Racing League (HRL) is a global sports league for competitive helicopter racing. Each year HRL holds the world championship and several regional league competitions where teams compete to earn a spot in the world championship. HRL offers a paid service to stream the races all over the world with live telemetry and predictions throughout each race.</p><h2>Solution concept</h2><p>HRL wants to migrate their existing service to a new platform to expand their use of managed AI and ML services to facilitate race predictions. Additionally, as new fans engage with the sport, particularly in emerging regions, they want to move the serving of their content, both real-time and recorded, closer to their users.</p><h2>Existing technical environment</h2><p>HRL is a public cloud-first company; the core of their mission-critical applications runs on their current public cloud provider. Video recording and editing is performed at the race tracks, and the content is encoded and transcoded, where needed, in the cloud. Enterprise-grade connectivity and local compute is provided by truck-mounted mobile data centers. Their race prediction services are hosted exclusively on their existing public cloud provider. Their existing technical environment is as follows:</p><ul><li>Existing content is stored in an object storage service on their existing public cloud provider.</li><li>Video encoding and transcoding is performed on VMs created for each job.</li><li>Race predictions are performed using TensorFlow running on VMs in the current public cloud provider.</li></ul><h2>Business requirements</h2><ul><li>Support ability to expose the predictive models to partners.</li><li>Increase predictive capabilities during and before races:<ul><li>Race results</li><li>Mechanical failures</li><li>Crowd sentiment</li></ul></li><li>Increase telemetry and create additional insights.</li><li>Measure fan engagement with new predictions.</li><li>Enhance global availability and quality of the broadcasts.</li><li>Increase the number of concurrent viewers.</li><li>Minimize operational complexity.</li><li>Ensure compliance with regulations.</li><li>Create a merchandising revenue stream.</li></ul><h2>Technical requirements</h2><ul><li>Maintain or increase prediction throughput and accuracy.</li><li>Reduce viewer latency.</li><li>Increase transcoding performance.</li><li>Create real-time analytics of viewer consumption patterns and engagement.</li><li>Create a data mart to enable processing of large volumes of race data.</li></ul><h2>Executive statement</h2><p>Our CEO, S. Hawke, wants to bring high-adrenaline racing to fans all around the world. We listen to our fans, and they want enhanced video streams that include predictions of events within the race (e.g., overtaking). Our current platform allows us to predict race outcomes but lacks the facility to support real-time predictions during races and the capacity to process season-long results.</p>"},cs_jencostore:{caseStudyId:"cs_jencostore",caseStudyText:"<h1>JencoMart</h1><h2>Introductory Info</h2><h3>Company overview:</h3><p>JencoMart is a global retailer with over 10,000 stores in 16 countries. The stores carry a range of goods, such as groceries, tires, and jewelry. One of the companys core values is excellent customer service. In addition, they recently introduced an environmental policy to reduce their carbon output by 50% over the next 5 years.</p><h3>Company background:</h3><p>JencoMart started as a general store in 1931, and has grown into one of the world's leading brands, known for great value and customer service. Over time, the company transitioned from only physical stores to a stores and online hybrid model, with 25% of sales online. Currently, JencoMart has little presence in Asia, but considers that market key for future growth.</p><h2>Solution Concept</h2><p>JencoMart wants to migrate several critical applications to the cloud but has not completed a technical review to determine their suitability for the cloud and the engineering required for migration. They currently host all of these applications on infrastructure that is at its end of life and is no longer supported.</p><h2>Existing Technical Environment</h2><p>JencoMart hosts all of its applications in 4 data centers: 3 in North American and 1 in Europe; most applications are dual-homed. JencoMart understands the dependencies and resource usage metrics of their on-premises architecture.</p><p><strong>Application:</strong> Customer loyalty portal LAMP (Linux, Apache, MySQL and PHP) application served from the two JencoMart-owned U.S. data centers.</p><h3>Database</h3><ul><li><strong>Oracle Database stores user profiles</strong><ul><li>20 TB</li><li>Complex table structure</li><li>Well maintained, clean data</li><li>Strong backup strategy</li></ul></li><li><strong>PostgreSQL database stores user credentials</strong><ul><li>Single-homed in US West</li><li>No redundancy</li><li>Backed up every 12 hours</li><li>100% uptime service level agreement (SLA)</li><li>Authenticates all users</li></ul></li></ul><h3>Compute</h3><ul><li><strong>30 machines in US West Coast, each machine has:</strong><ul><li>Twin, dual core CPUs</li><li>32 GB of RAM</li><li>Twin 250 GB HDD (RAID 1)</li></ul></li><li><strong>20 machines in US East Coast, each machine has:</strong><ul><li>Single, dual-core CPU</li><li>24 GB of RAM</li><li>Twin 250 GB HDD (RAID 1)</li></ul></li></ul><h3>Storage</h3><ul><li>Access to shared 100 TB SAN in each location</li><li>Tape backup every week</li></ul><h2>Business Requirements</h2><ul><li>Optimize for capacity during peak periods and value during off-peak periods</li><li>Guarantee service availability and support</li><li>Reduce on-premises footprint and associated financial and environmental impact</li><li>Move to outsourcing model to avoid large upfront costs associated with infrastructure purchase</li><li>Expand services into Asia</li></ul><h2>Technical Requirements</h2><ul><li>Assess key application for cloud suitability</li><li>Modify applications for the cloud</li><li>Move applications to a new infrastructure</li><li>Leverage managed services wherever feasible</li><li>Sunset 20% of capacity in existing data centers</li><li>Decrease latency in Asia</li></ul><h2>Executive Statements</h2><h3>CEO Statement</h3><p>JencoMart will continue to develop personal relationships with our customers as more people access the web. The future of our retail business is in the global market and the connection between online and in-store experiences. As a large, global company, we also have a responsibility to the environment through green initiatives and policies.</p><h3>CTO Statement</h3><p>The challenges of operating data centers prevent focus on key technologies critical to our long-term success. Migrating our data services to a public cloud infrastructure will allow us to focus on big data and machine learning to improve our service to customers.</p><h3>CFO Statement</h3><p>Since its founding, JencoMart has invested heavily in our data services infrastructure. However, because of changing market trends, we need to outsource our infrastructure to ensure our long-term success. This model will allow us to respond to increasing customer demand during peak periods and reduce costs.</p>"},cs_mountkirk_main:{caseStudyId:"cs_mountkirk_main",caseStudyText:"<h1>Mountkirk Games</h1><h2>Company overview</h2><p>Mountkirk Games makes online, session-based, multiplayer games for mobile platforms. They have recently started expanding to other platforms after successfully migrating their on-premises environments to Google Cloud.</p><p>Their most recent endeavor is to create a retro-style first-person shooter (FPS) game that allows hundreds of simultaneous players to join a geo-specific digital arena from multiple platforms and locations. A real-time digital banner will display a global leaderboard of all the top players across every active arena.</p><h2>Solution concept</h2><p>Mountkirk Games is building a new multiplayer game that they expect to be very popular. They plan to deploy the game's backend on Google Kubernetes Engine so they can scale rapidly and use Google's global load balancer to route players to the closest regional game arenas. In order to keep the global leader board in sync, they plan to use a multi-region Spanner cluster.</p><h2>Existing technical environment</h2><p>The existing environment was recently migrated to Google Cloud, and five games came across using lift-and-shift virtual machine migrations, with a few minor exceptions. Each new game exists in an isolated Google Cloud project nested below a folder that maintains most of the permissions and network policies. Legacy games with low traffic have been consolidated into a single project. There are also separate environments for development and testing.</p><h2>Business requirements</h2><ul><li>Support multiple gaming platforms.</li><li>Support multiple regions.</li><li>Support rapid iteration of game features.</li><li>Minimize latency.</li><li>Optimize for dynamic scaling.</li><li>Use managed services and pooled resources.</li><li>Minimize costs.</li></ul><h2>Technical requirements</h2><ul><li>Dynamically scale based on game activity.</li><li>Publish scoring data on a near real-time global leaderboard.</li><li>Store game activity logs in structured files for future analysis.</li><li>Use GPU processing to render graphics server-side for multi-platform support.</li><li>Support eventual migration of legacy games to this new platform.</li></ul><h2>Executive statement</h2><p>Our last game was the first time we used Google Cloud, and it was a tremendous success. We were able to analyze player behavior and game telemetry in ways that we never could before. This success allowed us to bet on a full migration to the cloud and to start building all-new games using cloud-native design principles. Our new game is our most ambitious to date and will open up doors for us to support more gaming platforms beyond mobile. Latency is our top priority, although cost management is the next most important challenge. As with our first cloud-based game, we have grown to expect the cloud to enable advanced analytics capabilities so we can rapidly iterate on our deployments of bug fixes and new functionality.</p>"},cs_terram_main:{caseStudyId:"cs_terram_main",caseStudyText:"<h1>TerramEarth</h1><h2>Company overview</h2><p>TerramEarth manufactures heavy equipment for the mining and agricultural industries. They currently have over 500 dealers and service centers in 100 countries. Their mission is to build products that make their customers more productive.</p><h2>Solution concept</h2><p>There are 2 million TerramEarth vehicles in operation currently, and we see 20% yearly growth. Vehicles collect telemetry data from many sensors during operation. A small subset of critical data is transmitted from the vehicles in real time to facilitate fleet management. The rest of the sensor data is collected, compressed, and uploaded daily when the vehicles return to home base. Each vehicle usually generates 200 to 500 megabytes of data per day.</p><h2>Existing technical environment</h2><p>TerramEarth's vehicle data aggregation and analysis infrastructure resides in Google Cloud and serves clients from all around the world. A growing amount of sensor data is captured from their two main manufacturing plants and sent to private data centers that contain their legacy inventory and logistics management systems. The private data centers have multiple network interconnects configured to Google Cloud.</p><p>The web frontend for dealers and customers is running in Google Cloud and allows access to stock management and analytics.</p><h2>Business requirements</h2><ul><li>Predict and detect vehicle malfunction and rapidly ship parts to dealerships for just-in-time repair where possible.</li><li>Decrease cloud operational costs and adapt to seasonality.</li><li>Increase speed and reliability of development workflow.</li><li>Allow remote developers to be productive without compromising code or data security.</li><li>Create a flexible and scalable platform for developers to create custom API services for dealers and partners.</li></ul><h2>Technical requirements</h2><ul><li>Create a new abstraction layer for HTTP API access to their legacy systems to enable a gradual move into the cloud without disrupting operations.</li><li>Modernize all CI/CD pipelines to allow developers to deploy container-based workloads in highly scalable environments.</li><li>Allow developers to run experiments without compromising security and governance requirements.</li><li>Create a self-service portal for internal and partner developers to create new projects, request resources for data analytics jobs, and centrally manage access to the API endpoints.</li><li>Use cloud-native solutions for keys and secrets management and optimize for identity-based access.</li><li>Improve and standardize tools necessary for application and network monitoring and troubleshooting.</li></ul><h2>Executive statement</h2><p>Our competitive advantage has always been our focus on the customer, with our ability to provide excellent customer service and minimize vehicle downtimes. After moving multiple systems into Google Cloud, we are seeking new ways to provide best-in-class online fleet management services to our customers and improve operations of our dealerships. Our 5-year strategic plan is to create a partner ecosystem of new products by enabling access to our data, increasing autonomous operation capabilities of our vehicles, and creating a path to move the remaining legacy systems to the cloud.</p>"}},od=[{id:1,topic:"AI & ML",question:"Your customer runs a web service used by e-commerce sites to offer product recommendations to users. The company has begun experimenting with a machine learning model on Google Cloud to improve the quality of results. What should the customer do to improve their model's results over time?",options:{A:"Export Cloud Machine Learning Engine performance metrics from Cloud Monitoring to BigQuery, to be used to analyze the efficiency of the model",B:"Build a roadmap to move the machine learning model training from Cloud GPUs to Cloud TPUs, which offer better results.",C:"Monitor Compute Engine announcements for availability of newer CPU architectures, and deploy the model to them as soon as they are available for additional performance.",D:"Save a history of recommendations and results of the recommendations in BigQuery, to be used as training data."},correctAnswer:["D"],explanation:{correct:"This is the core principle of improving ML models. By creating a feedback loop where the model's predictions (recommendations) and their real-world outcomes (clicks, purchases, etc.) are collected and stored (e.g., in BigQuery), you create a rich dataset for retraining. This allows the model to continuously learn from its successes and failures, adapting to new user behaviors and product trends, which directly improves its predictive quality over time.",incorrect:{A:"Performance metrics from Cloud Monitoring (e.g., CPU/GPU usage, serving latency) are for analyzing the operational efficiency and cost of the model's infrastructure, not its predictive accuracy or the quality of its recommendations.",B:"Moving from GPUs to TPUs can drastically speed up training for certain types of models, but it does not inherently improve the model's predictive results. The quality of the model is primarily dependent on the data, algorithm, and feature engineering, not just the training hardware.",C:"Newer CPU architectures can improve serving speed (inference latency) or training speed for CPU-based training, but like switching to TPUs, this is a hardware optimization and does not directly improve the model's predictive accuracy."}},conditions:["Product recommendation service using ML model on GCP","Goal is to improve model's predictive results over time"],caseStudyId:null},{id:2,topic:"AI & ML",question:"HRL wants better prediction accuracy from their ML prediction models. They want you to use Google's AI Platform so HRL can understand and interpret the predictions.",options:{A:"Use Explainable AI.",B:"Use Vision AI.",C:"Use Google Cloud's operations suite.",D:"Use Jupyter Notebooks."},correctAnswer:["A"],explanation:{correct:"The key requirement is to 'understand and interpret the predictions.' Explainable AI (XAI), a feature of Vertex AI (formerly AI Platform), is specifically designed for this purpose. It provides insights into why a model made a specific prediction by calculating feature attributions, which helps build trust, debug models, and ensure fairness.",incorrect:{B:"Vision AI is a suite of pre-trained models for image and video analysis. It is a specific application of AI, not a general tool for interpreting custom ML models.",C:"Google Cloud's operations suite (Cloud Monitoring, Logging) is for monitoring the operational health and performance of cloud resources, not for interpreting the internal logic of ML model predictions.",D:"Jupyter Notebooks are an interactive environment for developing and experimenting with ML models, but they are not the feature itself that provides explainability. You would use XAI tools within a notebook, but the tool itself is Explainable AI."}},conditions:["Improve ML model prediction accuracy","Need to understand and interpret model predictions","Use Google's AI Platform (Vertex AI)"],caseStudyId:"cs_hrl"},{id:3,topic:"AI & ML",question:"Operational parameters such as oil pressure are adjustable on each of TerramEarth's vehicles to increase their efficiency, depending on their environmental conditions. Your primary goal is to increase the operating efficiency of all 20 million cellular and unconnected vehicles in the field. How can you accomplish this goal?",options:{A:"Have you engineers inspect the data for patterns, and then create an algorithm with rules that make operational adjustments automatically",B:"Capture all operating data, train machine learning models that identify ideal operations, and run locally to make operational adjustments automatically",C:"Implement a Google Cloud Dataflow streaming job with a sliding window, and use Google Cloud Messaging (GCM) to make operational adjustments automatically",D:"Capture all operating data, train machine learning models that identify ideal operations, and host in Google Cloud Machine Learning (ML) Platform to make operational adjustments automatically"},correctAnswer:["B"],explanation:{correct:"This is the most robust and scalable solution. The key challenge is that the solution must work for *unconnected* vehicles as well. This means the decision-making logic must reside on the vehicle itself (at the edge). By training ML models in the cloud on comprehensive data and then deploying these models to run locally on the vehicles, you can make intelligent, real-time adjustments without requiring constant cloud connectivity. This 'train centrally, deploy locally' pattern is ideal for IoT/edge ML.",incorrect:{A:"A rule-based algorithm is likely too brittle and difficult to maintain for complex, multi-variable environmental conditions. ML models are better at learning these complex patterns from data.",C:"This solution relies on a streaming job in the cloud, which will not work for the unconnected vehicles that cannot stream data in real time. Also, GCM is a legacy service.",D:"This solution requires the vehicles to connect to a cloud-hosted ML model to get adjustments. This is not feasible for the unconnected vehicles."}},conditions:["Adjust vehicle operational parameters for efficiency","Solution must work for both connected and unconnected vehicles","Increase operating efficiency for all 20 million vehicles"],caseStudyId:"cs_terram_main"},{id:4,topic:"API Management",question:"The TerramEarth development team wants to create an API to meet the company's Business Requirements. You want the development team to focus their development effort on business value versus creating a custom framework. Which method should they use?",options:{A:"Use Google App Engine with Google Cloud Endpoints. Focus on an API for dealers and partners.",B:"Use Google App Engine with a JAX-RS Jersey Java-based framework. Focus on an API for the public.",C:"Use Google App Engine with the Swagger (open API Specification) framework. Focus on an API for the public.",D:"Use Google Container Engine with a Django Python container. Focus on an API for the public.",E:"Use Google Container Engine with a Tomcat container with the Swagger (Open API Specification) framework. Focus on an API for dealers and partners."},correctAnswer:["A"],explanation:{correct:"This option combines a managed compute platform (App Engine) with a managed API gateway (Cloud Endpoints). This combination allows developers to focus on writing the business logic for the API without worrying about infrastructure management, scaling, or building out API management features like authentication, monitoring, and quotas from scratch. The case study's business requirements clearly mention supporting dealers and partners, making this the correct target audience.",incorrect:{B:"Using a specific Java framework like JAX-RS requires the team to manage that framework's lifecycle and doesn't provide the out-of-the-box API management features that Cloud Endpoints does.",C:"While Cloud Endpoints uses the OpenAPI Specification (Swagger), this option doesn't mention using the managed Cloud Endpoints service, implying more custom work. It also incorrectly targets the public instead of dealers and partners.",D:"Google Kubernetes Engine (GKE) requires more operational overhead than App Engine, which contradicts the goal of letting the team focus on business value. It also targets the wrong audience.",E:"Using GKE is less managed than App Engine. While it mentions the correct audience, the combination of GKE and a self-managed framework is less optimal for minimizing custom framework development compared to App Engine with Cloud Endpoints."}},conditions:["Create API to support dealers and partners","Minimize custom framework development","Focus on business value"],caseStudyId:"cs_terram_main"},{id:5,topic:"API Management",question:"Your company provides a recommendation engine for retail customers. You are providing retail customers with an API where they can submit a user ID and the API returns a list of recommendations for that user. You are responsible for the API lifecycle and want to ensure stability for your customers in case the API makes backward-incompatible changes. You want to follow Google-recommended practices. What should you do?",options:{A:"Create a distribution list of all customers to inform them of an upcoming backward-incompatible change at least one month before replacing the old API with the new API.",B:"Create an automated process to generate API documentation, and update the public API documentation as part of the CI/CD process when deploying an update to the API.",C:"Use a versioning strategy for the APIs that increases the version number on every backward-incompatible change.",D:"Use a versioning strategy for the APIs that adds the suffix 'DEPRECATED' to the current API version number on every backward-incompatible change. Use the current version number for the new API."},correctAnswer:["C"],explanation:{correct:"This is the standard and Google-recommended practice for managing backward-incompatible (breaking) changes in an API. By introducing a new version (e.g., /v2/recommendations), you allow existing clients to continue using the old version (/v1/recommendations) without interruption. This provides a stable transition period for customers to migrate their code to the new version at their own pace.",incorrect:{A:"Communication is crucial, but it's a supporting activity, not the technical mechanism for ensuring stability. Simply replacing the old API after a month would still break customer integrations.",B:"Automated documentation is a good practice but does not prevent existing integrations from breaking when a backward-incompatible change is deployed.",D:"This is a non-standard and confusing versioning strategy. The standard is to increment the major version number for breaking changes, not to add suffixes."}},conditions:["Provide an API to external customers","Ensure stability during backward-incompatible changes","Follow Google-recommended practices"],caseStudyId:null},{id:6,topic:"API Management",question:"Your development team has created a structured API to retrieve vehicle data. They want to allow third parties to develop tools for dealerships that use this vehicle event data. You want to support delegated authorization against this data. What should you do?",options:{A:"Build or leverage an OAuth-compatible access control system",B:"Build SAML 2.0 SSO compatibility into your authentication system",C:"Restrict data access based on the source IP address of the partner systems",D:"Create secondary credentials for each dealer that can be given to the trusted third party"},correctAnswer:["A"],explanation:{correct:"OAuth 2.0 is the industry-standard protocol for delegated authorization. It allows a user (the dealer) to grant a third-party application (the tool) limited access to their resources (the vehicle data via the API) without sharing their own credentials. This is exactly what is required.",incorrect:{B:"SAML 2.0 is primarily for authentication and single sign-on (SSO), allowing a user to log in to multiple applications with one set of credentials. It is not designed for delegated API access by third-party applications.",C:"IP-based restriction is a brittle and insecure method for authorization. It doesn't scale, doesn't work well with dynamic clients, and doesn't provide granular, user-delegated control.",D:"Sharing credentials, even secondary ones, is a poor security practice. It lacks traceability, makes revocation difficult, and doesn't allow for scoped (limited) access."}},conditions:["API retrieves vehicle data","Allow 3rd parties to build tools for dealerships using the API","Support delegated authorization (dealer authorizes tool)"],caseStudyId:"cs_terram_main"},{id:7,topic:"App Engine",question:"Your company has an application running on App Engine that allows users to upload music files and share them with other people. You want to allow users to upload files directly into Cloud Storage from their browser session. The payload should not be passed through the backend. What should you do?",options:{A:"1. Set a CORS configuration in the target Cloud Storage bucket where the base URL of the App Engine application is an allowed origin. 2. Use the Cloud Storage Signed URL feature to generate a POST URL.",B:"1. Set a CORS configuration in the target Cloud Storage bucket where the base URL of the App Engine application is an allowed origin. 2. Assign the Cloud Storage WRITER role to users who upload files.",C:"1. Use the Cloud Storage Signed URL feature to generate a POST URL. 2. Use App Engine default credentials to sign requests against Cloud Storage.",D:"1. Assign the Cloud Storage WRITER role to users who upload files. 2. Use App Engine default credentials to sign requests against Cloud Storage."},correctAnswer:["A"],explanation:{correct:"This is the standard and secure method for direct browser uploads. The CORS (Cross-Origin Resource Sharing) configuration on the Cloud Storage bucket is required to allow the user's browser (running your App Engine app's frontend code) to make requests to the GCS domain. The Signed URL, generated by your App Engine backend, provides a short-lived, permission-scoped token that authorizes the browser to perform the upload directly to GCS without the file needing to pass through your backend servers.",incorrect:{B:"Assigning the Storage Writer role directly to end-users is not secure or scalable. It grants them broad permissions to write to the bucket and would require them to have Google accounts and be managed via IAM.",C:"App Engine default credentials are for the backend service itself to talk to other GCP services. They cannot be used by a user's browser session for direct uploads. A signed URL is needed.",D:"This option has two incorrect parts: assigning broad roles to users and attempting to use backend credentials on the frontend."}},conditions:["App Engine application","Allow direct file uploads from browser to Cloud Storage","Payload should not pass through backend servers"],caseStudyId:null},{id:8,topic:"App Engine",question:"Your customer is receiving reports that their recently updated Google App Engine application is taking approximately 30 seconds to load for some of their users. This behavior was not reported before the update. What strategy should you take?",options:{A:"Work with your ISP to diagnose the problem.",B:"Open a support ticket to ask for network capture and flow data to diagnose the problem, then roll back your application.",C:"Roll back to an earlier known good release initially, then use Cloud Trace and logging to diagnose the problem in a development/test/staging environment.",D:"Roll back to an earlier known good release, then push the release again at a quieter period to investigate. Then use Cloud Trace and logging to diagnose the problem."},correctAnswer:["C"],explanation:{correct:"This follows the best practices for incident response. The immediate priority is to restore service for users, which is achieved by rolling back to the previous stable version. After service is restored, the problematic version can be safely investigated in a non-production environment using tools like Cloud Trace (for latency analysis) and Cloud Logging (for errors) to find the root cause without impacting users.",incorrect:{A:"Since the issue started immediately after a code update, it is highly likely an application issue, not an ISP problem.",B:"Rolling back should be the first step to mitigate user impact, not the last. A support ticket is a good step, but initial diagnosis should be performed by the team.",D:"Pushing a known-bad release back to production, even during a quiet period, is risky and unnecessary. The issue should be diagnosed in a proper staging/test environment."}},conditions:["App Engine application recently updated","New version has high load times (~30s)","Need to resolve the issue"],caseStudyId:null},{id:9,topic:"Application Modernization",question:"Your company has developed a monolithic, 3-tier application to allow external users to upload and share files. The solution cannot be easily enhanced and lacks reliability. The development team would like to re-architect the application to adopt microservices and a fully managed service approach, but they need to convince their leadership that the effort is worthwhile. Which advantage(s) should they highlight to leadership?",options:{A:"The new approach will be significantly less costly, make it easier to manage the underlying infrastructure, and automatically manage the CI/CD pipelines.",B:"The monolithic solution can be converted to a container with Docker. The generated container can then be deployed into a Kubernetes cluster.",C:"The new approach will make it easier to decouple infrastructure from application, develop and release new features, manage the underlying infrastructure, manage CI/CD pipelines and perform A/B testing, and scale the solution if necessary.",D:"The process can be automated with Migrate for Compute Engine."},correctAnswer:["C"],explanation:{correct:"This option provides the most comprehensive and accurate list of business and technical advantages of moving from a monolith to a microservices architecture on managed services. It covers key benefits like increased agility (easier to develop/release features), reduced operational overhead (easier to manage infrastructure), improved scalability, and enabling modern development practices like CI/CD and A/B testing.",incorrect:{A:"While a microservices approach can be cost-effective at scale, it's not guaranteed to be 'significantly less costly' initially. Also, CI/CD pipelines still need to be built and managed by the team; the approach doesn't manage them automatically.",B:"This describes containerizing a monolith (a 'lift and shift' to containers), not re-architecting to microservices. It's a valid modernization step but doesn't capture the full benefits of a microservices approach.",D:"Migrate for Compute Engine is a tool for migrating virtual machines (lift and shift), not for re-architecting an application into microservices."}},conditions:["Existing monolithic application has reliability and enhancement issues","Plan is to re-architect to microservices on managed services","Need to justify the effort to leadership by highlighting advantages"],caseStudyId:null},{id:10,topic:"Billing",question:"Your finance team wants to receive a daily email summarizing the costs incurred for each Google Cloud project within your organization. They want this information broken down by service (e.g., Compute Engine, BigQuery). What is the most straightforward way to achieve this using built-in GCP features?",options:{A:"Configure Cloud Monitoring dashboards for each project and schedule PDF exports.",B:"Enable detailed Billing Export to BigQuery and create a scheduled query to generate the report.",C:"Set up Budgets and Alerts for each project, configuring email notifications.",D:"Use the Cost Table report in the Cloud Console Billing section and export it daily."},correctAnswer:["B"],explanation:{correct:"This is the most powerful and automatable solution. Enabling detailed Billing Export to BigQuery provides granular, queryable data. You can then use BigQuery's scheduled queries to run a daily aggregation query that groups costs by project and service. The results can be automatically sent to a new table, which can then be connected to a tool like Looker Studio (formerly Data Studio) to create a report that can be scheduled for email delivery. This is a fully automated, customizable, and scalable reporting solution.",incorrect:{A:"Cloud Monitoring dashboards are designed for operational metrics (CPU, latency, etc.), not for detailed cost breakdowns. While some cost metrics exist, it's not the primary tool for this kind of financial reporting.",C:"Budgets and Alerts are for sending notifications when spending thresholds are crossed. They do not provide a daily summary report with a service-level breakdown.",D:"The Cost Table report is an interactive tool in the console. Exporting it daily would be a manual process, which is not straightforward or automated."}},conditions:["Receive a daily email summarizing costs","Information broken down by project and by service","Use straightforward, built-in GCP features"],caseStudyId:null},{id:11,topic:"Case Study Analysis",question:"TerramEarth's CTO wants to use the raw data from connected vehicles to help identify approximately when a vehicle in the field will have a catastrophic failure. You want to allow analysts to centrally query the vehicle data. Which architecture should you recommend?",options:{A:"FTP -> GCLB -> GKE -> PubSub -> Dataflow -> BigQuery -> Analysts",B:"FTP -> App Engine Flex -> PubSub -> Dataflow -> BigQuery -> Analysts",C:"FTP -> GCLB -> GKE -> PubSub -> Dataflow -> Cloud SQL -> Analysts",D:"FTP -> App Engine Flex -> PubSub -> Dataflow -> Cloud SQL -> Analysts"},correctAnswer:["A"],explanation:{correct:"This architecture represents a robust, scalable data pipeline for IoT and analytics. Pub/Sub acts as a scalable ingest buffer. Dataflow is used for stream/batch processing. BigQuery is the ideal service for storing and centrally querying terabytes of raw vehicle telemetry for complex analytics at scale, which is required for predictive failure analysis. GKE provides a flexible compute layer for any custom ingestion logic needed.",incorrect:{B:"While App Engine Flex could also work for the ingestion layer, GKE often provides more flexibility for complex ingestion services. The core of the correct architecture (Pub/Sub -> Dataflow -> BigQuery) is the same.",C:"Cloud SQL is a relational OLTP database and is not suitable for large-scale analytical querying on massive telemetry datasets. It would be slow and cost-prohibitive.",D:"This option is incorrect for the same reason as C; Cloud SQL is the wrong tool for the analytics data warehouse."}},conditions:["Use raw data from connected vehicles","Predict vehicle failures","Allow central querying by analysts"],caseStudyId:"cs_terram_main"},{id:12,topic:"Case Study Analysis",question:"Which of TerramEarth's legacy enterprise processes will experience significant change as a result of increased Google Cloud Platform adoption.",options:{A:"Opex/capex allocation, LAN changes, capacity planning",B:"Capacity planning, TCO calculations, opex/capex allocation",C:"Capacity planning, utilization measurement, data center expansion",D:"Data Center expansion, TCO calculations, utilization measurement"},correctAnswer:["B"],explanation:{correct:"Adopting GCP fundamentally changes these three core processes. Capacity Planning shifts from long-term hardware procurement to dynamic, on-demand resource scaling. TCO (Total Cost of Ownership) calculations must be completely re-evaluated to account for pay-as-you-go pricing and managed services. Opex/Capex allocation sees a major shift from upfront capital expenditure (CapEx) to ongoing operational expenditure (OpEx).",incorrect:{A:"LAN changes are typically less impacted at an enterprise process level than core financial and capacity planning.",C:"Data center expansion becomes largely irrelevant or is significantly reduced, so it's a process that diminishes rather than changes.",D:"Similar to C, data center expansion is a process that is being replaced, not just changed."}},conditions:["Identify legacy enterprise processes significantly changed by GCP adoption"],caseStudyId:"cs_terram_main"},{id:13,topic:"Case Study Analysis",question:"You are responsible for ensuring that EHR's use of Google Cloud will pass an upcoming privacy compliance audit. What should you do? (Choose two.)",options:{A:"Verify EHR's product usage against the list of compliant products on the Google Cloud compliance page.",B:"Advise EHR to execute a Business Associate Agreement (BAA) with Google Cloud.",C:"Use Firebase Authentication for EHRs user facing applications.",D:"Implement Prometheus to detect and prevent security breaches on EHR's web-based applications.",E:"Use GKE private clusters for all Kubernetes workloads."},correctAnswer:["A","B"],explanation:{correct:"For a healthcare company, compliance with regulations like HIPAA is paramount. A privacy audit will focus on two key areas: 1) Ensuring that the specific cloud services used to handle Protected Health Information (PHI) are covered under Google's compliance program (e.g., are HIPAA-eligible). 2) Ensuring the necessary legal agreements, specifically a Business Associate Agreement (BAA), are in place between the covered entity (EHR) and the business associate (Google Cloud). These are fundamental administrative and legal prerequisites for compliance.",incorrect:{C:"Using Firebase Authentication is a technical choice for identity management. While it can be part of a secure solution, it is not a direct requirement for passing the overall privacy compliance audit itself.",D:"Implementing Prometheus is a technical choice for monitoring. It contributes to security but is not a primary compliance document or agreement that an auditor would check for first.",E:"Using GKE private clusters is a security best practice to reduce the attack surface. It's a strong technical control, but like C and D, it's an implementation detail, not a foundational compliance requirement like the BAA or service eligibility."}},conditions:["Ensure GCP usage passes privacy compliance audit","Healthcare context implies HIPAA or similar regulations"],caseStudyId:"cs_ehr_healthcare"},{id:14,topic:"Compute",question:"The Dress4Win security team has disabled external SSH access into production virtual machines (VMs) on Google Cloud. The operations team needs to remotely manage the VMs, build and push Docker containers, and manage Google Cloud Storage objects. What can they do?",options:{A:"Grant the operations engineers access to use Google Cloud Shell.",B:"Configure a VPN connection to GCP to allow SSH access to the cloud VMs.",C:"Develop a new access request process that grants temporary SSH access to cloud VMs when an operations engineer needs to perform a task.",D:"Have the development team build an API service that allows the operations team to execute specific remote procedure calls to accomplish their tasks."},correctAnswer:["A"],explanation:{correct:"Cloud Shell provides a pre-configured, browser-based command-line environment with the gcloud CLI, gsutil, and Docker pre-installed and authenticated. It allows the operations team to perform all required tasks: manage VMs (including SSHing via IAP tunneling, which doesn't require external IPs), manage GCS objects with gsutil, and build/push Docker containers, all without needing direct external SSH access from their workstations.",incorrect:{B:"Configuring a VPN is a valid way to get private network access, but Cloud Shell is a simpler, more direct solution that doesn't require managing VPN infrastructure for this use case.",C:"A temporary access process is more cumbersome and less efficient for daily operational tasks compared to the persistent availability of Cloud Shell.",D:"Building a custom API service is overly complex and unnecessary for standard operational tasks that are already supported by standard CLI tools available in Cloud Shell."}},conditions:["External SSH access to production VMs is disabled","Operations team needs to remotely manage VMs","Operations team needs to build and push Docker containers","Operations team needs to manage GCS objects"],caseStudyId:"cs_dress4win_v1"},{id:15,topic:"Compute",question:"Your company has an application running on Compute Engine that allows users to play their favorite music. There are a fixed number of instances. Files are stored in Cloud Storage and data is streamed directly to users. Users are reporting that they sometimes need to attempt to play popular songs multiple times before they are successful. You need to improve the performance of the application. What should you do?",options:{A:"1. Copy popular songs into CloudSQL as a blob. 2. Update application code to retrieve data from CloudSQL when Cloud Storage is overloaded",B:"1. Create a managed instance group with Compute Engine instances. 2. Create a global load balancer and configure it with two backends: a Managed instance group and a Cloud Storage bucket. 3. Enable Cloud CDN on the bucket backend",C:"1. Mount the Cloud Storage bucket using gcsfuse on all backend Compute Engine instances. 2. Serve music files directly from the backend Compute Engine instance",D:"1. Create a Cloud Filestore NFS volume and attach it to the backend Compute Engine instances. 2. Download popular songs in Cloud Filestore. 3. Serve music files directly from the backend Compute Engine instance"},correctAnswer:["B"],explanation:{correct:"This option addresses all potential bottlenecks. A Managed Instance Group provides scalability for the application servers. A global load balancer distributes traffic effectively. Most importantly, enabling Cloud CDN on the Cloud Storage bucket backend will cache popular songs at Google's edge locations, dramatically reducing latency for users and offloading the GCS bucket, which is the likely cause of the intermittent failures for popular content.",incorrect:{A:"Storing large binary files like music in a relational database like Cloud SQL is highly inefficient and not cost-effective.",C:"gcsfuse can introduce its own performance limitations and is not the optimal solution for high-throughput, low-latency content delivery compared to a CDN.",D:"Cloud Filestore is a network file system, which is not designed for globally distributed static content delivery. Cloud Storage with a CDN is the purpose-built solution for this."}},conditions:["Music streaming app on a fixed number of GCE instances","Files are stored in Cloud Storage","Intermittent failures when playing popular songs","Need to improve performance"],caseStudyId:null},{id:16,topic:"Compute",question:"Your company is developing a new application that will allow globally distributed users to upload pictures and share them with other selected users. The application will support millions of concurrent users. You want to allow developers to focus on just building code without having to create and maintain the underlying infrastructure. Which service should you use to deploy the application?",options:{A:"App Engine",B:"Cloud Endpoints",C:"Compute Engine",D:"Google Kubernetes Engine"},correctAnswer:["A"],explanation:{correct:"App Engine is a fully managed, serverless Platform-as-a-Service (PaaS) that handles all infrastructure provisioning, scaling, patching, and load balancing automatically. This directly meets the requirement for developers to focus only on code without managing infrastructure, and it's designed to scale to millions of users.",incorrect:{B:"Cloud Endpoints is an API management service, not a compute platform for deploying applications.",C:"Compute Engine is Infrastructure-as-a-Service (IaaS), which requires significant manual effort to manage and maintain the underlying virtual machines.",D:"Google Kubernetes Engine (GKE) is a container orchestration service. While more managed than raw Compute Engine, it still requires more operational overhead (managing clusters, deployments, etc.) than the 'no-ops' experience of App Engine."}},conditions:["New application for picture uploading and sharing","Globally distributed users","Support for millions of concurrent users","Developers should focus only on code (no infrastructure management)"],caseStudyId:null},{id:17,topic:"Compute",question:"Your company has a Compute Engine managed instance group that adds and removes Compute Engine instances from the group in response to the load of your application. The instances have a shutdown script that removes REDIS database entries associated with the instance. You see that many database entries have not been removed, and you suspect that the shutdown script is the problem. You need to ensure that the commands in the shutdown script are run reliably every time an instance is shut down. You create a Cloud Function to remove the database entries. What should you do next?",options:{A:"Modify the shutdown script to wait for 30 seconds before triggering the Cloud Function.",B:"Do not use the Cloud Function. Modify the shutdown script to restart if it has not completed in 30 seconds.",C:"Set up a Cloud Monitoring sink that triggers the Cloud Function after an instance removal log message arrives in Cloud Logging.",D:"Modify the shutdown script to wait for 30 seconds and then publish a message to a Pub/Sub queue."},correctAnswer:["C"],explanation:{correct:"Shutdown scripts can be unreliable because they have a limited execution window and can be preempted. A more robust, event-driven approach is to react to the instance deletion event itself. An instance removal action is captured in Cloud Audit Logs, which are sent to Cloud Logging. You can create a log sink that filters for these specific log entries and sends them to a Pub/Sub topic, which in turn triggers the Cloud Function to perform the cleanup. This decouples the cleanup logic from the lifecycle of the instance itself, making it much more reliable.",incorrect:{A:"This still relies on the unreliable shutdown script to initiate the action.",B:"This doesn't solve the core problem that the shutdown script might be terminated before it can complete or restart.",D:"This also relies on the unreliable shutdown script to publish the message."}},conditions:["Autoscaled MIG with a shutdown script for Redis cleanup","Shutdown script is proving to be unreliable","Need a reliable method for cleanup execution","A Cloud Function has already been created for the cleanup task"],caseStudyId:null},{id:18,topic:"Compute",question:"You are managing several projects on Google Cloud and need to interact on a daily basis with BigQuery, Bigtable, and Kubernetes Engine using the gcloud CLI tool. You are travelling a lot and work on different workstations during the week. You want to avoid having to manage the gcloud CLI manually. What should you do?",options:{A:"Use Google Cloud Shell in the Google Cloud Console to interact with Google Cloud.",B:"Create a Compute Engine instance and install gcloud on the instance. Connect to this instance via SSH to always use the same gcloud installation when interacting with Google Cloud.",C:"Install gcloud on all of your workstations. Run the command gcloud components auto-update on each workstation",D:"Use a package manager to install gcloud on your workstations instead of installing it manually."},correctAnswer:["A"],explanation:{correct:"Cloud Shell is the perfect solution for this use case. It provides a consistent, browser-based command-line environment that is accessible from anywhere. It comes pre-installed with the gcloud CLI and other necessary tools, is always authenticated with your console credentials, and is managed and updated by Google. This completely eliminates the need to install, configure, or manage the CLI on different workstations.",incorrect:{B:"This creates an unnecessary extra step (connecting to a VM) and incurs the cost of running the VM. Cloud Shell is free.",C:"This is exactly the manual management that the user wants to avoid.",D:"Using a package manager simplifies installation but doesn't solve the problem of managing configurations and authentications across multiple different workstations."}},conditions:["Manage multiple GCP projects daily","Need to use gcloud CLI for BigQuery, Bigtable, and GKE","Work from different workstations frequently","Want to avoid manual management of the gcloud CLI"],caseStudyId:null},{id:19,topic:"Compute",question:"Your company has an application running as a Deployment in a Google Kubernetes Engine (GKE) cluster. When releasing new versions of the application via a rolling deployment, the team has been causing outages. The root cause of the outages is misconfigurations with parameters that are only used in production. You want to put preventive measures for this in the platform to prevent outages. What should you do?",options:{A:"Configure liveness and readiness probes in the Pod specification.",B:"Configure health checks on the managed instance group.",C:"Create a Scheduled Task to check whether the application is available.",D:"Configure an uptime alert in Cloud Monitoring."},correctAnswer:["A"],explanation:{correct:"Readiness probes are a core Kubernetes feature designed to prevent this exact problem. During a rolling update, Kubernetes will not direct traffic to a new pod until its readiness probe passes. If a misconfiguration prevents the new version of the application from starting correctly, the readiness probe will fail, and the faulty pod will never receive production traffic, thus preventing an outage.",incorrect:{B:"Health checks on the managed instance group monitor the health of the underlying nodes (VMs), not the application running inside the GKE pods.",C:"A scheduled task is an external check that is reactive, not a preventative measure integrated into the deployment process.",D:"A Cloud Monitoring alert is also reactive. It will notify you that an outage has occurred but will not prevent it from happening."}},conditions:["Application is a Deployment on GKE","Rolling deployments are causing outages due to misconfigurations","Need a preventative measure to avoid outages"],caseStudyId:null},{id:20,topic:"Compute",question:"Your company uses Google Kubernetes Engine (GKE) as a platform for all workloads. Your company has a single large GKE cluster that contains batch, stateful, and stateless workloads. The GKE cluster is configured with a single node pool with 200 nodes. Your company needs to reduce the cost of this cluster but does not want to compromise availability. What should you do?",options:{A:"Create a second GKE cluster for the batch workloads only. Allocate the 200 original nodes across both clusters.",B:"Configure CPU and memory limits on the namespaces in the cluster. Configure all Pods to have a CPU and memory limits.",C:"Configure a HorizontalPodAutoscaler for all stateless workloads and for all compatible stateful workloads. Configure the cluster to use node auto scaling.",D:"Change the node pool to use Spot VMs."},correctAnswer:["C"],explanation:{correct:"This is the most effective strategy for cost optimization without compromising availability. The Horizontal Pod Autoscaler (HPA) scales the number of application pods based on load, ensuring you only run as many as you need. The Cluster Autoscaler (node auto scaling) scales the number of underlying nodes based on the number of pods that need to be scheduled. This combination ensures that both application and infrastructure resources dynamically match the actual demand, scaling down to save costs during low-traffic periods.",incorrect:{A:"Splitting the cluster adds management overhead and doesn't inherently reduce costs without also implementing autoscaling.",B:"Configuring requests and limits is a best practice for resource management and stability, but it doesn't dynamically scale down the infrastructure to save costs.",D:"Using Spot VMs (preemptible) can significantly reduce costs but is not suitable for all workloads, especially stateful ones, as it can compromise availability."}},conditions:["Single large GKE cluster with 200 nodes","Mixed workloads (batch, stateful, stateless)","Need to reduce cost without compromising availability"],caseStudyId:null},{id:21,topic:"Compute",question:"You have a Compute Engine application that you want to autoscale when total memory usage exceeds 80%. You have installed the Cloud Monitoring agent and configured the autoscaling policy with Metric identifier: agent.googleapis.com/memory/percent_used, Filter: metric.label.state = 'used', Target utilization level: 80, Target type: GAUGE. You observe that the application does not scale under high load. You want to resolve this. What should you do?",options:{A:"Change the Target type to DELTA_PER_MINUTE.",B:"Change the Metric identifier to agent.googleapis.com/memory/bytes_used.",C:"Change the filter to metric.label.state = used AND metric.label.state = buffered AND metric.label.state = cached AND metric.label.state slab.",D:"Change the filter to metric.label.state = free and the Target utilization to 20."},correctAnswer:["C"],explanation:{correct:"On Linux systems, the 'used' memory metric alone is often misleading. Memory used for buffers and cache is reclaimable by the OS when applications need it. To get an accurate picture of memory pressure, you need to consider the memory that is truly unavailable. The most accurate agent metric for this is often `agent.googleapis.com/memory/utilization`, which typically includes used, buffered, and cached memory. Since that metric isn't an option, the next best thing is to construct a filter that approximates it by including these other states. A filter that looks at 'used', 'buffered', 'cached', and 'slab' memory provides a much more accurate representation of total memory consumption, which will allow the autoscaler to trigger correctly.",incorrect:{A:"The target type GAUGE is correct for a utilization percentage. DELTA is for tracking rates of change.",B:"Changing from percent to bytes doesn't solve the fundamental problem of which memory states are being measured.",D:"While scaling on low free memory is a valid strategy, it's often more intuitive to scale on high *used* memory. The core issue is the definition of 'used', which C addresses most directly."}},conditions:["Autoscale GCE application when memory usage exceeds 80%","Cloud Monitoring agent is installed","Autoscaler is configured with `percent_used` metric and filter `state='used'`","Application fails to scale under high load"],caseStudyId:null},{id:22,topic:"Compute",question:"You are developing an application using different microservices that should remain internal to the cluster. You want to be able to configure each microservice with a specific number of replicas. You also want to be able to address a specific microservice from any other microservice in a uniform way, regardless of the number of replicas the microservice scales to. You need to implement this solution on Google Kubernetes Engine. What should you do?",options:{A:"Deploy each microservice as a Deployment. Expose the Deployment in the cluster using a Service, and use the Service DNS name to address it from other microservices within the cluster.",B:"Deploy each microservice as a Deployment. Expose the Deployment in the cluster using an Ingress, and use the Ingress IP address to address the Deployment from other microservices within the cluster.",C:"Deploy each microservice as a Pod. Expose the Pod in the cluster using a Service, and use the Service DNS name to address the microservice from other microservices within the cluster.",D:"Deploy each microservice as a Pod. Expose the Pod in the cluster using an Ingress, and use the Ingress IP address name to address the Pod from other microservices within the cluster."},correctAnswer:["A"],explanation:{correct:"This describes the standard Kubernetes pattern for microservices. A 'Deployment' manages a set of replica Pods, allowing you to specify the desired number of replicas and handle updates. A 'Service' (of type ClusterIP for internal traffic) provides a stable, internal IP address and DNS name that load balances traffic to the pods managed by the Deployment. This allows other microservices to use the stable DNS name to communicate, regardless of individual pod IPs or scaling events.",incorrect:{B:"An Ingress is used to expose services to traffic *outside* the cluster, not for internal service-to-service communication.",C:"Deploying bare Pods without a Deployment controller is not recommended as you lose self-healing, scaling, and update management capabilities.",D:"This is incorrect for two reasons: it uses bare Pods and an Ingress for internal communication."}},conditions:["Microservices application on GKE","Services should remain internal to the cluster","Need to configure replica count per microservice","Need a uniform way to address microservices regardless of scaling"],caseStudyId:null},{id:23,topic:"Compute",question:"The development team has provided you with a Kubernetes Deployment file. You have no infrastructure yet and need to deploy the application. What should you do?",options:{A:"Use gcloud to create a Kubernetes cluster. Use Deployment Manager to create the deployment.",B:"Use gcloud to create a Kubernetes cluster. Use kubectl to create the deployment.",C:"Use kubectl to create a Kubernetes cluster. Use Deployment Manager to create the deployment.",D:"Use kubectl to create a Kubernetes cluster. Use kubectl to create the deployment."},correctAnswer:["B"],explanation:{correct:"This is the standard workflow. First, you need to provision the infrastructure, which is the GKE cluster itself. This is a GCP resource and is created using `gcloud` (or Terraform, Deployment Manager). Once the cluster exists, you use `kubectl`, the native Kubernetes command-line tool, to interact with the cluster and deploy your application using the provided Deployment file (`kubectl apply -f <filename>`).",incorrect:{A:"While Deployment Manager can deploy Kubernetes resources, `kubectl` is the standard and more common tool for applying Kubernetes manifests.",C:"`kubectl` cannot create a GKE cluster; it's for managing resources within an existing cluster.",D:"`kubectl` cannot create a GKE cluster."}},conditions:["Have a Kubernetes Deployment file","No existing infrastructure","Need to deploy the application"],caseStudyId:null},{id:24,topic:"Compute",question:"You are designing an application for use only during business hours. For the minimum viable product release, youd like to use a managed product that automatically scales to zero so you dont incur costs when there is no activity. Which primary compute resource should you choose?",options:{A:"Cloud Functions",B:"Compute Engine",C:"Google Kubernetes Engine",D:"AppEngine flexible environment"},correctAnswer:["A"],explanation:{correct:"Cloud Functions and Cloud Run are serverless compute platforms designed to automatically scale down to zero instances when there are no incoming requests. This makes them extremely cost-effective for applications with intermittent or idle periods, as you only pay for the compute time you actually use. For an MVP, this is an excellent choice.",incorrect:{B:"Compute Engine instances incur costs as long as they are running, even if they are idle.",C:"Google Kubernetes Engine requires at least one node in the cluster to be running, which incurs costs even with no application traffic.",D:"App Engine flexible environment is designed for more customization and typically keeps at least one instance running to serve traffic, making it less cost-effective for scale-to-zero scenarios compared to App Engine Standard or Cloud Functions/Cloud Run."}},conditions:["Application used only during business hours","MVP release","Need a managed product","Requirement for automatic 'scale to zero' for cost savings"],caseStudyId:null},{id:25,topic:"Compute",question:"You have a Python web application with many dependencies that requires 0.1 CPU cores and 128 MB of memory to operate in production. You want to monitor and maximize machine utilization. You also want to reliably deploy new versions of the application. Which set of steps should you take?",options:{A:"Perform the following: 1. Create a managed instance group with f1-micro type machines. 2. Use a startup script to clone the repository, check out the production branch, install the dependencies, and start the Python app. 3. Restart the instances to automatically deploy new production releases.",B:"Perform the following: 1. Create a managed instance group with n1-standard-1 type machines. 2. Build a Compute Engine image from the production branch that contains all of the dependencies and automatically starts the Python app. 3. Rebuild the Compute Engine image, and update the instance template to deploy new production releases.",C:"Perform the following: 1. Create a Google Kubernetes Engine (GKE) cluster with n1-standard-1 type machines. 2. Build a Docker image from the production branch with all of the dependencies, and tag it with the version number. 3. Create a Kubernetes Deployment with the imagePullPolicy set to 'IfNotPresent' in the staging namespace, and then promote it to the production namespace after testing.",D:"Perform the following: 1. Create a GKE cluster with n1-standard-4 type machines. 2. Build a Docker image from the master branch with all of the dependencies, and tag it with latest. 3. Create a Kubernetes Deployment in the default namespace with the imagePullPolicy set to Always. Restart the pods to automatically deploy new production releases."},correctAnswer:["C"],explanation:{correct:"GKE is excellent for maximizing machine utilization through 'bin packing', where it schedules many small containers (pods) with fine-grained resource requests (like 0.1 CPU, 128MB RAM) onto larger nodes. This approach also follows best practices for reliable deployments by building versioned, immutable Docker images and using a structured promotion process from staging to production.",incorrect:{A:"Using startup scripts to install dependencies on every new instance is slow and inefficient. f1-micro machines may be too small and lead to inefficient packing.",B:"Using GCE MIGs is less efficient for bin-packing many small, distinct workloads compared to GKE's pod scheduling.",D:"This option follows several bad practices: using oversized nodes (n1-standard-4) for a small application is wasteful, using the 'latest' tag for production is unreliable, and manually restarting pods is not a proper deployment strategy."}},conditions:["Python web app with many dependencies","Small resource requirements (0.1 CPU, 128 MB RAM)","Goal to maximize machine utilization","Need a reliable deployment process"],caseStudyId:null},{id:26,topic:"Compute",question:"Your company is running a stateless application on a Compute Engine instance. The application is used heavily during regular business hours and lightly outside of business hours. Users are reporting that the application is slow during peak hours. You need to optimize the application's performance. What should you do?",options:{A:"Create a snapshot of the existing disk. Create an instance template from the snapshot. Create an autoscaled managed instance group from the instance template.",B:"Create a snapshot of the existing disk. Create a custom image from the snapshot. Create an autoscaled managed instance group from this custom image.",C:"Create a custom image from the existing disk. Create an instance template from the custom image. Create an autoscaled managed instance group from the instance template.",D:"Create an instance template from the existing disk. Create a custom image from the instance template. Create an autoscaled managed instance group from the custom image."},correctAnswer:["C"],explanation:{correct:"This option describes the correct and standard workflow for converting a single, manually configured VM into a scalable, automated deployment. First, you create a reusable custom image from the existing disk. Then, you define an instance template that uses this image. Finally, you create an autoscaled Managed Instance Group (MIG) from the template. The MIG will then automatically scale the number of instances up during peak hours to handle the load and scale down during off-hours to save costs.",incorrect:{A:"You cannot create an instance template directly from a snapshot. You must create an image or a disk from the snapshot first.",B:"This workflow is valid but less direct. You create a snapshot, then an image from the snapshot, then the MIG from the image. Option C is more direct by creating the image from the disk, then the template from the image.",D:"The order is incorrect. You cannot create a custom image from an instance template. You create an instance template from a custom image."}},conditions:["Stateless application on a single GCE instance","Heavy use during business hours, light otherwise","Application is slow during peak hours","Need to optimize performance"],caseStudyId:null},{id:27,topic:"Compute",question:"You want your Google Kubernetes Engine cluster to automatically add or remove nodes based on CPU load. What should you do?",options:{A:"Configure a HorizontalPodAutoscaler with a target CPU usage. Enable the Cluster Autoscaler from the GCP Console.",B:"Configure a HorizontalPodAutoscaler with a target CPU usage. Enable autoscaling on the managed instance group for the cluster using the gcloud command.",C:"Create a deployment and set the maxUnavailable and maxSurge properties. Enable the Cluster Autoscaler using the gcloud command.",D:"Create a deployment and set the maxUnavailable and maxSurge properties. Enable autoscaling on the cluster managed instance group from the GCP Console."},correctAnswer:["A"],explanation:{correct:"Scaling nodes based on CPU load in GKE is a two-step process. First, the Horizontal Pod Autoscaler (HPA) scales the number of *pods* up when their CPU usage exceeds a target. Second, if the new pods cannot be scheduled on existing nodes, the Cluster Autoscaler will provision new *nodes* to accommodate them. Both components must be enabled to achieve node scaling based on workload CPU load.",incorrect:{B:"You should not manage the autoscaling of a GKE cluster's managed instance group directly. The GKE Cluster Autoscaler handles this in a Kubernetes-aware manner.",C:"`maxUnavailable` and `maxSurge` are parameters that control the rolling update strategy for a deployment, not load-based scaling.",D:"This is incorrect for the same reasons as B and C."}},conditions:["GKE cluster","Automatically add or remove nodes based on CPU load"],caseStudyId:null},{id:28,topic:"Compute",question:"Your web application uses Google Kubernetes Engine to manage several workloads. One workload requires a consistent set of hostnames even after pod scaling and relaunches. Which feature of Kubernetes should you use to accomplish this?",options:{A:"StatefulSets",B:"Role-based access control",C:"Container environment variables",D:"Persistent Volumes"},correctAnswer:["A"],explanation:{correct:"StatefulSets are the Kubernetes controller designed specifically for stateful applications that require stable, unique network identifiers. Pods managed by a StatefulSet are given predictable, ordinal hostnames (e.g., web-0, web-1) that are maintained across restarts and scaling events.",incorrect:{B:"Role-based access control (RBAC) is for managing authorization and permissions within the cluster.",C:"Container environment variables are for passing configuration data into a container, not for defining its network identity.",D:"Persistent Volumes provide stable storage, which is often used in conjunction with StatefulSets, but they do not provide stable hostnames."}},conditions:["Application running on GKE","A workload requires consistent hostnames across pod scaling and relaunches"],caseStudyId:null},{id:29,topic:"Compute",question:"You have an App Engine application that needs to be updated. You want to test the update with production traffic before replacing the current application version. What should you do?",options:{A:"Deploy the update using the Instance Group Updater to create a partial rollout, which allows for canary testing.",B:"Deploy the update as a new version in the App Engine application, and split traffic between the new and current versions.",C:"Deploy the update in a new VPC, and use Googles global HTTP load balancing to split traffic between the update and current applications.",D:"Deploy the update as a new App Engine application, and use Googles global HTTP load balancing to split traffic between the new and current applications."},correctAnswer:["B"],explanation:{correct:"App Engine has native, built-in support for versioning and traffic splitting. You can deploy the updated code as a new version without affecting the current production version. Then, you can configure traffic splitting to send a small percentage of production traffic to the new version for testing (canary testing). This allows you to validate the update with real traffic before migrating all traffic.",incorrect:{A:"Instance Group Updater is for Compute Engine Managed Instance Groups, not App Engine.",C:"This is overly complex. App Engine provides this functionality natively without needing a new VPC and external load balancer.",D:"Deploying a completely new App Engine application is also unnecessarily complex and costly compared to using the built-in versioning."}},conditions:["App Engine application update","Need to test the update with production traffic before full rollout"],caseStudyId:null},{id:30,topic:"Compute",question:"A development team at your company has created a dockerized HTTPS web application. You need to deploy the application on Google Kubernetes Engine (GKE) and make sure that the application scales automatically. How should you deploy to GKE?",options:{A:"Use the Horizontal Pod Autoscaler and enable cluster autoscaling. Use an Ingress resource to load-balance the HTTPS traffic.",B:"Use the Horizontal Pod Autoscaler and enable cluster autoscaling on the Kubernetes cluster. Use a Service resource of type LoadBalancer to load-balance the HTTPS traffic.",C:"Enable autoscaling on the Compute Engine instance group. Use an Ingress resource to load-balance the HTTPS traffic.",D:"Enable autoscaling on the Compute Engine instance group. Use a Service resource of type LoadBalancer to load-balance the HTTPS traffic."},correctAnswer:["A"],explanation:{correct:"This option combines all the correct components for a scalable HTTPS application on GKE. The Horizontal Pod Autoscaler scales the application pods, the Cluster Autoscaler scales the cluster nodes, and an Ingress resource is the standard Kubernetes way to manage external HTTPS traffic, providing L7 features like SSL termination and path-based routing by provisioning a Google Cloud HTTP(S) Load Balancer.",incorrect:{B:"A Service of type LoadBalancer creates an L4 Network Load Balancer, which is less suitable for HTTPS traffic as it doesn't handle SSL termination or host/path routing.",C:"You should not manage autoscaling on the underlying Compute Engine instance group directly; use the GKE-native Cluster Autoscaler.",D:"This option is incorrect for two reasons: it uses direct instance group autoscaling and an L4 load balancer for an L7 application."}},conditions:["Deploy a dockerized HTTPS web application on GKE","Ensure the application scales automatically"],caseStudyId:null},{id:31,topic:"Compute",question:"Your company is deploying a PHP App Engine Standard service with Cloud SQL as the backend. You want to minimize the number of queries to the database. What should you do?",options:{A:"Set the memcache service level to dedicated. Create a key from the hash of the query, and return database values from memcache before issuing a query to Cloud SQL.",B:"Set the memcache service level to dedicated. Create a cron task that runs every minute to populate the cache with keys containing query results.",C:"Set the memcache service level to shared. Create a cron task that runs every minute to save all expected queries to a key called cached_queries.",D:"Set the memcache service level to shared. Create a key called cached_queries, and return database values from the key before using a query to Cloud SQL."},correctAnswer:["A"],explanation:{correct:"This describes the cache-aside pattern, which is a standard and effective way to reduce database load. The application first checks the cache (Memcache) for the data. If it's a cache hit, it returns the data from the cache. If it's a cache miss, it queries the database, stores the result in the cache for next time, and then returns the data. Using dedicated Memcache provides more predictable performance than shared Memcache, which is better for a production application.",incorrect:{B:"Proactively populating the cache with a cron job can be inefficient, may not cover all possible queries, and can lead to stale data.",C:"Shared Memcache offers no performance guarantees. Using a cron job has the same issues as in B.",D:"This describes a simplistic caching approach that is less robust than the cache-aside pattern described in A. Shared Memcache is also less ideal for production."}},conditions:["PHP App Engine Standard service with Cloud SQL backend","Goal is to minimize the number of queries to the database"],caseStudyId:null},{id:32,topic:"Compute",question:"Your company is planning to perform a lift and shift migration of their Linux RHEL 6.5+ virtual machines. The virtual machines are running in an on-premises VMware environment. You want to migrate them to Compute Engine following Google-recommended practices. What should you do?",options:{A:"1. Define a migration plan based on the list of the applications and their dependencies. 2. Migrate all virtual machines into Compute Engine individually with Migrate for Compute Engine.",B:"1. Perform an assessment of virtual machines running in the current VMware environment. 2. Create images of all disks. Import disks on Compute Engine. 3. Create standard virtual machines where the boot disks are the ones you have imported.",C:"1. Perform an assessment of virtual machines running in the current VMware environment. 2. Define a migration plan, prepare a Migrate for Compute Engine migration RunBook, and execute the migration.",D:"1. Perform an assessment of virtual machines running in the current VMware environment. 2. Install a third-party agent on all selected virtual machines. 3. Migrate all virtual machines into Compute Engine."},correctAnswer:["C"],explanation:{correct:"This outlines the standard, phased approach for a successful migration using Google's recommended tool. The process involves assessing the current environment, creating a detailed plan (RunBook) that groups VMs into migration waves, and then using Migrate for Compute Engine to execute the migration with features like test-cloning and minimal downtime.",incorrect:{A:"This option skips the critical assessment phase.",B:"Manually creating and importing disks is a much more labor-intensive and error-prone process than using a dedicated migration tool like Migrate for Compute Engine.",D:"Using a third-party agent is not the Google-recommended practice when a native, purpose-built tool is available."}},conditions:["Lift-and-shift migration of Linux RHEL 6.5+ VMs","Source environment is on-premises VMware","Target is Compute Engine","Follow Google-recommended practices"],caseStudyId:null},{id:33,topic:"Compute",question:"You need to deploy an application on Google Cloud that must run on a Debian Linux environment. The application requires extensive configuration in order to operate correctly. You want to ensure that you can install Debian distribution updates with minimal manual intervention whenever they become available. What should you do?",options:{A:"Create a Compute Engine instance template using the most recent Debian image. Create an instance from this template, and install and configure the application as part of the startup script. Repeat this process whenever a new Google-managed Debian image becomes available.",B:"Create a Debian-based Compute Engine instance, install and configure the application, and use OS patch management to install available updates.",C:"Create an instance with the latest available Debian image. Connect to the instance via SSH, and install and configure the application on the instance. Repeat this process whenever a new Google-managed Debian image becomes available.",D:"Create a Docker container with Debian as the base image. Install and configure the application as part of the Docker image creation process. Host the container on Google Kubernetes Engine and restart the container whenever a new update is available."},correctAnswer:["B"],explanation:{correct:"OS Patch Management is a Compute Engine feature designed to automate the application of OS updates to your VMs. After the initial extensive configuration, you can set up a patch deployment schedule to automatically apply Debian updates with minimal manual intervention, which directly meets the requirement.",incorrect:{A:"Recreating the instance from a new template and re-running the extensive configuration via startup script for every OS update is not 'minimal manual intervention'.",C:"This describes a fully manual process which is inefficient and error-prone.",D:"This describes updating the application container. While good practice, it doesn't address patching the underlying Debian OS on the GKE nodes themselves. OS Patch Management is the direct solution for patching the VM's OS."}},conditions:["Deploy application on a Debian Linux environment","Application requires extensive configuration","Need to install Debian updates with minimal manual intervention"],caseStudyId:null},{id:34,topic:"Compute",question:"You are deploying an application to Google Cloud. The application receives traffic via TCP and reads and writes data to the filesystem. The application does not support horizontal scaling. The application process requires full control over the data on the file system because concurrent access causes corruption. The business is willing to accept a downtime when an incident occurs, but the application must be available 24/7 to support their business operations. You need to design the architecture of this application on Google Cloud. What should you do?",options:{A:"Use a managed instance group with instances in multiple zones, use Cloud Filestore, and use an HTTP load balancer in front of the instances.",B:"Use a managed instance group with instances in multiple zones, use Cloud Filestore, and use a network load balancer in front of the instances.",C:"Use an unmanaged instance group with an active and standby instance in different zones, use a regional persistent disk, and use an HTTP load balancer in front of the instances.",D:"Use an unmanaged instance group with an active and standby instance in different zones, use a regional persistent disk, and use a network load balancer in front of the instances."},correctAnswer:["D"],explanation:{correct:"This architecture correctly addresses all constraints. An active/standby setup in different zones provides high availability. A regional persistent disk ensures data is synchronously replicated to the standby zone and can be attached upon failover, preventing data loss. A network load balancer (L4) is required for generic TCP traffic and can use health checks to direct traffic to the active instance and fail over to the standby.",incorrect:{A:"A MIG is for horizontal scaling, which the app doesn't support. Cloud Filestore is for shared access, which causes corruption. An HTTP load balancer is for L7 traffic, not generic TCP.",B:"Incorrect for the same reasons as A regarding MIG and Filestore.",C:"An HTTP load balancer is incorrect for generic TCP traffic."}},conditions:["Application receives TCP traffic","Reads/writes to a filesystem","Does not support horizontal scaling","Requires exclusive filesystem access","Needs 24/7 availability (implies a failover mechanism)"],caseStudyId:null},{id:35,topic:"Compute",question:"Your company has an application deployed on Anthos clusters (formerly Anthos GKE) that is running multiple microservices. The cluster has both Anthos Service Mesh and Anthos Config Management configured. End users inform you that the application is responding very slowly. You want to identify the microservice that is causing the delay. What should you do?",options:{A:"Use the Service Mesh visualization in the Cloud Console to inspect the telemetry between the microservices.",B:"Use Anthos Config Management to create a ClusterSelector selecting the relevant cluster. On the Google Cloud Console page for Google Kubernetes Engine, view the Workloads and filter on the cluster. Inspect the configurations of the filtered workloads.",C:"Use Anthos Config Management to create a namespaceSelector selecting the relevant cluster namespace. On the Google Cloud Console page for Google Kubernetes Engine, visit the workloads and filter on the namespace. Inspect the configurations of the filtered workloads.",D:"Reinstall istio using the default istio profile in order to collect request latency. Evaluate the telemetry between the microservices in the Cloud Console."},correctAnswer:["A"],explanation:{correct:"Anthos Service Mesh automatically collects detailed telemetry, including latency metrics, for all traffic between microservices. The Cloud Console provides rich visualizations of this data, including a service topology graph and latency dashboards. This is the primary tool for identifying performance bottlenecks in a service mesh by tracing request paths and pinpointing services with high latency.",incorrect:{B:"Anthos Config Management is for managing configuration and policies across clusters, not for performance monitoring.",C:"Anthos Config Management is for managing configuration and policies, not for performance monitoring.",D:"Reinstalling Istio is unnecessary and disruptive; Anthos Service Mesh already provides the required telemetry."}},conditions:["Application on Anthos clusters with multiple microservices","Anthos Service Mesh and Config Management are enabled","Application is responding very slowly","Need to identify the microservice causing the delay"],caseStudyId:null},{id:36,topic:"Compute",question:"Your operations team has asked you to help diagnose a performance issue in a production application that runs on Compute Engine. The application is dropping requests that reach it when under heavy load. The process list for affected instances shows a single application process that is consuming all available CPU, and autoscaling has reached the upper limit of instances. There is no abnormal load on any other related systems, including the database. You want to allow production traffic to be served again as quickly as possible. Which action should you recommend?",options:{A:"Change the autoscaling metric to agent.googleapis.com/memory/percent_used.",B:"Restart the affected instances on a staggered schedule.",C:"SSH to each instance and restart the application process.",D:"Increase the maximum number of instances in the autoscaling group."},correctAnswer:["D"],explanation:{correct:"The symptoms clearly indicate that the application is CPU-bound and has exhausted its configured scaling capacity. The autoscaler is trying to add more instances but is prevented by the maximum limit. The quickest way to restore service is to raise this limit, allowing the autoscaler to provision more instances and distribute the high CPU load across a larger pool of VMs.",incorrect:{A:"The current problem is CPU load, and the autoscaler has already hit its maximum limit. Changing the metric won't allow it to scale further.",B:"Restarting instances will not solve a sustained high-load issue if the root cause is insufficient capacity.",C:"Restarting the application process will have the same result as restarting the instance; it won't address the capacity shortfall."}},conditions:["Production application on Compute Engine is dropping requests under heavy load","Application process is consuming all available CPU","Autoscaling has reached the maximum number of instances","Need the quickest way to restore service"],caseStudyId:null},{id:37,topic:"Compute",question:"You are implementing the infrastructure for a web service on Google Cloud. The web service needs to receive and store the data from 500,000 requests per second. The data will be queried later in real time, based on exact matches of a known set of attributes. There will be periods when the web service will not receive any requests. The business wants to keep costs low. Which web service platform and database should you use for the application?",options:{A:"Cloud Run and BigQuery",B:"Cloud Run and Cloud Bigtable",C:"A Compute Engine autoscaling managed instance group and BigQuery",D:"A Compute Engine autoscaling managed instance group and Cloud Bigtable"},correctAnswer:["B"],explanation:{correct:"This combination is ideal for the requirements. Cloud Run is a serverless platform that scales to zero, making it extremely cost-effective during idle periods. Cloud Bigtable is a high-throughput NoSQL database designed for massive-scale ingestion (handling 500k req/sec) and low-latency, real-time queries based on row keys (which can be designed from the known attributes).",incorrect:{A:"BigQuery is an analytical data warehouse, not a database optimized for high-frequency transactional writes or low-latency point lookups.",C:"A Compute Engine MIG does not scale to zero, incurring costs during idle periods. BigQuery is not suitable for the database.",D:"A Compute Engine MIG does not scale to zero, which violates the low-cost requirement during idle periods."}},conditions:["Web service with 500,000 requests/sec ingestion rate","Real-time queries based on exact attribute matches","Periods of no requests","Need to keep costs low"],caseStudyId:null},{id:38,topic:"Compute",question:"Your company has a Kubernetes application that pulls messages from Pub/Sub and stores them in Filestore. Because the application is simple, it was deployed as a single pod. The infrastructure team has analyzed Pub/Sub metrics and discovered that the application cannot process the messages in real time. Most of them wait for minutes before being processed. You need to scale the elaboration process that is I/O-intensive. What should you do?",options:{A:"Use kubectl autoscale deployment APP_NAME --max 6 -min 2 --cpu-percent 50 to configure Kubernetes autoscaling deployment.",B:"Configure a Kubernetes autoscaling deployment based on the subscription/push_request_latencies metric.",C:"Use the --enable-autoscaling flag when you create the Kubernetes cluster.",D:"Configure a Kubernetes autoscaling deployment based on the subscription/num_undelivered_messages metric."},correctAnswer:["D"],explanation:{correct:"The problem is a message backlog, which is directly measured by the Pub/Sub metric `subscription/num_undelivered_messages`. To scale the processing pods based on the size of this backlog, you should configure a Horizontal Pod Autoscaler (HPA) with a custom metric based on this specific Pub/Sub metric. This will add more pods when the backlog grows and remove them as it shrinks.",incorrect:{A:"Scaling based on CPU is incorrect because the problem states the process is I/O-intensive and the bottleneck is a message backlog, not CPU utilization.",B:"Push request latency is a metric for push subscriptions and might not be the most direct indicator of a backlog in a pull-based system.",C:"This enables the Cluster Autoscaler, which scales the number of nodes in the cluster, not the number of application pods. You need to scale the pods first."}},conditions:["Kubernetes application processes Pub/Sub messages","Application is I/O-intensive (writes to Filestore)","Cannot process messages in real time, resulting in a backlog","Need to scale the processing application"],caseStudyId:null},{id:39,topic:"Compute",question:"Your company has an enterprise application running on Compute Engine that requires high availability and high performance. The application has been deployed on two instances in two zones in the same region in active-passive mode. The application writes data to a persistent disk. In the case of a single zone outage, that data should be immediately made available to the other instance in the other zone. You want to maximize performance while minimizing downtime and data loss. What should you do?",options:{A:"1. Attach a persistent SSD disk to the first instance. 2. Create a snapshot every hour. 3. In case of a zone outage, recreate a persistent SSD disk in the second instance where data is coming from the created snapshot.",B:"1. Create a Cloud Storage bucket. 2. Mount the bucket into the first instance with gcs-fuse. 3. In case of a zone outage, mount the Cloud Storage bucket to the second instance with gcs-fuse.",C:"1. Attach a regional SSD persistent disk to the first instance. 2. In case of a zone outage, force-attach the disk to the other instance.",D:"1. Attach a local SSD to the first instance disk. 2. Execute an rsync command every hour where the target is a persistent SSD disk attached to the second instance. 3. In case of a zone outage, use the second instance."},correctAnswer:["C"],explanation:{correct:"Regional Persistent Disks are designed for this exact use case. They synchronously replicate data between two zones in a region. In case of a failure of the primary instance in one zone, you can force-attach the same regional disk to the standby instance in the other zone. This provides a Recovery Point Objective (RPO) of zero (no data loss) and a low Recovery Time Objective (RTO), meeting the requirements for minimal downtime and data loss. Using an SSD variant ensures high performance.",incorrect:{A:"Snapshots are asynchronous and point-in-time. This would lead to data loss of up to an hour (RPO=1 hour) and a higher RTO due to the time needed to create a disk from the snapshot.",B:"gcsfuse is not suitable for high-performance, primary application data disks due to its performance characteristics and consistency model.",D:"Local SSDs are ephemeral, meaning data is lost if the instance stops or fails, making them unsuitable for persistent data. Rsync is asynchronous and would also lead to data loss."}},conditions:["Enterprise application on GCE requiring high availability and performance","Active-passive deployment across two zones","Data on persistent disk must be immediately available in other zone on failure","Maximize performance, minimize downtime and data loss"],caseStudyId:null},{id:40,topic:"Compute",question:"You are working at an institution that processes medical data. You are migrating several workloads onto Google Cloud. Company policies require all workloads to run on physically separated hardware, and workloads from different clients must also be separated. You created a sole-tenant node group and added a node for each client. You need to deploy the workloads on these dedicated hosts. What should you do?",options:{A:"Add the node group name as a network tag when creating Compute Engine instances in order to host each workload on the correct node group.",B:"Add the node name as a network tag when creating Compute Engine instances in order to host each workload on the correct node.",C:"Use node affinity labels based on the node group name when creating Compute Engine instances in order to host each workload on the correct node group.",D:"Use node affinity labels based on the node name when creating Compute Engine instances in order to host each workload on the correct node."},correctAnswer:["C"],explanation:{correct:"Sole-tenant nodes provide dedicated physical hardware. To control the placement of VMs onto these specific nodes, you use node affinity labels. By applying a unique label to each client's node group (e.g., `client=client-a`) and then specifying that same node affinity label when creating the client's VMs, you ensure that those VMs are scheduled only on the hardware dedicated to that client.",incorrect:{A:"Network tags are used for applying firewall rules and network routes, not for controlling VM placement on sole-tenant hardware.",B:"Network tags are for networking, not VM scheduling.",D:"While you can target a specific node name, it's more flexible and common to target a node group by its labels. This allows Compute Engine to place the VM on any available node within that group, providing more scheduling flexibility if you have multiple nodes per group."}},conditions:["Workloads must run on physically separated hardware","Workloads from different clients must be separated","Sole-tenant nodes have been created for this purpose","Need to deploy workloads to the correct dedicated hosts"],caseStudyId:null},{id:41,topic:"Compute",question:"Your company's test suite is a custom C++ application that runs tests throughout each day on Linux virtual machines. The full test suite takes several hours to complete, running on a limited number of on-premises servers reserved for testing. Your company wants to move the testing infrastructure to the cloud, to reduce the amount of time it takes to fully test a change to the system, while changing the tests as little as possible. Which cloud infrastructure should you recommend?",options:{A:"Google Compute Engine unmanaged instance groups and Network Load Balancer",B:"Google Compute Engine managed instance groups with auto-scaling",C:"Google Cloud Dataproc to run Apache Hadoop jobs to process each test",D:"Google App Engine with Google StackDriver for logging"},correctAnswer:["B"],explanation:{correct:"The best way to reduce test time is to parallelize the tests. A Managed Instance Group (MIG) with auto-scaling can spin up a large number of VMs on-demand to run tests in parallel and then scale down to zero when finished, which is both fast and cost-effective. This allows the existing C++ application to run on standard Linux VMs with minimal changes.",incorrect:{A:"Unmanaged instance groups lack the auto-scaling and auto-healing benefits that are crucial for an elastic and resilient testing environment.",C:"Cloud Dataproc is for Hadoop/Spark workloads and is not suitable for running a custom C++ test suite.",D:"App Engine is a PaaS for web applications and does not support running arbitrary C++ applications in this manner."}},conditions:["Custom C++ test suite running on Linux VMs","Current process is slow due to limited on-premises servers","Goal is to move to the cloud to reduce test time","Minimize changes to the existing test application"],caseStudyId:null},{id:42,topic:"Compute",question:"You want to make a copy of a production Linux virtual machine in the US-Central region. You want to manage and replace the copy easily if there are changes on the production virtual machine. You will deploy the copy as a new instance in a different project in the US-East region. What steps must you take?",options:{A:"Use the Linux dd and netcat commands to copy and stream the root disk contents to a new virtual machine instance in the US-East region",B:"Create a snapshot of the root disk and select the snapshot as the root disk when you create a new virtual machine instance in the US-East region.",C:"Create an image file from the root disk with Linux dd command, create a new virtual machine instance in the US-East region",D:"Create a snapshot of the root disk, create an image file in Google Cloud Storage from the snapshot, and create a new virtual machine instance in the US-East region using the image file the root disk."},correctAnswer:["D"],explanation:{correct:"This describes the standard and most manageable process for copying a VM across regions and projects. First, a snapshot creates a point-in-time backup. Then, creating a custom image from that snapshot makes it a reusable, cross-regional resource. Finally, you can easily create a new instance in any region or project (with appropriate permissions) from this custom image. This process is easily manageable and repeatable.",incorrect:{A:"Using `dd` and `netcat` is a manual, error-prone, and inefficient method for copying disks in a cloud environment.",B:"Snapshots are regional resources. You cannot directly create a VM in a different region from a snapshot in the source region. You must first create an image from the snapshot or copy the snapshot to the target region.",C:"This describes a manual process of creating an image file, which is less efficient and integrated than using GCP's native snapshot-to-image workflow."}},conditions:["Copy a production Linux VM from us-central","Deploy the copy to a different project in us-east","Need an easily manageable and replaceable process"],caseStudyId:null},{id:43,topic:"Compute",question:"You want to enable your running Google Kubernetes Engine cluster to scale as demand for your application changes. What should you do?",options:{A:"Add additional nodes to your Kubernetes Engine cluster using the following command: gcloud container clusters resize CLUSTER_NAME --size 10",B:"Add a tag to the instances in the cluster with the following command: gcloud compute instances add-tags INSTANCE --tags enable --autoscaling max-nodes-10",C:"Update the existing Kubernetes Engine cluster with the following command: gcloud alpha container clusters update mycluster --enable-autoscaling --min-nodes=1 -- max-nodes=10",D:"Create a new Kubernetes Engine cluster with the following command: gcloud alpha container clusters create mycluster --enable-autocaling --min-nodes=1 --max-nodes=10 and redeploy your application."},correctAnswer:["C"],explanation:{correct:"To enable the Cluster Autoscaler on an *existing* GKE cluster, you use the `gcloud container clusters update` command with the `--enable-autoscaling` flag and specify the scaling limits (`--min-nodes`, `--max-nodes`). This will allow GKE to automatically add or remove nodes based on the resource demands of your workloads (i.e., pending pods).",incorrect:{A:"The `resize` command manually sets a fixed number of nodes; it does not enable automatic scaling.",B:"You should not manage the autoscaling of GKE nodes at the Compute Engine instance level; this must be managed through the GKE control plane.",D:"This creates a new cluster, which is not what was asked. The question is about enabling scaling on a *running* cluster."}},conditions:["Have a running GKE cluster","Need to enable it to scale automatically based on demand"],caseStudyId:null},{id:44,topic:"Compute",question:"Your company's marketing department wants to send out a promotional email campaign. The development team wants to minimize direct operation management. They project a wide range of possible customer responses, from 100 to 500,000 click-throughs per day. The link leads to a simple website that explains the promotion and collects user information and preferences. Which infrastructure should you recommend?",options:{A:"Use Google App Engine to serve the website and Google Cloud Datastore to store user data.",B:"Use a Google Container Engine cluster to serve the website and store data to persistent disk.",C:"Use a managed instance group to serve the website and Google Cloud Bigtable to store user data.",D:"Use a single Compute Engine virtual machine (VM) to host a web server, backend by Google Cloud SQL."},correctAnswer:["A"],explanation:{correct:"This combination of fully managed, serverless services is ideal for the requirements. App Engine automatically handles scaling for the highly variable traffic with minimal operational management. Cloud Datastore (Firestore) is a scalable NoSQL database that is well-suited for storing user profile and preference data and integrates seamlessly with App Engine.",incorrect:{B:"GKE (Container Engine) requires more operational management than App Engine. Storing user data on a persistent disk is not a scalable database solution.",C:"A managed instance group requires more management than App Engine. Cloud Bigtable is designed for very large-scale analytical or operational workloads and would be overly complex and likely not cost-effective for this simple user data collection use case.",D:"A single VM cannot handle the potential peak load of 500,000 clicks and lacks reliability and scalability."}},conditions:["Promotional website and data collection form","Minimize operational management","Handle highly variable traffic (100 to 500,000 clicks/day)"],caseStudyId:null},{id:45,topic:"Compute",question:"Your company just finished a rapid lift and shift to Google Compute Engine for your compute needs. You have another 9 months to design and deploy a more cloud-native solution. Specifically, you want a system that is no-ops and auto-scaling. Which two compute products should you choose?",options:{A:"Compute Engine with containers",B:"Google Kubernetes Engine with containers",C:"Google App Engine Standard Environment",D:"Compute Engine with custom instance types",E:"Compute Engine with managed instance groups"},correctAnswer:["B","C"],explanation:{correct:"Both GKE and App Engine Standard Environment are excellent choices for a cloud-native, auto-scaling, and low-ops solution. App Engine (C) provides the most 'no-ops' experience, as it's a fully managed PaaS. GKE (B) offers more flexibility and control for containerized applications while still managing the control plane and providing robust auto-scaling for both pods and nodes, significantly reducing operational burden compared to raw VMs.",incorrect:{A:"Running containers on Compute Engine requires manual management of the container runtime and orchestration, which is not 'no-ops'.",D:"Custom instance types are a feature of Compute Engine, which is IaaS and requires significant operations.",E:"MIGs provide auto-scaling for VMs, but this is still an IaaS solution that requires managing the underlying instances, OS, and patching."}},conditions:["Design a cloud-native solution after a lift-and-shift","Requirement: 'no-ops' (minimal infrastructure management)","Requirement: auto-scaling"],caseStudyId:null},{id:46,topic:"Compute",question:"You want to enable your running Google Kubernetes Engine cluster to scale as demand for your application changes. What should you do?",options:{A:"Add additional nodes to your Kubernetes Engine cluster using the following command: gcloud container clusters resize CLUSTER_Name --size 10",B:"Add a tag to the instances in the cluster with the following command: gcloud compute instances add-tags INSTANCE --tags enable --autoscaling max-nodes-10",C:"Update the existing Kubernetes Engine cluster with the following command: gcloud alpha container clusters update mycluster --enable-autoscaling --min-nodes=1 -- max-nodes=10",D:"Create a new Kubernetes Engine cluster with the following command: gcloud alpha container clusters create mycluster --enable-autocaling --min-nodes=1 --max-nodes=10 and redeploy your application."},correctAnswer:["C"],explanation:{correct:"To enable the Cluster Autoscaler on an *existing* GKE cluster, you use the `gcloud container clusters update` command with the `--enable-autoscaling` flag and specify the scaling limits (`--min-nodes`, `--max-nodes`). This will allow GKE to automatically add or remove nodes based on the resource demands of your workloads (i.e., pending pods).",incorrect:{A:"The `resize` command manually sets a fixed number of nodes; it does not enable automatic scaling.",B:"You should not manage the autoscaling of GKE nodes at the Compute Engine instance level; this must be managed through the GKE control plane.",D:"This creates a new cluster, which is not what was asked. The question is about enabling scaling on a *running* cluster."}},conditions:["Running GKE cluster","Enable autoscaling based on demand"],caseStudyId:null},{id:47,topic:"Compute",question:"Your marketing department wants to send out a promotional email campaign. The development team wants to minimize direct operation management. They project a wide range of possible customer responses, from 100 to 500,000 click-throughs per day. The link leads to a simple website that explains the promotion and collects user information and preferences. Which infrastructure should you recommend?",options:{A:"Use Google App Engine to serve the website and Google Cloud Datastore to store user data.",B:"Use a Google Container Engine cluster to serve the website and store data to persistent disk.",C:"Use a managed instance group to serve the website and Google Cloud Bigtable to store user data.",D:"Use a single Compute Engine virtual machine (VM) to host a web server, backend by Google Cloud SQL."},correctAnswer:["A"],explanation:{correct:"This combination of fully managed, serverless services is ideal for the requirements. App Engine automatically handles scaling for the highly variable traffic with minimal operational management. Cloud Datastore (Firestore) is a scalable NoSQL database that is well-suited for storing user profile and preference data and integrates seamlessly with App Engine.",incorrect:{B:"GKE (Container Engine) requires more operational management than App Engine. Storing user data on a persistent disk is not a scalable database solution.",C:"A managed instance group requires more management than App Engine. Cloud Bigtable is designed for very large-scale analytical or operational workloads and would be overly complex and likely not cost-effective for this simple user data collection use case.",D:"A single VM cannot handle the potential peak load of 500,000 clicks and lacks reliability and scalability."}},conditions:["Promotional website and data collection form","Minimize operational management","Highly variable traffic (100 - 500,000 clicks/day)"],caseStudyId:null},{id:48,topic:"Compute",question:"You have a Compute Engine managed instance group that adds and removes Compute Engine instances from the group in response to the load of your application. The instances have a shutdown script that removes REDIS database entries associated with the instance. You see that many database entries have not been removed, and you suspect that the shutdown script is the problem. You need to ensure that the commands in the shutdown script are run reliably every time an instance is shut down. You create a Cloud Function to remove the database entries. What should you do next?",options:{A:"Modify the shutdown script to wait for 30 seconds before triggering the Cloud Function.",B:"Do not use the Cloud Function. Modify the shutdown script to restart if it has not completed in 30 seconds.",C:"Set up a Cloud Monitoring sink that triggers the Cloud Function after an instance removal log message arrives in Cloud Logging.",D:"Modify the shutdown script to wait for 30 seconds and then publish a message to a Pub/Sub queue."},correctAnswer:["C"],explanation:{correct:"Shutdown scripts can be unreliable because they have a limited execution window and can be preempted. A more robust, event-driven approach is to react to the instance deletion event itself. An instance removal action is captured in Cloud Audit Logs, which are sent to Cloud Logging. You can create a log sink that filters for these specific log entries and sends them to a Pub/Sub topic, which in turn triggers the Cloud Function to perform the cleanup. This decouples the cleanup logic from the lifecycle of the instance itself, making it much more reliable.",incorrect:{A:"This still relies on the unreliable shutdown script to initiate the action.",B:"This doesn't solve the core problem that the shutdown script might be terminated before it can complete or restart.",D:"This also relies on the unreliable shutdown script to publish the message."}},conditions:["Autoscaled MIG","Unreliable shutdown script for Redis cleanup","Need reliable cleanup","Cloud Function created for cleanup"],caseStudyId:null},{id:49,topic:"Compute",question:"You are managing several projects on Google Cloud and need to interact on a daily basis with BigQuery, Bigtable, and Kubernetes Engine using the gcloud CLI tool. You are travelling a lot and work on different workstations during the week. You want to avoid having to manage the gcloud CLI manually. What should you do?",options:{A:"Use Google Cloud Shell in the Google Cloud Console to interact with Google Cloud.",B:"Create a Compute Engine instance and install gcloud on the instance. Connect to this instance via SSH to always use the same gcloud installation when interacting with Google Cloud.",C:"Install gcloud on all of your workstations. Run the command gcloud components auto-update on each workstation",D:"Use a package manager to install gcloud on your workstations instead of installing it manually."},correctAnswer:["A"],explanation:{correct:"Cloud Shell is the perfect solution for this use case. It provides a consistent, browser-based command-line environment that is accessible from anywhere. It comes pre-installed with the gcloud CLI and other necessary tools, is always authenticated with your console credentials, and is managed and updated by Google. This completely eliminates the need to install, configure, or manage the CLI on different workstations.",incorrect:{B:"This creates an unnecessary extra step (connecting to a VM) and incurs the cost of running the VM. Cloud Shell is free.",C:"This is exactly the manual management that the user wants to avoid.",D:"Using a package manager simplifies installation but doesn't solve the problem of managing configurations and authentications across multiple different workstations."}},conditions:["Manage multiple GCP projects","Daily gcloud use (BQ, BT, GKE)","Use different workstations","Avoid manual gcloud management"],caseStudyId:null},{id:50,topic:"Compute",question:"You are managing several internal applications that are deployed on Compute Engine. Business users inform you that an application has become very slow over the past few days. You want to find the underlying cause in order to solve the problem. What should you do first?",options:{A:"Inspect the logs and metrics from the instances in Cloud Logging and Cloud Monitoring.",B:"Change the Compute Engine Instances behind the application to a machine type with more CPU and memory.",C:"Restore a backup of the application database from a time before the application became slow.",D:"Deploy the applications on a managed instance group with autoscaling enabled. Add a load balancer in front of the managed instance group, and have the users connect to the IP of the load balancer."},correctAnswer:["A"],explanation:{correct:"The first and most critical step in troubleshooting any performance issue is to gather data. Inspecting logs (for errors or unusual activity) and metrics (CPU, memory, disk I/O, network) in Cloud Logging and Monitoring will provide visibility into what the system is doing and help identify the bottleneck or root cause. Only after a proper diagnosis should you take corrective action.",incorrect:{B:"This is a potential solution, but making this change without diagnosis is just guessing. The problem might not be CPU or memory.",C:"This is a drastic action that could cause data loss and should only be considered if database corruption is identified as the cause.",D:"Re-architecting the deployment is a significant change that should only be done after understanding the root cause of the current performance issue."}},conditions:["Internal GCE app became slow","Find the cause"],caseStudyId:null},{id:51,topic:"Compute",question:"Your company has an application running as a Deployment in a Google Kubernetes Engine (GKE) cluster. When releasing new versions of the application via a rolling deployment, the team has been causing outages. The root cause of the outages is misconfigurations with parameters that are only used in production. You want to put preventive measures for this in the platform to prevent outages. What should you do?",options:{A:"Configure liveness and readiness probes in the Pod specification.",B:"Configure health checks on the managed instance group.",C:"Create a Scheduled Task to check whether the application is available.",D:"Configure an uptime alert in Cloud Monitoring."},correctAnswer:["A"],explanation:{correct:"Readiness probes are a core Kubernetes feature designed to prevent this exact problem. During a rolling update, Kubernetes will not direct traffic to a new pod until its readiness probe passes. If a misconfiguration prevents the new version of the application from starting correctly, the readiness probe will fail, and the faulty pod will never receive production traffic, thus preventing an outage.",incorrect:{B:"Health checks on the managed instance group monitor the health of the underlying nodes (VMs), not the application running inside the GKE pods.",C:"A scheduled task is an external check that is reactive, not a preventative measure integrated into the deployment process.",D:"A Cloud Monitoring alert is also reactive. It will notify you that an outage has occurred but will not prevent it from happening."}},conditions:["GKE Deployment","Rolling deployments cause outages due to misconfiguration","Need preventative measure"],caseStudyId:null},{id:52,topic:"Compute",question:"Your company uses Google Kubernetes Engine (GKE) as a platform for all workloads. Your company has a single large GKE cluster that contains batch, stateful, and stateless workloads. The GKE cluster is configured with a single node pool with 200 nodes. Your company needs to reduce the cost of this cluster but does not want to compromise availability. What should you do?",options:{A:"Create a second GKE cluster for the batch workloads only. Allocate the 200 original nodes across both clusters.",B:"Configure CPU and memory limits on the namespaces in the cluster. Configure all Pods to have a CPU and memory limits.",C:"Configure a HorizontalPodAutoscaler for all stateless workloads and for all compatible stateful workloads. Configure the cluster to use node auto scaling.",D:"Change the node pool to use spot VMs."},correctAnswer:["C"],explanation:{correct:"This is the most effective strategy for cost optimization without compromising availability. The Horizontal Pod Autoscaler (HPA) scales the number of application pods based on load, ensuring you only run as many as you need. The Cluster Autoscaler (node auto scaling) scales the number of underlying nodes based on the number of pods that need to be scheduled. This combination ensures that both application and infrastructure resources dynamically match the actual demand, scaling down to save costs during low-traffic periods.",incorrect:{A:"Splitting the cluster adds management overhead and doesn't inherently reduce costs without also implementing autoscaling.",B:"Configuring requests and limits is a best practice for resource management and stability, but it doesn't dynamically scale down the infrastructure to save costs.",D:"Using Spot VMs (preemptible) can significantly reduce costs but is not suitable for all workloads, especially stateful ones, as it can compromise availability."}},conditions:["Single large GKE cluster (200 nodes)","Mixed workloads","Reduce cost, maintain availability"],caseStudyId:null},{id:53,topic:"Compute",question:"You have a Compute Engine application that you want to autoscale when total memory usage exceeds 80%. You have installed the Cloud Monitoring agent and configured the autoscaling policy as follows: Metric identifier: agent.googleapis.com/memory/percent_used, Filter: metric.label.state = used, Target utilization level: 80, Target type: GAUGE. You observe that the application does not scale under high load. You want to resolve this. What should you do?",options:{A:"Change the Target type to DELTA_PER_MINUTE.",B:"Change the Metric identifier to agent.googleapis.com/memory/bytes_used.",C:"Change the filter to metric.label.state = used AND metric.label.state = buffered AND metric.label.state = cached AND metric.label.state = slab.",D:"Change the filter to metric.label.state = free and the Target utilization to 20."},correctAnswer:["C"],explanation:{correct:"On Linux systems, the 'used' memory metric alone is often misleading. Memory used for buffers and cache is reclaimable by the OS when applications need it. To get an accurate picture of memory pressure, you need to consider the memory that is truly unavailable. The most accurate agent metric for this is often `agent.googleapis.com/memory/utilization`, which typically includes used, buffered, and cached memory. Since that metric isn't an option, the next best thing is to construct a filter that approximates it by including these other states. A filter that looks at 'used', 'buffered', 'cached', and 'slab' memory provides a much more accurate representation of total memory consumption, which will allow the autoscaler to trigger correctly.",incorrect:{A:"The target type GAUGE is correct for a utilization percentage. DELTA is for tracking rates of change.",B:"Changing from percent to bytes doesn't solve the fundamental problem of which memory states are being measured.",D:"While scaling on low free memory is a valid strategy, it's often more intuitive to scale on high *used* memory. The core issue is the definition of 'used', which C addresses most directly."}},conditions:["Autoscale GCE app when memory usage > 80%","Monitoring agent installed","Autoscaler configured with `percent_used` metric, filter `state='used'`, target 80%","App fails to scale under high load"],caseStudyId:null},{id:54,topic:"Compute",question:"Your company is planning to migrate their Windows Server 2022 from their on-premises data center to Google Cloud. You need to bring the licenses that are currently in use in on-premises virtual machines into the target cloud environment. What should you do?",options:{A:"1. Create an image of the on-premises virtual machines and upload into Cloud Storage. 2. Import the image as a virtual disk on Compute Engine.",B:"1. Create standard instances on Compute Engine. 2. Select as the OS the same Microsoft Windows version that is currently in use in the on-premises environment.",C:"1. Create an image of the on-premises virtual machine. 2. Import the image as a virtual disk on Compute Engine. 3. Create a standard instance on Compute Engine, selecting as the OS the same Microsoft Windows version that is currently in use in the on-premises environment. 4. Attach a data disk that includes data that matches the created image.",D:"1. Create an image of the on-premises virtual machines. 2. Import the image as a virtual disk on Compute Engine using --os=windows-2022-dc-v. 3. Create a sole-tenancy instance on Compute Engine that uses the imported disk as a boot disk."},correctAnswer:["D"],explanation:{correct:"Bringing your own license (BYOL) for Windows Server to Google Cloud requires using sole-tenant nodes. These are physical servers dedicated to your project, which is a requirement for license compliance. The process involves importing your on-premises image and then creating an instance from that image on a sole-tenant node.",incorrect:{A:"This process is incomplete and doesn't address the licensing requirement of using sole-tenancy.",B:"This approach uses Google's pay-as-you-go licenses, not your own.",C:"This describes a confusing process and still uses standard instances, which is not compliant with BYOL for Windows Server."}},conditions:["Migrate Windows Server 2022 from on-premises","Bring existing licenses (BYOL)"],caseStudyId:null},{id:55,topic:"Compute",question:"A few days after JencoMart migrates the user credentials database to Google Cloud and shuts down the old server, the new database server stops responding to SSH connections. It is still serving database requests to the application servers correctly. What three steps should you take to diagnose the problem?",options:{A:"Delete the virtual machine (VM) and disks and create a new one",B:"Delete the instance, attach the disk to a new VM, and investigate",C:"Take a snapshot of the disk and connect to a new machine to investigate",D:"Check inbound firewall rules for the network the machine is connected to",E:"Connect the machine to another network with very simple firewall rules and investigate",F:"Print the Serial Console output for the instance for troubleshooting, activate the interactive console, and investigate"},correctAnswer:["C","D","F"],explanation:{correct:"D and F are the least disruptive initial steps. Checking firewall rules (D) is a common cause for connectivity issues. The Serial Console (F) provides out-of-band access to check the OS status, SSH daemon logs, and resource usage even if networking is down. If these fail, creating a non-disruptive snapshot (C) and attaching it to a new VM for offline analysis is a sound next step to investigate the filesystem without touching the running (but partially failed) instance.",incorrect:{A:"This is a destructive action that prevents any diagnosis of the root cause.",B:"Deleting the instance is also destructive and should only be done after investigation, if at all.",E:"Moving the machine to another network is a more complex operation than simply checking the firewall rules on the existing network."}},conditions:["Migrated PostgreSQL credentials DB to GCE","New DB server is unresponsive to SSH","DB server is still serving application requests correctly","Need to diagnose the SSH issue"],caseStudyId:"cs_jencostore"},{id:56,topic:"Compute",question:"A recent finance audit of cloud infrastructure noted an exceptionally high number of Compute Engine instances are allocated to do video encoding and transcoding. You suspect that these Virtual Machines are zombie machines that were not deleted after their workloads completed. You need to quickly get a list of which VM instances are idle.",options:{A:"Log into each Compute Engine instance and collect disk, CPU, memory, and network usage statistics for analysis.",B:"Use the gcloud compute instances list to list the virtual machine instances that have the idle: true label set.",C:"Use the gcloud recommender command to list the idle virtual machine instances.",D:"From the Google Console, identify which Compute Engine instances in the managed instance groups are no longer responding to health check probes."},correctAnswer:["C"],explanation:{correct:"Google Cloud's Active Assist includes the Recommender service, which automatically analyzes resource usage. The Idle VM Recommender specifically identifies instances with low CPU and network utilization over a period of time. Using the `gcloud recommender recommendations list` command (or viewing recommendations in the console) is the fastest and most automated way to get this list.",incorrect:{A:"This is a manual, time-consuming, and inefficient process for a large number of instances.",B:"This relies on instances having been manually labeled as 'idle', which is unlikely to have happened for 'zombie' machines.",D:"Unhealthy instances are different from idle instances. An idle instance could be perfectly healthy but simply not doing any work."}},conditions:["High number of GCE instances used for video encoding","Suspect 'zombie' (idle) VMs were not deleted after jobs completed","Need a quick way to list idle VMs"],caseStudyId:"cs_hrl"},{id:57,topic:"Compute",question:"You are responsible for designing the Google Cloud network architecture for Google Kubernetes Engine. You want to follow Google best practices. Considering the EHR Healthcare business and technical requirements, what should you do to reduce the attack surface?",options:{A:"Use a private cluster with a private endpoint with master authorized networks configured.",B:"Use a public cluster with firewall rules and Virtual Private Cloud (VPC) routes.",C:"Use a private cluster with a public endpoint with master authorized networks configured.",D:"Use a public cluster with master authorized networks enabled and firewall rules."},correctAnswer:["A"],explanation:{correct:"This option combines multiple security layers for maximum attack surface reduction. A private cluster ensures nodes have no public IPs. A private endpoint makes the GKE control plane accessible only within the VPC. Master authorized networks further restrict access to the private endpoint to only whitelisted internal IP ranges. This is the most secure configuration.",incorrect:{B:"A public cluster exposes nodes and the master to the internet, creating a large attack surface.",C:"A private cluster with a public endpoint still exposes the control plane to the internet, which is less secure than a private endpoint.",D:"A public cluster with master authorized networks is better than no restrictions, but the master endpoint is still publicly accessible, making it more vulnerable than a private endpoint."}},conditions:["Design GKE network architecture","Follow Google best practices","Reduce the attack surface","Healthcare context implies high security needs"],caseStudyId:"cs_ehr_healthcare"},{id:58,topic:"Compute",question:"Mountkirk Games gaming servers are not automatically scaling properly. Last month, they rolled out a new feature, which suddenly became very popular. A record number of users are trying to use the service, but many of them are getting 503 errors and very slow response times. What should they investigate first?",options:{A:"Verify that the database is online",B:"Verify that the project quota hasn't been exceeded",C:"Verify that the new feature code did not introduce any performance bugs",D:"Verify that the load-testing team is not running their tool against production"},correctAnswer:["B"],explanation:{correct:"When an autoscaler fails to add new instances during a legitimate traffic spike, a common and easily verifiable cause is hitting a resource quota. This could be a quota on the number of VMs, CPUs, IP addresses, or other resources in the region. If the quota is exhausted, the autoscaler cannot provision new resources, leading to system overload and 503 errors. This should be one of the first things to check.",incorrect:{A:"While a database issue could cause errors, the primary symptom described is a failure to scale, which points more towards an infrastructure constraint.",C:"Investigating code for performance bugs is a necessary step, but it's often more time-consuming than quickly checking project quotas for hard limits.",D:"While possible, this is less likely than a quota issue triggered by a real, sudden increase in popularity."}},conditions:["Autoscaled game servers are failing to scale up","Sudden spike in user popularity","Symptoms include 503 errors and slow responses","Need to determine the first thing to investigate"],caseStudyId:"cs_mountkirk_main"},{id:59,topic:"Compute",question:"You need to analyze and define the technical architecture for the compute workloads for your company, Mountkirk Games. Considering the Mountkirk Games business and technical requirements, what should you do?",options:{A:"Create network load balancers. Use preemptible Compute Engine instances.",B:"Create network load balancers. Use non-preemptible Compute Engine instances.",C:"Create a global load balancer with managed instance groups and autoscaling policies. Use preemptible Compute Engine instances.",D:"Create a global load balancer with managed instance groups and autoscaling policies. Use non-preemptible Compute Engine instances."},correctAnswer:["D"],explanation:{correct:"This architecture meets all the key requirements. A global load balancer supports the global footprint and reduces latency. Managed instance groups with autoscaling policies provide the required dynamic scaling. Non-preemptible instances are essential for a production game backend where uptime is critical ('downtime is loss of players').",incorrect:{A:"Network load balancers are regional, not global. Preemptible instances are not suitable for a production, user-facing workload that requires high uptime.",B:"Network load balancers are regional, which doesn't meet the global footprint and low latency goals as well as a global load balancer.",C:"Preemptible instances compromise uptime, which is a critical business requirement."}},conditions:["Define compute workload architecture","Business Requirements: global footprint, improve uptime, reduce latency","Technical requirements: dynamically scale up or down based on game activity"],caseStudyId:"cs_mountkirk_main"},{id:60,topic:"Compute",question:"You are in charge of the new Game Backend Platform architecture. The game communicates with the backend over a REST API. You want to follow Google-recommended practices. How should you design the backend?",options:{A:"Create an instance template for the backend. For every region, deploy it on a multi-zone managed instance group. Use an L4 load balancer",B:"Create an instance template for the backend. For every region, deploy it on a single-zone managed instance group. Use an L4 load balancer",C:"Create an instance template for the backend. For every region, deploy it on a multi-zone managed instance group. Use an L7 load balancer",D:"Create an instance template for the backend. For every region, deploy it on a single-zone managed instance group. Use an L7 load balancer"},correctAnswer:["C"],explanation:{correct:"This design follows best practices for a resilient and scalable REST API on Compute Engine. Using a multi-zone managed instance group provides high availability against zonal failures. An L7 (HTTP/S) load balancer is the appropriate choice for a REST API, providing features like path-based routing, SSL offload, and integration with services like Cloud Armor and CDN.",incorrect:{A:"An L4 (Network) load balancer is not ideal for a REST API as it lacks L7 features.",B:"A single-zone MIG is not resilient to zonal failures. An L4 load balancer is not ideal.",D:"A single-zone MIG is not resilient to zonal failures."}},conditions:["Design Game Backend Platform architecture for a REST API","Follow Google-recommended practices for resilience and scalability"],caseStudyId:"cs_mountkirk_main"},{id:61,topic:"Compute",question:"You have an application that will run on Compute Engine. You need to design an architecture that takes into account a disaster recovery plan that requires your application to fail over to another region in case of a regional outage. What should you do?",options:{A:"Deploy the application on two Compute Engine instances in the same project but in a different region. Use the first instance to serve traffic, and use the HTTP load balancing service to fail over to the standby instance in case of a disaster.",B:"Deploy the application on a Compute Engine instance. Use the instance to serve traffic, and use the HTTP load balancing service to fail over to an instance on your premises in case of a disaster.",C:"Deploy the application on two Compute Engine instance groups, each in the same project but in a different region. Use the first instance group to serve traffic, and use the HTTP load balancing service to fail over to the standby instance group in case of a disaster.",D:"Deploy the application on two Compute Engine instance groups, each in separate project and a different region. Use the first instance group to server traffic, and use the HTTP load balancing service to fail over to the standby instance in case of a disaster."},correctAnswer:["C"],explanation:{correct:"This describes the standard architecture for regional high availability. By deploying Managed Instance Groups (MIGs) in two different regions and placing them behind a Global HTTP(S) Load Balancer, you achieve both scalability within each region and resilience against a complete regional outage. The load balancer's health checks will detect the failure of the primary region's instance group and automatically redirect traffic to the healthy standby group in the other region.",incorrect:{A:"Using single instances instead of instance groups is not resilient or scalable.",B:"This describes a hybrid DR scenario, not a multi-region GCP architecture.",D:"While possible, using separate projects adds unnecessary complexity for a single application's DR plan unless required by specific organizational policies."}},conditions:["Application on Compute Engine","Need a disaster recovery plan for regional failover"],caseStudyId:null},{id:62,topic:"Compute",question:"Dress4Win has asked you to recommend machine types they should deploy their application servers to. How should you proceed?",options:{A:"Perform a mapping of the on-premises physical hardware cores and RAM to the nearest machine types in the cloud.",B:"Recommend that Dress4Win deploy application servers to machine types that offer the highest RAM to CPU ratio available.",C:"Recommend that Dress4Win deploy into production with the smallest instances available, monitor them over time, and scale the machine type up until the desired performance is reached.",D:"Identify the number of virtual cores and RAM associated with the application server virtual machines align them to a custom machine type in the cloud, monitor performance, and scale the machine types up until the desired performance is reached."},correctAnswer:["D"],explanation:{correct:"This is the most methodical and cost-effective approach. Start by matching the current (virtual) resource allocation to a custom machine type to avoid initial overprovisioning. Then, deploy the application, monitor its actual performance and resource consumption under load, and use the data (and GCP's Rightsizing Recommendations) to iteratively adjust the machine type until the optimal balance of performance and cost is found. This aligns with the business requirement to 'analyze and optimize architecture for performance in the cloud.'",incorrect:{A:"Mapping from physical hardware often leads to overprovisioning, as on-premises physical servers are typically purchased with significant headroom.",B:"Choosing the highest RAM-to-CPU ratio is arbitrary and may not match the application's actual needs, leading to wasted resources.",C:"Starting with the smallest instances may lead to poor performance from the outset and a lengthy and disruptive tuning process if the application has significant baseline resource requirements."}},conditions:["Recommend GCE machine types for web app servers","Business requirement: Analyze and optimize for performance in the cloud"],caseStudyId:"cs_dress4win_v1"},{id:63,topic:"Compute",question:"Which of the compute services should be migrated as-is and would still be an optimized architecture for performance in the cloud?",options:{A:"Web applications deployed using App Engine standard environment",B:"RabbitMQ deployed using an unmanaged instance group",C:"Hadoop/Spark deployed using Cloud Dataproc Regional in High Availability mode",D:"Jenkins, monitoring, bastion hosts, security scanners services deployed on custom machine types"},correctAnswer:["C"],explanation:{correct:"Cloud Dataproc is Google's managed Hadoop and Spark service. It is designed to run existing Hadoop/Spark jobs with minimal or no changes ('as-is'). Migrating on-premises Hadoop/Spark workloads to Dataproc is considered an optimized architecture because Dataproc provides a scalable, performant, and managed environment specifically for these types of jobs, often with better performance than self-managed clusters.",incorrect:{A:"Migrating existing Tomcat/Java microservices to App Engine Standard is a significant re-architecture, not a migration 'as-is'.",B:"Deploying RabbitMQ on an unmanaged instance group is not an optimized architecture. The cloud-native, optimized approach would be to migrate to a managed service like Cloud Pub/Sub.",D:"Simply lifting and shifting miscellaneous servers to GCE instances is not inherently an 'optimized architecture for performance' without further analysis and potential use of managed alternatives (e.g., Cloud Build, Cloud Monitoring)."}},conditions:["Identify a compute service to migrate 'as-is'","The resulting cloud architecture must be optimized for performance"],caseStudyId:"cs_dress4win_v1"},{id:64,topic:"Compute",question:"You are managing a Google Kubernetes Engine cluster where developers deploy multiple applications. You want to ensure that specific applications are only scheduled on nodes that have a particular CPU type (e.g., Intel Broadwell). What Kubernetes feature should you use?",options:{A:"Node Affinity",B:"Network Policies",C:"Taints and Tolerations",D:"Pod Security Policies"},correctAnswer:["A"],explanation:{correct:"Node Affinity is the Kubernetes feature specifically designed to attract Pods to a set of nodes based on labels on those nodes. You would label the nodes with the desired CPU type and then add a `nodeAffinity` rule to your application's Pod specification to require scheduling on nodes with that label.",incorrect:{B:"Network Policies are for controlling network traffic between Pods.",C:"Taints and Tolerations are used to repel Pods from nodes. While they can be used to achieve a similar outcome, Node Affinity is the more direct and idiomatic way to express an attraction to a specific type of node.",D:"Pod Security Policies (now deprecated and replaced by Pod Security Admission) are for enforcing security constraints on Pods, not for controlling scheduling based on node hardware."}},conditions:["GKE cluster with multiple applications","Need to schedule specific applications only on nodes with a particular CPU type"],caseStudyId:null},{id:65,topic:"Compute",question:"You need to run a batch processing job once a week that requires significant compute resources (e.g., 64 vCPUs, 256 GB RAM) for approximately 4 hours. The job is fault-tolerant and can be restarted if interrupted. You want to minimize the cost of running this job. Which Compute Engine purchasing option is most suitable?",options:{A:"On-demand instances",B:"Spot VMs (or Preemptible VMs)",C:"1-year Committed Use Discounts",D:"3-year Committed Use Discounts"},correctAnswer:["B"],explanation:{correct:"Spot VMs (formerly Preemptible VMs) offer the largest discounts (up to 91%) on Compute Engine instances. They are ideal for fault-tolerant and non-time-critical batch jobs because they can be reclaimed (preempted) by Google Cloud. Since the job is fault-tolerant and can be restarted, using Spot VMs will provide the most significant cost savings.",incorrect:{A:"On-demand instances are the most expensive option and provide no discount.",C:"Committed Use Discounts are for workloads with consistent, predictable usage. Committing to use 64 vCPUs for a full year to run a 4-hour weekly job would be extremely cost-inefficient.",D:"A 3-year commitment is even less suitable for this infrequent workload than a 1-year commitment."}},conditions:["Weekly batch processing job","High compute resources needed","Short duration (~4 hours)","Job is fault-tolerant and can be restarted","Goal is to minimize cost"],caseStudyId:null},{id:66,topic:"Compute",question:"You are deploying a stateful application (e.g., a traditional database) onto a single Compute Engine instance. High availability is critical, and you need the instance to automatically restart on a different physical host within the same zone if the original host undergoes maintenance or fails. What setting should you configure for the instance?",options:{A:"Enable automatic restart in the Availability policies.",B:"Set the scheduling onHostMaintenance policy to 'MIGRATE'.",C:"Configure the instance as part of a Managed Instance Group with size 1.",D:"Attach a Regional Persistent Disk."},correctAnswer:["B"],explanation:{correct:"Setting the `onHostMaintenance` policy to 'MIGRATE' enables live migration. During planned host maintenance, Google will automatically move your running VM to another host in the same zone with minimal disruption. This is the primary mechanism for maintaining availability during scheduled events. The 'automatic restart' policy (which is on by default) handles restarts after unexpected hardware failures.",incorrect:{A:"Automatic restart is for recovering from unexpected failures, while the 'MIGRATE' policy proactively handles planned maintenance, which is a key part of HA.",C:"A MIG with size 1 provides autohealing (recreation on failure) but live migration is generally a more graceful way to handle host maintenance for a single stateful instance.",D:"A Regional Persistent Disk provides data durability across zones but does not control the behavior of the VM instance itself during a host failure or maintenance event within a single zone."}},conditions:["Stateful application on a single GCE instance","High availability is critical","Need automatic restart/migration if the host fails or undergoes maintenance"],caseStudyId:null},{id:67,topic:"Compute",question:"You want to prevent accidental deletion of a critical Compute Engine instance that runs a legacy workload. While the instance data is backed up, recreating and configuring the instance is a time-consuming process you want to avoid. What is the most effective way to protect this specific instance from accidental deletion?",options:{A:"Apply the compute.instances.delete IAM permission deny policy directly to the instance.",B:"Remove the Project Editor/Owner roles from users who might accidentally delete it.",C:"Enable deletion protection on the Compute Engine instance.",D:"Store the instance configuration in Terraform and use state locking."},correctAnswer:["C"],explanation:{correct:"Compute Engine provides a specific 'deletion protection' flag for VM instances. When this flag is enabled, any attempt to delete the instance via the API, gcloud CLI, or console will fail. To delete the instance, an authorized user must first explicitly disable this protection. This provides a direct and effective safeguard against accidental deletion.",incorrect:{A:"IAM Deny policies are a more complex feature and may not be as straightforward to apply for this specific use case as the built-in deletion protection flag.",B:"Removing broad roles is good practice for least privilege but doesn't specifically protect one instance from a user who still has legitimate (but perhaps more granular) deletion permissions.",D:"Terraform state locking prevents concurrent modifications but does not prevent an authorized user from intentionally or unintentionally running `terraform destroy` or from deleting the instance via other means."}},conditions:["Prevent accidental deletion of a specific critical GCE instance","Recreating the instance is time-consuming"],caseStudyId:null},{id:68,topic:"Compute",question:"Your existing application runs on Ubuntu Linux VMs in an on-premises hypervisor. You want to deploy the application to Google Cloud with minimal refactoring. What should you do?",options:{A:"Set up a Google Kubernetes Engine (GKE) cluster, and then create a deployment with an autoscaler.",B:"Isolate the core features that the application provides. Use Cloud Run to deploy each feature independently as a microservice.",C:"Use Dedicated or Partner Interconnect to connect the on-premises network where your application is running to your VPC. Configure an endpoint for a global external Application Load Balancer that connects to the existing VMs.",D:"Write Terraform scripts to deploy the application as Compute Engine instances."},correctAnswer:["D"],explanation:{correct:"This describes a classic 'lift and shift' migration. The most direct path with minimal refactoring is to move the application from on-premises VMs to Compute Engine VMs. Using an Infrastructure as Code (IaC) tool like Terraform to define and deploy these instances is a best practice for creating a repeatable and manageable cloud environment.",incorrect:{A:"Migrating to GKE requires containerizing the application, which is more than 'minimal refactoring'.",B:"Re-architecting the application into microservices for Cloud Run is a major refactoring effort, not a lift and shift.",C:"This describes a hybrid architecture where the application remains on-premises, which is not a deployment *to* Google Cloud."}},conditions:["Existing application on on-premises Ubuntu VMs","Goal is to deploy to Google Cloud","Requirement is minimal refactoring"],caseStudyId:null},{id:69,topic:"Compute",question:"Cymbal Direct wants a layered approach to security when setting up Compute Engine instances. What are some options you could use to make your Compute Engine instances more secure?",options:{A:"Use labels to allow traffic only from certain sources and ports. Turn on Secure boot and vTPM.",B:"Use labels to allow traffic only from certain sources and ports. Use a Compute Engine service account.",C:"Use network tags to allow traffic only from certain sources and ports. Turn on Secure boot and vTPM.",D:"Use network tags to allow traffic only from certain sources and ports. Use a Compute Engine service account."},correctAnswer:["C"],explanation:{correct:"This option provides two distinct and strong layers of security. Network tags are used to apply VPC firewall rules, controlling network access (Layer 3/4). Secure Boot and vTPM are features of Shielded VMs that provide boot-level and runtime integrity verification, protecting against rootkits and bootkits. This combination secures both the network and the instance's software integrity.",incorrect:{A:"Labels are for organization and billing; network tags are used for firewall rules.",B:"Labels are incorrect for firewalls. Service accounts are for identity and authorization, which is another layer of security, but Secure Boot/vTPM provides a more fundamental instance integrity layer.",D:"While using a service account is a critical security practice, Secure Boot and vTPM provide a more foundational layer of security for the instance itself, which aligns well with a 'layered approach' starting from the hardware up."}},conditions:["Implement a layered security approach for Compute Engine instances"],caseStudyId:"CS001"},{id:70,topic:"Compute",question:"You have deployed your frontend web application in Kubernetes. Based on historical use, you need three pods to handle normal demand. Occasionally your load will roughly double. A load balancer is already in place. How could you configure your environment to efficiently meet that demand?",options:{A:"Edit your pod's configuration file and change the number of replicas to six.",B:"Edit your deployment's configuration file and change the number of replicas to six.",C:"Use the 'kubectl autoscale' command to change the pod's maximum number of instances to six.",D:"Use the 'kubectl autoscale' command to change the deployment's maximum number of instances to six."},correctAnswer:["D"],explanation:{correct:"The `kubectl autoscale deployment` command creates a HorizontalPodAutoscaler (HPA) resource. The HPA automatically adjusts the number of replicas in a Deployment based on observed metrics (like CPU utilization). This is the most efficient way to handle fluctuating demand, as it scales up to the maximum (e.g., 6) during peaks and scales back down to the minimum (e.g., 3) during normal load, optimizing resource usage and cost.",incorrect:{A:"You don't directly edit a Pod's configuration file to scale; Pods are managed by controllers like Deployments. Also, this would be a manual change.",B:"Manually editing the Deployment to a fixed number of 6 replicas is inefficient. You would be overprovisioned and paying for unnecessary pods during periods of normal demand. Autoscaling is the required efficient approach.",C:"You autoscale a controller like a `Deployment`, not an individual `Pod`. Pods are ephemeral and managed by the Deployment."}},conditions:["Kubernetes app","3 pods for normal demand, load doubles occasionally","Need efficient scaling"],caseStudyId:null},{id:71,topic:"Compute",question:"You have an application implemented on Compute Engine. You want to increase the durability of your application. What should you do?",options:{A:"Implement a scheduled snapshot on your Compute Engine instances.",B:"Implement a regional managed instance group.",C:"Monitor your application's usage metrics and implement autoscaling.",D:"Perform health checks on your Compute Engine instances."},correctAnswer:["B"],explanation:{correct:"A regional Managed Instance Group (MIG) is the primary solution for increasing application durability on Compute Engine. It distributes application instances across multiple zones within a region. If an entire zone fails, the application remains available on instances in the other healthy zones. MIGs also provide autohealing to automatically recreate failed instances within a zone.",incorrect:{A:"Snapshots provide *data* durability (backups), not application runtime durability. They help recover data after a failure but don't keep the application running during an outage.",C:"Autoscaling addresses performance and cost by adjusting the number of instances to match the load. While it contributes to availability under heavy load, it doesn't inherently protect against a zonal failure like a regional MIG does.",D:"Health checks are a mechanism used by MIGs and load balancers to *detect* failures. They are a necessary component of a durable system but are not the architectural solution itself. Implementing the regional MIG is the action that increases durability."}},conditions:["Application on Compute Engine","Need to increase application durability"],caseStudyId:null},{id:72,topic:"Compute",question:"The pilot subsystem in your Delivery by Drone service is critical to your service. You want to ensure that connections to the pilots can survive a VM outage without affecting connectivity. What should you do?",options:{A:"Configure proper startup scripts for your VMs.",B:"Deploy a load balancer to distribute traffic across multiple machines.",C:"Create persistent disk snapshots.",D:"Implement a managed instance group and load balancer."},correctAnswer:["D"],explanation:{correct:"This combination provides a complete high-availability solution. The Managed Instance Group (MIG) ensures that a healthy pool of VM instances is always running, automatically recreating any that fail (autohealing). The load balancer distributes incoming connections across the healthy instances in the MIG. If one VM fails, the load balancer's health checks detect this and stop sending traffic to it, while the MIG works to replace it, ensuring continuous service availability.",incorrect:{A:"Startup scripts configure a VM when it boots, but they do not prevent an outage if the VM itself fails.",B:"A load balancer needs a resilient group of backend instances. If you only have one VM, or manually managed VMs, a load balancer alone cannot protect against a VM outage.",C:"Persistent disk snapshots are for data backup and disaster recovery, not for maintaining live connectivity during a VM outage. They don't keep the application running."}},conditions:["Critical subsystem must survive a VM outage","Connectivity must not be affected"],caseStudyId:"CS001"},{id:73,topic:"Data Analytics",question:"Your company captures all web traffic data in Google Analytics 360 and stores it in BigQuery. Each country has its own dataset. Each dataset has multiple tables. You want analysts from each country to be able to see and query only the data for their respective countries. How should you configure the access rights?",options:{A:"Create a group per country. Add analysts to their respective country-groups. Create a single group all_analysts, and add all country-groups as members. Grant the all_analysts group the IAM role of BigQuery Job User. Share the appropriate dataset with view access with each respective analyst country-group.",B:"Create a group per country. Add analysts to their respective country-groups. Create a single group all_analysts, and add all country-groups as members. Grant the all_analysts group the IAM role of BigQuery Job User. Share the appropriate tables with view access with each respective analyst country-group.",C:"Create a group per country. Add analysts to their respective country-groups. Create a single group all_analysts, and add all country-groups as members. Grant the all_analysts group the IAM role of BigQuery Data Viewer. Share the appropriate dataset with view access with each respective analyst country-group.",D:"Create a group per country. Add analysts to their respective country-groups. Create a single group all_analysts, and add all country-groups as members. Grant the all_analysts group the IAM role of BigQuery Data Viewer. Share the appropriate table with view access with each respective analyst country-group."},correctAnswer:["A"],explanation:{correct:"This is the correct separation of permissions. Granting the `BigQuery Job User` role at the project level gives analysts permission to run query jobs (which incurs cost in the project). Then, granting `Viewer` access on a *per-dataset* basis ensures they can only read data from the specific datasets shared with their country's group. This enforces both the ability to query and the restriction of data visibility.",incorrect:{B:"Sharing at the table level is more granular than necessary and harder to manage if all tables in a dataset share the same access policy. Dataset-level sharing is more efficient here.",C:"Granting the `BigQuery Data Viewer` role at the project level would give analysts view access to *all* datasets in the project, violating the requirement to restrict access to only their own country's data.",D:"This is incorrect for two reasons: it grants project-level data viewer access and shares at the table level, which is less efficient."}},conditions:["GA360 data in BigQuery","Data is separated into a dataset per country","Analysts must only be able to see and query data for their own country"],caseStudyId:null},{id:74,topic:"Data Analytics",question:"Your company has a Google Cloud project that uses BigQuery for data warehousing on a pay-per-use basis. You want to monitor queries in real time to discover the most costly queries and which users spend the most. What should you do?",options:{A:"1. In the BigQuery dataset that contains all the tables to be queried, add a label for each user that can launch a query. 2. Open the Billing page of the project. 3. Select Reports. 4. Select BigQuery as the product and filter by the user you want to check.",B:"1. Create a Cloud Logging sink to export BigQuery data access logs to BigQuery. 2. Perform a BigQuery query on the generated table to extract the information you need.",C:"1. Create a Cloud Logging sink to export BigQuery data access logs to Cloud Storage. 2. Develop a Dataflow pipeline to compute the cost of queries split by users.",D:"1. Activate billing export into BigQuery. 2. Perform a BigQuery query on the billing table to extract the information you need."},correctAnswer:["B"],explanation:{correct:"BigQuery's audit logs (specifically data access logs) contain rich metadata about every query job, including the user who ran it and the total bytes billed. By creating a sink to export these logs to another BigQuery dataset, you can perform near real-time SQL queries on this log data to aggregate costs by user and identify the most expensive queries. This is the most direct and detailed method.",incorrect:{A:"Billing reports in the console are aggregated and not suitable for real-time monitoring of individual queries.",C:"Using Cloud Storage and Dataflow is overly complex when you can directly export and query the logs within BigQuery itself.",D:"Billing export to BigQuery provides cost data but often with more delay and less granular detail about individual query executions compared to the audit logs."}},conditions:["BigQuery data warehouse on a pay-per-use basis","Need to monitor queries in real time","Identify the most costly queries","Identify which users spend the most"],caseStudyId:null},{id:75,topic:"Data Analytics",question:"Your company is using BigQuery as its enterprise data warehouse. Data is distributed over several Google Cloud projects. All queries on BigQuery need to be billed on a single project. You want to make sure that no query costs are incurred on the projects that contain the data. Users should be able to query the datasets, but not edit them. How should you configure users access roles?",options:{A:"Add all users to a group. Grant the group the role of BigQuery user on the billing project and BigQuery dataViewer on the projects that contain the data.",B:"Add all users to a group. Grant the group the roles of BigQuery dataViewer on the billing project and BigQuery user on the projects that contain the data.",C:"Add all users to a group. Grant the group the roles of BigQuery jobUser on the billing project and BigQuery dataViewer on the projects that contain the data.",D:"Add all users to a group. Grant the group the roles of BigQuery dataViewer on the billing project and BigQuery jobUser on the projects that contain the data."},correctAnswer:["C"],explanation:{correct:"This correctly separates permissions. The `BigQuery Job User` role on the 'billing project' grants the permission to run query jobs, and those jobs will be billed to that project. The `BigQuery Data Viewer` role on the 'data projects' grants the necessary read-only access to the datasets in those projects without granting the permission to run jobs (and thus incur costs) in those projects.",incorrect:{A:"The `BigQuery User` role is slightly broader than `Job User` and includes permissions to create datasets, which may not be intended. `Job User` is more aligned with the principle of least privilege here.",B:"This reverses the roles. `Data Viewer` on the billing project does nothing useful, and `User` on the data projects would cause queries to be billed to the wrong projects.",D:"This also reverses the roles, with the same incorrect outcome as B."}},conditions:["BigQuery DWH across multiple projects","Centralize query billing on a single project","Prevent query costs on data-hosting projects","Users need read-only query access"],caseStudyId:null},{id:76,topic:"Data Analytics",question:"You are tasked with building an online analytical processing (OLAP) marketing analytics and reporting tool. This requires a relational database that can operate on hundreds of terabytes of data. What is the Google recommended tool for such applications?",options:{A:"Cloud Spanner, because it is globally distributed",B:"Cloud SQL, because it is a fully managed relational database",C:"Cloud Firestore, because it offers real-time synchronization across devices",D:"BigQuery, because it is designed for large-scale processing of tabular data"},correctAnswer:["D"],explanation:{correct:"BigQuery is Google Cloud's serverless, petabyte-scale data warehouse, specifically optimized for OLAP workloads like complex analytics and reporting on massive datasets. It uses a SQL interface and is designed to handle hundreds of terabytes of tabular data efficiently.",incorrect:{A:"Cloud Spanner is a globally distributed relational database, but it is primarily designed for Online Transaction Processing (OLTP) workloads, not large-scale OLAP.",B:"Cloud SQL is a managed relational database service suitable for OLTP applications but does not scale to hundreds of terabytes for analytical workloads.",C:"Cloud Firestore is a NoSQL document database, not a relational database, and is not designed for OLAP on massive structured datasets."}},conditions:["Build an OLAP analytics and reporting tool","Requires a relational (SQL-based) interface","Must operate on hundreds of terabytes of data"],caseStudyId:null},{id:77,topic:"Data Analytics",question:"Your BigQuery project has several users. For audit purposes, you need to see how many queries each user ran in the last month. What should you do?",options:{A:"Connect Google Data Studio to BigQuery. Create a dimension for the users and a metric for the amount of queries per user.",B:"In the BigQuery interface, execute a query on the JOBS table to get the required information.",C:"Use 'bq show ' to list all jobs. Per job, use bq ls to list job information and get the required information.",D:"Use Cloud Audit Logging to view Cloud Audit Logs, and create a filter on the query operation to get the required information."},correctAnswer:["B"],explanation:{correct:"BigQuery provides `INFORMATION_SCHEMA` views (e.g., `INFORMATION_SCHEMA.JOBS_BY_PROJECT`) which can be queried directly with SQL. These views contain metadata about all executed jobs, including the user, creation time, and job type. A simple SQL query can filter for query jobs in the last month and group by user to get the required count. This is the most direct and efficient method for this ad-hoc audit task.",incorrect:{A:"Data Studio is for visualization and would require the data to be queried and structured first. It's not the primary tool for performing the audit query itself.",C:"Using the `bq` command-line tool in a loop is highly inefficient for aggregating data across a large number of jobs compared to a single SQL query.",D:"While audit logs in Cloud Logging contain this information, querying it directly within BigQuery's `INFORMATION_SCHEMA` is often more convenient and doesn't require setting up log exports."}},conditions:["BigQuery project with multiple users","Need to audit and count the number of queries each user ran in the last month"],caseStudyId:null},{id:78,topic:"Data Analytics",question:"Your BigQuery project has several users. For audit purposes, you need to see how many queries each user ran in the last month. What should you do?",options:{A:"Connect Google Data Studio to BigQuery. Create a dimension for the users and a metric for the amount of queries per user.",B:"In the BigQuery interface, execute a query on the JOBS table to get the required information.",C:"Use 'bq show ' to list all jobs. Per job, use bq ls to list job information and get the required information.",D:"Use Cloud Audit Logging to view Cloud Audit Logs, and create a filter on the query operation to get the required information."},correctAnswer:["B"],explanation:{correct:"BigQuery provides `INFORMATION_SCHEMA` views which are system tables containing metadata. The `INFORMATION_SCHEMA.JOBS_BY_PROJECT` (or `JOBS_BY_USER`) view can be queried directly with SQL to get information about all jobs, including the user, creation time, and job type. A simple `SELECT COUNT(*)` with a `GROUP BY user_email` and a `WHERE` clause for the timeframe and job type is the most direct way to get this audit information.",incorrect:{A:"Data Studio is a visualization tool, not the primary method for performing the audit query itself.",C:"Using the `bq` command-line tool in a loop to fetch details for each job is highly inefficient compared to a single SQL query.",D:"While the information is in Cloud Audit Logs, querying the `INFORMATION_SCHEMA` is more direct from within BigQuery and doesn't require navigating to a different service or setting up log sinks."}},conditions:["BigQuery project with multiple users","Need to audit and count queries per user in the last month"],caseStudyId:null},{id:79,topic:"Data Analytics",question:"Your BigQuery project has several users. There are some tables that contain personally identifiable information (PII). Only the compliance team may access the PII. The other information in the tables must be available to the data science team. You want to minimize cost and the time it takes to assign appropriate access to the tables. What should you do?",options:{A:"1. From the dataset where you have the source data, create views of tables that you want to share, excluding PII. 2. Assign an appropriate project-level IAM role to the members of the data science team. 3. Assign access controls to the dataset that contains the view.",B:"1. From the dataset where you have the source data, create materialized views of tables that you want to share, excluding PII. 2. Assign an appropriate project-level IAM role to the members of the data science team. 3. Assign access controls to the dataset that contains the view.",C:"1. Create a dataset for the data science team. 2. Create views of tables that you want to share, excluding PII. 3. Assign an appropriate project-level IAM role to the members of the data science team. 4. Assign access controls to the dataset that contains the view. 5. Authorize the view to access the source dataset.",D:"1. Create a dataset for the data science team. 2. Create materialized views of tables that you want to share, excluding PII. 3. Assign an appropriate project-level IAM role to the members of the data science team. 4. Assign access controls to the dataset that contains the view. 5. Authorize the view to access the source dataset."},correctAnswer:["C"],explanation:{correct:"This describes the best practice for column-level security using authorized views. Creating a separate dataset for the data science team provides a clean separation of concerns. The views in this dataset select only the non-PII columns. Authorizing the view (not the users) to access the source data means the data science team can query the view without needing any permissions on the sensitive source dataset, perfectly enforcing the security requirement. Since views don't store data, this is the most cost-effective solution.",incorrect:{A:"This approach is less secure because granting a project-level IAM role might inadvertently grant access to the source dataset. The authorized view pattern is more explicit and secure.",B:"Materialized views store data, which would duplicate the non-PII data and incur unnecessary storage costs, violating the cost-minimization requirement.",D:"This is incorrect for the same reason as B; materialized views incur extra storage costs."}},conditions:["BigQuery tables contain PII","Compliance team needs full access","Data science team needs access to non-PII columns only","Minimize cost and setup time"],caseStudyId:null},{id:80,topic:"Data Analytics",question:"HRL is looking for a cost-effective approach for storing their race data such as telemetry. They want to keep all historical records, train models using only the previous season's data, and plan for data growth in terms of volume and information collected. You need to propose a data solution. Considering HRL's Business Requirements and the goals expressed by CEO S. Hawke, what should you do?",options:{A:"Use Firestore for its scalable and flexible document-based database. Use collections to aggregate race data by season and event. a",B:"Use Cloud Spanner for its scalability and ability to version schemas with zero downtime. Split race data using season as a primary key.",C:"Use BigQuery for its scalability and ability to add columns to a schema. Partition race data based on season.",D:"Use Cloud SQL for its ability to automatically manage storage increases and compatibility with MySQL. Use separate database instances for each season."},correctAnswer:["C"],explanation:{correct:"BigQuery is the ideal service for this use case. It is a highly scalable data warehouse designed for large-scale analytics, fitting the need to store all historical records and handle data growth. Partitioning the data by season (or a more granular time unit) is a key BigQuery feature that makes it highly efficient and cost-effective to query specific time periods, such as the 'previous season's data' required for model training.",incorrect:{A:"Firestore is a NoSQL document database, not optimized for the complex analytical queries typically performed on large historical datasets for model training.",B:"Cloud Spanner is a globally distributed relational database designed for OLTP (transactional) workloads, not for large-scale OLAP (analytical) workloads.",D:"Cloud SQL is a relational database service that does not scale to the same extent as BigQuery for large analytical workloads. Managing separate instances for each season is inefficient and complex."}},conditions:["Cost-effective storage for large volumes of race and telemetry data","Keep all historical records and plan for data growth","Need to train models using only the previous season's data","Create a data mart for processing large volumes of data"],caseStudyId:"cs_hrl"},{id:81,topic:"Data Analytics",question:"TerramEarths 20 million vehicles are scattered around the world. Based on the vehicles location, its telemetry data is stored in a Google Cloud Storage (GCS) regional bucket (US, Europe, or Asia). The CTO has asked you to run a report on the raw telemetry data to determine why vehicles are breaking down after 100 K miles. You want to run this job on all the data. What is the most cost-effective way to run this job?",options:{A:"Move all the data into 1 zone, then launch a Cloud Dataproc cluster to run the job",B:"Move all the data into 1 region, then launch a Google Cloud Dataproc cluster to run the job",C:"Launch a cluster in each region to preprocess and compress the raw data, then move the data into a multi-region bucket and use a Dataproc cluster to finish the job",D:"Launch a cluster in each region to preprocess and compress the raw data, then move the data into a region bucket and use a Cloud Dataproc cluster to finish the job"},correctAnswer:["D"],explanation:{correct:"To minimize cost, especially network egress costs, data should be processed as close to its location as possible. This 'map-and-reduce' style approach is the most cost-effective. First, run a preprocessing job (e.g., on a Dataproc cluster) in each region to filter, aggregate, and compress the raw data, significantly reducing its volume. Then, move the much smaller, processed results to a single regional bucket for the final aggregation and reporting job.",incorrect:{A:"Moving all raw data to a single zone would incur massive inter-regional network egress costs.",B:"Moving all raw data to a single region would also incur massive inter-regional network egress costs.",C:"Moving the intermediate data to a multi-region bucket is likely more expensive than a regional bucket if the final processing step is also regional."}},conditions:["Data is stored in regional GCS buckets (US, Europe, Asia)","Need to run a report on all the raw data","Goal is to find the most cost-effective way to run the job"],caseStudyId:"cs_terram_main"},{id:82,topic:"Data Analytics",question:"You need to implement a reliable, scalable GCP solution for the data warehouse for your company, TerramEarth. Considering the TerramEarth business and technical requirements, what should you do?",options:{A:"Replace the existing data warehouse with BigQuery. Use table partitioning.",B:"Replace the existing data warehouse with a Compute Engine instance with 96 CPUs.",C:"Replace the existing data warehouse with BigQuery. Use federated data sources.",D:"Replace the existing data warehouse with a Compute Engine instance with 96 CPUs. Add an additional Compute Engine preemptible instance with 32 CPUs."},correctAnswer:["A"],explanation:{correct:"The case study describes a legacy PostgreSQL data warehouse that is struggling with scale. BigQuery is Google's managed, serverless, and highly scalable data warehouse, making it the ideal replacement. Using table partitioning (e.g., by date) is a key best practice for managing large time-series datasets in BigQuery, as it improves query performance and helps control costs.",incorrect:{B:"Replacing one self-managed database server with a larger self-managed database server on Compute Engine doesn't solve the fundamental scalability and management challenges. It is not a cloud-native solution.",C:"While federated sources are a useful feature, the primary task is to replace the core data warehouse itself with a scalable solution, which is BigQuery's native storage.",D:"This is another variation of a self-managed solution that is less scalable and reliable than BigQuery."}},conditions:["Implement a reliable and scalable data warehouse solution","Replace an existing PostgreSQL server","Handle large volumes of data for analytics"],caseStudyId:"cs_terram_main"},{id:83,topic:"Data Ingestion and Processing",question:"Cymbal Direct drones continuously send data during deliveries. You need to process and analyze the incoming telemetry data. After processing, the data should be retained, but it will only be accessed once every month or two. Your CIO has issued a directive to incorporate managed services wherever possible. You want a cost-effective solution to process the incoming streams of data. What should you do?",options:{A:"Ingest data with an IoT Core like service, process it with Dataprep, and store it in a Coldline Cloud Storage bucket.",B:"Ingest data with an IoT Core like service, and then publish to Pub/Sub. Use Dataflow to process the data, and store it in a Nearline Cloud Storage bucket.",C:"Ingest data with an IoT Core like service, and then publish to Pub/Sub. Use BigQuery to process the data, and store it in a Standard Cloud Storage bucket.",D:"Ingest data with an IoT Core like service, and then store it in BigQuery."},correctAnswer:["B"],explanation:{correct:"This architecture follows a standard, robust, and fully managed pattern for IoT data. Pub/Sub is the scalable ingestion buffer. Dataflow is the powerful stream processing engine. For storage with infrequent access (monthly or bi-monthly), Nearline Storage offers a good balance between low storage cost and reasonable retrieval cost, making it more cost-effective than Standard storage.",incorrect:{A:"Dataprep is primarily a visual tool for batch data preparation, not for real-time stream processing. Coldline storage has a 90-day minimum storage duration charge, which might be less cost-effective than Nearline if data is accessed every month or two.",C:"BigQuery is an analytics data warehouse, not a stream processing engine like Dataflow. Standard Cloud Storage is too expensive for data that is accessed infrequently.",D:"This skips the processing step entirely. While data can be streamed into BigQuery, Dataflow is often needed for complex transformations, enrichments, and processing before data lands in the analytical store."}},conditions:["Process and analyze streaming telemetry data","Retain data with infrequent access (monthly or bi-monthly)","Use managed services","Cost-effective solution required"],caseStudyId:"CS001"},{id:84,topic:"Data Processing",question:"TerramEarth plans to connect all 20 million vehicles in the field to the cloud. This increases the volume to 20 million 600 byte records a second for 40 TB an hour. How should you design the data ingestion?",options:{A:"Vehicles write data directly to GCS.",B:"Vehicles write data directly to Google Cloud Pub/Sub.",C:"Vehicles stream data directly to Google BigQuery.",D:"Vehicles continue to write data using the existing system (FTP)."},correctAnswer:["B"],explanation:{correct:"Cloud Pub/Sub is a globally distributed, highly scalable messaging service designed for this exact use case. It can easily handle millions of messages per second, providing a durable and reliable buffer that decouples the data-ingesting vehicles from the downstream processing systems (like Dataflow or Cloud Functions).",incorrect:{A:"Writing millions of small files per second directly to GCS can lead to performance issues due to object creation overhead and potential 'hotspotting' if naming conventions are not carefully randomized.",C:"BigQuery's streaming insert API has quotas that are lower than what Pub/Sub can handle. Pub/Sub is the recommended buffer for this scale of ingestion before data lands in BigQuery.",D:"The existing FTP system is a batch-oriented process and completely inadequate for handling real-time streaming data at this massive scale."}},conditions:["Connect 20 million vehicles","Data rate: 20 million records/sec (40 TB/hour)","Design a scalable data ingestion mechanism"],caseStudyId:"cs_terram_main"},{id:85,topic:"Data Processing",question:"You need to analyze and define the technical architecture for the data workloads for your company, TerramEarth. Considering the TerramEarth business and technical requirements, what should you do?",options:{A:"Ingest the data using Cloud Dataflow. Use Cloud Pub/Sub to store the ingested data. Use BigQuery for data analytics.",B:"Ingest the data using Cloud Pub/Sub. Use Cloud Dataflow to process the ingested data. Use BigQuery for data analytics.",C:"Ingest the data using Cloud Pub/Sub. Use Cloud Dataflow to process the ingested data. Use Cloud Spanner for data analytics.",D:"Ingest the data using Cloud Pub/Sub. Use Cloud Dataproc to process the ingested data. Use BigQuery for data analytics."},correctAnswer:["B"],explanation:{correct:"This option describes the standard and recommended architecture for large-scale streaming analytics on GCP. Cloud Pub/Sub is used for scalable ingestion, Cloud Dataflow provides a unified model for stream and batch processing, and BigQuery serves as the scalable, serverless data warehouse for analytics.",incorrect:{A:"This reverses the roles of Pub/Sub and Dataflow. Pub/Sub is for ingestion, and Dataflow is for processing.",C:"Cloud Spanner is a transactional (OLTP) database, not a data warehouse for large-scale analytics (OLAP). BigQuery is the correct choice for analytics.",D:"While Dataproc can be used for data processing, Dataflow is often preferred for its serverless nature and unified stream/batch programming model, which aligns well with the modern, cloud-native requirements of the case study."}},conditions:["Define the technical architecture for data workloads","Handle large volumes of streaming and batch data","Support large-scale analytics","Align with TerramEarth's business and technical requirements"],caseStudyId:"cs_terram_main"},{id:86,topic:"Data Processing",question:"You are developing a globally scaled frontend for a legacy streaming backend data API. This API expects events in strict chronological order with no repeat data for proper processing. Which products should you deploy to ensure guaranteed-once FIFO (first-in, first-out) delivery of data?",options:{A:"Cloud Pub/Sub alone",B:"Cloud Pub/Sub to Cloud Dataflow",C:"Cloud Pub/Sub to Stackdriver",D:"Cloud Pub/Sub to Cloud SQL"},correctAnswer:["B"],explanation:{correct:"This is a classic complex event processing challenge. Pub/Sub with ordering keys can provide ordering for messages with the same key, but for a globally scaled system, messages may still arrive out of order. Cloud Dataflow is designed to solve this by using event-time windowing and watermarks to buffer and reorder out-of-order data correctly. It also provides tools and stateful processing capabilities to perform deduplication, ensuring exactly-once processing before sending the data in strict order to the legacy API.",incorrect:{A:"Pub/Sub alone cannot guarantee strict global FIFO and exactly-once processing for a consumer. It provides at-least-once delivery, and ordering is only guaranteed for messages with the same ordering key within a region.",C:"Stackdriver (Cloud Operations) is for monitoring and logging, not data processing.",D:"Cloud SQL is a relational database. While it could be used to buffer and order data, Dataflow is the purpose-built stream processing service for this task."}},conditions:["Globally scaled frontend","Legacy streaming backend API","Requires strict chronological order (FIFO)","Requires no repeat data (exactly-once processing)"],caseStudyId:null},{id:87,topic:"Data Processing",question:"You are working with a data warehousing team that performs data analysis. The team needs to process data from external partners, but the data contains personally identifiable information (PII). You need to process and store the data without storing any of the PII data. What should you do?",options:{A:"Create a Dataflow pipeline to retrieve the data from the external sources. As part of the pipeline, use the Cloud Data Loss Prevention (Cloud DLP) API to remove any PII data. Store the result in BigQuery.",B:"Create a Dataflow pipeline to retrieve the data from the external sources. As part of the pipeline, store all non-PII data in BigQuery and store all PII data in a Cloud Storage bucket that has a retention policy set.",C:"Ask the external partners to upload all data on Cloud Storage. Configure Bucket Lock for the bucket. Create a Dataflow pipeline to read the data from the bucket. As part of the pipeline, use the Cloud Data Loss Prevention (Cloud DLP) API to remove any PII data. Store the result in BigQuery.",D:"Ask the external partners to import all data in your BigQuery dataset. Create a dataflow pipeline to copy the data into a new table. As part of the Dataflow bucket, skip all data in columns that have PII data"},correctAnswer:["A"],explanation:{correct:"This is the most secure and efficient approach. By integrating the Cloud DLP API directly into the Dataflow ingestion pipeline, PII is redacted or tokenized *in-flight* before it is ever stored in the final data warehouse (BigQuery). This ensures that the sensitive data never lands in the analytical system, meeting the core requirement.",incorrect:{B:"This approach still involves storing PII, which the requirement explicitly aims to avoid in the final processed data.",C:"This involves landing the raw, sensitive PII data in Cloud Storage first. While this is a common staging pattern, processing it out before it lands in the final DWH is cleaner and reduces the number of places sensitive data is stored.",D:"This is the least desirable option as it involves loading the raw PII directly into BigQuery first before cleaning it. It's better to clean the data before it enters the data warehouse."}},conditions:["Process external partner data containing PII","Need to process and store the data without storing any of the PII"],caseStudyId:null},{id:88,topic:"Data Processing",question:"Mountkirk Games wants to set up a real-time analytics platform for their new game. The new platform must meet their technical requirements. Which combination of Google technologies will meet all of their requirements?",options:{A:"Kubernetes Engine, Cloud Pub/Sub, and Cloud SQL",B:"Cloud Dataflow, Cloud Storage, Cloud Pub/Sub, and BigQuery",C:"Cloud SQL, Cloud Storage, Cloud Pub/Sub, and Cloud Dataflow",D:"Cloud Dataproc, Cloud Pub/Sub, Cloud SQL, and Cloud Dataflow",E:"Cloud Pub/Sub, Compute Engine, Cloud Storage, and Cloud Dataproc"},correctAnswer:["B"],explanation:{correct:"This combination of fully managed services perfectly matches the technical requirements from the v1 case study. Pub/Sub for scalable, real-time ingestion. Dataflow for processing both streaming data on-the-fly and batch files, including handling late data. Cloud Storage for landing user-uploaded files. BigQuery for storing and allowing SQL queries on 10+ TB of historical data.",incorrect:{A:"Cloud SQL cannot handle 10+ TB of analytical queries efficiently.",C:"Cloud SQL is not suitable for the analytics platform's historical data store.",D:"Cloud SQL is not suitable. While Dataproc could be used, Dataflow is often a better fit for a unified stream/batch platform.",E:"This architecture lacks a scalable database for historical SQL queries; Dataproc would query files in GCS, which is generally less performant than BigQuery for ad-hoc analytics."}},conditions:["Set up a real-time analytics platform","Meet all technical requirements for the Game Analytics Platform (v1)","Process streaming data, late data, and batch files","Allow SQL queries on 10+ TB of historical data"],caseStudyId:"cs_mountkirk_main"},{id:89,topic:"Data Processing",question:"TerramEarth plans to connect all 20 million vehicles in the field to the cloud. This increases the volume to 20 million 600 byte records a second for 40 TB an hour. How should you design the data ingestion?",options:{A:"Vehicles write data directly to GCS.",B:"Vehicles write data directly to Google Cloud Pub/Sub.",C:"Vehicles stream data directly to Google BigQuery.",D:"Vehicles continue to write data using the existing system (FTP)."},correctAnswer:["B"],explanation:{correct:"Cloud Pub/Sub is a globally distributed, highly scalable messaging service designed for this exact use case. It can easily handle millions of messages per second, providing a durable and reliable buffer that decouples the data-ingesting vehicles from the downstream processing systems (like Dataflow or Cloud Functions).",incorrect:{A:"Writing millions of small files per second directly to GCS can lead to performance issues due to object creation overhead and potential 'hotspotting' if naming conventions are not carefully randomized.",C:"BigQuery's streaming insert API has quotas that are lower than what Pub/Sub can handle. Pub/Sub is the recommended buffer for this scale of ingestion before data lands in BigQuery.",D:"The existing FTP system is a batch-oriented process and completely inadequate for handling real-time streaming data at this massive scale."}},conditions:["Connect 20 million vehicles","Ingestion rate: 20M records/sec (~40TB/hr)","Design data ingestion"],caseStudyId:"cs_terram_main"},{id:90,topic:"Data Processing",question:"You analyzed TerramEarths business requirement to reduce downtime, and found that they can achieve a majority of time saving by reducing customers wait time for parts. You decided to focus on the reduction of the 3 weeks aggregate reporting time. Which modifications to the companys processes should you recommend?",options:{A:"Migrate from CSV to binary format, migrate from FTP to SFTP transport, and develop machine learning analysis of metrics",B:"Migrate from FTP to streaming transport, migrate from CSV to binary format, and develop machine learning analysis of metrics",C:"Increase fleet cellular connectivity to 80%, migrate from FTP to streaming transport, and develop machine learning analysis of metrics",D:"Migrate from FTP to SFTP transport, develop machine learning analysis of metrics, and increase dealer local inventory by a fixed factor"},correctAnswer:["C"],explanation:{correct:"The 3-week delay is caused by stale data from a slow, batch FTP process from a limited number of vehicles. The most impactful changes are: 1) Increase the number of data sources by getting more vehicles connected. 2) Change the data transport from slow batch (FTP) to real-time streaming. 3) Use this fresh, comprehensive data to develop ML models for prediction. This option addresses all three key areas.",incorrect:{A:"SFTP is still a batch process and doesn't solve the data freshness problem. Changing format is a minor optimization.",B:"This is a good step, but without increasing the number of connected vehicles, the data will still be sparse and less effective for fleet-wide predictions.",D:"SFTP is still batch. Increasing inventory is a reaction to poor prediction, not a way to improve the prediction process."}},conditions:["Goal is to reduce 3-week aggregate reporting time","Current process is slow due to batch FTP from a limited number of vehicles","Recommend process modifications to get fresher data"],caseStudyId:"cs_terram_main"},{id:91,topic:"Data Processing",question:"To speed up data retrieval, more vehicles will be upgraded to cellular connections and be able to transmit data to the ETL process. The current FTP process is error-prone and restarts the data transfer from the start of the file when connections fail, which happens often. You want to improve the reliability of the solution and minimize data transfer time on the cellular connections. What should you do?",options:{A:"Use one Google Container Engine cluster of FTP servers. Save the data to a Multi-Regional bucket. Run the ETL process using data in the bucket",B:"Use multiple Google Container Engine clusters running FTP servers located in different regions. Save the data to multi-regional buckets in US, EU, and Asia. Run the ETL process using the data in the bucket",C:"Directly transfer the files to different Google Cloud Multi-Regional Storage bucket locations in US, EU, and Asia using Google APIs over HTTP(S). Run the ETL process using the data in the bucket",D:"Directly transfer the files to a different Google Cloud Regional Storage bucket location in US, EU, and Asia using Google APIs over HTTP(S). Run the ETL process to retrieve the data from each regional bucket"},correctAnswer:["C"],explanation:{correct:"The current FTP process is unreliable. Google Cloud Storage APIs over HTTP(S) support resumable uploads, which is critical for unreliable connections like cellular. If a transfer is interrupted, it can be resumed from where it left off. Using geographically distributed Multi-Regional buckets allows vehicles to upload to a closer endpoint, reducing latency and improving success rates.",incorrect:{A:"Continuing to use FTP, even on GKE, does not solve the underlying protocol reliability issues.",B:"This is also incorrect for the same reason as A.",D:"While regional buckets are an option, multi-regional buckets often provide better performance and availability for globally distributed clients (the vehicles)."}},conditions:["Increase in cellular-connected vehicles","Current FTP process is error-prone and not resumable","Need to improve reliability and minimize transfer time over cellular"],caseStudyId:"cs_terram_main"},{id:92,topic:"Data Processing",question:"A new architecture that writes all incoming data to BigQuery has been introduced. You notice that the data is dirty, and want to ensure data quality on an automated daily basis while managing cost. What should you do?",options:{A:"Set up a streaming Cloud Dataflow job, receiving data by the ingestion process. Clean the data in a Cloud Dataflow pipeline.",B:"Create a Cloud Function that reads data from BigQuery and cleans it. Trigger the Cloud Function from a Compute Engine instance.",C:"Create a SQL statement on the data in BigQuery, and save it as a view, Run the view daily, and save the result to a new table,",D:"Use Cloud Dataprep and configure the BigQuery tables as the source. Schedule a daily job to clean the data."},correctAnswer:["D"],explanation:{correct:"Cloud Dataprep is a managed data preparation service designed for visually exploring, cleaning, and preparing data for analysis. It integrates directly with BigQuery as a source and sink and allows you to schedule recurring jobs to apply your data cleaning 'recipe'. This is the most suitable tool for an automated, daily data quality process on data already in BigQuery.",incorrect:{A:"A streaming Dataflow job is for cleaning data *during* ingestion, not for cleaning data that is already stored in BigQuery on a daily batch basis.",B:"This is a custom, less integrated solution. Dataprep provides a more powerful and user-friendly interface for this task.",C:"While SQL views can perform some transformations, Dataprep offers a much richer set of data cleaning and transformation capabilities and better job scheduling and management for this type of pipeline."}},conditions:["Incoming data is already stored in BigQuery","The data is dirty","Need an automated daily data quality process","Need to manage cost"],caseStudyId:"cs_terram_main"},{id:93,topic:"Data Processing",question:"Considering the technical requirements, how should you reduce the unplanned vehicle downtime in GCP?",options:{A:"Use BigQuery as the data warehouse. Connect all vehicles to the network and stream data into BigQuery using Cloud Pub/Sub and Cloud Dataflow. Use Google Data Studio for analysis and reporting.",B:"Use BigQuery as the data warehouse. Connect all vehicles to the network and upload gzip files to a Multi-Regional Cloud Storage bucket using gcloud. Use Google Data Studio for analysis and reporting.",C:"Use Cloud Dataproc Hive as the data warehouse. Upload gzip files to a Multi-Regional Cloud Storage bucket. Upload this data into BigQuery using gcloud. Use Google Data Studio for analysis and reporting.",D:"Use Cloud Dataproc Hive as the data warehouse. Directly stream data into partitioned Hive tables. Use Pig scripts to analyze data."},correctAnswer:["A"],explanation:{correct:"To reduce downtime, TerramEarth needs to move from slow, batch-based reporting to a real-time or near real-time analytics pipeline. This architecture (Pub/Sub for streaming ingest -> Dataflow for stream processing -> BigQuery for analytics) allows for the timely analysis of vehicle telemetry data. This enables better predictive maintenance models and faster insights, which in turn helps to anticipate needs and reduce unplanned downtime.",incorrect:{B:"Uploading gzip files is a batch process and would reintroduce the data staleness problem that the current system suffers from.",C:"This describes a convoluted process. BigQuery should be the primary data warehouse, not Dataproc Hive for this use case.",D:"This relies on older big data technologies (Hive, Pig) and is less aligned with a modern, scalable cloud-native approach compared to using Pub/Sub, Dataflow, and BigQuery."}},conditions:["Reduce unplanned vehicle downtime","Leverage GCP services","Improve upon the old system with stale, batch-processed data"],caseStudyId:"cs_terram_main"},{id:94,topic:"Data Processing",question:"You have a streaming data pipeline using Pub/Sub and Dataflow that processes sensor data. Occasionally, sensors send duplicate readings with the same timestamp and sensor ID within a short timeframe (e.g., 5 minutes). You need to ensure that only one reading per sensor ID is processed within any 5-minute window. How should you configure your Dataflow pipeline?",options:{A:"Use a Fixed time window of 5 minutes and group by sensor ID.",B:"Use a Sliding time window of 5 minutes and group by sensor ID, then take the first element.",C:"Use Pub/Sub message deduplication feature.",D:"Use a Session window grouped by sensor ID, then apply a Combine transform to select one value."},correctAnswer:["A"],explanation:{correct:"This is the classic use case for fixed time windows for deduplication. By applying a 5-minute fixed window, Dataflow groups all events into discrete, non-overlapping 5-minute chunks based on their timestamp. Then, by grouping by the sensor ID within each window, you get all readings for a specific sensor in that 5-minute interval. From there, you can easily apply a transform to pick only one element (e.g., the first one), effectively deduplicating the readings within that window.",incorrect:{B:"Sliding windows overlap, which is not what you want for this type of deduplication. You would end up processing the same reading in multiple windows.",C:"Pub/Sub's message deduplication is based on message IDs to handle at-least-once delivery semantics. It does not understand the content of your message (like sensor ID and timestamp) to perform business-logic-level deduplication.",D:"Session windows are for grouping events based on periods of activity followed by gaps of inactivity, which is a different pattern from the fixed 5-minute intervals required."}},conditions:["Streaming pipeline using Pub/Sub and Dataflow","Sensors send duplicate readings (same timestamp and sensor ID)","Need to process only one reading per sensor within any 5-minute window"],caseStudyId:null},{id:95,topic:"Data Retention and Compliance",question:"Cymbal Direct's user account management app allows users to delete their accounts whenever they like. Cymbal Direct also has a very generous 60-day return policy for users. The customer service team wants to make sure that they can still refund or replace items for a customer even if the customer's account has been deleted. What can you do to ensure that the customer service team has access to relevant account information?",options:{A:"Temporarily disable the account for 30 days. Export account information to Cloud Storage, and enable lifecycle management to delete the data in 60 days.",B:"Ensure that the user clearly understands that after they delete their account, all their information will also be deleted. Remind them to download a copy of their order history and account information before deleting their account. Have the support agent copy any open or recent orders to a shared spreadsheet.",C:"Restore a previous copy of the user information database from a snapshot. Have a database administrator capture needed information about the customer.",D:"Disable the account. Export account information to Cloud Storage. Have the customer service team permanently delete the data after 30 days."},correctAnswer:["A"],explanation:{correct:"This solution provides a 'soft delete' workflow that respects the user's request while meeting the business requirement. Exporting necessary order/account information to Cloud Storage makes it available to the customer service team. Using a Cloud Storage lifecycle policy to automatically delete the data after 60 days ensures compliance with data retention limits and automates the final deletion process.",incorrect:{B:"This places the burden on the user and the support agent (manual copy) and is not a reliable or scalable process.",C:"Restoring an entire database snapshot to retrieve a single user's information is extremely inefficient, costly, and slow.",D:"This option suggests deleting the data after 30 days, which does not cover the full 60-day return policy window."}},conditions:["Users can delete their accounts","A 60-day return policy must be supported","Customer service needs access to relevant account/order information even after account deletion"],caseStudyId:"CS001"},{id:96,topic:"Database Selection",question:"Cymbal Direct is evaluating database options to store the analytics data from its experimental drone deliveries. You're currently using a small cluster of MongoDB NoSQL database servers. You want to move to a managed NoSQL database service with consistent low latency that can scale throughput seamlessly and can handle the petabytes of data you expect after expanding to additional markets. What should you do?",options:{A:"Extract the data from MongoDB. Insert the data into Firestore using Datastore mode.",B:"Create a Bigtable instance, extract the data from MongoDB, and insert the data into Bigtable.",C:"Extract the data from MongoDB. Insert the data into Firestore using Native mode.",D:"Extract the data from MongoDB, and insert the data into BigQuery."},correctAnswer:["B"],explanation:{correct:"Cloud Bigtable is the best fit for these requirements. It is a managed NoSQL wide-column store designed for very large (petabyte-scale) analytical and operational workloads. It provides consistent low latency and is built to scale throughput seamlessly, making it the ideal choice to replace MongoDB for this high-volume analytics use case.",incorrect:{A:"Firestore is a document database that is excellent for many use cases, but it is not typically recommended for petabyte-scale analytics workloads where Bigtable's throughput and scale are superior.",C:"Similar to A, Firestore is not the optimal choice for this scale of analytics data.",D:"BigQuery is a data warehouse for OLAP (Online Analytical Processing). While it handles petabytes of data, it is not a low-latency NoSQL database for real-time operational analytics in the same way Bigtable is. Bigtable is a better fit for a 'database' while BigQuery is a 'data warehouse'."}},conditions:["Store analytics data from drone deliveries","Currently using MongoDB","Requires a managed NoSQL database service","Needs consistent low latency and seamless throughput scaling","Must handle petabytes of data"],caseStudyId:"CS001"},{id:97,topic:"Databases",question:"Dress4Win has asked you for advice on how to migrate their on-premises MySQL deployment to the cloud. They want to minimize downtime and performance impact to their on-premises solution during the migration. Which approach should you recommend?",options:{A:"Create a dump of the on-premises MySQL master server, and then shut it down, upload it to the cloud environment, and load into a new MySQL cluster.",B:"Setup a MySQL replica server/slave in the cloud environment, and configure it for asynchronous replication from the MySQL master server on-premises until cutover.",C:"Create a new MySQL cluster in the cloud, configure applications to begin writing to both on premises and cloud MySQL masters, and destroy the original cluster at cutover.",D:"Create a dump of the MySQL replica server into the cloud environment, load it into: Google Cloud Datastore, and configure applications to read/write to Cloud Datastore at cutover."},correctAnswer:["B"],explanation:{correct:"This is the standard and recommended approach for a minimal downtime database migration. By setting up the cloud database as a replica of the on-premises master, data is continuously synchronized while the on-premises system remains fully operational. The final cutover involves a very short maintenance window to stop writes, wait for replication to catch up, promote the replica, and repoint the application.",incorrect:{A:"This 'offline' migration method requires significant downtime while the database is shut down, dumped, transferred, and loaded.",C:"Configuring dual-writes is complex, error-prone, and difficult to manage to ensure data consistency between the two masters.",D:"Migrating from MySQL (relational) to Cloud Datastore (NoSQL) is a major re-architecture, not a like-for-like migration. It would require significant application code changes."}},conditions:["Migrate an on-premises MySQL deployment to the cloud","Requirement to minimize downtime","Requirement to minimize performance impact on the on-premises solution during migration"],caseStudyId:"cs_dress4win_v1"},{id:98,topic:"Databases",question:"You have found an error in your App Engine application caused by missing Cloud Datastore indexes. You have created a YAML file with the required indexes and want to deploy these new indexes to Cloud Datastore. What should you do?",options:{A:"Point gcloud datastore create-indexes to your configuration file",B:"Upload the configuration file to App Engine's default Cloud Storage bucket, and have App Engine detect the new indexes",C:"In the GCP Console, use Datastore Admin to delete the current indexes and upload the new configuration file",D:"Create an HTTP request to the built-in python module to send the index configuration file to your application"},correctAnswer:["A"],explanation:{correct:"The `gcloud datastore indexes create <index-file.yaml>` command is the standard and correct way to deploy or update Cloud Datastore indexes from a local configuration file. It will upload the definitions and initiate the index building process.",incorrect:{B:"App Engine does not automatically detect or deploy index files from a Cloud Storage bucket.",C:"There is no need to delete current indexes. The `gcloud` command will update existing indexes and create new ones as defined in the file.",D:"There is no built-in application module or HTTP request mechanism for deploying index configuration files; this is a management plane operation handled by `gcloud`."}},conditions:["App Engine application error due to missing Datastore indexes","A YAML file with the required index definitions has been created","Need to deploy the new indexes"],caseStudyId:null},{id:99,topic:"Databases",question:"You need to set up Microsoft SQL Server on GCP. Management requires that there's no downtime in case of a data center outage in any of the zones within a GCP region. What should you do?",options:{A:"Configure a Cloud SQL instance with high availability enabled.",B:"Configure a Cloud Spanner instance with a regional instance configuration.",C:"Set up SQL Server on Compute Engine, using Always On Availability Groups using Windows Failover Clustering. Place nodes in different subnets.",D:"Set up SQL Server Always On Availability Groups using Windows Failover Clustering. Place nodes in different zones."},correctAnswer:["D"],explanation:{correct:"To achieve high availability against a zonal outage for a self-managed SQL Server on Compute Engine, you must use a multi-zone architecture. SQL Server Always On Availability Groups built on Windows Server Failover Clustering (WSFC) is the standard technology for this. Placing the WSFC nodes in different zones within the same region ensures that if one zone fails, a secondary node in another zone can take over.",incorrect:{A:"While Cloud SQL for SQL Server with HA is also a valid solution, this question presents the self-managed GCE option. Between C and D, D is correct.",B:"Cloud Spanner is a different database service, not Microsoft SQL Server.",C:"Placing nodes in different subnets within the *same zone* does not protect against a zonal outage."}},conditions:["Set up Microsoft SQL Server on GCP","Requirement for no downtime in case of a zonal outage within a region"],caseStudyId:null},{id:100,topic:"Databases",question:"You are using a single Cloud SQL instance to serve your application from a specific zone. You want to introduce high availability. What should you do?",options:{A:"Create a read replica instance in a different region",B:"Create a failover replica instance in a different region",C:"Create a read replica instance in the same region, but in a different zone",D:"Create a failover replica instance in the same region, but in a different zone"},correctAnswer:["D"],explanation:{correct:"Cloud SQL's High Availability (HA) configuration is designed to provide resilience against zonal failures. It works by creating a standby instance, known as a failover replica, in a different zone within the same region as the primary instance. Data is synchronously replicated to this replica, and in the event of a primary instance or zone failure, Cloud SQL automatically fails over to the standby.",incorrect:{A:"Read replicas are for scaling read traffic and use asynchronous replication; they do not provide automatic failover for HA.",B:"A replica in a different region is for disaster recovery, not intra-region high availability. Failover is typically a manual process with higher RTO/RPO.",C:"This describes a read replica, not a failover replica for HA."}},conditions:["Have a single Cloud SQL instance in one zone","Need to introduce high availability"],caseStudyId:null},{id:101,topic:"Databases",question:"You are using Cloud SQL as the database backend for a large CRM deployment. You want to scale as usage increases and ensure that you dont run out of storage, maintain 75% CPU usage cores, and keep replication lag below 60 seconds. What are the correct steps to meet your requirements?",options:{A:"1) Enable automatic storage increase for the instance. 2) Create a Cloud Monitoring alert when CPU usage exceeds 75%, and change the instance type to reduce CPU usage. 3) Create a Cloud Monitoring alert for replication lag, and shard the database to reduce replication time.",B:"1) Enable automatic storage increase for the instance. 2) Change the instance type to a 32-core machine type to keep CPU usage below 75%. 3) Create a Cloud Monitoring alert for replication lag, and shard the database to reduce replication time.",C:"1) Create a Cloud Monitoring alert when storage exceeds 75%, and increase the available storage on the instance to create more space. 2) Deploy memcached to reduce CPU load. 3) Change the instance type to a 32-core machine type to reduce replication lag.",D:"1) Create a Cloud Monitoring alert when storage exceeds 75%, and increase the available storage on the instance to create more space. 2) Deploy memcached to reduce CPU load. 3) Create a Cloud Monitoring alert for replication lag, and change the instance type to a 32-core machine type to reduce replication lag."},correctAnswer:["A"],explanation:{correct:"This option provides a comprehensive and practical strategy. Enabling 'automatic storage increase' is a proactive feature to prevent storage issues. Using Cloud Monitoring alerts for CPU and replication lag allows for reactive scaling and troubleshooting, which is a standard operational practice. The suggested actions (changing instance type for CPU, investigating sharding for persistent replication lag) are appropriate responses.",incorrect:{B:"Preemptively scaling to a very large instance type (32-core) may not be cost-effective. It's better to scale reactively based on monitoring.",C:"Relying on a manual alert for storage is less ideal than the automatic increase feature. Changing instance type is not a guaranteed fix for replication lag.",D:"Similar to C, this relies on manual storage management and makes assumptions about the cause of replication lag."}},conditions:["Cloud SQL backend for a large CRM","Need to scale with usage","Prevent running out of storage","Maintain CPU usage at or below 75%","Keep replication lag below 60 seconds"],caseStudyId:null},{id:102,topic:"Databases",question:"You have deployed an application to Kubernetes Engine, and are using the Cloud SQL proxy container to make the Cloud SQL database available to the services running on Kubernetes. You are notified that the application is reporting database connection issues. Your company policies require a post-mortem. What should you do?",options:{A:"Use gcloud sql instances restart.",B:"Validate that the Service Account used by the Cloud SQL proxy container still has the Cloud Build Editor role.",C:"In the GCP Console, navigate to Cloud Logging. Consult logs for Kubernetes Engine and Cloud SQL.",D:"In the GCP Console, navigate to Cloud SQL. Restore the latest backup. Use kubectl to restart all pods."},correctAnswer:["C"],explanation:{correct:"The first step in any post-mortem is to gather data to understand the root cause. Cloud Logging is the central place to find logs from both GKE (including the application pods and the Cloud SQL proxy sidecar container) and the Cloud SQL instance itself. Analyzing these logs is the essential first step to diagnose connection errors, permission issues, or problems with the database instance.",incorrect:{A:"Restarting the instance is a mitigation step, not a diagnostic one. Doing this before investigation may destroy evidence of the root cause.",B:"The `Cloud Build Editor` role is incorrect for the Cloud SQL proxy. The proxy typically needs the `Cloud SQL Client` role. While checking permissions is part of the investigation, this option specifies the wrong role.",D:"Restoring a backup is a drastic recovery measure for data corruption, not for diagnosing connection issues. Restarting pods might temporarily fix the issue but won't reveal the cause."}},conditions:["GKE application using Cloud SQL Proxy","Application is reporting database connection issues","A post-mortem is required"],caseStudyId:null},{id:103,topic:"Databases",question:"Your company has an application running on Google Cloud that is collecting data from thousands of physical devices that are globally distributed. Data is published to Pub/Sub and streamed in real time into an SSD Cloud Bigtable cluster via a Dataflow pipeline. The operations team informs you that your Cloud Bigtable cluster has a hotspot, and queries are taking longer than expected. You need to resolve the problem and prevent it from happening in the future. What should you do?",options:{A:"Advise your clients to use HBase APIs instead of NodeJS APIs.",B:"Delete records older than 30 days.",C:"Review your RowKey strategy and ensure that keys are evenly spread across the alphabet.",D:"Double the number of nodes you currently have."},correctAnswer:["C"],explanation:{correct:"Hotspotting in Cloud Bigtable is almost always caused by a poorly designed row key strategy that leads to a high concentration of reads or writes on a narrow range of keys. Because Bigtable stores data lexicographically, sequential keys (like timestamps or sequential IDs at the beginning of the key) cause new writes to target a single node. The fundamental solution is to redesign the row key to distribute writes randomly across the keyspace (e.g., by adding a random salt or hash as a prefix).",incorrect:{A:"The client API used has no bearing on the row key design or data distribution.",B:"Deleting old data might temporarily reduce the size of the hotspot but will not fix the underlying access pattern that is causing it.",D:"Adding more nodes might temporarily alleviate the problem by splitting the hot tablet, but if the poor row key design persists, a new hotspot will likely form on one of the new nodes."}},conditions:["Data pipeline: Devices -> Pub/Sub -> Dataflow -> Bigtable","Problem: Cloud Bigtable cluster has a hotspot","Symptom: Queries are taking longer than expected","Need to resolve and prevent the issue"],caseStudyId:null},{id:104,topic:"Databases",question:"JencoMart wants to move their User Profiles database to Google Cloud. Which Google Database should they use?",options:{A:"Cloud Spanner",B:"Google BigQuery",C:"Google Cloud SQL",D:"Google Cloud Datastore"},correctAnswer:["D"],explanation:{correct:"The user profiles database is currently a 20TB Oracle DB with a complex structure. For a user profile store, a NoSQL document database like Cloud Datastore (now Firestore in Datastore mode) is often an excellent fit. It provides a flexible schema, automatic scaling, and is a fully managed service, which aligns with the technical requirement to 'leverage managed services wherever feasible'. Migrating a complex relational schema for user profiles to a more flexible NoSQL model can often simplify the application and improve scalability.",incorrect:{A:"Cloud Spanner is a globally distributed relational database. While powerful, it might be overly complex and expensive for a user profile store, and migrating a 20TB Oracle schema would be a significant effort.",B:"BigQuery is an analytical data warehouse, completely unsuitable for serving a transactional application's user profile data.",C:"Cloud SQL is a managed relational database. Migrating a 20TB complex Oracle database to Cloud SQL (MySQL or PostgreSQL) would be a challenging migration project and might not offer the same effortless scalability as Datastore for this use case."}},conditions:["Migrate a 20TB Oracle database for User Profiles","Requirement to leverage managed services"],caseStudyId:"cs_jencostore"},{id:105,topic:"Databases",question:"You need to analyze and define the technical architecture for the database workloads for your company, Mountkirk Games. Considering the business and technical requirements, what should you do?",options:{A:"Use Cloud SQL for time series data, and use Cloud Bigtable for historical data queries.",B:"Use Cloud SQL to replace MySQL, and use Cloud Spanner for historical data queries.",C:"Use Cloud Bigtable to replace MySQL, and use BigQuery for historical data queries.",D:"Use Cloud Bigtable for time series data, use Cloud Spanner for transactional data, and use BigQuery for historical data queries."},correctAnswer:["D"],explanation:{correct:"This option correctly maps each distinct database requirement from the case study to the most appropriate GCP service: Cloud Spanner for the transactional database (user profiles, game state); Cloud Bigtable for the time-series database (game activity); and BigQuery for the analytics platform (historical data queries). This 'purpose-built database' approach is a Google-recommended best practice.",incorrect:{A:"This misaligns the services. Cloud SQL is not a time-series database, and Bigtable is not a general-purpose historical query engine in the same way as BigQuery.",B:"This misaligns the services. Spanner is for transactional data, not historical analytics.",C:"This is partially correct (BigQuery for historical data), but Bigtable is not a direct replacement for a transactional MySQL database; Spanner is a better fit for the transactional requirement."}},conditions:["Define the database architecture for Mountkirk Games","Requirement for a transactional database service","Requirement for a time-series database service","Requirement for querying 10+ TB of historical data"],caseStudyId:"cs_mountkirk_main"},{id:106,topic:"Databases",question:"Which managed storage option meets Mountkirk's technical requirement for storing game activity in a time series database service?",options:{A:"Cloud Bigtable",B:"Cloud Spanner",C:"BigQuery",D:"Cloud Datastore"},correctAnswer:["A"],explanation:{correct:"Cloud Bigtable is Google Cloud's managed NoSQL wide-column database, and it is specifically optimized for large-scale, low-latency workloads, including time-series data like game activity events. Its data model and performance characteristics are an excellent fit for this requirement.",incorrect:{B:"Cloud Spanner is a relational database for transactional workloads.",C:"BigQuery is a data warehouse for analytical queries, not a low-latency time-series database for real-time ingestion.",D:"Cloud Datastore is a NoSQL document database, which is less optimized for high-volume time-series data patterns compared to Bigtable."}},conditions:["Meet the technical requirement for a time-series database service","Store game activity data"],caseStudyId:"cs_mountkirk_main"},{id:107,topic:"Databases",question:"Your company is building a new web application for internal users. The application needs to store user session information. You want to use a managed service and minimize costs. Which GCP service should you use?",options:{A:"Memorystore for Redis",B:"Firestore",C:"Cloud SQL",D:"Cloud Spanner"},correctAnswer:["A"],explanation:{correct:"Memorystore for Redis is a fully managed, in-memory data store that provides extremely low latency, which is ideal for session management. As a managed service, it reduces operational overhead. For session storage, you can often use smaller, more cost-effective instances compared to a full-fledged persistent database.",incorrect:{B:"Firestore is a persistent NoSQL database. While it can store session data, its latency will be higher than an in-memory store like Redis, and it may be less cost-effective for this specific caching use case.",C:"Cloud SQL is a relational database, which is typically overkill in terms of complexity, cost, and performance for simple key-value session storage.",D:"Cloud Spanner is a globally distributed relational database, which is far too complex and expensive for a simple internal application's session store."}},conditions:["Internal web application","Need to store user session information","Use a managed service","Minimize costs"],caseStudyId:null},{id:108,topic:"Databases",question:"You are deploying an application that requires a relational database with strong consistency and horizontal scalability across multiple regions to serve a global user base with low latency reads and writes. Which database service is most appropriate?",options:{A:"Cloud SQL with cross-region replicas",B:"Cloud Spanner",C:"Firestore",D:"Cloud Bigtable"},correctAnswer:["B"],explanation:{correct:"Cloud Spanner is the only database service on this list that meets all the requirements. It is a relational database (with SQL support and ACID transactions) that is globally distributed, provides strong external consistency, and scales horizontally for both reads and writes across multiple regions.",incorrect:{A:"Cloud SQL provides cross-region replicas for read scaling and disaster recovery, but all writes must go to a single primary region, and replication is asynchronous, so it does not offer multi-region writes or global strong consistency.",C:"Firestore is a NoSQL document database, not relational. While it has multi-region capabilities, it offers eventual consistency for global reads, not strong consistency.",D:"Cloud Bigtable is a NoSQL wide-column store, not a relational database, and it provides eventual consistency for replication."}},conditions:["Relational database required","Strong consistency required","Horizontal scalability across multiple regions required","Global user base with low latency reads and writes"],caseStudyId:null},{id:109,topic:"DevOps",question:"Mountkirk Games wants to set up a continuous delivery pipeline. Their architecture includes many small services that they want to be able to update and roll back quickly. Mountkirk Games has the following requirements: Services are deployed redundantly across multiple regions in the US and Europe. Only frontend services are exposed on the public internet. They can provide a single frontend IP for their fleet of services. Deployment artifacts are immutable. Which set of products should they use?",options:{A:"Google Cloud Storage, Google Cloud Dataflow, Google Compute Engine",B:"Google Cloud Storage, Google App Engine, Google Network Load Balancer",C:"Google Kubernetes Registry, Google Container Engine, Google HTTP(S) Load Balancer",D:"Google Cloud Functions, Google Cloud Pub/Sub, Google Cloud Deployment Manager"},correctAnswer:["C"],explanation:{correct:"This combination is ideal for a containerized microservices architecture with continuous delivery. Artifact Registry (formerly GCR) stores immutable Docker images. Google Kubernetes Engine (GKE) orchestrates the deployment of these services across multiple regions. A Global HTTP(S) Load Balancer provides a single frontend IP and can route traffic to the appropriate regional GKE clusters, exposing only the necessary frontend services.",incorrect:{A:"This set of products is primarily for data processing and storage, not for serving containerized web services.",B:"A Network Load Balancer (Layer 4) is less suitable for HTTP(S) traffic than an HTTP(S) Load Balancer (Layer 7) as it doesn't understand URLs or headers for routing.",D:"This is a serverless stack, which might not be suitable for deploying a complex fleet of existing small services without significant re-architecture."}},conditions:["Architecture includes many small services","Need for quick updates and rollbacks","Redundant deployment across multiple regions (US, Europe)","Only frontend services exposed to the public internet","Requirement for a single frontend IP","Deployment artifacts must be immutable"],caseStudyId:"cs_mountkirk_main"},{id:110,topic:"DevOps",question:"Mountkirk Games wants you to design their new testing strategy. How should the test coverage differ from their existing backends on the other platforms?",options:{A:"Tests should scale well beyond the prior approaches.",B:"Unit tests are no longer required, only end-to-end tests.",C:"Tests should be applied after the release is in the production environment.",D:"Tests should include directly testing the Google Cloud Platform (GCP) infrastructure."},correctAnswer:["A"],explanation:{correct:"The case study explicitly mentions that their previous game did not scale well, which was a primary motivation for moving to GCP. Therefore, a key difference in the new testing strategy must be a focus on performance and load testing at a scale that far exceeds what was previously possible or tested. This is to validate that the new cloud-native architecture can handle the expected dynamic scaling.",incorrect:{B:"Unit tests are always a fundamental part of good software engineering and are still required.",C:"Testing should be done *before* a release is in the production environment to prevent outages.",D:"You should test your application *on* GCP, not test the GCP infrastructure itself. Google is responsible for testing its own infrastructure."}},conditions:["Designing a new testing strategy for a GCP-based backend","Previous platform had scaling issues","The new GCP backend is designed for dynamic scaling"],caseStudyId:"cs_mountkirk_main"},{id:111,topic:"DevOps",question:"Dress4Win has end-to-end tests covering 100% of their endpoints. They want to ensure that the move to the cloud does not introduce any new bugs. Which additional testing methods should the developers employ to prevent an outage?",options:{A:"They should enable Google Stackdriver Debugger on the application code to show errors in the code.",B:"They should add additional unit tests and production scale load tests on their cloud staging environment.",C:"They should run the end-to-end tests in the cloud staging environment to determine if the code is working as intended.",D:"They should add canary tests so developers can measure how much of an impact the new release causes to latency."},correctAnswer:["B"],explanation:{correct:"While running existing end-to-end tests is a necessary step (as in C), it's not sufficient. A migration can introduce subtle changes. Additional unit tests help catch regressions at a granular level. Crucially, production-scale load tests are needed to uncover performance bottlenecks, race conditions, and other issues that only appear under significant load in the new cloud environment. This is critical for preventing outages.",incorrect:{A:"Cloud Debugger is a tool for troubleshooting and debugging live code, not a pre-release testing method.",C:"This is a good and necessary step, but it's not an *additional* testing method and may not be sufficient on its own to prevent all types of new bugs (especially performance-related ones).",D:"Canary tests are a deployment strategy for gradually rolling out a release to production, not a testing method to be employed beforehand."}},conditions:["Existing 100% end-to-end test coverage","Migrating to the cloud","Goal is to prevent new bugs and outages","Need for additional testing methods beyond existing end-to-end tests"],caseStudyId:"cs_dress4win_v1"},{id:112,topic:"DevOps",question:"Your company is developing a web-based application. You need to make sure that production deployments are linked to source code commits and are fully auditable. What should you do?",options:{A:"Make sure a developer is tagging the code commit with the date and time of commit",B:"Make sure a developer is adding a comment to the commit that links to the deployment.",C:"Make the container tag match the source code commit hash.",D:"Make sure the developer is tagging the commits with :latest"},correctAnswer:["C"],explanation:{correct:"Using the unique Git commit hash as the tag for the container image (or any build artifact) creates a direct, immutable, and unambiguous link between the deployed artifact and the exact version of the source code it was built from. This is a fundamental practice for creating a traceable and auditable CI/CD pipeline.",incorrect:{A:"Tagging with date and time is less precise than a commit hash and can be prone to errors or ambiguity.",B:"Relying on manual comments is not a reliable or automatable process for auditing.",D:"Using the `:latest` tag is an anti-pattern for production deployments because it is mutable (it can point to different images over time), which breaks traceability and makes it impossible to know which version of the code is actually running."}},conditions:["Developing a web-based application","Need to link production deployments to source code commits","Deployments must be fully auditable"],caseStudyId:null},{id:113,topic:"DevOps",question:"You are building a continuous deployment pipeline for a project stored in a Git source repository and want to ensure that code changes can be verified deploying to production. What should you do?",options:{A:"Use Spinnaker to deploy builds to production using the red/black deployment strategy so that changes can easily be rolled back.",B:"Use Spinnaker to deploy builds to production and run tests on production deployments.",C:"Use Jenkins to build the staging branches and the master branc Build and deploy changes to production for 10% of users before doing a complete rollout.",D:"Use Jenkins to monitor tags in the repositor Deploy staging tags to a staging environment for testing.After testing, tag the repository for production and deploy that to the production environment."},correctAnswer:["D"],explanation:{correct:"This option describes a robust and standard CI/CD workflow that enforces verification. It uses a dedicated staging environment for testing and uses Git tags to clearly signal which code versions are intended for which environment. This process ensures that code is thoroughly tested in a separate, production-like environment before it is ever promoted and deployed to production.",incorrect:{A:"A red/black (or blue/green) deployment is a strategy for the production deployment itself, but it doesn't describe the critical pre-production verification step.",B:"Running tests directly on production deployments is risky and should be avoided.",C:"This describes a canary release, which is a good strategy for rolling out to production, but option D better describes the distinct verification phase in a staging environment that should happen *before* any production exposure."}},conditions:["Building a continuous deployment pipeline","Source code is in a Git repository","Need to ensure changes are verified before deploying to production"],caseStudyId:null},{id:114,topic:"DevOps",question:"You want to automate the creation of a managed instance group and a startup script to install the OS package dependencies. You want to minimize the startup time for VMs in the instance group. What should you do?",options:{A:"Use Terraform to create the managed instance group and a startup script to install the OS package dependencies.",B:"Create a custom VM image with all OS package dependencies. Use Deployment Manager to create the managed instance group with the VM image.",C:"Use Puppet to create the managed instance group and install the OS package dependencies.",D:"Use Deployment Manager to create the managed instance group and Ansible to install the OS package dependencies."},correctAnswer:["B"],explanation:{correct:"The key to minimizing startup time is to 'bake' dependencies into a custom VM image. By pre-installing all OS packages and application dependencies into an image, new VMs created from this image can start and become ready to serve traffic much faster because they don't have to run lengthy installation scripts at boot time. Using an Infrastructure as Code tool like Deployment Manager (or Terraform) to automate the MIG creation from this custom image is a best practice.",incorrect:{A:"This uses a startup script to install dependencies, which directly contradicts the goal of minimizing startup time.",C:"This uses a configuration management tool at startup, which is slower than using a pre-baked image.",D:"This also uses a configuration management tool at startup, which is slower than using a pre-baked image."}},conditions:["Automate the creation of a Managed Instance Group","VMs have OS package dependencies that need to be installed","Requirement to minimize the startup time for VMs"],caseStudyId:null},{id:115,topic:"DevOps",question:"You need to develop procedures to test a disaster plan for a mission-critical application. You want to use Google-recommended practices and native capabilities within GCP. What should you do?",options:{A:"Use Deployment Manager to automate service provisioning. Use Activity Logs to monitor and debug your tests.",B:"Use Deployment Manager to automate service provisioning. Use Stackdriver to monitor and debug your tests.",C:"Use gcloud scripts to automate service provisioning. Use Activity Logs to monitor and debug your tests.",D:"Use gcloud scripts to automate service provisioning. Use Stackdriver to monitor and debug your tests."},correctAnswer:["B"],explanation:{correct:"This option combines two Google-recommended best practices. Cloud Deployment Manager allows you to automate the provisioning of your DR environment using an Infrastructure as Code approach, ensuring consistency and repeatability. The Cloud Operations suite (formerly Stackdriver) provides the necessary tools (Monitoring, Logging, etc.) to comprehensively monitor the health and performance of the system during a DR test and to debug any issues that arise.",incorrect:{A:"Activity Logs show API calls but are not sufficient for monitoring the performance and health of an application during a DR test.",C:"While gcloud scripts can be used for automation, Deployment Manager provides a more robust, declarative IaC approach. Activity logs are insufficient for monitoring.",D:"Deployment Manager is generally preferred over gcloud scripts for managing complex infrastructure as code."}},conditions:["Develop disaster recovery test procedures","Use Google-recommended practices and native GCP capabilities"],caseStudyId:null},{id:116,topic:"DevOps",question:"You need to ensure reliability for your application and operations by supporting reliable task a scheduling for compute on GCP. Leveraging Google best practices, what should you do?",options:{A:"Using the Cron service provided by App Engine, publishing messages directly to a message-processing utility service running on Compute Engine instances.",B:"Using the Cron service provided by App Engine, publish messages to a Cloud Pub/Sub topic. Subscribe to that topic using a message-processing utility service running on Compute Engine instances.",C:"Using the Cron service provided by Google Kubernetes Engine (GKE), publish messages directly to a message-processing utility service running on Compute Engine instances.",D:"Using the Cron service provided by GKE, publish messages to a Cloud Pub/Sub topic. Subscribe to that topic using a message-processing utility service running on Compute Engine instances."},correctAnswer:["B"],explanation:{correct:"This describes a robust, decoupled, and reliable architecture. A scheduler (App Engine Cron or Cloud Scheduler) triggers the task by publishing a message to a Pub/Sub topic. Pub/Sub acts as a durable and scalable buffer. A separate worker service (on Compute Engine, GKE, Cloud Run, etc.) subscribes to the topic to process the tasks. This decouples the scheduler from the worker, provides at-least-once delivery guarantees, and allows for independent scaling of components.",incorrect:{A:"Directly calling a service from a cron job creates a tight coupling and is less reliable. If the worker service is down, the task is lost. Pub/Sub provides a buffer and retries.",C:"GKE uses CronJobs, which run inside the cluster. A direct call to a GCE instance is less reliable than using Pub/Sub.",D:"This is a valid pattern, but App Engine Cron (or Cloud Scheduler) is a more common choice for a general-purpose, managed scheduler than a GKE CronJob for triggering external services."}},conditions:["Ensure reliable task scheduling for compute on GCP","Leverage Google best practices"],caseStudyId:null},{id:117,topic:"DevOps",question:"Your team will start developing a new application using microservices architecture on Kubernetes Engine. As part of the development lifecycle, any code change that has been pushed to the remote develop branch on your GitHub repository should be built and tested automatically. When the build and test are successful, the relevant microservice will be deployed automatically in the development environment. You want to ensure that all code deployed in the development environment follows this process. What should you do?",options:{A:"Have each developer install a pre-commit hook on their workstation that tests the code and builds the container when committing on the development branch. After a successful commit, have the developer deploy the newly built container image on the development cluster.",B:"Install a post-commit hook on the remote git repository that tests the code and builds the container when code is pushed to the development branch. After a successful commit, have the developer deploy the newly built container image on the development cluster.",C:"Create a Cloud Build trigger based on the development branch that tests the code, builds the container, and stores it in Container Registry. Create a deployment pipeline that watches for new images and deploys the new image on the development cluster. Ensure only the deployment tool has access to deploy new versions.",D:"Create a Cloud Build trigger based on the development branch to build a new container image and store it in Container Registry. Rely on Vulnerability Scanning to ensure the code tests succeed. As the final step of the Cloud Build process, deploy the new container image on the development cluster. Ensure only Cloud Build has access to deploy new versions."},correctAnswer:["C"],explanation:{correct:"This describes a robust, server-side CI/CD pipeline. Using a Cloud Build trigger automates the build and test process. A separate, automated deployment pipeline (e.g., Cloud Deploy, Spinnaker, or another Cloud Build step) that triggers on a new successful image provides a clean separation of concerns. Crucially, enforcing this process by restricting deployment permissions to only the automated tool's service account prevents manual deployments that bypass the pipeline.",incorrect:{A:"Client-side hooks are unreliable as they can be easily bypassed by developers.",B:"This still relies on a manual deployment step by the developer, which is not fully automated and less controlled.",D:"Vulnerability Scanning is for security, not for ensuring that functional code tests succeed. These are two different types of checks."}},conditions:["New microservices application on GKE","Automated CI/CD pipeline triggered on push to 'develop' branch","Process must include automatic build, test, and deployment to dev environment","Need to enforce that all deployments follow this process"],caseStudyId:null},{id:118,topic:"DevOps",question:"You are developing your microservices application on Google Kubernetes Engine. During testing, you want to validate the behavior of your application in case a specific microservice should suddenly crash. What should you do?",options:{A:"Add a taint to one of the nodes of the Kubernetes cluster. For the specific microservice, configure a pod anti-affinity label that has the name of the tainted node as a value.",B:"Use Istios fault injection on the particular microservice whose faulty behavior you want to simulate.",C:"Destroy one of the nodes of the Kubernetes cluster to observe the behavior.",D:"Configure Istios traffic management features to steer the traffic away from a crashing microservice."},correctAnswer:["B"],explanation:{correct:"Istio's (or Anthos Service Mesh's) fault injection feature is designed for exactly this type of resilience testing, often called chaos engineering. You can configure it to introduce failures, such as returning HTTP 503 errors (simulating a crash) for a percentage of requests to a specific microservice, allowing you to test how upstream services handle the failure without actually crashing any pods.",incorrect:{A:"Taints and anti-affinity are used to control pod scheduling, not to simulate application crashes.",C:"Destroying a node is a much more drastic infrastructure failure test, not a test for a specific microservice crash.",D:"Steering traffic away is a resiliency pattern to *handle* a failure, not a method to *simulate* a failure for testing purposes."}},conditions:["Microservices application on GKE","During testing, need to validate application behavior when a specific microservice crashes"],caseStudyId:null},{id:119,topic:"DevOps",question:"Your team is developing a web application that will be deployed on Google Kubernetes Engine (GKE). Your CTO expects a successful launch and you need to ensure your application can handle the expected load of tens of thousands of users. You want to test the current deployment to ensure the latency of your application stays below a certain threshold. What should you do?",options:{A:"Use a load testing tool to simulate the expected number of concurrent users and total requests to your application, and inspect the results.",B:"Enable autoscaling on the GKE cluster and enable horizontal pod autoscaling on your application deployments. Send curl requests to your application, and validate if the auto scaling works.",C:"Replicate the application over multiple GKE clusters in every Google Cloud region. Configure a global HTTP(S) load balancer to expose the different clusters over a single global IP address.",D:"Use Cloud Debugger in the development environment to understand the latency between the different microservices."},correctAnswer:["A"],explanation:{correct:"Load testing is the standard method for validating an application's performance and scalability under a specific expected load. Using a dedicated tool (like JMeter, k6, Locust) allows you to simulate realistic traffic patterns and measure key performance indicators like latency and error rate to see if they meet the required thresholds.",incorrect:{B:"Sending a few `curl` requests is not a load test. This only validates that autoscaling is configured, not how the application performs at scale.",C:"This describes a production deployment strategy for high availability and global performance, not a method for testing the capacity of the current deployment.",D:"Cloud Debugger is for inspecting the state of code during execution; it is not a tool for performance or load testing."}},conditions:["Web application deployed on GKE","Need to ensure it can handle an expected load of tens of thousands of users","Test the latency to ensure it stays below a certain threshold"],caseStudyId:null},{id:120,topic:"DevOps",question:"Your company's test suite is a custom C++ application that runs tests throughout each day on Linux virtual machines. The full test suite takes several hours to complete, running on a limited number of on-premises servers reserved for testing. Your company wants to move the testing infrastructure to the cloud, to reduce the amount of time it takes to fully test a change to the system, while changing the tests as little as possible. Which cloud infrastructure should you recommend?",options:{A:"Google Compute Engine unmanaged instance groups and Network Load Balancer",B:"Google Compute Engine managed instance groups with auto-scaling",C:"Google Cloud Dataproc to run Apache Hadoop jobs to process each test",D:"Google App Engine with Google StackDriver for logging"},correctAnswer:["B"],explanation:{correct:"The best way to reduce test time is to parallelize the tests. A Managed Instance Group (MIG) with auto-scaling can spin up a large number of VMs on-demand to run tests in parallel and then scale down to zero when finished, which is both fast and cost-effective. This allows the existing C++ application to run on standard Linux VMs with minimal changes.",incorrect:{A:"Unmanaged instance groups lack the auto-scaling and auto-healing benefits that are crucial for an elastic and resilient testing environment.",C:"Cloud Dataproc is for Hadoop/Spark workloads and is not suitable for running a custom C++ test suite.",D:"App Engine is a PaaS for web applications and does not support running arbitrary C++ applications in this manner."}},conditions:["Custom C++ test suite running on Linux VMs","Current process is slow due to limited on-premises servers","Goal is to move to the cloud to reduce test time","Minimize changes to the existing test application"],caseStudyId:null},{id:121,topic:"DevOps",question:"You are helping the QA team to roll out a new load-testing tool to test the scalability of your primary cloud services that run on Google Compute Engine with Cloud Bigtable. Which three requirements should they include?",options:{A:"Ensure that the load tests validate the performance of Cloud Bigtable.",B:"Create a separate Google Cloud project to use for the load-testing environment.",C:"Schedule the load-testing tool to regularly run against the production environment.",D:"Ensure all third-party systems your services use are capable of handling high load.",E:"Instrument the production services to record every transaction for replay by the load- testing tool.",F:"Instrument the load-testing tool and the target services with detailed logging and metrics collection."},correctAnswer:["A","B","F"],explanation:{correct:"A, B, and F represent core principles of effective load testing. You must test the critical components (A: Cloud Bigtable). You must isolate your test environment to avoid impacting production (B: separate project). You must have comprehensive monitoring in place for both the load generator and the system under test to analyze the results (F: detailed logging and metrics).",incorrect:{C:"Running load tests against production is extremely risky and should only be done with extreme caution and careful planning, if at all. It is not a general requirement.",D:"While important for overall system health, ensuring third-party systems are ready might be out of scope for the initial rollout of a tool to test *your* primary services.",E:"Recording and replaying production traffic can be a valid testing strategy, but it's complex and not a universal requirement. It also may not accurately reflect future traffic patterns."}},conditions:["Roll out a new load-testing tool","Target services run on GCE with Cloud Bigtable","Goal is to test scalability"],caseStudyId:null},{id:122,topic:"DevOps",question:"Your company places a high value on being responsive and meeting customer needs quickly. Their primary business objectives are release speed and agility. You want to reduce the chance of security errors being accidentally introduced. Which two actions can you take?",options:{A:"Ensure every code check-in is peer reviewed by a security SME.",B:"Use source code security analyzers as part of the CI/CD pipeline.",C:"Ensure you have stubs to unit test all interfaces between components.",D:"Enable code signing and a trusted binary repository integrated with your CI/CD pipeline.",E:"Run a vulnerability security scanner as part of your continuous-integration /continuous- delivery (CI/CD) pipeline."},correctAnswer:["B","E"],explanation:{correct:"To balance agility with security ('DevSecOps'), automating security checks is key. Integrating source code analyzers (B: Static Application Security Testing - SAST) and vulnerability scanners (E: Dynamic Application Security Testing - DAST) into the CI/CD pipeline allows for automated, early detection of security flaws without creating a manual bottleneck.",incorrect:{A:"Requiring a security SME to review every check-in would create a significant bottleneck and hinder release speed and agility.",C:"Unit testing interfaces is good for functional correctness but is not a primary security measure.",D:"Code signing and trusted repositories are for supply chain security (ensuring integrity of artifacts), not for finding security bugs in the code itself."}},conditions:["Primary business objectives are release speed and agility","Need to reduce the chance of introducing security errors"],caseStudyId:null},{id:123,topic:"DevOps",question:"Mountkirk Games wants you to design their new testing strategy. How should the test coverage differ from their existing backends on the other platforms?",options:{A:"Tests should scale well beyond the prior approaches",B:"Unit tests are no longer required, only end-to-end tests",C:"Tests should be applied after the release is in the production environment",D:"Tests should include directly testing the Google Cloud Platform (GCP) infrastructure"},correctAnswer:["A"],explanation:{correct:"The case study explicitly states that their previous game 'did not scale well,' which was a primary motivation for moving to GCP. Therefore, a key difference in the new testing strategy must be a focus on performance and load testing at a scale that far exceeds what was previously possible or tested. This is to validate that the new cloud-native architecture can handle the expected dynamic scaling.",incorrect:{B:"Unit tests are always a fundamental part of good software engineering and are still required.",C:"Testing should be done *before* a release is in the production environment to prevent outages.",D:"You should test your application *on* GCP, not test the GCP infrastructure itself. Google is responsible for testing its own infrastructure."}},conditions:["Design new testing strategy for GCP","Compare with previous testing","Previous platform had scaling problems"],caseStudyId:"cs_mountkirk_main"},{id:124,topic:"DevOps",question:"Mountkirk Games has deployed their new backend on Google Cloud Platform (GCP). You want to create a through testing process for new versions of the backend before they are released to the public. You want the testing environment to scale in an economical way. How should you design the process?",options:{A:"Create a scalable environment in GCP for simulating production load",B:"Use the existing infrastructure to test the GCP-based backend at scale",C:"Build stress tests into each component of your application using resources internal to GCP to simulate load",D:"Create a set of static environments in GCP to test different levels of load for example, high, medium, and low"},correctAnswer:["A"],explanation:{correct:"To properly test a cloud-native backend, the test environment must be able to replicate production scale. Creating a dedicated, scalable test environment in GCP allows you to spin up the necessary resources to simulate production load, run the tests, and then tear the resources down to save costs. This is both effective and economical.",incorrect:{B:"Existing (presumably on-premises or smaller) infrastructure is unlikely to be able to generate sufficient load to test a scalable GCP backend.",C:"Internal stress tests are good for component-level testing but don't replace a full, end-to-end load test that simulates real user traffic.",D:"Static environments are not economical. It's expensive to keep a 'high' load environment running permanently. A dynamic, on-demand environment is more cost-effective."}},conditions:["New backend deployed on GCP","Need thorough pre-release testing","Testing environment must scale economically"],caseStudyId:"cs_mountkirk_main"},{id:125,topic:"DevOps",question:"Mountkirk Games wants to set up a continuous delivery pipeline. Their architecture includes many small services that they want to be able to update and roll back quickly. Mountkirk Games has the following requirements: Services are deployed redundantly across multiple regions in the US and Europe. Only frontend services are exposed on the public internet. They can provide a single frontend IP for their fleet of services. Deployment artifacts are immutable. Which set of products should they use?",options:{A:"Google Cloud Storage, Google Cloud Dataflow, Google Compute Engine",B:"Google Cloud Storage, Google App Engine, Google Network Load Balancer",C:"Google Kubernetes Registry, Google Container Engine, Google HTTP(S) Load Balancer",D:"Google Cloud Functions, Google Cloud Pub/Sub, Google Cloud Deployment Manager"},correctAnswer:["C"],explanation:{correct:"This combination is ideal for a containerized microservices architecture with continuous delivery. Artifact Registry (formerly GCR) stores immutable Docker images. Google Kubernetes Engine (GKE) orchestrates the deployment of these services across multiple regions. A Global HTTP(S) Load Balancer provides a single frontend IP and can route traffic to the appropriate regional GKE clusters, exposing only the necessary frontend services.",incorrect:{A:"This set of products is primarily for data processing and storage, not for serving containerized web services.",B:"A Network Load Balancer (Layer 4) is less suitable for HTTP(S) traffic than an HTTP(S) Load Balancer (Layer 7) as it doesn't understand URLs or headers for routing.",D:"This is a serverless stack, which might not be suitable for deploying a complex fleet of existing small services without significant re-architecture."}},conditions:["CD pipeline for small services","Quick updates/rollbacks","Multi-region deployment (US, EU)","Only frontend public","Single frontend IP","Immutable artifacts"],caseStudyId:"cs_mountkirk_main"},{id:126,topic:"DevOps",question:"Mountkirk Games wants you to design their solution for the future in order to take advantage of cloud and technology improvements as they become available. Which two steps should they take?",options:{A:"Store as much analytics and game activity data as financially feasible today so it can be used to train machine learning models to predict user behavior in the future.",B:"Begin packaging their game backend artifacts in container images and running them on Google Kubernetes Engine to improve the ability to scale up or down based on game activity.",C:"Set up a CI/CD pipeline using Jenkins and Spinnaker to automate canary deployments and improve development velocity.",D:"Adopt a schema versioning tool to reduce downtime when adding new game features that require storing additional player data in the database.",E:"Implement a weekly rolling maintenance process for the Linux virtual machines so they can apply critical kernel patches and package updates and reduce the risk of 0-day vulnerabilities."},correctAnswer:["B","C"],explanation:{correct:"To future-proof the solution, adopting modern cloud-native practices is key. Containerizing the application (B) and running it on an orchestrator like GKE provides portability, scalability, and makes it easier to adopt new technologies. Implementing a sophisticated CI/CD pipeline (C) with automated canary deployments improves release velocity and reliability, which are crucial for staying competitive and iterating quickly.",incorrect:{A:"Storing data for ML is a good data strategy but is not a foundational architectural step for future-proofing the deployment and operations of the application itself.",D:"Schema versioning is a good practice but is a specific detail of database management, not a broad architectural step.",E:"While VM maintenance is important, a move towards managed container platforms like GKE reduces the burden of direct VM maintenance, making it a less forward-looking action compared to B and C."}},conditions:["Design solution for the future","Take advantage of future cloud/tech improvements"],caseStudyId:"cs_mountkirk_main"},{id:127,topic:"DevOps",question:"Mountkirk Games wants you to design a way to test the analytics platforms resilience to changes in mobile network latency. What should you do?",options:{A:"Deploy failure injection software to the game analytics platform that can inject additional latency to mobile client analytics traffic.",B:"Build a test client that can be run from a mobile phone emulator on a Compute Engine virtual machine, and run multiple copies in Google Cloud Platform regions all over the world to generate realistic traffic.",C:"Add the ability to introduce a random amount of delay before beginning to process analytics files uploaded from mobile devices.",D:"Create an opt-in beta of the game that runs on players mobile devices and collects response times from analytics endpoints running in Google Cloud Platform regions all over the world."},correctAnswer:["C"],explanation:{correct:"The requirement is to test the *analytics platform's resilience* to the *effects* of mobile network latency. The primary effect is that data arrives late or out of order. By adding an artificial, random delay *at the processing stage* (C), you can directly simulate this condition in a controlled way and test if the platform (e.g., a Dataflow pipeline) correctly handles late-arriving data without needing to simulate the network itself.",incorrect:{A:"Injecting latency *within* the platform tests internal component resilience, not the platform's ability to handle data that *arrives* late from the outside world.",B:"This can generate traffic but doesn't provide a controlled way to test the specific resilience logic for handling late data within the processing pipeline.",D:"A beta program is for gathering real-world data, not for controlled resilience testing of specific failure modes."}},conditions:["Test analytics platform resilience","Simulate changes in mobile network latency","The platform processes uploaded files and must handle late data"],caseStudyId:"cs_mountkirk_main"},{id:128,topic:"DevOps",question:"Your development team has created a mobile game app. You want to test the new mobile app on Android and iOS devices with a variety of configurations. You need to ensure that testing is efficient and cost-effective. What should you do?",options:{A:"Upload your mobile app to the Firebase Test Lab, and test the mobile app on Android and iOS devices.",B:"Create Android and iOS VMs on Google Cloud, install the mobile app on the VMs, and test the mobile app.",C:"Create Android and iOS containers on Google Kubernetes Engine (GKE), install the mobile app on the containers, and test the mobile app.",D:"Upload your mobile app with different configurations to Firebase Hosting and test each configuration."},correctAnswer:["A"],explanation:{correct:"Firebase Test Lab is a cloud-based service specifically designed for testing mobile apps on a wide range of real and virtual devices. It is the most efficient and cost-effective way to get broad device coverage without the overhead of maintaining a physical device lab or managing emulators manually.",incorrect:{B:"GCP does not provide standard Android and iOS VMs for app testing. Running emulators on standard Linux/Windows VMs is possible but inefficient to manage at scale.",C:"You cannot run Android or iOS apps directly in standard GKE containers.",D:"Firebase Hosting is for web content, not for testing native mobile applications on devices."}},conditions:["Test a new mobile game app on both Android and iOS","Need to test on a variety of device configurations","Testing must be efficient and cost-effective"],caseStudyId:"cs_mountkirk_main"},{id:129,topic:"DevOps",question:"You are building a microservice-based application for TerramEarth. The application is based on Docker containers. You want to follow Google-recommended practices to build the application continuously and store the build artifacts. What should you do?",options:{A:"Configure a trigger in Cloud Build for new source changes. Invoke Cloud Build to build container images for each microservice, and tag them using the code commit hash. Push the images to the Container Registry.",B:"Configure a trigger in Cloud Build for new source changes. The trigger invokes build jobs and build container images for the microservices. Tag the images with a version number, and push them to Cloud Storage.",C:"Create a Scheduler job to check the repo every minute. For any new change, invoke Cloud Build to build container images for the microservices. Tag the images using the current timestamp, and push them to the Container Registry.",D:"Configure a trigger in Cloud Build for new source changes. Invoke Cloud Build to build one container image, and tag the image with the tag latest. Push the image to the Container Registry."},correctAnswer:["A"],explanation:{correct:"This option describes a best-practice CI pipeline. Using a Cloud Build trigger on source changes automates the process. Building images for each microservice is correct for a microservices architecture. Tagging with the commit hash provides immutable, traceable artifacts. Pushing to Artifact Registry (formerly Container Registry) is the correct place to store container images.",incorrect:{B:"Container images should be pushed to Artifact Registry, not Cloud Storage.",C:"Polling the repository with a scheduler is inefficient compared to using event-driven triggers.",D:"Using the 'latest' tag is an anti-pattern for CI/CD as it's mutable and breaks versioning. You should also build an image for each microservice, not just one."}},conditions:["Build a microservice-based application using Docker containers","Build the application continuously","Store the build artifacts","Follow Google-recommended practices"],caseStudyId:"cs_terram_main"},{id:130,topic:"DevOps",question:"Considering the given Business Requirements, how would you automate the deployment of web and transactional data layers?",options:{A:"Deploy Nginx and Tomcat using Cloud Deployment Manager to Compute Engine. Deploy a Cloud SQL server to replace MySQL. Deploy Jenkins using Cloud Deployment Manager.",B:"Deploy Nginx and Tomcat using Cloud Launcher. Deploy a MySQL server using Cloud Launcher. Deploy Jenkins to Compute Engine using Cloud Deployment Manager scripts.",C:"Migrate Nginx and Tomcat to App Engine. Deploy a Cloud Datastore server to replace the MySQL server in a high-availability configuration. Deploy Jenkins to Compute Engine using Cloud Launcher.",D:"Migrate Nginx and Tomcat to App Engine. Deploy a MySQL server using Cloud Launcher. Deploy Jenkins to Compute Engine using Cloud Launcher."},correctAnswer:["A"],explanation:{correct:"The requirement is to automate deployment and provisioning. Cloud Deployment Manager is Google's native Infrastructure as Code (IaC) service. It can be used to declaratively define and automate the creation of all required resources: the Compute Engine instances for Nginx/Tomcat, the Cloud SQL instance to replace MySQL, and even the Jenkins instance for CI/CD.",incorrect:{B:"Cloud Launcher (Marketplace) is for deploying pre-packaged solutions, not for automating your custom infrastructure deployments from code.",C:"Migrating to App Engine and Datastore is a major re-architecture, not just automating the deployment of the existing architectural style (web servers + MySQL).",D:"This is also a major re-architecture, not an automated deployment of the existing layers."}},conditions:["Automate deployment of web layer (Nginx/Tomcat)","Automate deployment of transactional data layer (MySQL)","Requirement to implement an automation framework"],caseStudyId:"cs_dress4win_v1"},{id:131,topic:"DevOps",question:"Your company's user-feedback portal comprises a standard LAMP stack replicated across two zones. It is deployed in the us-central1 region and uses autoscaled managed instance groups on all layers, except the database. Currently, only a small group of select customers have access to the portal. The portal meets a 99.99% availability SLA under these conditions However next quarter, your company will be making the portal available to all users, including unauthenticated users. You need to develop a resiliency testing strategy to ensure the system maintains the SLA once they introduce additional user load. What should you do?",options:{A:"Capture existing users input, and replay captured user load until autoscale is triggered on all layers. At the same time, terminate all resources in one of the zones.",B:"Create synthetic random user input, replay synthetic load until autoscale logic is triggered on at least one layer, and introduce 'chaos' to the system by terminating random resources on both zones.",C:"Expose the new system to a larger group of users, and increase group size each day until autoscale logic is triggered on all layers. At the same time, terminate random resources on both zones.",D:"Capture existing users input, and replay captured user load until resource utilization crosses 80%. Also, derive estimated number of users based on existing users usage of the app, and deploy enough resources to handle 200% of expected load."},correctAnswer:["A"],explanation:{correct:"This is a comprehensive resiliency test. It combines realistic load testing (replaying user input until autoscaling is triggered) with a significant failure scenario (terminating all resources in one zone). This test will validate if the system can handle the increased load *and* survive a zonal failure simultaneously, which is crucial for maintaining a 99.99% SLA.",incorrect:{B:"Introducing 'chaos' by terminating random resources is a valid chaos engineering practice, but terminating a whole zone is a more specific and critical test for a multi-zone architecture.",C:"Involving real users in potentially disruptive resiliency tests is not a good practice. These tests should be done in a controlled environment.",D:"This option focuses on load testing and capacity planning but completely misses the critical failure injection component required for *resiliency* testing."}},conditions:["LAMP stack portal with multi-zone MIGs","Currently meets 99.99% SLA","Will be opened to a much larger user base","Need a resiliency testing strategy to ensure the SLA is maintained"],caseStudyId:null},{id:132,topic:"DevOps",question:"You are managing infrastructure using Terraform. You have defined your production environment in Terraform configuration files stored in a Git repository. You want to ensure that only planned changes reviewed and approved via pull requests are applied to the production environment, and that manual changes via the Cloud Console or gcloud CLI are prevented or reverted. What should you do?",options:{A:"Use Cloud Build triggers to automatically apply Terraform configurations on every push to the main branch.",B:"Implement a GitOps workflow using a tool like Config Connector or Flux CD that monitors the Git repository and automatically reconciles the GCP state with the desired state defined in Terraform.",C:"Restrict IAM permissions to remove resource modification rights from all users, granting apply permissions only to a CI/CD service account.",D:"Run a nightly Cloud Function that uses terraform plan to detect drift and alerts the operations team."},correctAnswer:["C"],explanation:{correct:"The most effective way to prevent manual changes and enforce an Infrastructure-as-Code workflow is to control permissions. By removing modification rights from human users and granting them only to a CI/CD service account, you create a chokepoint. The only way to make changes is through the automated pipeline, which is triggered by approved merges to the main branch in Git. This technically enforces the desired workflow.",incorrect:{A:"This describes the automation part, but without restricting IAM permissions, developers could still make manual changes, causing drift.",B:"GitOps tools are a great way to implement the reconciliation, but they also rely on having the correct IAM permissions locked down to be truly effective at preventing manual drift.",D:"This is a reactive approach that only detects drift after it has happened. The goal is to prevent it in the first place."}},conditions:["Managing infrastructure with Terraform and Git","Ensure only approved changes from pull requests are applied","Prevent or revert manual changes (drift)"],caseStudyId:null},{id:133,topic:"DevOps",question:"Cymbal Direct wants to create a pipeline to automate the building of new application releases. What sequence of steps should you use?",options:{A:"Set up a source code repository. Run unit tests. Check in code. Deploy. Build a Docker container.",B:"Check in code. Set up a source code repository. Run unit tests. Deploy. Build a Docker container.",C:"Set up a source code repository. Check in code. Run unit tests. Build a Docker container. Deploy.",D:"Run unit tests. Deploy. Build a Docker container. Check in code. Set up a source code repository."},correctAnswer:["C"],explanation:{correct:"This option follows the logical and standard sequence of a CI/CD pipeline: 1) Have a place for code (repository). 2) A developer commits a change. 3) The change is automatically tested (unit tests). 4) If tests pass, a deployable artifact is built (Docker container). 5) The artifact is deployed.",incorrect:{A:"The steps are out of order. You can't run unit tests or deploy before checking in code, and you build the container before deploying it.",B:"The steps are out of order. You set up the repository before checking in code.",D:"The steps are completely out of logical order."}},conditions:["Automate the building of new application releases","Determine the correct sequence of steps"],caseStudyId:"CS001"},{id:134,topic:"DevOps",question:"Cymbal Direct needs to use a tool to deploy its infrastructure. You want something that allows for repeatable deployment processes, uses a declarative language, and allows parallel deployment. You also want to deploy infrastructure as code on Google Cloud and other cloud providers. What should you do?",options:{A:"Automate the deployment with Terraform scripts.",B:"Automate the deployment using scripts containing gcloud commands.",C:"Use Google Kubernetes Engine (GKE) to create deployments and manifests for your applications.",D:"Develop in Docker containers for portability and ease of deployment."},correctAnswer:["A"],explanation:{correct:"Terraform is an industry-standard Infrastructure as Code (IaC) tool that perfectly matches all the requirements. It uses a declarative language (HCL), supports repeatable and parallel deployments, and is multi-cloud, with extensive support for Google Cloud.",incorrect:{B:"`gcloud` commands are imperative, not declarative. Scripts can be repeatable but are generally less robust for managing state than Terraform.",C:"GKE and its manifests are for deploying applications *onto* Kubernetes, not for deploying the general cloud infrastructure itself.",D:"Docker is for containerizing applications, not for provisioning infrastructure."}},conditions:["Need a tool for infrastructure deployment","Must be repeatable, declarative, and allow parallel deployment","Must support Infrastructure as Code (IaC)","Must support Google Cloud and other cloud providers"],caseStudyId:"CS001"},{id:135,topic:"DevOps",question:"You are asked to implement a lift and shift operation for Cymbal Direct's Social Media Highlighting service. You compose a Terraform configuration file to build all the necessary Google Cloud resources. What is the next step in the Terraform workflow for this effort?",options:{A:"Commit the configuration file to your software repository.",B:"Run terraform plan to verify the contents of the Terraform configuration file.",C:"Run terraform apply to deploy the resources described in the configuration file.",D:"Run terraform init to download the necessary provider modules."},correctAnswer:["D"],explanation:{correct:"The standard Terraform workflow after writing a new configuration file is to first run `terraform init`. This command initializes the working directory by downloading the required provider plugins (in this case, the Google Cloud provider) and setting up the backend for state storage. You must run `init` before you can run `plan` or `apply`.",incorrect:{A:"Committing the file is a good practice for version control, but it is not the next step in the Terraform execution workflow.",B:"You cannot run `terraform plan` until you have first run `terraform init` to initialize the directory and providers.",C:"You should always run `terraform plan` to review the proposed changes before running `terraform apply`."}},conditions:["Implementing a lift and shift with Terraform","A Terraform configuration file has just been composed","Determine the next step in the Terraform workflow"],caseStudyId:"CS001"},{id:136,topic:"DevOps",question:"You have implemented a manual CI/CD process for the container services required for the next implementation of the Cymbal Direct's Drone Delivery project. You want to automate the process. What should you do?",options:{A:"Implement and reference a source repository in your Cloud Build configuration file.",B:"Implement a build trigger that applies your build configuration when a new software update is committed to Cloud Source Repositories.",C:"Specify the name of your Container Registry in your Cloud Build configuration.",D:"Configure and push a manifest file into an environment repository in Cloud Source Repositories."},correctAnswer:["B"],explanation:{correct:"To move from a manual to an automated CI/CD process, the key is to trigger the pipeline automatically based on a source code event. A Cloud Build trigger can be configured to watch a specific branch or tag in a repository (like Cloud Source Repositories). When a new commit is pushed, the trigger will automatically execute the defined build steps, thus automating the process.",incorrect:{A:"Referencing a source repository is part of the configuration, but it does not automate the trigger.",C:"Specifying the registry is a necessary step in the build configuration, but it does not automate the process.",D:"Pushing a manifest is part of a GitOps-style deployment (CD) process, but the question is about automating the initial build (CI) process."}},conditions:["Have a manual CI/CD process for container services","Want to automate the process"],caseStudyId:"CS001"},{id:137,topic:"DevOps",question:"Developers on your team frequently write new versions of the code for one of your applications. You want to automate the build process when updates are pushed to Cloud Source Repositories. What should you do?",options:{A:"Implement a Cloud Build configuration file with build steps.",B:"Implement a build trigger that references your repository and branch.",C:"Set proper permissions for Cloud Build to access deployment resources.",D:"Upload application updates and Cloud Build configuration files to Cloud Source Repositories."},correctAnswer:["B"],explanation:{correct:"The core of automating a build process based on source code changes is the trigger. Implementing a Cloud Build trigger that is connected to your Cloud Source Repository and configured to watch a specific branch (e.g., `main` or `develop`) will automatically start the build process every time a new commit is pushed to that branch. The other options are necessary components of the overall process, but the trigger is what provides the automation.",incorrect:{A:"You need a configuration file, but it doesn't automate the process on its own; it only defines what the process does when triggered.",C:"Permissions are necessary for the pipeline to work, but setting them doesn't create the automation.",D:"Uploading the code is the event that you want to trigger the automation, it's not the automation itself."}},conditions:["Automate the build process","Trigger the process when code updates are pushed to Cloud Source Repositories"],caseStudyId:null},{id:138,topic:"DevOps",question:"Your development team used Cloud Source Repositories, Cloud Build, and Artifact Registry to successfully implement the build portion of an application's CI/CD process. However, the deployment process is erroring out. Initial troubleshooting shows that the runtime environment does not have access to the build images. You need to advise the team on how to resolve the issue. What could cause this problem?",options:{A:"The runtime environment does not have permissions to the Artifact Registry in your current project.",B:"The runtime environment does not have permissions to Cloud Source Repositories in your current project.",C:"The Artifact Registry might be in a different project.",D:"You need to specify the Artifact Registry image by name."},correctAnswer:["A"],explanation:{correct:"This is a classic IAM permission issue. The service account associated with the runtime environment (e.g., the GKE node service account, the Cloud Run service identity, or the Compute Engine VM service account) needs permission to pull images from Artifact Registry. The most common cause of a deployment failing with an access error for an image is that this service account lacks the `Artifact Registry Reader` (`roles/artifactregistry.reader`) role.",incorrect:{B:"Permissions to Cloud Source Repositories are needed for the *build* phase, which is already successful. They are not needed for the *deployment* (runtime) phase.",C:"While having the registry in a different project can complicate permissions, the root cause would still be a lack of permissions for the runtime service account on that other project's registry.",D:"Specifying the image by its full name is required, but an incorrect name would typically result in an 'Image not found' or 'Manifest unknown' error, not an access/permission denied error."}},conditions:["CI/CD process: Build phase (CSR -> Cloud Build -> Artifact Registry) is successful","Deployment phase is failing","Error indicates the runtime environment cannot access the build images"],caseStudyId:null},{id:139,topic:"DevOps",question:"Cymbal Direct wants to improve its drone pilot interface. You want to collect feedback on proposed changes from the community of pilots before rolling out updates systemwide. What type of deployment pattern should you implement?",options:{A:"You should implement canary testing.",B:"You should implement A/B testing.",C:"You should implement a blue/green deployment.",D:"You should implement an in-place release."},correctAnswer:["A"],explanation:{correct:"Canary testing (or a canary release) is the practice of rolling out a new version of an application to a small subset of users or servers. This allows you to test the new version with real production traffic and gather feedback from a specific group (like the community of pilots) before exposing it to the entire user base. This minimizes the risk and impact of any potential issues.",incorrect:{B:"A/B testing is typically used to compare two or more versions of a feature to see which one performs better against a specific business metric (e.g., conversion rate). While related, canary testing is more focused on ensuring the stability and functionality of a new release with a small group before a full rollout.",C:"A blue/green deployment involves running two identical production environments, Blue and Green. You deploy the new version to the inactive environment and then switch traffic over. This is primarily for fast, low-risk rollbacks, not for gathering feedback from a subset of users.",D:"An in-place release (or rolling update) updates the existing servers with the new version, which is not suitable for collecting feedback from a specific, limited group before a full rollout."}},conditions:["Improve an interface","Collect feedback on changes from a small subset of users","Roll out updates systemwide only after feedback"],caseStudyId:"CS001"},{id:140,topic:"DevOps",question:"Cymbal Direct releases new versions of its drone delivery software every 1.5 to 2 months. Although most releases are successful, you have experienced three problematic releases that made drone delivery unavailable while software developers rolled back the release. You want to increase the reliability of software releases and prevent similar problems in the future. What should you do?",options:{A:"Adopt a 'waterfall' development process. Maintain the current release schedule. Ensure that documentation explains how all the features interact. Ensure that the entire application is tested in a staging environment before the release. Ensure that the process to roll back the release is documented. Use Cloud Monitoring, Cloud Logging, and Cloud Alerting to ensure visibility.",B:"Adopt a 'waterfall' development process. Maintain the current release schedule. Ensure that documentation explains how all the features interact. Automate testing of the application. Ensure that the process to roll back the release is well documented. Use Cloud Monitoring, Cloud Logging, and Cloud Alerting to ensure visibility.",C:"Adopt an 'agile' development process. Maintain the current release schedule. Automate build processes from a source repository. Automate testing after the build process. Use Cloud Monitoring, Cloud Logging, and Cloud Alerting to ensure visibility. Deploy the previous version if problems are detected and you need to roll back.",D:"Adopt an 'agile' development process. Reduce the time between releases as much as possible. Automate the build process from a source repository, which includes versioning and self-testing. Use Cloud Monitoring, Cloud Logging, and Cloud Alerting to ensure visibility. Use a canary deployment to detect issues that could cause rollback."},correctAnswer:["D"],explanation:{correct:"This option encompasses modern DevOps and SRE best practices for increasing release reliability. Adopting an agile process and reducing release frequency lowers the amount of change (and thus risk) in each release. Automating the build and test process catches bugs earlier. A canary deployment is a crucial strategy that exposes the new version to a small subset of traffic first, allowing you to detect problems with real traffic before they cause a full-scale outage.",incorrect:{A:"A waterfall process and infrequent releases are generally associated with higher-risk deployments. Manual testing and documentation are less effective than automation.",B:"This is similar to A and still relies on an outdated waterfall process.",C:"This is a good step towards automation, but it lacks the key risk-mitigation strategy of a canary deployment, which is specifically designed to detect issues that could cause a rollback *before* a full release."}},conditions:["Problematic releases are causing outages","Release cycle is 1.5 to 2 months","Need to increase the reliability of software releases","Prevent future rollback-causing problems"],caseStudyId:"CS001"},{id:141,topic:"GKE Workload Migration",question:"You are working with a client who is using Google Kubernetes Engine (GKE) to migrate applications from a virtual machine-based environment to a microservices-based architecture. Your client has a complex legacy application that stores a significant amount of data on the file system of its VM. You do not want to re-write the application to use an external service to store the file system data. What should you do?",options:{A:"In Cloud Shell, create a YAML file defining your Deployment called deployment.yaml. Create a Deployment in GKE by running the command kubectl apply -f deployment.yaml",B:"In Cloud Shell, create a YAML file defining your Container called build.yaml. Create a Container in GKE by running the command gcloud builds submit --config build.yaml .",C:"In Cloud Shell, create a YAML file defining your StatefulSet called statefulset.yaml. Create a StatefulSet in GKE by running the command kubectl apply -f statefulset.yaml",D:"In Cloud Shell, create a YAML file defining your Pod called pod.yaml. Create a Pod in GKE by running the command kubectl apply -f pod.yaml"},correctAnswer:["C"],explanation:{correct:"StatefulSets are the Kubernetes workload API object specifically designed for managing stateful applications. They provide stable, unique network identifiers and, crucially, stable persistent storage. When migrating a legacy VM that relies on its local filesystem, a StatefulSet combined with a PersistentVolumeClaim is the correct pattern to provide each pod with its own persistent disk, effectively mimicking the stateful nature of the original VM without rewriting the application.",incorrect:{A:"Deployments are intended for stateless applications. While you can attach persistent storage, they don't provide the stable network identity or ordered scaling guarantees that are often important for stateful applications.",B:"This command is for Cloud Build, which builds container images; it does not deploy applications to GKE.",D:"You should not create bare Pods directly. They should be managed by a controller like a Deployment or a StatefulSet to provide self-healing, scaling, and update capabilities."}},conditions:["Migrating a VM-based application to GKE","The legacy application is stateful (stores significant data on its filesystem)","Do not want to re-write the application to use an external storage service"],caseStudyId:null},{id:142,topic:"IAM & Security",question:"Mountkirk Games needs to create a repeatable and configurable mechanism for deploying isolated application environments. Developers and testers can access each other's environments and resources, but they cannot access staging or production resources. The staging environment needs access to some services from production. What should you do to isolate development environments from staging and production?",options:{A:"Create a project for development and test and another for staging and production.",B:"Create a network for development and test and another for staging and production.",C:"Create one subnetwork for development and another for staging and production.",D:"Create one project for development, a second for staging and a third for production."},correctAnswer:["D"],explanation:{correct:"GCP Projects are the primary boundary for resource, IAM, and billing isolation. Creating separate projects for Development, Staging, and Production is the best practice for enforcing strong separation of duties. This allows you to grant developers broad permissions in the dev project, more restricted permissions in staging, and highly restricted (or no) permissions in production. Staging can be granted access to specific production services via a service account without compromising overall isolation.",incorrect:{A:"Combining staging and production in the same project makes it much harder to enforce proper separation of duties and increases the risk of accidental changes to production.",B:"Network boundaries do not provide IAM isolation. A developer with permissions on the project could still access resources in the production network.",C:"Subnetwork boundaries, like network boundaries, do not provide IAM isolation."}},conditions:["Need to create isolated application environments (Dev, Test, Staging, Prod)","Developers/testers can access dev/test but not staging/production","Staging environment needs access to some production services"],caseStudyId:"cs_mountkirk_main"},{id:143,topic:"IAM & Security",question:"The JencoMart security team requires that all Google Cloud Platform infrastructure is deployed using a least privilege model with separation of duties for administration between production and development resources. What Google domain and project structure should you recommend?",options:{A:"Create two G Suite accounts to manage users: one for development/test/staging and one for production Each account should contain one project for every application.",B:"Create two G Suite accounts to manage users: one with a single project for all development applications and one with a single project for all production applications.",C:"Create a single G Suite account to manage users with each stage of each application in its own project.",D:"Create a single G Suite account to manage users with one project for the development/test/staging environment and one project for the production environment."},correctAnswer:["D"],explanation:{correct:"This structure is a standard best practice. Using a single Google Workspace (formerly G Suite) or Cloud Identity domain simplifies user and group management. Using separate projects for production and non-production environments creates a strong isolation boundary for resources and IAM. You can then assign different roles to the same user/group on each project, enforcing separation of duties (e.g., a developer can be an Editor on the dev project but only a Viewer on the prod project).",incorrect:{A:"Creating two separate G Suite (Google Workspace) accounts adds unnecessary complexity to user management.",B:"This is also incorrect due to the complexity of managing two separate user domains.",C:"While creating a project for each stage of each application provides high granularity, a fundamental separation between all of production and all of non-production is a more crucial first step for enforcing administrative separation of duties."}},conditions:["Deploy GCP infrastructure using a least privilege model","Enforce separation of duties for administration between production and development"],caseStudyId:"cs_jencostore"},{id:144,topic:"IAM & Security",question:"Your company has stored sensitive data in a Cloud Storage bucket. For regulatory reasons, your company must be able to rotate the encryption key used to encrypt the data in the bucket. The data will be processed in Dataproc. You want to follow Google-recommended practices for security. What should you do?",options:{A:"Create a key with Cloud Key Management Service (KMS). Encrypt the data using the encrypt method of Cloud KMS.",B:"Create a key with Cloud Key Management Service (KMS). Set the encryption key on the bucket to the Cloud KMS key.",C:"Generate a GPG key pair. Encrypt the data using the GPG key. Upload the encrypted data to the bucket.",D:"Generate an AES-256 encryption key. Encrypt the data in the bucket using the customer-supplied encryption keys feature."},correctAnswer:["B"],explanation:{correct:"This describes using Customer-Managed Encryption Keys (CMEK). By creating a key in KMS and setting it as the default key for the bucket, you get the benefits of managed encryption (GCS handles the encryption/decryption transparently) while maintaining control over the key, including rotation, in KMS. This integrates seamlessly with other GCP services like Dataproc, which can be granted permission to use the KMS key.",incorrect:{A:"This describes client-side encryption. This is more complex to manage, and services like Dataproc would need custom logic to decrypt the data.",C:"This is another form of client-side encryption that is complex and doesn't integrate well with GCP services.",D:"This describes Customer-Supplied Encryption Keys (CSEK). While it provides key control, CMEK (Option B) is generally the recommended practice as it offers better integration with GCP services and KMS-managed key lifecycle operations like rotation."}},conditions:["Sensitive data stored in a Cloud Storage bucket","Must be able to rotate the encryption key for regulatory reasons","Data will be processed by Dataproc","Follow Google-recommended security practices"],caseStudyId:null},{id:145,topic:"IAM & Security",question:"Mountkirk Games wants you to secure the connectivity from the new gaming application platform to Google Cloud. You want to streamline the process and follow Google-recommended practices. What should you do?",options:{A:"Configure Workload Identity and service accounts to be used by the application platform.",B:"Use Kubernetes Secrets, which are obfuscated by default. Configure these Secrets to be used by the application platform.",C:"Configure Kubernetes Secrets to store the secret, enable Application-Layer Secrets Encryption, and use Cloud Key Management Service (Cloud KMS) to manage the encryption keys. Configure these Secrets to be used by the application platform.",D:"Configure HashiCorp Vault on Compute Engine, and use customer managed encryption keys and Cloud Key Management Service (Cloud KMS) to manage the encryption keys. Configure these Secrets to be used by the application platform."},correctAnswer:["A"],explanation:{correct:"Workload Identity is the Google-recommended and most secure method for applications running in GKE to authenticate to Google Cloud services. It allows you to map a Kubernetes service account to a Google service account, enabling pods to get short-lived GCP credentials automatically without managing or storing service account key files. This is streamlined, secure, and follows the principle of least privilege.",incorrect:{B:"Kubernetes Secrets are for application-level secrets (e.g., database passwords), not for authenticating the workload itself to GCP APIs. They are only base64 encoded, not truly encrypted by default.",C:"This describes how to secure Kubernetes secrets, but not how to authenticate the application to GCP services.",D:"While Vault is a valid secrets management tool, Workload Identity is the native, simpler, and recommended solution for authenticating GKE workloads to GCP."}},conditions:["Secure connectivity from a GKE application platform to Google Cloud services","Streamline the process","Follow Google-recommended practices"],caseStudyId:"cs_mountkirk_main"},{id:146,topic:"IAM & Security",question:"Your company has a Google Workspace account and Google Cloud Organization. Some developers in the company have created Google Cloud projects outside of the Google Cloud Organization. You want to create an Organization structure that allows developers to create projects, but prevents them from modifying production projects. You want to manage policies for all projects centrally and be able to set more restrictive policies for production projects. You want to minimize disruption to users and developers when business needs change in the future. You want to follow Google-recommended practices. How should you design the Organization structure?",options:{A:"1. Create a second Google Workspace account and Organization. 2. Grant all developers the Project Creator IAM role on the new Organization. 3. Move the developer projects into the new Organization. 4. Set the policies for all projects on both Organizations. 5. Additionally, set the production policies on the original Organization.",B:"1. Create a folder under the Organization resource named Production. 2. Grant all developers the Project Creator IAM role on the new Organization. 3. Move the developer projects into the new Organization. 4. Set the policies for all projects on the Organization. 5. Additionally, set the production policies on the Production folder.",C:"1. Create folders under the Organization resource named Development and Production. 2. Grant all developers the Project Creator IAM role on the Development folder. 3. Move the developer projects into the Development folder. 4. Set the policies for all projects on the Organization. 5. Additionally, set the production policies on the Production folder.",D:"1. Designate the Organization for production projects only. 2. Ensure that developers do not have the Project Creator IAM role on the Organization. 3. Create development projects outside of the Organization using the developer Google Workspace accounts. 4. Set the policies for all projects on the Organization. 5. Additionally, set the production policies on the individual production projects."},correctAnswer:["C"],explanation:{correct:"This is the ideal structure. It uses a single Organization for central management. Folders are used to group projects by environment (Development, Production). IAM permissions are applied at the correct scope: developers get the `Project Creator` role only on the Development folder, and more restrictive policies are applied to the Production folder. This is scalable, manageable, and enforces separation of duties.",incorrect:{A:"Creating a second Organization adds unnecessary complexity to user and policy management.",B:"This is close, but it grants developers Project Creator on the new Organization, which is too broad. Permissions should be scoped to the Development folder.",D:"Allowing projects to be created outside the Organization defeats the purpose of central governance and policy management."}},conditions:["Existing Google Workspace and Organization","Some developer projects are outside the Organization","Allow developers to create projects but prevent them from modifying production projects","Manage policies centrally and have more restrictive policies for production","Follow Google-recommended practices"],caseStudyId:null},{id:147,topic:"IAM & Security",question:"Your company has just recently activated Cloud Identity to manage users. The Google Cloud Organization has been configured as well. The security team needs to secure projects that will be part of the Organization. They want to prohibit IAM users outside the domain from gaining permissions from now on. What should they do?",options:{A:"Configure an organization policy to restrict identities by domain.",B:"Configure an organization policy to block creation of service accounts.",C:"Configure Cloud Scheduler to trigger a Cloud Function every hour that removes all users that don't belong to the Cloud identity domain from all projects.",D:"Create a technical user (e g . crawler@yourdomain com), and give it the protect owner rote at root organization level Write a bash script that Lists all me IAM rules of all projects within the organization Deletes all users that do not belong to the company domainCreate a Compute Engine instance m a project within the Organization and configure gcloud to be executed with technical user credentials Configure a cron job that executes the bash script every hour."},correctAnswer:["A"],explanation:{correct:"Organization Policies provide preventative controls. The `constraints/iam.allowedPolicyMemberDomains` constraint is specifically designed to restrict which domains can be added to IAM policies. By setting this at the Organization level to your company's Cloud Identity domain, you prevent any user from an outside domain from being granted permissions.",incorrect:{B:"Blocking service account creation is a different security measure and does not address the issue of restricting external user access.",C:"This is a reactive approach that tries to clean up permissions after they have been granted. A preventative control like an Organization Policy is much more secure.",D:"This is another complex and reactive scripting approach. It is less secure and more difficult to manage than using the built-in Organization Policy."}},conditions:["Cloud Identity and Organization are configured","Need to secure projects within the Organization","Prohibit granting IAM permissions to users from outside the company domain"],caseStudyId:null},{id:148,topic:"IAM & Security",question:"Google Cloud Platform resources are managed hierarchically using organization, folders, and projects. When Cloud Identity and Access Management (IAM) policies exist at these different levels, what is the effective policy at a particular node of the hierarchy?",options:{A:"The effective policy is determined only by the policy set at the node",B:"The effective policy is the policy set at the node and restricted by the policies of its ancestors",C:"The effective policy is the union of the policy set at the node and policies inherited from its ancestors",D:"The effective policy is the intersection of the policy set at the node and policies inherited from its ancestors"},correctAnswer:["C"],explanation:{correct:"GCP IAM policies are hierarchical and additive. The effective policy for a resource is the union of the policy set directly on that resource and all policies inherited from its ancestors (Project, Folder(s), Organization). This means permissions granted at a higher level are automatically available at lower levels.",incorrect:{A:"This is incorrect; policies are inherited from parent resources.",B:"Inherited policies add permissions, they don't restrict them (with the exception of Deny policies, which are evaluated separately).",D:"An intersection would be overly restrictive and is not how IAM inheritance works for allow policies."}},conditions:["Understanding the GCP Resource Hierarchy (Organization, Folder, Project)","How IAM policies are evaluated at different levels"],caseStudyId:null},{id:149,topic:"IAM & Security",question:"Your web application must comply with the requirements of the European Unions General Data Protection Regulation (GDPR). You are responsible for the technical architecture of your web application. What should you do?",options:{A:"Ensure that your web application only uses native features and services of Google Cloud Platform, because Google already has various certifications and provides pass-on compliance when you use native features.",B:"Enable the relevant GDPR compliance setting within the GCP Console for each of the services in use within your application.",C:"Ensure that Cloud Security Scanner is part of your test planning strategy in order to pick up any compliance gaps.",D:"Define a design for the security of data in your web application that meets GDPR requirements."},correctAnswer:["D"],explanation:{correct:"GDPR compliance is a shared responsibility. While Google provides a compliant infrastructure, you (the customer) are responsible for how you build your application and manage data on that infrastructure. This means you must design your application's data handling, security measures, consent management, and processes for data subject rights to be compliant with GDPR principles.",incorrect:{A:"There is no such thing as 'pass-on' compliance. You are always responsible for your application's compliance.",B:"There is no single 'GDPR compliance setting' in the GCP Console. Compliance requires a holistic approach to architecture and process.",C:"Cloud Security Scanner is a tool for finding web application vulnerabilities. While it contributes to security (which is part of GDPR), it does not cover the full scope of GDPR compliance requirements."}},conditions:["Web application must comply with GDPR","You are responsible for the technical architecture"],caseStudyId:null},{id:150,topic:"IAM & Security",question:"Your company pushes batches of sensitive transaction data from its application server VMs to Cloud Pub/Sub for processing and storage, What is the Google-recommended way for your application to authenticate to the required Google Cloud services?",options:{A:"Ensure that VM service accounts are granted the appropriate Cloud Pub/Sub IAM roles.",B:"Ensure that VM service accounts do not have access to Cloud Pub/Sub, and use VM access scopes to grant the appropriate Cloud Pub/Sub IAM roles.",C:"Generate an OAuth2 access token for accessing Cloud Pub/Sub, encrypt it, and store it in Cloud Storage for access from each VM.",D:"Create a gateway to Cloud Pub/Sub using a Cloud Function, and grant the Cloud Function service account the appropriate Cloud Pub/Sub IAM roles."},correctAnswer:["A"],explanation:{correct:"The most secure and recommended method for a VM to authenticate to GCP services is by using its attached service account. The application running on the VM can automatically obtain credentials from the metadata server without any need to manage or store key files on the instance. You grant the necessary IAM roles (e.g., `Pub/Sub Publisher`) directly to this service account.",incorrect:{B:"Access scopes are a legacy mechanism. IAM roles provide more granular control and are the recommended practice.",C:"Manually managing credentials like OAuth2 tokens or service account keys is less secure and adds operational overhead. Using the attached service account avoids this.",D:"Creating a gateway adds unnecessary complexity when direct authentication via service accounts is available and simpler."}},conditions:["Application server VMs need to push data to Cloud Pub/Sub","Need the Google-recommended way for the application to authenticate"],caseStudyId:null},{id:151,topic:"IAM & Security",question:"Your company has a networking team and a development team. The development team runs applications on Compute Engine instances that contain sensitive data. The development team requires administrative permissions for Compute Engine. Your company requires all network resources to be managed by the networking team. The development team does not want the networking team to have access to the sensitive data on the instances. What should you do?",options:{A:"1. Create a project with a standalone VPC and assign the Network Admin role to the networking team. 2. Create a second project with a standalone VPC and assign the Compute Admin role to the development team. 3. Use Cloud VPN to join the two VPCs.",B:"1. Create a project with a standalone Virtual Private Cloud (VPC), assign the Network Admin role to the networking team, and assign the Compute Admin role to the development team.",C:"1. Create a project with a Shared VPC and assign the Network Admin role to the networking team. 2. Create a second project without a VPC, configure it as a Shared VPC service project, and assign the Compute Admin role to the development team.",D:"1. Create a project with a standalone VPC and assign the Network Admin role to the networking team. 2. Create a second project with a standalone VPC and assign the Compute Admin role to the development team. 3. Use VPC Peering to join the two VPCs."},correctAnswer:["C"],explanation:{correct:"Shared VPC is designed for this exact use case. It allows for centralized network administration in a 'host project' while delegating resource administration (like Compute Engine instances) to 'service projects'. This perfectly enforces separation of duties: the networking team controls the network in the host project without having permissions to access instances in the service projects, and the development team manages their instances in the service project without being able to modify the shared network.",incorrect:{A:"This creates two separate networks and requires a VPN, which is less integrated and efficient than Shared VPC.",B:"Putting both teams in a single project makes it very difficult to enforce the required separation of duties.",D:"This is similar to A but uses VPC Peering. Shared VPC is the purpose-built solution for this administrative model."}},conditions:["Separate networking and development teams","Development team needs administrative permissions for Compute Engine instances","Networking team must manage all network resources","Networking team should not have access to sensitive data on instances"],caseStudyId:null},{id:152,topic:"IAM & Security",question:"Your company is using Google Cloud. You have two folders under the Organization: Finance and Shopping. The members of the development team are in a Google Group. The development team group has been assigned the Project Owner role on the Organization. You want to prevent the development team from creating resources in projects in the Finance folder. What should you do?",options:{A:"Assign the development team group the Project Viewer role on the Finance folder, and assign the development team group the Project Owner role on the Shopping folder.",B:"Assign the development team group only the Project Viewer role on the Finance folder.",C:"Assign the development team group the Project Owner role on the Shopping folder, and remove the development team group Project Owner role from the Organization.",D:"Assign the development team group only the Project Owner role on the Shopping folder."},correctAnswer:["C"],explanation:{correct:"IAM policies are inherited and additive. The `Project Owner` role at the Organization level gives the developers owner permissions on *everything* below it, including the Finance folder. To restrict their access, you must first remove the overly broad permission at the Organization level. Then, you can grant them the `Project Owner` role at a more granular level where they actually need it, such as the Shopping folder.",incorrect:{A:"Adding a more restrictive role (Viewer) at a lower level does not override the inherited Owner role. The effective permission will be the union of both, which is still Owner.",B:"This is incorrect for the same reason as A.",D:"This step is necessary, but it's insufficient if you don't also remove the broad permission from the Organization level."}},conditions:["Organization has two folders: Finance and Shopping","Development team group has the Project Owner role at the Organization level","Need to prevent the development team from creating resources in the Finance folder"],caseStudyId:null},{id:153,topic:"IAM & Security",question:"You are responsible for the Google Cloud environment in your company. Multiple departments need access to their own projects, and the members within each department will have the same project responsibilities. You want to structure your Google Cloud environment for minimal maintenance and maximum overview of IAM permissions as each department's projects start and end. You want to follow Google-recommended practices. What should you do?",options:{A:"Grant all department members the required IAM permissions for their respective projects.",B:"Create a Google Group per department and add all department members to their respective groups. Create a folder per department and grant the respective group the required IAM permissions at the folder level. Add the projects under the respective folders.",C:"Create a folder per department and grant the respective members of the department the required IAM permissions at the folder level. Structure all projects for each department under the respective folders.",D:"Create a Google Group per department and add all department members to their respective groups. Grant each group the required IAM permissions for their respective projects."},correctAnswer:["B"],explanation:{correct:"This is the epitome of Google-recommended best practices for IAM at scale. Using Google Groups centralizes user management. Using Folders to reflect the organizational structure allows you to apply permissions to the groups at the folder level. All projects within that folder then inherit these permissions, making the system easy to manage, audit, and scale. When a user changes roles, you only need to change their group membership.",incorrect:{A:"Granting permissions to individual users is not scalable and becomes very difficult to manage and audit.",C:"This is better than A, but still grants permissions to individuals. Using groups is the best practice.",D:"This is good, but applying permissions at the folder level is even better and more scalable than applying them to each project individually."}},conditions:["Manage GCP environment for multiple departments","Members within each department have the same responsibilities","Need a structure with minimal maintenance and maximum overview of IAM permissions","Follow Google-recommended practices"],caseStudyId:null},{id:154,topic:"IAM & Security",question:"You are designing a Data Warehouse on Google Cloud and want to store sensitive data in BigQuery. Your company requires you to generate the encryption keys outside of Google Cloud. You need to implement a solution. What should you do?",options:{A:"Generate a new key in Cloud Key Management Service (Cloud KMS). Store all data in Cloud Storage using the customer-managed key option and select the created key. Set up a Dataflow pipeline to decrypt the data and to store it in a new BigQuery dataset.",B:"Generate a new key in Cloud Key Management Service (Cloud KMS). Create a dataset in BigQuery using the customer-managed key option and select the created key.",C:"Import a key in Cloud KMS. Store all data in Cloud Storage using the customer-managed key option and select the created key. Set up a Dataflow pipeline to decrypt the data and to store it in a new BigQuery dataset.",D:"Import a key in Cloud KMS. Create a dataset in BigQuery using the customer-supplied key option and select the created key."},correctAnswer:["D"],explanation:{correct:"The requirement is to use keys generated *outside* of Google Cloud. This can be achieved using Cloud KMS's key import feature. You generate the key material externally, then import it into a KMS key. This imported key can then be used as a Customer-Managed Encryption Key (CMEK) to encrypt a BigQuery dataset. This gives you control over an externally generated key while leveraging the integration of CMEK with BigQuery.",incorrect:{A:"This is overly complex and generates the key inside KMS, which violates the requirement.",B:"This generates the key inside KMS, violating the requirement.",C:"This is overly complex. BigQuery supports CMEK directly, so there is no need for an intermediate step with Cloud Storage and Dataflow."}},conditions:["Design a Data Warehouse on BigQuery with sensitive data","Encryption keys must be generated outside of Google Cloud"],caseStudyId:null},{id:155,topic:"IAM & Security",question:"Your company has a Google Cloud project that uses BigQuery for data warehousing. They have a VPN tunnel between the on-premises environment and Google Cloud that is configured with Cloud VPN. The security team wants to avoid data exfiltration by malicious insiders, compromised code, and accidental oversharing. What should they do?",options:{A:"Configure Private Google Access for on-premises only.",B:"Perform the following tasks: 1. Create a service account. 2. Give the BigQuery JobUser role and Storage Reader role to the service account. 3. Remove all other IAM access from the project.",C:"Configure VPC Service Controls and configure Private Google Access.",D:"Configure Private Google Access."},correctAnswer:["C"],explanation:{correct:"VPC Service Controls are specifically designed to prevent data exfiltration. By creating a service perimeter around your BigQuery and other sensitive services, you can restrict data from leaving the perimeter. Combining this with Private Google Access for on-premises hosts ensures that legitimate on-premises traffic to BigQuery stays within Google's network and can be allowed by the perimeter, while data exfiltration to the public internet is blocked.",incorrect:{A:"Private Google Access alone facilitates private connectivity but does not prevent a compromised identity from exfiltrating data over that connection or other paths.",B:"IAM controls are essential for least privilege but do not prevent an authorized (but compromised) identity from exfiltrating data.",D:"This is the same as A and is insufficient on its own."}},conditions:["BigQuery data warehouse with a VPN connection to on-premises","Need to avoid data exfiltration by insiders, compromised code, or accidental sharing"],caseStudyId:null},{id:156,topic:"IAM & Security",question:"Your customer is moving their corporate applications to Google Cloud Platform. The security team wants detailed visibility of all projects in the organization. You provision the Google Cloud Resource Manager and set up yourself as the org admin. What Google Cloud Identity and Access Management (Cloud IAM) roles should you give to the security team'?",options:{A:"Org viewer, project owner",B:"Org viewer, project viewer",C:"Org admin, project browser",D:"Project owner, network admin"},correctAnswer:["B"],explanation:{correct:"To provide read-only visibility across the entire organization, you should grant roles at the organization level. The `Organization Viewer` role allows the team to see the resource hierarchy (folders, projects). The `Project Viewer` role, when granted at the organization level, is inherited by all projects and allows the team to view the resources within each project. This combination provides comprehensive visibility without granting any modification rights, adhering to the principle of least privilege.",incorrect:{A:"Project Owner is an administrative role and grants far too much permission for a visibility requirement.",C:"Org Admin is the highest-level administrative role and is highly privileged. Project Browser only allows listing projects, not viewing their contents.",D:"Project Owner and Network Admin are both administrative roles that are inappropriate for a read-only visibility requirement."}},conditions:["Security team needs detailed visibility of all projects in the organization"],caseStudyId:null},{id:157,topic:"IAM & Security",question:"Your company has just recently activated Cloud Identity to manage users. The Google Cloud Organization has been configured as well. The security team needs to secure projects that will be part of the Organization. They want to prohibit IAM users outside the domain from gaining permissions from now on. What should they do?",options:{A:"Configure an organization policy to restrict identities by domain.",B:"Configure an organization policy to block creation of service accounts.",C:"Configure Cloud Scheduler to trigger a Cloud Function every hour that removes all users that don't belong to the Cloud Identity domain from all projects.",D:"Create a technical user (e.g., crawler@yourdomain.com), and give it the project owner role at root organization level. Write a bash script that: a Lists all the IAM rules of all projects within the organization. a Deletes all users that do not belong to the company domain. Create a Compute Engine instance in a project within the Organization and configure gcloud to be executed with technical user credentials. Configure a cron job that executes the bash script every hour."},correctAnswer:["A"],explanation:{correct:"Organization Policies provide preventative controls. The `constraints/iam.allowedPolicyMemberDomains` constraint is specifically designed to restrict which domains can be added to IAM policies. By setting this at the Organization level to your company's Cloud Identity domain, you prevent any user from an outside domain from being granted permissions.",incorrect:{B:"Blocking service account creation is a different security measure and does not address the issue of restricting external user access.",C:"This is a reactive approach that tries to clean up permissions after they have been granted. A preventative control like an Organization Policy is much more secure.",D:"This is another complex and reactive scripting approach. It is less secure and more difficult to manage than using the built-in Organization Policy."}},conditions:["Cloud Identity & Org configured","Secure projects within Org","Prohibit granting IAM permissions to external users"],caseStudyId:null},{id:158,topic:"IAM & Security",question:"Your company has a Google Workspace account and Google Cloud Organization. Some developers in the company have created Google Cloud projects outside of the Google Cloud Organization. You want to create an Organization structure that allows developers to create projects, but prevents them from modifying production projects. You want to manage policies for all projects centrally and be able to set more restrictive policies for production projects. You want to minimize disruption to users and developers when business needs change in the future. You want to follow Google-recommended practices. How should you design the Organization structure?",options:{A:"1. Create a second Google Workspace account and Organization. 2. Grant all developers the Project Creator IAM role on the new Organization. 3. Move the developer projects into the new Organization. 4. Set the policies for all projects on both Organizations. 5. Additionally, set the production policies on the original Organization.",B:"1. Create a folder under the Organization resource named Production. 2. Grant all developers the Project Creator IAM role on the new Organization. 3. Move the developer projects into the new Organization. 4. Set the policies for all projects on the Organization. 5. Additionally, set the production policies on the Production folder.",C:"1. Create folders under the Organization resource named Development and Production. 2. Grant all developers the Project Creator IAM role on the Development folder. 3. Move the developer projects into the Development folder. 4. Set the policies for all projects on the Organization. 5. Additionally, set the production policies on the Production folder.",D:"1. Designate the Organization for production projects only. 2. Ensure that developers do not have the Project Creator IAM role on the Organization. 3. Create development projects outside of the Organization using the developer Google Workspace accounts. 4. Set the policies for all projects on the Organization. 5. Additionally, set the production policies on the individual production projects."},correctAnswer:["C"],explanation:{correct:"This is the ideal structure. It uses a single Organization for central management. Folders are used to group projects by environment (Development, Production). IAM permissions are applied at the correct scope: developers get the `Project Creator` role only on the Development folder, and more restrictive policies are applied to the Production folder. This is scalable, manageable, and enforces separation of duties.",incorrect:{A:"Creating a second Organization adds unnecessary complexity to user and policy management.",B:"This is close, but it grants developers Project Creator on the new Organization, which is too broad. Permissions should be scoped to the Development folder.",D:"Allowing projects to be created outside the Organization defeats the purpose of central governance and policy management."}},conditions:["Existing Google Workspace & Org","Some dev projects outside Org","Allow dev project creation (only in Dev)","Prevent dev modification of Prod","Central policy management","Restrictive policies for Prod","Minimize future disruption","Follow Google recommended practices"],caseStudyId:null},{id:159,topic:"IAM & Security",question:"The JencoMart security team requires that all Google Cloud Platform infrastructure is deployed using a least privilege model with separation of duties for administration between production and development resources. What Google domain and project structure should you recommend?",options:{A:"Create two G Suite accounts to manage users: one for development/test/staging and one for production. Each account should contain on project for every application",B:"Create two G Suite accounts to manage users: one with a single project for all development applications and one with a single project for all production applications",C:"Create a single G Suite account to manage users with each stage of each application in its own project",D:"Create a single G Suite account to manage users with one project for the development/test/staging environment and one project for the production environment"},correctAnswer:["D"],explanation:{correct:"This structure is a standard best practice. Using a single Google Workspace (formerly G Suite) or Cloud Identity domain simplifies user and group management. Using separate projects for production and non-production environments creates a strong isolation boundary for resources and IAM. You can then assign different roles to the same user/group on each project, enforcing separation of duties (e.g., a developer can be an Editor on the dev project but only a Viewer on the prod project).",incorrect:{A:"Creating two separate G Suite (Google Workspace) accounts adds unnecessary complexity to user management.",B:"This is also incorrect due to the complexity of managing two separate user domains.",C:"While creating a project for each stage of each application provides high granularity, a fundamental separation between all of production and all of non-production is a more crucial first step for enforcing administrative separation of duties."}},conditions:["Least privilege model","Separation of duties (Prod vs. Dev admin)"],caseStudyId:"cs_jencostore"},{id:160,topic:"IAM & Security",question:"JencoMart has decided to migrate user profile storage to Google Cloud Datastore and the application servers to Google Compute Engine (GCE). During the migration, the existing infrastructure will need access to Datastore to upload the data. What service account key-management strategy should you recommend?",options:{A:"Provision service account keys for the on-premises infrastructure and for the GCE virtual machines (VMs)",B:"Authenticate the on-premises infrastructure with a user account and provision service account keys for the VMs",C:"Provision service account keys for the on-premises infrastructure and use Google Cloud Platform (GCP) managed keys for the VMs",D:"Deploy a custom authentication service on GCE/Google Kubernetes Engine (GKE) for the on-premises infrastructure and use GCP managed keys for the VMs"},correctAnswer:["C"],explanation:{correct:"This is the recommended best practice. For on-premises systems that need to authenticate to GCP, you must use downloadable service account key files. For GCE VMs, you should always use the attached service account, which leverages GCP-managed keys via the metadata server. This avoids the security risk and management overhead of storing key files on the VMs.",incorrect:{A:"Provisioning service account keys for GCE VMs is not a best practice; attached service accounts should be used.",B:"Using a user account for programmatic access from on-premises is not recommended; service accounts are for services. Also, provisioning keys for VMs is not a best practice.",D:"A custom authentication service is overly complex and unnecessary."}},conditions:["Migrating to Datastore and GCE","On-premises infrastructure needs access to Datastore during migration","Recommend a service account key-management strategy"],caseStudyId:"cs_jencostore"},{id:161,topic:"IAM & Security",question:"Mountkirk Games needs to create a repeatable and configurable mechanism for deploying isolated application environments. Developers and testers can access each others environments and resources, but they cannot access staging or production resources. The staging environment needs access to some services from production. What should you do to isolate development environments from staging and production?",options:{A:"Create a project for development and test and another for staging and production",B:"Create a network for development and test and another for staging and production",C:"Create one subnetwork for development and another for staging and production",D:"Create one project for development, a second for staging and a third for production"},correctAnswer:["D"],explanation:{correct:"GCP Projects are the primary boundary for resource, IAM, and billing isolation. Creating separate projects for Development, Staging, and Production is the best practice for enforcing strong separation of duties. This allows you to grant developers broad permissions in the dev project, more restricted permissions in staging, and highly restricted (or no) permissions in production. Staging can be granted access to specific production services via a service account without compromising overall isolation.",incorrect:{A:"Combining staging and production in the same project makes it much harder to enforce proper separation of duties and increases the risk of accidental changes to production.",B:"Network boundaries do not provide IAM isolation. A developer with permissions on the project could still access resources in the production network.",C:"Subnetwork boundaries, like network boundaries, do not provide IAM isolation."}},conditions:["Deploy isolated environments (Dev, Test, Staging, Prod)","Dev/Testers access each other, not Staging/Prod","Staging needs access to some Prod services"],caseStudyId:"cs_mountkirk_main"},{id:162,topic:"IAM & Security",question:"You are implementing Firestore for Mountkirk Games. Mountkirk Games wants to give a new game programmatic access to a legacy games Firestore database. Access should be as restricted as possible. What should you do?",options:{A:"Create a service account (SA) in the legacy games Google Cloud project, add a second SA in the new games IAM page, and then give the Organization Admin role to both SAs.",B:"Create a service account (SA) in the legacy games Google Cloud project, give the SA the Organization Admin role, and then give it the Firebase Admin role in both projects.",C:"Create a service account (SA) in the legacy games Google Cloud project, add this SA in the new games IAM page, and then give it the Firebase Admin role in both projects.",D:"Create a service account (SA) in the legacy games Google Cloud project, give it the Firebase Admin role, and then migrate the new game to the legacy games project."},correctAnswer:["C"],explanation:{correct:"The correct pattern for cross-project service access is to create a service account in the project that owns the resource (the legacy project). Then, grant that service account the necessary permissions on the resource it needs to access (the Firestore database). Finally, you grant the identity of the new game's application (e.g., its own service account in the new project) the permission to impersonate or use the service account in the legacy project. Option C is the closest description, though a bit imprecise. The key is creating an SA in the legacy project, granting it permissions there, and then making that SA's identity usable by the new game's project.",incorrect:{A:"Organization Admin is an extremely excessive permission and violates the principle of least privilege.",B:"This is also incorrect for the same reason as A.",D:"Migrating the new game to the legacy project is a major architectural change and not just a way to grant access."}},conditions:["New game needs programmatic access to a legacy game's Firestore database","Access should be as restricted as possible"],caseStudyId:"cs_mountkirk_main"},{id:163,topic:"IAM & Security",question:"Mountkirk Games wants to limit the physical location of resources to their operating Google Cloud regions. What should you do?",options:{A:"Configure an organizational policy which constrains where resources can be deployed.",B:"Configure IAM conditions to limit what resources can be configured.",C:"Configure the quotas for resources in the regions not being used to 0.",D:"Configure a custom alert in Cloud Monitoring so you can disable resources as they are created in other regions."},correctAnswer:["A"],explanation:{correct:"Organization Policies are designed for this exact purpose. The `constraints/gcp.resourceLocations` constraint allows you to define a list of allowed GCP regions. When this policy is enforced at the organization or folder level, any attempt to create resources outside of the allowed locations will be blocked. This is a preventative and highly effective governance control.",incorrect:{B:"IAM conditions control *when* an IAM permission is granted (e.g., based on time of day or resource type), not *where* a resource can be physically located.",C:"Setting quotas to 0 is a workaround that is not comprehensive (doesn't cover all resource types) and can be overridden by users with quota management permissions. It's not the recommended way to enforce location policies.",D:"This is a reactive approach. It would alert you after a non-compliant resource has already been created, rather than preventing it in the first place."}},conditions:["Limit the physical location of GCP resources","Restrict deployment to specific operating regions"],caseStudyId:"cs_mountkirk_main"},{id:164,topic:"IAM & Security",question:"How can you enable users defined in Microsoft Active Directory to access GCP resources using their on-premises usernames and passwords?",options:{A:"Use the Password Sync tool to replicate the passwords into GCP.",B:"Use Google Cloud Directory Sync (GCDS) to synchronize users with Cloud Identity and configure single sign-on (SSO) via SAML 2.0.",C:"Provision users in Cloud Identity and require users to set their passwords to match their on-premises ones.",D:"Configure Cloud Identity to authenticate directly against Active Directory using the LDAP protocol."},correctAnswer:["B"],explanation:{correct:"This describes the standard and secure identity federation pattern. GCDS provisions user identities (without passwords) into Cloud Identity. SAML 2.0 SSO is then configured to redirect authentication requests to the on-premises Active Directory (via ADFS or another IdP). This keeps AD as the single source of truth for authentication and avoids storing passwords in the cloud.",incorrect:{A:"Password Sync is generally considered less secure than federation because it involves replicating password hashes to the cloud.",C:"This is a manual, insecure, and unmanageable process that defeats the purpose of single sign-on.",D:"Cloud Identity does not use LDAP to directly authenticate users against an on-premises AD for GCP resource access; SAML is the standard protocol for this federation."}},conditions:["Enable users from on-premises Microsoft Active Directory to access GCP resources","Users should use their on-premises usernames and passwords"],caseStudyId:null},{id:165,topic:"IAM & Security",question:"You need to ensure that Compute Engine instances, hosted in a Shared VPC service project, can access a corporate API running on-premises. The connectivity between Google Cloud and the on-premises data center is already established via a dedicated Interconnect connection terminated in the Shared VPC host project. Instances in the service project do not have external IP addresses. How should you configure access?",options:{A:"Configure a NAT gateway in the Shared VPC host project that applies only to the subnets of the service project.",B:"Configure a firewall rule in the Shared VPC host project to allow egress traffic from the service project instances to the on-premises network.",C:"Configure Private Google Access for the subnets of the service project.",D:"Configure VPC Network Peering between the service project and the Shared VPC host project."},correctAnswer:["B"],explanation:{correct:"In a Shared VPC model, all network controls, including firewall rules, are managed in the host project. Since the Interconnect provides the routing path, the final step is to create a firewall rule in the host project that allows egress traffic from the service project's instances (identified by tag or service account) to the on-premises API's IP range.",incorrect:{A:"A NAT gateway is for accessing the public internet, not for private connectivity to an on-premises network via Interconnect.",C:"Private Google Access is for accessing Google APIs and services, not on-premises resources.",D:"VPC Peering is for connecting two separate VPCs. In a Shared VPC, the service project is already using the host project's VPC, so peering is not applicable."}},conditions:["Instances are in a Shared VPC service project with no external IPs","Need to access an on-premises API","Connectivity is via a Dedicated Interconnect in the host project"],caseStudyId:null},{id:166,topic:"IAM & Security",question:"A security audit revealed that several developers have the Project Owner role on production projects, violating the principle of least privilege. You need to grant these developers the ability to deploy new versions of applications to App Engine and view resources within the projects, but restrict their ability to modify other resources or IAM policies. Which predefined IAM role should you assign instead of Project Owner?",options:{A:"App Engine Deployer and Project Viewer",B:"App Engine Admin and Project Browser",C:"App Engine Service Admin and Monitoring Viewer",D:"Compute Instance Admin and Security Reviewer"},correctAnswer:["A"],explanation:{correct:"This option perfectly follows the principle of least privilege. The `App Engine Deployer` role grants the specific permissions needed to deploy new application versions without any other administrative rights. The `Project Viewer` role provides the required read-only access to all resources in the project. Combining these two roles meets all requirements without granting excessive permissions.",incorrect:{B:"`App Engine Admin` is too broad, and `Project Browser` is too restrictive (it only allows listing projects, not viewing their contents).",C:"`App Engine Service Admin` is for managing App Engine settings, not deploying code. `Monitoring Viewer` is too specific if general resource viewing is needed.",D:"These roles are completely unrelated to App Engine deployment."}},conditions:["Developers have excessive Project Owner permissions on production projects","Need to grant them permission to deploy to App Engine","Need to grant them permission to view resources","Must restrict all other modification permissions"],caseStudyId:null},{id:167,topic:"Identity",question:"Your company wants to start using Google Cloud resources but wants to retain their on-premises Active Directory domain controller for identity management. What should you do?",options:{A:"Use the Admin Directory API to authenticate against the Active Directory domain controller.",B:"Use Google Cloud Directory Sync to synchronize Active Directory usernames with cloud identities and configure SAML SSO.",C:"Use Cloud Identity-Aware Proxy configured to use the on-premises Active Directory domain controller as an identity provider.",D:"Use Compute Engine to create an Active Directory (AD) domain controller that is a replica of the on-premises AD domain controller using Google Cloud Directory Sync."},correctAnswer:["B"],explanation:{correct:"This is the standard and recommended identity federation pattern. Google Cloud Directory Sync (GCDS) provisions the user identities (without passwords) into Cloud Identity. SAML-based Single Sign-On (SSO) is then configured to redirect authentication requests to the on-premises Active Directory (via ADFS or another IdP). This keeps AD as the single source of truth for authentication.",incorrect:{A:"The Admin Directory API is for managing Cloud Identity/Workspace resources, not for direct authentication against an on-premises AD.",C:"Cloud IAP is an access control service that relies on an identity provider; it is not the mechanism for setting up the identity federation itself.",D:"Replicating an AD domain controller to GCE is a different architectural pattern (domain extension). GCDS is used to sync identities to Cloud Identity, not to replicate AD controllers."}},conditions:["Start using GCP resources","Retain on-premises Active Directory for identity management"],caseStudyId:null},{id:168,topic:"IoT",question:"You are asked to design a new architecture for the ingestion of the data of the 200,000 vehicles that are connected to a cellular network. You want to follow Google-recommended practices. Considering the technical requirements, which components should you use for the ingestion of the data?",options:{A:"Google Kubernetes Engine with an SSL Ingress",B:"Cloud loT Core with public/private key pairs",C:"Compute Engine with project-wide SSH keys",D:"Compute Engine with specific SSH keys"},correctAnswer:["B"],explanation:{correct:"Cloud IoT Core is the purpose-built, managed service for securely connecting, managing, and ingesting data from a large number of IoT devices like connected vehicles. It handles device authentication (e.g., using public/private key pairs), protocol bridges (MQTT/HTTP), and scalable data ingestion, integrating seamlessly with downstream services like Pub/Sub. This directly meets the requirement for secure and scalable data transfer from equipment.",incorrect:{A:"Using GKE would require you to build and manage your own custom ingestion service, which is not a recommended practice when a managed service like IoT Core exists.",C:"SSH keys are for administrative access to VMs, not for secure data ingestion from IoT devices.",D:"This is also incorrect for the same reason as C."}},conditions:["Design a new data ingestion architecture","Source is 200,000 connected vehicles","Follow Google-recommended practices","Technical requirement: Increase security of data transfer"],caseStudyId:"cs_terram_main"},{id:169,topic:"Messaging",question:"You are a developer on the EHR customer portal team. Your team recently migrated the customer portal application to Google Cloud. The load has increased on the application servers, and now the application is logging many timeout errors. You recently incorporated Pub/Sub into the application architecture, and the application is not logging any Pub/Sub publishing errors. You want to improve publishing latency. What should you do?",options:{A:"Increase the Pub/Sub Total Timeout retry value.",B:"Move from a Pub/Sub subscriber pull model to a push model.",C:"Turn off Pub/Sub message batching.",D:"Create a backup Pub/Sub message queue."},correctAnswer:["C"],explanation:{correct:"Pub/Sub client libraries use message batching by default to improve throughput and reduce API call costs. However, this introduces a small amount of latency for each message as it waits for the batch to fill or for a time limit to be reached. If the application is extremely sensitive to publishing latency and this is contributing to timeouts, turning off or reducing the batching settings can force messages to be sent more immediately, thus reducing the publish-side latency.",incorrect:{A:"Retry timeout is for handling publish *failures*, which the question states are not occurring. It will not reduce latency.",B:"The subscriber model (pull vs. push) affects message *consumption*, not the latency of message *publishing*.",D:"A backup queue is for redundancy and disaster recovery, not for improving latency."}},conditions:["Application is logging timeout errors under high load","Pub/Sub is used for publishing, but no publishing errors are logged","Goal is to improve Pub/Sub publishing latency"],caseStudyId:"cs_ehr_healthcare"},{id:170,topic:"Migration",question:"Your web application has several VM instances running within a VPC. You want to restrict communications between instances to only the paths and ports you authorize, but you don't want to rely on static IP addresses or subnets because the app can autoscale. How should you restrict communications?",options:{A:"Use separate VPCs to restrict traffic.",B:"Use firewall rules based on network tags attached to the compute instances.",C:"Use Cloud DNS and only allow connections from authorized hostnames.",D:"Use service accounts and configure the web application to authorize particular service accounts to have access."},correctAnswer:["B"],explanation:{correct:"VPC firewall rules can use network tags (or service accounts) as sources and targets. This is the ideal way to manage firewall rules in a dynamic, autoscaled environment. You can assign tags like `frontend` and `backend` to your instances via instance templates. Then, you can create a firewall rule that allows traffic from the `frontend` tag to the `backend` tag on a specific port, regardless of the individual IP addresses of the instances.",incorrect:{A:"Using separate VPCs is a much heavier form of isolation and is overly complex for managing intra-application traffic within a single deployment.",C:"Cloud DNS is for name resolution, not for enforcing network traffic policies.",D:"While application-level authorization using service accounts is a good practice for security, this question specifically asks about restricting communication paths and ports at the network level, which is the job of firewall rules."}},conditions:["Multiple VM instances in one VPC","Need to restrict inter-instance communication (specific paths/ports)","Cannot rely on static IPs or subnets due to autoscaling"],caseStudyId:null},{id:171,topic:"Migration",question:"You need to evaluate your team readiness for a new GCP project. You must perform the evaluation and create a skills gap plan incorporates the business goal of cost optimization. Your team has deployed two GCP projects successfully to date. What should you do?",options:{A:"Allocate budget for team training. Set a deadline for the new GCP project.",B:"Allocate budget for team training. Create a roadmap for your team to achieve Google Cloud certification based on job role.",C:"Allocate budget to hire skilled external consultant Set a deadline for the new GCP project.",D:"Allocate budget to hire skilled external consultant Create a roadmap for your team to achieve Google Cloud certification based on job role."},correctAnswer:["B"],explanation:{correct:"A structured approach is best. Creating a roadmap for role-based certifications provides a clear, structured learning path for the team. This ensures they learn not just the technical aspects but also the best practices, including cost optimization principles, that are part of the certification curriculum. This invests in the long-term capability of the internal team.",incorrect:{A:"Simply setting a deadline without a structured plan to address the skill gaps is a recipe for failure.",C:"Hiring consultants is a valid strategy to fill immediate gaps, but investing in internal team training builds more sustainable, long-term capability. Also, this option lacks a structured plan.",D:"This is a good option, but investing in the existing team first (as in B) is often preferred, especially since they already have some GCP experience."}},conditions:["Evaluate team readiness for a new GCP project","Create a skills gap plan","Plan must incorporate the business goal of cost optimization","The team has some prior GCP experience"],caseStudyId:null},{id:172,topic:"Migration",question:"Your company is planning to perform a lift and shift migration of their Linux RHEL 6.5+ virtual machines. The virtual machines are running in an on-premises VMware environment. You want to migrate them to Compute Engine following Google-recommended practices. What should you do?",options:{A:"1. Define a migration plan based on the list of the applications and their dependencies. 2. Migrate all virtual machines into Compute Engine individually with Migrate for Compute Engine.",B:"1. Perform an assessment of virtual machines running in the current VMware environment. 2. Create images of all disks. Import disks on Compute Engine. 3. Create standard virtual machines where the boot disks are the ones you have imported.",C:"1. Perform an assessment of virtual machines running in the current VMware environment. 2. Define a migration plan, prepare a Migrate for Compute Engine migration RunBook, and execute the migration.",D:"1. Perform an assessment of virtual machines running in the current VMware environment. 2. Install a third-party agent on all selected virtual machines. 3. Migrate all virtual machines into Compute Engine."},correctAnswer:["C"],explanation:{correct:"This outlines the standard, phased approach for a successful migration using Google's recommended tool. The process involves assessing the current environment, creating a detailed plan (RunBook) that groups VMs into migration waves, and then using Migrate for Compute Engine to execute the migration with features like test-cloning and minimal downtime.",incorrect:{A:"This option skips the critical assessment phase.",B:"Manually creating and importing disks is a much more labor-intensive and error-prone process than using a dedicated migration tool like Migrate for Compute Engine.",D:"Using a third-party agent is not the Google-recommended practice when a native, purpose-built tool is available."}},conditions:["Lift-and-shift migration of Linux RHEL 6.5+ VMs","Source environment is on-premises VMware","Target is Compute Engine","Follow Google-recommended practices"],caseStudyId:null},{id:173,topic:"Migration",question:"You are migrating third-party applications from optimized on-premises virtual machines to Google Cloud. You are unsure about the optimum CPU and memory options. The applications have a consistent usage pattern across multiple weeks. You want to optimize resource usage for the lowest cost. What should you do?",options:{A:"Create an instance template with the smallest available machine type, and use an image of the third-party application taken from a current on-premises virtual machine. Create a managed instance group that uses average CPU utilization to autoscale the number of instances in the group. Modify the average CPU utilization threshold to optimize the number of instances running.",B:"Create an App Engine flexible environment, and deploy the third-party application using a Dockerfile and a custom runtime. Set CPU and memory options similar to your application's current on-premises virtual machine in the app.yaml file.",C:"Create multiple Compute Engine instances with varying CPU and memory options. Install the Cloud Monitoring agent, and deploy the third party application on each of them. Run a load test with high traffic levels on the application, and use the results to determine the optimal settings.",D:"Create a Compute Engine instance with CPU and memory options similar to your application's current on-premises virtual machine. Install the Cloud Monitoring agent, and deploy the third-party application. Run a load test with normal traffic levels on the application, and follow the Rightsizing Recommendations in the Cloud Console."},correctAnswer:["D"],explanation:{correct:"This is a practical and data-driven approach. Start with a reasonable baseline by matching the current on-premises specs. Then, use the Cloud Monitoring agent to collect actual usage data under normal traffic conditions. Finally, leverage Google Cloud's built-in Rightsizing Recommendations, which analyze this historical data to suggest the most cost-effective machine type. This avoids guesswork and optimizes based on real-world performance.",incorrect:{A:"Starting with the smallest machine type may cause the application to fail or perform poorly from the outset, making it difficult to test.",B:"Migrating a third-party VM-based application to App Engine Flexible is a significant re-architecture (containerization), not a simple migration.",C:"Creating and testing multiple instances with varying configurations is more complex and less efficient than using the automated Rightsizing Recommendations."}},conditions:["Migrating third-party applications from on-premises VMs","Unsure about optimal CPU and memory options in GCP","Applications have a consistent usage pattern","Goal is to optimize resource usage for the lowest cost"],caseStudyId:null},{id:174,topic:"Migration",question:"The migration of JencoMart's application to Google Cloud is progressing too slowly. The infrastructure is shown in the diagram. You want to maximize throughput. What are three potential bottlenecks?",options:{A:"A single VPN tunnel, which limits throughput",B:"A tier of Google Cloud Storage that is not suited for this task",C:"A copy command that is not suited to operate over long distances",D:"Fewer virtual machines (VMs) in GCP than on-premises machines",E:"A separate storage layer outside the VMs, which is not suited for this task",F:"Complicated internet connectivity between the on-premises infrastructure and GCP"},correctAnswer:["A","C","F"],explanation:{correct:"A, C, and F are all common network-related bottlenecks for on-premises to cloud migrations over VPN. A single VPN tunnel (A) has a limited bandwidth capacity. Standard, single-threaded copy commands (C) are inefficient over high-latency WAN links. The underlying public internet connectivity (F) can be congested and unpredictable. These three factors directly impact the speed of data transfer.",incorrect:{B:"The storage tier of the destination GCS bucket is unlikely to be the bottleneck during the network transfer phase.",D:"The number of VMs is related to compute capacity, not the network throughput of the migration itself.",E:"The storage layer is not typically the bottleneck for the data transfer over the network."}},conditions:["Application migration from on-premises to GCP is too slow","Connectivity is via Cloud VPN","Need to identify three potential throughput bottlenecks"],caseStudyId:"cs_jencostore"},{id:175,topic:"Migration",question:"Mountkirk Games wants to migrate from their current analytics and statistics reporting model to one that meets their technical requirements on Google Cloud. Which two steps should be part of their migration plan?",options:{A:"Evaluate the impact of migrating their current batch ETL code to Cloud Dataflow.",B:"Write a schema migration plan to denormalize data for better performance in BigQuery.",C:"Draw an architecture diagram that shows how to move from a single MySQL database to a MySQL cluster.",D:"Load 10 TB of analytics data from a previous game into a Cloud SQL instance, and run test queries against the full dataset to confirm that they complete successfully.",E:"Integrate Cloud Armor to defend against possible SQL injection attacks in analytics files uploaded to Cloud Storage."},correctAnswer:["A","B"],explanation:{correct:"The new analytics platform will likely use Dataflow for ETL and BigQuery for the data warehouse. Therefore, key migration steps are to: A) Analyze and plan the migration of the existing ETL logic to Cloud Dataflow. B) Design a new, denormalized schema for BigQuery to optimize analytical query performance, which is a critical step when moving from a relational database like MySQL to a columnar store like BigQuery.",incorrect:{C:"Moving to a MySQL cluster does not address the requirement to handle 10+ TB of analytical data; this is a job for BigQuery, not MySQL.",D:"Cloud SQL is not suitable for 10 TB of analytical queries.",E:"Cloud Armor is a WAF for protecting web applications, not for scanning files in Cloud Storage for SQL injection."}},conditions:["Migrate current analytics model (ETL -> MySQL)","New platform must meet technical requirements (scaling, on-the-fly processing, 10+ TB historical data)","Identify two key steps for the migration plan"],caseStudyId:"cs_mountkirk_main"},{id:176,topic:"Migration",question:"Dress4Win would like to become familiar with deploying applications to the cloud by successfully deploying some applications quickly, as is. They have asked for your recommendation. What should you advise?",options:{A:"Identify self-contained applications with external dependencies as a first move to the cloud.",B:"Identify enterprise applications with internal dependencies and recommend these as a first move to the cloud.",C:"Suggest moving their in-house databases to the cloud and continue serving requests to on-premise applications.",D:"Recommend moving their message queuing servers to the cloud and continue handling requests to on-premise applications."},correctAnswer:["A"],explanation:{correct:"For an initial, low-risk migration to gain familiarity, the best candidates are applications that are relatively self-contained and have few, well-defined dependencies. These 'islands' are easier to move 'as-is' (lift and shift) without causing complex networking or latency issues with tightly coupled on-premises components.",incorrect:{B:"Applications with many internal dependencies are the most difficult to migrate and are poor candidates for a first move.",C:"Moving a database to the cloud while the application remains on-premises (a 'hybrid split-stack') often introduces significant latency that can severely degrade application performance. This is a risky first move.",D:"Similar to C, moving the message queue to the cloud while producers/consumers are on-premises can introduce latency and is not an ideal first migration."}},conditions:["Goal is to become familiar with deploying to the cloud","Want to deploy some applications quickly and 'as-is'","Recommend the best type of application for a first move"],caseStudyId:"cs_dress4win_v1"},{id:177,topic:"Migration",question:"You want to ensure that your on-premises architecture meets Business Requirements before you migrate your solution. What change in the on-premises architecture should you make?",options:{A:"Replace RabbitMQ with Google Pub/Sub.",B:"Downgrade MySQL to v5.7, which is supported by Cloud SQL for MySQL.",C:"Resize compute resources to match predefined Compute Engine machine types.",D:"Containerize the micro-services and host them in Google Kubernetes Engine."},correctAnswer:["B"],explanation:{correct:"The question asks for a change to be made *on-premises* before migration. If the on-premises MySQL version (5.8 in the case study) is not fully compatible with the target Cloud SQL version or its migration tools, performing a version alignment (e.g., downgrading to a fully supported version like 5.7) on-premises *before* starting the migration can de-risk and simplify the process. This is a common pre-migration preparatory step.",incorrect:{A:"Replacing RabbitMQ with Pub/Sub is part of the migration *to the cloud*, not a change made on-premises beforehand.",C:"Resizing on-premises resources to match cloud types is not a necessary pre-migration step.",D:"Containerizing and moving to GKE is a major part of the migration itself, not a preparatory on-premises change."}},conditions:["Make a change to the on-premises architecture *before* migration","The change should help meet business requirements for the migration","On-premises MySQL is version 5.8"],caseStudyId:"cs_dress4win_v1"},{id:178,topic:"Migration",question:"Your company wants to migrate a MySQL database currently running on-premises to Google Cloud. The database is 3 TB. The migration must minimize downtime. You also need to minimize effort and follow Google-recommended practices. Which migration strategy should you choose?",options:{A:"Export the data as CSV flies from the source database. Import the files to a Cloud SQL instance.",B:"Configure the source database as a master and a Cloud SQL instance as an external replica. After the Initial sync, promote the Cloud SQL instance to become the new master.",C:"Set up a Dataflow job that reads the data from the source database and writes it into a Cloud SQL instance.",D:"Use the Database Migration Service to continuously replicate data from the source database to the Cloud SQL Instance. After the initial sync, promote the Cloud SQL Instance to become the new master."},correctAnswer:["D"],explanation:{correct:"Google Cloud's Database Migration Service (DMS) is the purpose-built, serverless tool for this exact scenario. It automates the process of setting up continuous replication from an on-premises source to a Cloud SQL target, which is the key to minimizing downtime. It is the recommended practice and minimizes manual effort compared to setting up replication manually.",incorrect:{A:"This is an offline migration that would incur significant downtime for a 3 TB database.",B:"This is a valid technical approach, but DMS (Option D) automates and manages this process, which minimizes effort and is the more modern recommended practice.",C:"Dataflow is a data processing service, not a tool designed for homogeneous database migrations. It would be overly complex and not the right tool for the job."}},conditions:["Migrate a 3 TB on-premises MySQL database to Google Cloud","Migration must minimize downtime","Need to minimize effort","Follow Google-recommended practices"],caseStudyId:null},{id:179,topic:"Networking",question:"The current Dress4Win system architecture has high latency to some customers because it is located in one data center. As part of a future evaluation and optimizing for performance in the cloud, Dresss4Win wants to distribute its system architecture to multiple locations when using Google cloud platform. Which approach should they use?",options:{A:"Use regional managed instance groups and a global load balancer to increase performance because the regional managed instance groups can grow instances in each region separately based on traffic.",B:"Use a global load balancer with a set of virtual machines that forward the requests to a closer group of virtual machines managed by your operations team.",C:"Use regional managed instance groups and a global load balancer to increase reliability by providing automatic failover between zones in different regions.",D:"Use a global load balancer with a set of virtual machines that forward the requests to a closer group of virtual machines as part of a separate managed instance groups."},correctAnswer:["A"],explanation:{correct:"This describes the standard architecture for a globally distributed, high-performance application on GCP. Deploying regional Managed Instance Groups (MIGs) places the application closer to users in different parts of the world. A Global HTTP(S) Load Balancer provides a single anycast IP and routes users to the nearest healthy regional MIG, which directly reduces latency and improves performance. The independent scaling of each regional MIG ensures resources are used efficiently.",incorrect:{B:"This describes a custom, manual forwarding setup that is much less efficient and robust than using the managed Global Load Balancer directly with backend instance groups.",C:"While this architecture does increase reliability, the primary motivation stated in the question is to reduce latency and increase performance.",D:"This is similar to B and is an inefficient, non-standard approach."}},conditions:["Current architecture has high latency due to a single data center location","Need to distribute the system architecture to multiple locations on GCP","Goal is to optimize for performance (reduce latency)"],caseStudyId:"cs_dress4win_v1"},{id:180,topic:"Networking",question:"You want to establish a Compute Engine application in a single VPC across two regions. The application must communicate over VPN to an on-premises network. How should you deploy the VPN?",options:{A:"Use VPC Network Peering between the VPC and the on-premises network.",B:"Expose the VPC to the on-premises network using IAM and VPC Sharing.",C:"Create a global Cloud VPN Gateway with VPN tunnels from each region to the on- premises peer gateway.",D:"Deploy a Cloud VPN Gateway in each region. Ensure that each region has at least one VPN tunnel to the on-premises peer gateway."},correctAnswer:["D"],explanation:{correct:"Cloud VPN gateways are regional resources. For a multi-region VPC, the best practice for resilient connectivity to on-premises is to deploy a Cloud VPN gateway in each region where your application has a presence. Using Cloud Router with BGP (dynamic routing), the VPC's global routing will automatically choose the optimal path for traffic from any instance to the on-premises network via the closest regional VPN gateway.",incorrect:{A:"VPC Network Peering is for connecting two VPCs together, not for connecting a VPC to an on-premises network.",B:"IAM and VPC Sharing are unrelated to establishing VPN connectivity.",C:"There is no such thing as a 'global' Cloud VPN Gateway. They are regional resources."}},conditions:["GCE application in a single VPC across two regions","Application must communicate over VPN to an on-premises network"],caseStudyId:null},{id:181,topic:"Networking",question:"You need to implement a network ingress for a new game that meets the defined business and technical requirements. Mountkirk Games wants each regional game instance to be located in multiple Google Cloud regions. What should you do?",options:{A:"Configure a global load balancer connected to a managed instance group running Compute Engine instances.",B:"Configure kubemci with a global load balancer and Google Kubernetes Engine.",C:"Configure a global load balancer with Google Kubernetes Engine.",D:"Configure Ingress for Anthos with a global load balancer and Google Kubernetes Engine."},correctAnswer:["D"],explanation:{correct:"The case study explicitly states the plan is to use GKE and a global load balancer to route players to the closest regional game arenas. Ingress for Anthos (formerly Multi-cluster Ingress) is the Google-recommended, GKE-native solution for configuring a single Global HTTP(S) Load Balancer to distribute traffic across services running in multiple GKE clusters in different regions.",incorrect:{A:"This uses Compute Engine, but the case study specifies GKE for the new game.",B:"`kubemci` was the predecessor to Ingress for Anthos and is now deprecated.",C:"This is too generic. Ingress for Anthos is the specific feature/product you use to configure a global load balancer for a multi-cluster GKE setup."}},conditions:["Implement network ingress for a new game","Deploy game instances in multiple Google Cloud regions","Use GKE and a global load balancer (per case study)"],caseStudyId:"cs_mountkirk_main"},{id:182,topic:"Networking",question:"You need to upgrade the EHR connection to comply with their requirements. The new connection design must support business-critical needs and meet the same network and security policy requirements. What should you do?",options:{A:"Add a new Dedicated Interconnect connection.",B:"Upgrade the bandwidth on the Dedicated Interconnect connection to 100 G.",C:"Add three new Cloud VPN connections.",D:"Add a new Carrier Peering connection."},correctAnswer:["A"],explanation:{correct:"The case study calls for a 'secure and high-performance connection' and '99.9% availability'. For business-critical needs, high availability is achieved through redundancy. Adding a second, physically separate Dedicated Interconnect connection provides this redundancy, allowing the system to tolerate a failure of the first connection. This is the standard way to achieve a 99.99% SLA for Interconnect.",incorrect:{B:"Upgrading the bandwidth of a single connection increases capacity but does not add redundancy. It remains a single point of failure.",C:"While multiple VPNs provide some redundancy, Dedicated Interconnect offers higher, more consistent performance for business-critical needs.",D:"Carrier Peering is for connecting to Google's public network edge, not for private VPC connectivity."}},conditions:["Upgrade hybrid connectivity","Support business-critical needs","Meet network/security policies","Requirement for high performance and 99.9% availability"],caseStudyId:"cs_ehr_healthcare"},{id:183,topic:"Networking",question:"You need to define the technical architecture for hybrid connectivity between EHR's on-premises systems and Google Cloud. You want to follow Google's recommended practices for production-level applications. Considering the EHR Healthcare business and technical requirements, what should you do?",options:{A:"Configure two Partner Interconnect connections in one metro (City), and make sure the Interconnect connections are placed in different metro zones.",B:"Configure two VPN connections from on-premises to Google Cloud, and make sure the VPN devices on-premises are in separate racks.",C:"Configure Direct Peering between EHR Healthcare and Google Cloud, and make sure you are peering at least two Google locations.",D:"Configure two Dedicated Interconnect connections in one metro (City) and two connections in another metro, and make sure the Interconnect connections are placed in different metro zones."},correctAnswer:["D"],explanation:{correct:"This describes the most resilient architecture for Dedicated Interconnect, designed to achieve a 99.99% SLA. By having connections in two different metropolitan areas, you protect against a regional disaster. By having two connections in different edge availability domains (metro zones) within each metro, you protect against local failures within a single colocation facility. This is the recommended practice for mission-critical production applications.",incorrect:{A:"This provides redundancy within a single metro but is vulnerable to a metro-wide outage.",B:"VPNs do not typically offer the same level of performance and reliability as Dedicated Interconnect for high-performance needs.",C:"Direct Peering is for accessing Google's public services, not for private VPC connectivity."}},conditions:["Define hybrid connectivity architecture for production-level applications","Follow Google's recommended practices","Meet requirements for high availability (99.9%), security, and high performance"],caseStudyId:"cs_ehr_healthcare"},{id:184,topic:"Networking",question:"Your company recently acquired a company that has infrastructure in Google Cloud. Each company has its own Google Cloud organization. Each company is using a Shared Virtual Private Cloud (VPC) to provide network connectivity for its applications. Some of the subnets used by both companies overlap. In order for both businesses to integrate, the applications need to have private network connectivity. These applications are not on overlapping subnets. You want to provide connectivity with minimal re-engineering. What should you do?",options:{A:"Set up VPC peering and peer each Shared VPC together",B:"Configure SSH port forwarding on each application to provide connectivity between applications in the different Shared VPCs",C:"Migrate the projects from the acquired company into your company's Google Cloud organization. Re-launch the instances in your company's Shared VPC",D:"Set up a Cloud VPN gateway in each Shared VPC and peer Cloud VPNs"},correctAnswer:["D"],explanation:{correct:"VPC Peering cannot be used because the overall VPCs have overlapping subnets, even if the specific applications do not. The least disruptive way to connect the two networks is by using Cloud VPN. A VPN gateway can be set up in each VPC, and custom routes can be configured to route traffic specifically between the non-overlapping subnets where the applications reside.",incorrect:{A:"VPC Peering is not possible due to the overlapping subnet ranges in the VPCs.",B:"SSH port forwarding is not a scalable or robust solution for application network integration.",C:"Migrating the entire organization is a major re-engineering effort, which violates the requirement."}},conditions:["Two companies in separate Google Cloud Organizations, each with a Shared VPC","Some subnets between the VPCs overlap","Need private network connectivity for specific applications that are *not* on overlapping subnets","Minimize re-engineering effort"],caseStudyId:null},{id:185,topic:"Networking",question:"Your company and one of its partners each have a Google Cloud project in separate organizations. Your company's project (prj-a) runs in Virtual Private Cloud (vpc-a). The partner's project (prj-b) runs in vpc-b. There are two instances running on vpc-a and one instance running on vpc-b. Subnets defined in both VPCs are not overlapping. You need to ensure that all instances communicate with each other via internal IPs, minimizing latency and maximizing throughput. What should you do?",options:{A:"Set up a network peering between vpc-a and vpc-b.",B:"Set up a VPN between vpc-a and vpc-b using Cloud VPN.",C:"Configure [AP TCP forwarding on the instance in vpc-b, and then launch the following gcloud command from one of the instances in vpc-a gcloud: gcloud compute start-iap-tunnel INSTANCE_NAME_IN_VPC_8 22 \\ --local-host-port=localhost:22",D:"1. Create an additional instance in vpc-a. 2. Create an additional instance in vpc-b. 3. Install OpenVPN in newly created instances. 4. Configure a VPN tunnel between vpc-a and vpc-b with the help of OpenVPN."},correctAnswer:["A"],explanation:{correct:"VPC Network Peering is the ideal solution for this scenario. It allows private connectivity between VPCs (even in different organizations) using internal IPs. Traffic stays on Google's network, which provides the lowest latency and highest throughput. The key prerequisite of non-overlapping subnets is met.",incorrect:{B:"Cloud VPN would add encryption overhead and potentially higher latency as traffic might traverse parts of the public internet. Peering is more direct.",C:"IAP TCP forwarding is for secure administrative access to a single instance, not for general application network connectivity.",D:"This is a manual, self-managed VPN solution that is more complex and less performant than the managed VPC Peering service."}},conditions:["Two projects in separate Organizations (prj-a/vpc-a, pr-b/vpc-b)","Subnets do not overlap","Need communication via internal IPs","Minimize latency and maximize throughput"],caseStudyId:null},{id:186,topic:"Networking",question:"You are migrating your on-premises solution to Google Cloud in several phases. You will use Cloud VPN to maintain a connection between your on-premises systems and Google Cloud until the migration is completed. You want to make sure all your on-premise systems remain reachable during this period. How should you organize your networking in Google Cloud?",options:{A:"Use the same IP range on Google Cloud as you use on-premises",B:"Use the same IP range on Google Cloud as you use on-premises for your primary IP range and use a secondary range that does not overlap with the range you use on-premises",C:"Use an IP range on Google Cloud that does not overlap with the range you use on-premises",D:"Use an IP range on Google Cloud that does not overlap with the range you use on-premises for your primary IP range and use a secondary range with the same IP range as you use on-premises"},correctAnswer:["C"],explanation:{correct:"To establish network connectivity (via VPN, Interconnect, or Peering) between two networks, their IP address ranges must not overlap. Overlapping ranges create routing ambiguity, and systems will not be able to communicate correctly. Therefore, you must plan your GCP VPC subnets to use IP ranges that are completely distinct from your on-premises IP ranges.",incorrect:{A:"This will cause routing conflicts and break connectivity.",B:"This is incorrect for the same reason as A.",D:"This is also incorrect for the same reason as A."}},conditions:["Phased migration from on-premises to GCP","Using Cloud VPN to connect the two environments during migration","Need to ensure on-premises systems remain reachable from GCP"],caseStudyId:null},{id:187,topic:"Networking",question:"You need to develop procedures to verify resilience of disaster recovery for remote recovery using GCP. Your production environment is hosted on-premises. You need to establish a secure, redundant connection between your on premises network and the GCP network. What should you do?",options:{A:"Verify that Dedicated Interconnect can replicate files to GC Verify that direct peering can establish a secure connection between your networks if Dedicated Interconnect fails.",B:"Verify that Dedicated Interconnect can replicate files to GC Verify that Cloud VPN can establish a secure connection between your networks if Dedicated Interconnect fails.",C:"Verify that the Transfer Appliance can replicate files to GC Verify that direct peering can establish a secure connection between your networks if the Transfer Appliance fails.",D:"Verify that the Transfer Appliance can replicate files to GC Verify that Cloud VPN can establish a secure connection between your networks if the Transfer Appliance fails."},correctAnswer:["B"],explanation:{correct:"A common and recommended pattern for a redundant hybrid connection is to use Dedicated Interconnect as the primary, high-performance link, and a Cloud VPN tunnel as a backup. Disaster recovery procedures should include tests to verify that failover to the Cloud VPN works correctly if the primary Interconnect connection fails.",incorrect:{A:"Direct Peering is for connecting to Google's public services, not for private VPC connectivity, so it cannot act as a backup for Dedicated Interconnect.",C:"Transfer Appliance is for offline data transfer, not for establishing live network connectivity.",D:"This is also incorrect for the same reason as C."}},conditions:["Develop disaster recovery resilience testing procedures","Production environment is on-premises","Need a secure, redundant connection to GCP"],caseStudyId:null},{id:188,topic:"Networking",question:"You are using Cloud CDN to deliver static HTTP(S) website content hosted on a Compute Engine instance group. You want to improve the cache hit ratio. What should you do?",options:{A:"Customize the cache keys to omit the protocol from the key.",B:"Shorten the expiration time of the cached objects.",C:"Make sure the HTTP(S) header Cache-Region points to the closest region of your users.",D:"Replicate the static content in a Cloud Storage bucke Point CloudCDN toward a load balancer on that bucket."},correctAnswer:["A"],explanation:{correct:"By default, Cloud CDN includes the protocol (HTTP or HTTPS) in the cache key. This means that if a user requests `http://example.com/image.jpg` and another requests `https://example.com/image.jpg`, they will generate two separate cache entries for the same object. By customizing the cache key to omit the protocol, both requests will map to the same cache entry, increasing the chance of a cache hit.",incorrect:{B:"Shortening the expiration time would *decrease* the cache hit ratio, as objects would be evicted from the cache more frequently.",C:"`Cache-Region` is not a standard header used for this purpose.",D:"Changing the origin from Compute Engine to Cloud Storage is a good practice for reliability and performance, but it doesn't directly improve the cache hit ratio without also optimizing cache keys or headers."}},conditions:["Using Cloud CDN with a Compute Engine origin","Serving static HTTP(S) content","Want to improve the cache hit ratio"],caseStudyId:null},{id:189,topic:"Networking",question:"All Compute Engine instances in your VPC should be able to connect to an Active Directory server on specific ports. Any other traffic emerging from your instances is not allowed. You want to enforce this using VPC firewall rules. How should you configure the firewall rules?",options:{A:"Create an egress rule with priority 1000 to deny all traffic for all instances. Create another egress rule with priority 100 to allow the Active Directory traffic for all instances.",B:"Create an egress rule with priority 100 to deny all traffic for all instances. Create another egress rule with priority 1000 to allow the Active Directory traffic for all instances.",C:"Create an egress rule with priority 1000 to allow the Active Directory traffic. Rely on the implied deny egress rule with priority 100 to block all traffic for all instances.",D:"Create an egress rule with priority 100 to allow the Active Directory traffic. Rely on the implied deny egress rule with priority 1000 to block all traffic for all instances."},correctAnswer:["A"],explanation:{correct:"GCP firewall rules are evaluated in order of priority, from the lowest number (highest priority) to the highest. To implement a 'default deny' policy with a specific 'allow' exception, you create a high-priority (low number, e.g., 100) rule to allow the specific traffic. Then, you create a lower-priority (higher number, e.g., 1000) rule to deny all other traffic. The allow rule will be matched first for AD traffic, and all other traffic will fall through to be blocked by the deny rule.",incorrect:{B:"This reverses the priorities. The deny rule at priority 100 would block all traffic, including the AD traffic, before the allow rule is ever evaluated.",C:"There is an implied *allow* all egress rule at the lowest priority (65535), not an implied deny egress rule. You must create an explicit deny rule.",D:"This is incorrect for the same reason as C."}},conditions:["Allow all GCE instances to connect to an Active Directory server on specific ports","Deny all other outgoing (egress) traffic from the instances","Enforce this using VPC firewall rules"],caseStudyId:null},{id:190,topic:"Networking",question:"You need to design a solution for global load balancing based on the URL path being requested. You need to ensure operations reliability and end-to-end in- transit encryption based on Google best practices. What should you do?",options:{A:"Create a cross-region load balancer with URL Maps.",B:"Create an HTTPS load balancer with URL Maps.",C:"Create appropriate instance groups and instances. Configure SSL proxy load balancing.",D:"Create a global forwarding rule. Configure SSL proxy load balancing."},correctAnswer:["B"],explanation:{correct:"The External HTTP(S) Load Balancer is Google Cloud's global Layer 7 load balancer. It meets all the requirements: it's a global service, it uses URL Maps for routing based on URL path, and configuring it for HTTPS provides encryption between the client and the load balancer. For end-to-end encryption, you would also configure HTTPS on the backend service.",incorrect:{A:"'Cross-region load balancer' is a descriptive term, but 'HTTPS load balancer' is the specific product type that meets the requirements.",C:"SSL Proxy load balancing is for non-HTTP(S) TCP traffic and does not support URL path-based routing.",D:"This is also incorrect for the same reason as C."}},conditions:["Design a global load balancing solution","Route traffic based on the requested URL path","Ensure operational reliability and end-to-end in-transit encryption","Follow Google best practices"],caseStudyId:null},{id:191,topic:"Networking",question:"You have deployed several instances on Compute Engine. As a security requirement, instances cannot have a public IP address. There is no VPN connection between Google Cloud and your office, and you need to connect via SSH into a specific machine without violating the security requirements. What should you do?",options:{A:"Configure Cloud NAT on the subnet where the instance is hosted. Create an SSH connection to the Cloud NAT IP address to reach the instance.",B:"Add all instances to an unmanaged instance group. Configure TCP Proxy Load Balancing with the instance group as a backend. Connect to the instance using the TCP Proxy IP.",C:"Configure Identity-Aware Proxy (IAP) for the instance and ensure that you have the role of IAP-secured Tunnel User. Use the gcloud command line tool to ssh into the instance.",D:"Create a bastion host in the network to SSH into the bastion host from your office location. From the bastion host, SSH into the desired instance."},correctAnswer:["C"],explanation:{correct:"Identity-Aware Proxy (IAP) for TCP forwarding is designed for this exact use case. It allows you to securely connect to instances that do not have public IP addresses by tunneling the traffic through IAP's infrastructure. Access is controlled by IAM permissions, not network firewall rules, providing a zero-trust approach without requiring a VPN or bastion host.",incorrect:{A:"Cloud NAT provides outbound internet connectivity for private instances; it does not allow inbound connections.",B:"A TCP Proxy Load Balancer is for distributing application traffic, not for providing administrative SSH access to individual instances.",D:"A bastion host is a traditional solution, but it typically requires a public IP on the bastion host itself and adds an extra hop. IAP is the more modern, secure, and direct cloud-native solution."}},conditions:["GCE instances have no public IP addresses","No VPN connection from your office","Need to SSH into a specific machine","Must not violate the no-public-IP security requirement"],caseStudyId:null},{id:192,topic:"Networking",question:"Your team needs to create a Google Kubernetes Engine (GKE) cluster to host a newly built application that requires access to third-party services on the internet. Your company does not allow any Compute Engine instance to have a public IP address on Google Cloud. You need to create a deployment strategy that adheres to these guidelines. What should you do?",options:{A:"Configure the GKE cluster as a private cluster, and configure Cloud NAT Gateway for the cluster subnet.",B:"Configure the GKE cluster as a private cluster. Configure Private Google Access on the Virtual Private Cloud (VPC).",C:"Configure the GKE cluster as a route-based cluster. Configure Private Google Access on the Virtual Private Cloud (VPC).",D:"Create a Compute Engine instance, and install a NAT Proxy on the instance. Configure all workloads on GKE to pass through this proxy to access third-party services on the Internet."},correctAnswer:["A"],explanation:{correct:"This is the standard and recommended solution. A private GKE cluster ensures that the nodes do not have public IP addresses. Cloud NAT is the managed service that allows these private nodes to initiate outbound connections to the internet (for accessing third-party services) without needing public IPs themselves.",incorrect:{B:"Private Google Access only allows access to Google APIs and services, not to the general internet for third-party services.",C:"Route-based clusters are an older networking mode, and Private Google Access does not provide general internet access.",D:"Building a self-managed NAT proxy on a VM is more complex and less reliable than using the managed Cloud NAT service."}},conditions:["Create a GKE cluster","Application requires outbound access to third-party services on the internet","Company policy prohibits public IP addresses on any Compute Engine instance (including GKE nodes)"],caseStudyId:null},{id:193,topic:"Networking",question:"Your company has a support ticketing solution that uses App Engine Standard. The project that contains the App Engine application already has a Virtual Private Cloud (VPC) network fully connected to the company's on-premises environment through a Cloud VPN tunnel. You want to enable the App Engine application to communicate with a database that is running in the company's on-premises environment. What should you do?",options:{A:"Configure private Google access for on-premises hosts only.",B:"Configure private Google access.",C:"Configure private services access.",D:"Configure serverless VPC access."},correctAnswer:["D"],explanation:{correct:"App Engine Standard runs in a Google-managed environment outside of your VPC. To allow it to connect to resources inside your VPC (and by extension, to your on-premises network connected to that VPC), you must use a Serverless VPC Access connector. This connector acts as a bridge, allowing the App Engine application to send traffic to private IP addresses in your VPC.",incorrect:{A:"Private Google Access is for resources *inside* the VPC to reach Google APIs, not for App Engine to reach *into* the VPC.",B:"This is incorrect for the same reason as A.",C:"Private services access is for connecting your VPC to other Google-managed services like Cloud SQL, not for connecting App Engine to your VPC."}},conditions:["App Engine Standard application","VPC is connected to on-premises via Cloud VPN","Need to enable the App Engine application to communicate with an on-premises database"],caseStudyId:null},{id:194,topic:"Networking",question:"Your company has a stateless web API that performs scientific calculations. The web API runs on a single Google Kubernetes Engine (GKE) cluster. The cluster is currently deployed in us-central1. Your company has expanded to offer your API to customers in Asia. You want to reduce the latency for users in Asia. What should you do?",options:{A:"Create a second GKE cluster in asia-southeast1, and expose both APIs using a Service of type LoadBalancer. Add the public IPs to the Cloud DNS zone.",B:"Use a global HTTP(s) load balancer with Cloud CDN enabled.",C:"Create a second GKE cluster in asia-southeast1, and use kubemci to create a global HTTP(s) load balancer.",D:"Increase the memory and CPU allocated to the application in the cluster."},correctAnswer:["C"],explanation:{correct:"To reduce latency for users in Asia, you must serve traffic from a region closer to them. This requires deploying a second GKE cluster in an Asian region. To direct users to the closest cluster automatically, you need a single global entry point, which is provided by a Global HTTP(S) Load Balancer. `kubemci` (or its successor, Ingress for Anthos) is the tool used to configure this multi-cluster ingress, creating a global load balancer that fronts GKE clusters in multiple regions.",incorrect:{A:"A Service of type LoadBalancer creates a regional Network Load Balancer. This would give you two separate regional IPs, which doesn't provide automatic latency-based routing from a single entry point.",B:"A global load balancer is needed, but Cloud CDN is for caching static content, which is not effective for a dynamic computational API.",D:"Vertically scaling the cluster in us-central1 will not solve the network latency problem for users in Asia."}},conditions:["Stateless web API on a single GKE cluster in us-central1","New customers in Asia","Goal is to reduce latency for users in Asia"],caseStudyId:null},{id:195,topic:"Networking",question:"A lead software engineer tells you that his new application design uses websockets and HTTP sessions that are not distributed across the web servers. You want to help him ensure his application will run property on Google Cloud Platform. What should you do?",options:{A:"Help the engineer to convert his websocket code to use HTTP streaming.",B:"Review the encryption requirements for websocket connections with the security team.",C:"Meet with the cloud operations team and the engineer to discuss load balancer options.",D:"Help the engineer redesign the application to use a distributed user session service that does not rely on websockets and HTTP sessions."},correctAnswer:["C"],explanation:{correct:"The application's requirementsWebSockets and non-distributed sessionsdirectly impact the choice and configuration of a load balancer. The load balancer must support the WebSocket protocol and also be configured for session affinity ('sticky sessions') to ensure that requests from a user are always sent to the same backend server where their session state is stored. Discussing these specific load balancer options is the crucial first step to ensuring the application works on GCP.",incorrect:{A:"Converting from WebSockets might not be necessary, as GCP load balancers support them. This would be an unnecessary application change.",B:"Encryption is important, but the core functional problem is the routing of WebSocket and stateful traffic.",D:"While redesigning for distributed sessions is a good long-term goal for scalability, the immediate need is to make the *current* design work. This would be a significant re-architecture."}},conditions:["Application uses WebSockets","Application uses non-distributed (local) HTTP sessions","Need to ensure the application runs properly on GCP"],caseStudyId:null},{id:196,topic:"Networking",question:"The operations team in your company wants to save Cloud VPN log events for one year. You need to configure the cloud infrastructure to save the logs. What should you do?",options:{A:"Set up a filter in Cloud Logging and a Cloud Storage bucket as an export target for the logs you want to save.",B:"Enable the Compute Engine API, and then enable logging on the firewall rules that match the traffic you want to save.",C:"Set up a Cloud Logging Dashboard titled Cloud VPN Logs, and then add a chart that queries for the VPN metrics over a one-year time period.",D:"Set up a filter in Cloud Logging and a topic in Pub/Sub to publish the logs."},correctAnswer:["A"],explanation:{correct:"Cloud Logging's default retention may not be one year. To retain logs for a longer period, you must create a log sink. Configuring a sink to filter for Cloud VPN logs and export them to a Cloud Storage bucket is the standard and most cost-effective method for long-term log archival.",incorrect:{B:"This refers to firewall rule logging, not Cloud VPN logs.",C:"A dashboard is for viewing metrics, not for storing and retaining the raw log events themselves.",D:"Exporting to Pub/Sub is for real-time processing of logs, not for long-term archival. Messages in Pub/Sub have a limited retention period."}},conditions:["Save Cloud VPN log events for one year"],caseStudyId:null},{id:197,topic:"Networking",question:"You are deploying an application to Google Cloud. The application is part of a system. The application in Google Cloud must communicate over a private network with applications in a non-Google Cloud environment. The expected average throughput is 200 kbps. The business requires: as close to 100% system availability as possible and cost optimization. You need to design the connectivity between the locations to meet the business requirements. What should you provision?",options:{A:"An HA Cloud VPN gateway connected with two tunnels to an on-premises VPN gateway.",B:"Two Classic Cloud VPN gateways connected to two on-premises VPN gateways Configure each Classic Cloud VPN gateway to have two tunnels, each connected to different on-premises VPN gateways.",C:"Two HA Cloud VPN gateways connected to two on-premises VPN gateways Configure each HA Cloud VPN gateway to have two tunnels, each connected to different on-premises VPN gateways.",D:"A single Cloud VPN gateway connected to an on-premises VPN gateway."},correctAnswer:["A"],explanation:{correct:"For high availability (targeting 99.99% SLA) and cost optimization with low throughput, HA Cloud VPN is the ideal choice. A single HA VPN gateway in GCP with two tunnels connected to a redundant peer gateway provides the required availability. The throughput is low, so more expensive options like Interconnect are not needed, and the cost of a single HA VPN gateway is optimized.",incorrect:{B:"Classic VPN only offers a 99.9% SLA and is not recommended for new HA deployments.",C:"Using two HA VPN gateways is overly redundant and not cost-optimized for this scenario. A single HA gateway is designed to meet the 99.99% SLA.",D:"A single classic VPN gateway is a single point of failure and does not meet the availability requirement."}},conditions:["GCP app needs private communication with non-GCP app","Throughput: 200 kbps (low)","Requirement: Near 100% system availability","Requirement: Cost optimization"],caseStudyId:null},{id:198,topic:"Networking",question:"You are configuring the cloud network architecture for a newly created project in Google Cloud that will host applications in Compute Engine. Compute Engine virtual machine instances will be created in two different subnets (sub-a and sub-b) within a single region: Instances in sub-a will have public IP addresses. Instances in sub-b will have only private IP addresses. To download updated packages, instances must connect to a public repository outside the boundaries of Google Cloud. You need to allow sub-b to access the external repository. What should you do?",options:{A:"Enable Private Google Access on sub-b.",B:"Configure Cloud NAT and select sub-b in the NAT mapping section.",C:"Configure a bastion host instance in sub-a to connect to instances in sub-b.",D:"Enable Identity-Aware Proxy for TCP forwarding for instances in sub-b."},correctAnswer:["B"],explanation:{correct:"Cloud NAT is the managed service specifically designed to allow instances without external IP addresses to initiate outbound connections to the public internet. By configuring Cloud NAT for `sub-b`, the instances within it can reach the external package repository.",incorrect:{A:"Private Google Access only allows access to Google APIs and services (like GCS, BigQuery), not the general internet.",C:"A bastion host is for secure *inbound* access to private instances, not for enabling their outbound connections.",D:"IAP is also for secure *inbound* access, not outbound."}},conditions:["GCE instances in two subnets (sub-a with public IPs, sub-b with private IPs)","Instances in the private subnet (sub-b) need to connect to a public external repository"],caseStudyId:null},{id:199,topic:"Networking",question:"You are deploying an application to Google Cloud. The application is part of a system. The application in Google Cloud must communicate over a private network with applications in a non-Google Cloud environment. The expected average throughput is 200 kbps. The business requires: 99.99% system availability and cost optimization. You need to design the connectivity between the locations to meet the Business Requirements. What should you provision?",options:{A:"An HA Cloud VPN gateway connected with two tunnels to an on-premises VPN gateway.",B:"A Classic Cloud VPN gateway connected with two tunnels to an on-premises VPN gateway.",C:"Two HA Cloud VPN gateways connected to two on-premises VPN gateways. Configure each HA Cloud VPN gateway to have two tunnels, each connected to different on-premises VPN gateways.",D:"A Classic Cloud VPN gateway connected with one tunnel to an on-premises VPN gateway."},correctAnswer:["A"],explanation:{correct:"HA Cloud VPN is specifically designed to provide a 99.99% uptime SLA. For the low throughput requirement of 200 kbps, it is also the most cost-effective solution compared to options like Dedicated Interconnect. A single HA VPN gateway with two tunnels to a redundant peer gateway is the standard configuration to achieve this SLA.",incorrect:{B:"Classic Cloud VPN does not offer a 99.99% SLA.",C:"Using two HA VPN gateways is more expensive and generally unnecessary to meet the 99.99% SLA, making it not cost-optimized.",D:"A single Classic VPN tunnel is a single point of failure and does not meet the availability requirement."}},conditions:["Private connectivity between GCP and non-GCP environment","Low throughput (200 kbps)","Requirement: 99.99% system availability","Requirement: Cost optimization"],caseStudyId:null},{id:200,topic:"Networking",question:"Recently HRL started a new regional racing league in Cape Town, South Africa. In an effort to give customers in Cape Town a better user experience, HRL has partnered with the Content Delivery Network provider, Fastly. HRL needs to allow traffic coming from all of the Fastly IP address ranges into their Virtual Private Cloud network (VPC network). You are a member of the HRL security team and you need to configure the update that will allow only the Fastly IP address ranges through the External HTTP(S) load balancer. Which command should you use?",options:{A:"gcloud compute security-policies rules update 1000 --security-policy from-fastly --src-ip-ranges * --action 'allow'",B:"gcloud compute firewall rules update sourceiplist-fastly --priority 1000 --allow tcep:443",C:"gcloud compute firewall rules update hlr-policy --priority 1000 --target-tags=sourceiplist-fastly --allow tcep:443",D:"gcloud compute security-policies rules update 1000 --security-policy hlr-policy --expression 'evaluatePreconfiguredExpr(sourceiplist-fastly)' --action 'allow'"},correctAnswer:["D"],explanation:{correct:"To control traffic at the edge of the network, specifically for an External HTTP(S) Load Balancer, you must use Google Cloud Armor security policies. This command correctly updates a rule in a security policy (`hlr-policy`) to use a preconfigured expression that references a named IP list (`sourceiplist-fastly`), and sets the action to `allow`. This is the proper way to whitelist traffic from a CDN provider like Fastly at the load balancer level.",incorrect:{A:"This allows all source IPs (`*`), which is insecure and violates the requirement to only allow Fastly IPs.",B:"This configures a VPC firewall rule, which applies to VM instances, not the external load balancer.",C:"This also configures a VPC firewall rule, not a Cloud Armor policy."}},conditions:["Partnered with CDN provider Fastly","Need to allow traffic *only* from Fastly's IP address ranges","Traffic comes through an External HTTP(S) Load Balancer"],caseStudyId:"cs_hrl"},{id:201,topic:"Networking",question:"In the past, configuration errors put public IP addresses on backend servers that should not have been accessible from the Internet. You need to ensure that no one can put external IP addresses on backend Compute Engine instances and that external IP addresses can only be configured on frontend Compute Engine instances. What should you do?",options:{A:"Create an Organizational Policy with a constraint to allow external IP addresses only on the frontend Compute Engine instances.",B:"Revoke the compute.networkAdmin role from all users in the project with front end instances.",C:"Create an Identity and Access Management (IAM) policy that maps the IT staff to the compute.networkAdmin role for the organization.",D:"Create a custom Identity and Access Management (IAM) role named GCE_FRONTEND with the compute.addresses.create permission."},correctAnswer:["A"],explanation:{correct:"Organization Policies provide preventative controls. The `constraints/compute.vmExternalIpAccess` constraint is specifically designed for this purpose. You can set a policy at a high level (e.g., a folder for backend projects) to deny external IPs, and then create a more permissive policy (allowing external IPs) on the specific project or folder containing the frontend instances. This provides a centrally managed, preventative guardrail.",incorrect:{B:"Relying solely on IAM roles is not enough. A user with sufficient permissions could still make a mistake. Organization policies provide a safety net that applies to everyone, regardless of their IAM roles.",C:"This is an IAM configuration and doesn't prevent misconfigurations as effectively as an Organization Policy.",D:"This is also an IAM configuration and doesn't provide the preventative guardrail that an Organization Policy does."}},conditions:["Past issue: Accidental public IPs on backend servers","Requirement: Prevent external IPs on backend GCE instances","Requirement: Allow external IPs only on frontend GCE instances"],caseStudyId:"cs_ehr_healthcare"},{id:202,topic:"Networking",question:"Your company has a project in Google Cloud with three Virtual Private Clouds (VPCs). There is a Compute Engine instance on each VPC. Network subnets do not overlap and must remain separated. The network configuration is shown below. [Diagram shows VPC #1 (subnet #1, Instance #1), VPC #2 (subnet #2, Instance #2), VPC #3 (subnet #3, Instance #3) with no connections between them]. Instance #1 is an exception and must communicate directly with both Instance #2 and Instance #3 via internal IPs. How should you accomplish this?",options:{A:"Create a cloud router to advertise subnet #2 and subnet #3 to subnet #1.",B:"Add two additional NICs to Instance #1 with the following configuration:NIC1VPC: VPC #2SUBNETWORK: subnet #2NIC2VPC: VPC #3SUBNETWORK: subnet #3Update firewall rules to enable traffic between instances.",C:"Create two VPN tunnels via CloudVPN:1 between VPC #1 and VPC #2.1 between VPC #2 and VPC #3.Update firewall rules to enable traffic between the instances.",D:"Peer all three VPCs:Peer VPC #1 with VPC #2.Peer VPC #2 with VPC #3.Update firewall rules to enable traffic between the instances."},correctAnswer:["B"],explanation:{correct:"A Compute Engine instance can have multiple network interface controllers (NICs), each attached to a different VPC. By adding a second NIC to Instance #1 connected to VPC #2 and a third NIC connected to VPC #3, Instance #1 becomes multi-homed and can communicate directly with instances in all three VPCs using their internal IPs, while keeping the VPCs themselves separate.",incorrect:{A:"Cloud Router is for dynamic routing with VPN or Interconnect, not for connecting a single instance to multiple VPCs.",C:"This would establish connectivity between VPCs but is more complex than needed. Also, it would not provide direct connectivity between VPC #1 and VPC #3.",D:"VPC Peering is not transitive. Peering VPC #1 to #2 and #2 to #3 does not allow VPC #1 to communicate with VPC #3. You would need to peer #1 with #3 as well, which may not be desired if the networks should remain separated except for this one instance."}},conditions:["3 separate VPCs with instances","Non-overlapping subnets that must remain separated","Instance #1 needs to communicate directly with Instance #2 and Instance #3 via internal IPs"],caseStudyId:null},{id:203,topic:"Networking",question:"You have set up an Internal TCP/UDP Load Balancer to distribute traffic to a backend instance group containing web servers. You want to ensure that client requests from the same IP address are consistently directed to the same backend instance for the duration of their session. What configuration should you enable on the backend service?",options:{A:"Connection Draining",B:"Session Affinity: Client IP",C:"Session Affinity: Generated Cookie",D:"Health Checks"},correctAnswer:["B"],explanation:{correct:"Session Affinity is the feature specifically designed to ensure requests from the same client are sent to the same backend instance. For a TCP/UDP load balancer where HTTP cookies are not available, `Client IP` is the appropriate method to achieve this.",incorrect:{A:"Connection Draining ensures existing connections are handled gracefully when an instance is removed from a backend, but it doesn't control initial routing for new sessions.",C:"`Generated Cookie` is a session affinity method for HTTP(S) Load Balancers, not TCP/UDP Load Balancers.",D:"Health Checks are essential for ensuring traffic is only sent to healthy backends, but they do not provide session affinity."}},conditions:["Internal TCP/UDP Load Balancer","Need to direct requests from the same client IP to the same backend instance (sticky sessions)"],caseStudyId:null},{id:204,topic:"Networking",question:"You need to configure firewall rules for your VPC network to allow ingress traffic on TCP port 443 (HTTPS) only from a specific set of Compute Engine instances that act as web proxies. These proxy instances are identified by the network tag 'web-proxy'. What is the most appropriate configuration for the firewall rule?",options:{A:"Direction: Ingress, Action: Allow, Targets: 'web-proxy' tag, Source filter: IP ranges 0.0.0.0/0, Protocols/ports: tcp:443",B:"Direction: Ingress, Action: Allow, Targets: All instances, Source filter: 'web-proxy' tag, Protocols/ports: tcp:443",C:"Direction: Egress, Action: Allow, Targets: 'web-proxy' tag, Destination filter: IP ranges 0.0.0.0/0, Protocols/ports: tcp:443",D:"Direction: Ingress, Action: Allow, Targets: <tag_of_backend_servers>, Source filter: 'web-proxy' tag, Protocols/ports: tcp:443"},correctAnswer:["D"],explanation:{correct:"This configuration correctly defines the desired traffic flow. The rule applies to incoming traffic (`Direction: Ingress`) for the backend servers (`Targets: <tag_of_backend_servers>`). It specifically allows this traffic only if it comes from instances with the 'web-proxy' tag (`Source filter: 'web-proxy' tag`) on the required port (`tcp:443`).",incorrect:{A:"This rule incorrectly targets the proxies themselves and allows traffic from anywhere on the internet (`0.0.0.0/0`).",B:"This rule correctly identifies the source but is too broad on the target, applying to all instances in the VPC.",C:"This is an egress (outbound) rule, not an ingress (inbound) rule."}},conditions:["Configure a VPC firewall rule","Allow ingress traffic on TCP port 443","Source of traffic must be only from instances with the 'web-proxy' network tag"],caseStudyId:null},{id:205,topic:"Networking",question:"You are configuring network connectivity for a GKE cluster. You need to ensure that Pods can communicate with each other across different nodes within the cluster, but external traffic to the Pods should be blocked by default unless explicitly allowed. Which GKE networking mode should you use?",options:{A:"VPC-native using Alias IP ranges",B:"Routes-based cluster networking",C:"Network Policy enabled networking",D:"Shared VPC networking"},correctAnswer:["C"],explanation:{correct:"Enabling Network Policy enforcement (typically with Calico or GKE Dataplane V2) allows you to define firewall rules at the Pod and Namespace level using Kubernetes `NetworkPolicy` resources. This is the feature designed to implement a default-deny stance and then explicitly allow specific traffic flows, meeting the requirement to block external traffic by default.",incorrect:{A:"VPC-native networking is the recommended mode for GKE, but it does not inherently enforce traffic policies between pods. Network Policy must be enabled on top of it.",B:"Routes-based networking is an older mode and is not recommended for new clusters.",D:"Shared VPC is for network administration and sharing a VPC across projects; it does not control pod-to-pod traffic policies."}},conditions:["Configure GKE network connectivity","Allow Pod-to-Pod communication across nodes","Block external traffic to Pods by default","Allow specific external traffic explicitly"],caseStudyId:null},{id:206,topic:"Networking",question:"You need to provide private connectivity between your on-premises network and your Google Cloud VPC network. You require a highly available connection with a guaranteed SLA of 99.99% and need at least 20 Gbps of bandwidth. Which connectivity option should you choose?",options:{A:"HA VPN over the public internet",B:"Two Dedicated Interconnect connections, each 10 Gbps, in different interconnect locations (metros).",C:"Two Partner Interconnect connections, each 10 Gbps, terminating in the same interconnect location.",D:"Four Dedicated Interconnect connections, each 10 Gbps, with two connections in one metro (different edge availability domains) and two connections in a second metro (different edge availability domains)."},correctAnswer:["D"],explanation:{correct:"This describes the most resilient architecture for Dedicated Interconnect, designed to achieve a 99.99% SLA. By having connections in two different metropolitan areas, you protect against a regional disaster. By having two connections in different edge availability domains (EADs) within each metro, you protect against local failures within a single colocation facility. This setup also provides 40 Gbps of total capacity, meeting the bandwidth requirement.",incorrect:{A:"HA VPN cannot guarantee 20 Gbps of throughput.",B:"While this configuration can also achieve a 99.99% SLA, it only provides exactly 20 Gbps of capacity. The four-link option (D) provides more capacity and an even higher degree of redundancy.",C:"This configuration only provides a 99.9% SLA, not 99.99%."}},conditions:["Private connectivity between on-premises and GCP VPC","Highly available connection with a 99.99% SLA","At least 20 Gbps of bandwidth"],caseStudyId:null},{id:207,topic:"Networking",question:"You have multiple VPC networks in your Google Cloud organization. You want to establish private connectivity between all these VPCs without using external IP addresses or VPNs. The VPCs have non-overlapping IP address ranges. What should you configure?",options:{A:"Shared VPC",B:"VPC Network Peering",C:"Cloud Interconnect",D:"Cloud NAT"},correctAnswer:["B"],explanation:{correct:"VPC Network Peering is designed for this exact use case. It allows private communication between VPC networks using internal IP addresses, with traffic staying on Google's private network. The key prerequisite that the VPCs must have non-overlapping IP ranges is met.",incorrect:{A:"Shared VPC is a model for centralized network administration, not for connecting multiple existing, independent VPCs.",C:"Cloud Interconnect is for connecting an on-premises network to a GCP VPC.",D:"Cloud NAT is for providing outbound internet access to instances without external IPs."}},conditions:["Multiple VPC networks","Establish private connectivity between them","Without using external IPs or VPNs","VPCs have non-overlapping IP ranges"],caseStudyId:null},{id:208,topic:"Networking",question:"Cymbal Direct is working with Cymbal Retail, a separate, autonomous division of Cymbal with different staff, networking teams, and data center. Cymbal Direct and Cymbal Retail are not in the same Google Cloud organization. Cymbal Retail needs access to Cymbal Direct's web application for making bulk orders, but the application will not be available on the public internet. You want to ensure that Cymbal Retail has access to your application with low latency. You also want to avoid egress network charges if possible. What should you do?",options:{A:"Verify that the subnet range Cymbal Retail is using doesn't overlap with Cymbal Direct's subnet range, and then enable VPC Network Peering for the project.",B:"If Cymbal Retail does not have access to a Google Cloud data center, use Carrier Peering to connect the two networks.",C:"Specify Cymbal Direct's project as the Shared VPC host project, and then configure Cymbal Retail's project as a service project.",D:"Verify that the subnet Cymbal Retail is using has the same IP address range with Cymbal Direct's subnet range, and then enable VPC Network Peering for the project."},correctAnswer:["A"],explanation:{correct:"VPC Network Peering allows for private, low-latency connectivity between two VPCs, even if they are in different organizations. Traffic between peered networks stays on Google's backbone, which avoids public internet egress charges. The primary prerequisite is that the subnet IP ranges must not overlap.",incorrect:{B:"Carrier Peering is for connecting to Google's public edge network, not for private VPC-to-VPC communication.",C:"Shared VPC is a model for sharing a network within a single organization; it cannot be used across different organizations.",D:"VPC Peering requires that the subnet ranges do *not* overlap. Same or overlapping ranges will cause the peering connection to fail."}},conditions:["Connect two VPCs in different organizations","Provide private access to a web application","Ensure low latency and avoid egress charges"],caseStudyId:"CS001"},{id:209,topic:"Networking",question:"Cymbal Direct's employees will use Google Workspace. Your current on-premises network cannot meet the requirements to connect to Google's public infrastructure. What should you do?",options:{A:"Order a Dedicated Interconnect from a Google Cloud partner, and ensure that proper routes are configured.",B:"Connect the network to a Google point of presence, and enable Direct Peering.",C:"Order a Partner Interconnect from a Google Cloud partner, and ensure that proper routes are configured.",D:"Connect the on-premises network to Google's public infrastructure via a partner that supports Carrier Peering."},correctAnswer:["B"],explanation:{correct:"Google Workspace services are part of Google's public infrastructure. Direct Peering allows you to establish a direct, private connection between your on-premises network and Google's public edge network at a Google Point of Presence (PoP). This provides a more reliable and potentially lower-latency path to services like Google Workspace compared to using the general internet.",incorrect:{A:"Dedicated Interconnect is for private access to your VPC network, not for accessing public Google services like Workspace.",C:"Partner Interconnect is also for private access to your VPC network.",D:"Carrier Peering is a form of peering via a partner network, but Direct Peering is the most direct connection if you can meet at a Google PoP."}},conditions:["Need to connect to Google Workspace (public Google services)","Current on-premises network is insufficient"],caseStudyId:"CS001"},{id:210,topic:"Networking",question:"You need to deploy a load balancer for a web-based application with multiple backends in different regions. You want to direct traffic to the backend closest to the end user, but also to different backends based on the URL the user is accessing. Which of the following could be used to implement this?",options:{A:"The request is received by the global external Application Load Balancer. A global forwarding rule sends the request to a target proxy, which checks the URL map and selects the backend service. The backend service sends the request to Compute Engine instance groups in multiple regions.",B:"The request is matched by a URL map and then sent to a global external Application Load Balancer. A global forwarding rule sends the request to a target proxy, which selects a backend service. The backend service sends the request to Compute Engine instance groups in multiple regions.",C:"The request is received by the proxy Network Load Balancer, which uses a global forwarding rule to check the URL map, then sends the request to a backend service. The request is processed by Compute Engine instance groups in multiple regions.",D:"The request is matched by a URL map and then sent to a proxy Network Load Balancer. A global forwarding rule sends the request to a target proxy, which selects a backend service and sends the request to Compute Engine instance groups in multiple regions."},correctAnswer:["A"],explanation:{correct:"This option correctly describes the traffic flow for a Global External Application Load Balancer (also known as an HTTP(S) Load Balancer). It's a global service that uses a global forwarding rule to direct traffic to a target proxy. The proxy consults a URL map to route the request to the appropriate backend service based on the URL path. The backend service then distributes the request to an instance group in the closest region with available capacity.",incorrect:{B:"The request is received by the load balancer *before* it is matched by the URL map. The URL map is a component of the load balancer.",C:"A Network Load Balancer (Layer 4) does not use URL maps; it operates on IP addresses and ports.",D:"This is incorrect for the same reason as C."}},conditions:["Deploy a load balancer for a web application","Backends are in different regions","Direct traffic to the closest backend","Direct traffic based on the URL"],caseStudyId:null},{id:211,topic:"Networking",question:"Cymbal Direct needs to make sure its new social media integration service can't be accessed directly from the public internet. You want to allow access only through the web frontend store. How can you prevent access to the social media integration service from the outside world, but still allow access to the APIs of social media services?",options:{A:"Remove external IP addresses from the VM instances running the social media service and place them in a private VPC behind Cloud NAT. Any SSH connection for management should be done with Identity-Aware Proxy (IAP) or a bastion host (jump box) after allowing SSH access from IAP or a corporate network.",B:"Limit access to the external IP addresses of the VM instances using firewall rules and place them in a private VPC behind Cloud NAT. Any SSH connection for management should be done with Identity-Aware Proxy (IAP) or a bastion host (jump box) after allowing SSH access from IAP or a corporate network.",C:"Limit access to the external IP addresses of the VM instances using a firewall rule to block all outbound traffic. Any SSH connection for management should be done with Identity-Aware Proxy (IAP) or a bastion host (jump box) after allowing SSH access from IAP or a corporate network.",D:"Remove external IP addresses from the VM instances running the social media service and place them in a private VPC behind Cloud NAT. Any SSH connection for management should be restricted to corporate network IP addresses by Google Cloud Armor."},correctAnswer:["A"],explanation:{correct:"This is the most secure and correct architecture. Removing external IPs makes the service VMs unreachable from the internet. Placing them in a private VPC allows the frontend to communicate with them via internal IPs. Cloud NAT provides the necessary outbound-only internet connectivity for the service to call external social media APIs. IAP or a bastion host provides a secure method for administrative access.",incorrect:{B:"This is less secure because the VMs still have external IPs, which creates an unnecessary attack surface, even if firewalled.",C:"Blocking all outbound traffic would prevent the service from calling the external social media APIs.",D:"Cloud Armor is for protecting L7 applications from web-based attacks, not for restricting SSH access."}},conditions:["Prevent direct public internet access to a backend service","Allow access only from a web frontend","Allow the backend service to make outbound calls to external APIs","Provide secure administrative access"],caseStudyId:"CS001"},{id:212,topic:"Networking",question:"Cymbal Direct uses a proprietary service to manage on-call rotation and alerting. The on-call rotation service has an API for integration. Cymbal Direct wants to monitor its environment for service availability and ensure that the correct person is notified. What should you do?",options:{A:"Ensure that VPC firewall rules allow access from the IP addresses used by Google Cloud's uptime-check servers. Create a Pub/Sub topic for alerting as a monitoring notification channel in Google Cloud Observability. Create an uptime check for the appropriate resource's internal IP address, with an alerting policy set to use the Pub/Sub topic. Create a Cloud Run function that subscribes to the Pub/Sub topic to send the alert to the on-call API.",B:"Ensure that VPC firewall rules allow access from the IP addresses used by Google Cloud's uptime-check servers. Create a Pub/Sub topic for alerting as a monitoring notification channel in Google Cloud Observability. Create an uptime check for the appropriate resource's external IP address, with an alerting policy set to use the Pub/Sub topic. Create a Cloud Run function that subscribes to the Pub/Sub topic to send the alert to the on-call API.",C:"Ensure that VPC firewall rules allow access from the on-call API. Create a Cloud Run function to send the alert to the on-call API. Add Cloud Run functions as a monitoring notification channel in Google Cloud Observability. Create an uptime check for the appropriate resource's external IP address, with an alerting policy set to use the Cloud Run function.",D:"Ensure that VPC firewall rules allow access from the IP addresses used by Google Cloud's uptime-check servers. Add the URL for the on-call rotation API as a monitoring notification channel in Google Cloud Observability. Create an uptime check for the appropriate resource's internal IP address, with an alerting policy set to use the API."},correctAnswer:["B"],explanation:{correct:"This describes a robust, event-driven, and decoupled alerting pipeline. Uptime checks monitor the external endpoint. An alerting policy triggers a notification to a Pub/Sub topic. A serverless function (Cloud Run) is triggered by the Pub/Sub message, providing a flexible integration point to call the custom on-call API. This is the recommended pattern for integrating with third-party notification systems.",incorrect:{A:"Uptime checks monitor external service availability, so they should target an external IP address, not an internal one.",C:"While you could potentially trigger a function directly, using Pub/Sub as an intermediary provides better decoupling, durability, and retry capabilities for the alert notifications.",D:"Using a generic webhook notification channel might work, but it offers less flexibility for custom payloads, authentication, and logic than a Cloud Run function. Also, it incorrectly targets an internal IP for the uptime check."}},conditions:["Integrate Google Cloud monitoring with a proprietary on-call service via an API","Monitor service availability","Notify the correct on-call person"],caseStudyId:"CS001"},{id:213,topic:"Operations",question:"As part of Dress4Win's plans to migrate to the cloud, they want to be able to set up a managed logging and monitoring system so they can handle spikes in their traffic load. They want to ensure that: The infrastructure can be notified when it needs to scale up and down to handle the ebb and flow of usage throughout the day. Their administrators are notified automatically when their application reports errors. They can filter their aggregated logs down in order to debug one piece of the application across many hosts. Which Google Cloud Operations suite features should they use?",options:{A:"Logging, Alerts, Insights, Debug",B:"Monitoring, Trace, Debug, Logging",C:"Monitoring, Logging, Alerts, Error Reporting",D:"Monitoring, Logging, Debug, Error Reporting"},correctAnswer:["C"],explanation:{correct:"This option correctly maps each requirement to a specific feature of the Cloud Operations suite. Cloud Monitoring provides the metrics for scaling notifications and the Alerting feature. Cloud Logging provides centralized, filterable logs. Error Reporting automatically aggregates application errors for notification.",incorrect:{A:"'Insights' is a general term, not a specific feature name in this context. Debugger is not required for these specific needs.",B:"Trace and Debugger are for performance tracing and live debugging, respectively, which were not explicitly requested.",D:"Debugger is for live debugging, not the requirements listed."}},conditions:["Set up a managed logging and monitoring system","Need notifications for infrastructure scaling","Need automatic notifications for application errors","Need to filter aggregated logs for debugging"],caseStudyId:"cs_dress4win_v1"},{id:214,topic:"Operations",question:"Your development teams release new versions of games running on Google Kubernetes Engine (GKE) daily. You want to create service level indicators (SLIs) to evaluate the quality of the new versions from the user's perspective. What should you do?",options:{A:"Create CPU Utilization and Request Latency as service level indicators.",B:"Create GKE CPU Utilization and Memory Utilization as service level indicators.",C:"Create Request Latency and Error Rate as service level indicators.",D:"Create Server Uptime and Error Rate as service level indicators."},correctAnswer:["C"],explanation:{correct:"SLIs should measure what the user experiences. Request Latency (how fast the service is) and Error Rate (how reliable the service is) are two of the most fundamental user-facing SLIs. They directly reflect the quality of the service from the user's perspective.",incorrect:{A:"CPU Utilization is a system-level metric, not a direct measure of user experience. While high CPU can cause high latency, latency itself is the user-facing SLI.",B:"CPU and Memory Utilization are resource metrics, important for capacity planning but not for measuring user-perceived quality.",D:"Server Uptime is a very coarse metric. A server can be 'up' but still be serving errors or be unacceptably slow. Request-level metrics like latency and error rate are much better SLIs."}},conditions:["Games running on GKE with daily releases","Need to create Service Level Indicators (SLIs)","SLIs should evaluate quality from the user's perspective"],caseStudyId:"cs_mountkirk_main"},{id:215,topic:"Operations",question:"You are managing several internal applications that are deployed on Compute Engine. Business users inform you that an application has become very slow over the past few days. You want to find the underlying cause in order to solve the problem. What should you do first?",options:{A:"Inspect the logs and metrics from the instances in Cloud Logging and Cloud Monitoring.",B:"Change the Compute Engine Instances behind the application to a machine type with more CPU and memory.",C:"Restore a backup of the application database from a time before the application became slow.",D:"Deploy the applications on a managed instance group with autoscaling enabled. Add a load balancer in front of the managed instance group, and have the users connect to the IP of the load balancer."},correctAnswer:["A"],explanation:{correct:"The first and most critical step in troubleshooting any performance issue is to gather data. Inspecting logs (for errors or unusual activity) and metrics (CPU, memory, disk I/O, network) in Cloud Logging and Monitoring will provide visibility into what the system is doing and help identify the bottleneck or root cause. Only after a proper diagnosis should you take corrective action.",incorrect:{B:"This is a potential solution, but making this change without diagnosis is just guessing. The problem might not be CPU or memory.",C:"This is a drastic action that could cause data loss and should only be considered if database corruption is identified as the cause.",D:"Re-architecting the deployment is a significant change that should only be done after understanding the root cause of the current performance issue."}},conditions:["Internal application on Compute Engine has become slow","Need to find the underlying cause"],caseStudyId:null},{id:216,topic:"Operations",question:"Your customer wants to capture multiple GBs of aggregate real-time key performance indicators (KPIs) from their game servers running on Google Cloud Platform and monitor the KPIs with low latency. How should they capture the KPIs?",options:{A:"Store time-series data from the game servers in Google Bigtable, and view it using Google Data Studio.",B:"Output custom metrics to Stackdriver from the game servers, and create a Dashboard in Stackdriver Monitoring Console to view them.",C:"Schedule BigQuery load jobs to ingest analytics files uploaded to Cloud Storage every ten minutes, and visualize the results in Google Data Studio.",D:"Insert the KPIs into Cloud Datastore entities, and run ad hoc analysis and visualizations of them in Cloud Datalab."},correctAnswer:["B"],explanation:{correct:"Cloud Monitoring (formerly Stackdriver) is the purpose-built service for ingesting, storing, and visualizing time-series metrics with low latency. Applications can send custom metrics directly to the Monitoring API, which can then be displayed on real-time dashboards and used for alerting.",incorrect:{A:"While Bigtable can store time-series data, it is not a monitoring and dashboarding tool. Using Data Studio on top would add latency and is not designed for real-time operational monitoring.",C:"This is a batch analytics approach with a latency of at least ten minutes, which does not meet the real-time, low-latency requirement.",D:"Cloud Datastore is a document database, not a time-series monitoring system. Cloud Datalab is for data exploration, not real-time dashboards."}},conditions:["Capture multiple GBs of aggregate real-time KPIs from game servers","Monitor the KPIs with low latency"],caseStudyId:null},{id:217,topic:"Operations",question:"You are running a cluster on Kubernetes Engine (GKE) to serve a web application. Users are reporting that a specific part of the application is not responding anymore. You notice that all pods of your deployment keep restarting after 2 seconds. The application writes logs to standard output. You want to inspect the logs to find the cause of the issue. Which approach can you take?",options:{A:"Review the Stackdriver logs for each Compute Engine instance that is serving as a node in the cluster.",B:"Review the Stackdriver logs for the specific GKE container that is serving the unresponsive part of the application.",C:"Connect to the cluster using gcloud credentials and connect to a container in one of the pods to read the logs.",D:"Review the Serial Port logs for each Compute Engine instance that is serving as a node in the cluster."},correctAnswer:["B"],explanation:{correct:"When pods are in a crash-loop (restarting constantly), trying to view logs with `kubectl logs` is difficult because the pod is terminated quickly. Cloud Logging (formerly Stackdriver) captures logs from all pods, including those that have already terminated. By filtering in Cloud Logging for the specific GKE container, you can view the historical logs from all previous failed instances of the pod and find the error that is causing the crash at startup.",incorrect:{A:"Node-level logs are less likely to contain the specific application error than the container logs.",C:"This is difficult to do with a crash-looping pod. By the time you connect, the pod might have already been terminated and replaced.",D:"Serial port logs are for low-level OS and boot issues on the node, not for application errors inside a container."}},conditions:["Application running on GKE","Pods are in a crash-loop (restarting every 2 seconds)","Application logs to standard output","Need to inspect logs to find the cause of the crash"],caseStudyId:null},{id:218,topic:"Operations",question:"Your architecture calls for the centralized collection of all admin activity and VM system logs within your project. How should you collect these logs from both VMs and services?",options:{A:"All admin and VM system logs are automatically collected by Stackdriver.",B:"Stackdriver automatically collects admin activity logs for most services. The Stackdriver Logging agent must be installed on each instance to collect system logs.",C:"Launch a custom syslogd compute instance and configure your GCP project and VMs to forward all logs to it.",D:"Install the Stackdriver Logging agent on a single compute instance and let it collect all audit and access logs for your environment."},correctAnswer:["B"],explanation:{correct:"This correctly describes how Cloud Logging works. Admin Activity audit logs (and other audit logs) are generated by GCP services and collected automatically. However, to collect logs generated *within* a VM (e.g., syslog, application logs), you must install the Cloud Logging agent on that VM.",incorrect:{A:"This is incorrect because VM system logs are not collected automatically; the agent is required.",C:"This describes a custom, self-managed logging setup, which is not the recommended GCP-native approach.",D:"The logging agent runs on each VM to collect logs from that specific VM; it cannot collect logs from other services or instances remotely."}},conditions:["Centralized collection of all admin activity logs","Centralized collection of all VM system logs"],caseStudyId:null},{id:219,topic:"Operations",question:"You have an application that runs in Google Kubernetes Engine (GKE). Over the last 2 weeks, customers have reported that a specific part of the application returns errors very frequently. You currently have no logging or monitoring solution enabled on your GKE cluster. You want to diagnose the problem, but you have not been able to replicate the issue. You want to cause minimal disruption to the application. What should you do?",options:{A:"1. Update your GKE cluster to use Cloud Operations for GKE. 2. Use the GKE Monitoring dashboard to investigate logs from affected Pods",B:"1. Create a new GKE cluster with Cloud Operations for GKE enabled. 2. Migrate the affected Pods to the new cluster, and redirect traffic for those Pods to the new cluster. 3. Use the GKE Monitoring dashboard to investigate logs from affected Pods.",C:"1. Update your GKE cluster to use Cloud Operations for GKE, and deploy Prometheus. 2. Set an alert to trigger whenever the application returns an error.",D:"1. Create a new GKE cluster with Cloud Operations for GKE enabled, and deploy Prometheus. 2. Migrate the affected Pods to the new cluster, and redirect traffic for those Pods to the new cluster. 3. Set an alert to trigger whenever the application returns an error."},correctAnswer:["A"],explanation:{correct:"The least disruptive way to add observability is to enable the Cloud Operations for GKE integration on the existing cluster. This can typically be done via a cluster update. Once enabled, it will start collecting logs and metrics from the running workloads, allowing you to investigate the intermittent errors without needing to migrate the application or disrupt service.",incorrect:{B:"Creating a new cluster and migrating the application is a highly disruptive process.",C:"While deploying Prometheus is a valid monitoring strategy, the first step is to get basic logging and metrics, which the native Cloud Operations integration provides easily. Setting an alert doesn't help diagnose the past issue.",D:"This is also highly disruptive."}},conditions:["Application on GKE is experiencing frequent, intermittent errors","No logging or monitoring is currently enabled","Need to diagnose the problem with minimal disruption"],caseStudyId:null},{id:220,topic:"Operations",question:"Your operations team has asked you to help diagnose a performance issue in a production application that runs on Compute Engine. The application is dropping requests that reach it when under heavy load. The process list for affected instances shows a single application process that is consuming all available CPU, and autoscaling has reached the upper limit of instances. There is no abnormal load on any other related systems, including the database. You want to allow production traffic to be served again as quickly as possible. Which action should you recommend?",options:{A:"Change the autoscaling metric to agent.googleapis.com/memory/percent_used.",B:"Restart the affected instances on a staggered schedule.",C:"SSH to each instance and restart the application process.",D:"Increase the maximum number of instances in the autoscaling group."},correctAnswer:["D"],explanation:{correct:"The symptoms clearly indicate that the application is CPU-bound and has exhausted its configured scaling capacity. The autoscaler is trying to add more instances but is prevented by the maximum limit. The quickest way to restore service is to raise this limit, allowing the autoscaler to provision more instances and distribute the high CPU load across a larger pool of VMs.",incorrect:{A:"The current problem is CPU load, and the autoscaler has already hit its maximum limit. Changing the metric won't allow it to scale further.",B:"Restarting instances will not solve a sustained high-load issue if the root cause is insufficient capacity.",C:"Restarting the application process will have the same result as restarting the instance; it won't address the capacity shortfall."}},conditions:["Production application on Compute Engine is dropping requests under heavy load","Application process is consuming all available CPU","Autoscaling has reached the maximum number of instances","Need the quickest way to restore service"],caseStudyId:null},{id:221,topic:"Operations",question:"Your company sends all Google Cloud logs to Cloud Logging. Your security team wants to monitor the logs. You want to ensure that the security team can react quickly if an anomaly such as an unwanted firewall change or server breach is detected. You want to follow Google-recommended practices. What should you do?",options:{A:"Schedule a cron job with Cloud Scheduler. The scheduled job queries the logs every minute for the relevant events.",B:"Export logs to BigQuery, and trigger a query in BigQuery to process the log data for the relevant events.",C:"Export logs to a Pub/Sub topic, and trigger Cloud Function with the relevant log events.",D:"Export logs to a Cloud Storage bucket, and trigger Cloud Run with the relevant log events."},correctAnswer:["C"],explanation:{correct:"This describes a real-time, event-driven alerting architecture. A log sink in Cloud Logging can filter for specific security events and export them to a Pub/Sub topic. A Cloud Function can then be triggered by messages on this topic to perform immediate actions, such as sending a notification to the security team. This provides the quickest reaction time.",incorrect:{A:"Periodic querying with a cron job introduces latency (up to a minute in this case) and is less efficient than an event-driven approach.",B:"Exporting to BigQuery is for batch analysis and has higher latency, making it unsuitable for quick reaction.",D:"Exporting to Cloud Storage is for archival. While it can trigger a Cloud Run service, Pub/Sub is the more direct and appropriate choice for streaming log events for real-time processing."}},conditions:["All GCP logs are in Cloud Logging","Security team needs to monitor for anomalies (e.g., firewall changes)","Need to ensure a quick reaction to detected anomalies","Follow Google-recommended practices"],caseStudyId:null},{id:222,topic:"Operations",question:"You have deployed an application on Anthos clusters (formerly Anthos GKE). According to the SRE practices at your company, you need to be alerted if request latency is above a certain threshold for a specified amount of time. What should you do?",options:{A:"Install Anthos Service Mesh on your cluster. Use the Google Cloud Console to define a Service Level Objective (SLO), and create an alerting policy based on this SLO.",B:"Enable the Cloud Trace API on your project, and use Cloud Monitoring Alerts to send an alert based on the Cloud Trace metrics.",C:"Use Cloud Profiler to follow up the request latency. Create a custom metric in Cloud Monitoring based on the results of Cloud Profiler, and create an Alerting policy in case this metric exceeds the threshold.",D:"Configure Anthos Config Management on your cluster, and create a yaml file that defines the SLO and alerting policy you want to deploy in your cluster."},correctAnswer:["A"],explanation:{correct:"This is the standard SRE approach on GCP. Anthos Service Mesh provides the necessary latency metrics (SLIs). Cloud Monitoring is the tool used to define an SLO based on these SLIs (e.g., 99% of requests should be faster than 200ms). You can then create an alerting policy that triggers if the service is not meeting its SLO (i.e., its error budget is burning too fast).",incorrect:{B:"Cloud Trace is for analyzing the latency of individual requests (distributed tracing), not for defining and alerting on aggregate SLOs.",C:"Cloud Profiler is for analyzing CPU and memory usage of your code, not request latency.",D:"Anthos Config Management is for managing cluster configurations and policies as code, not for defining and monitoring runtime SLOs."}},conditions:["Application is on Anthos clusters","Follow SRE practices","Need to be alerted if request latency is above a threshold for a specified time"],caseStudyId:null},{id:223,topic:"Operations",question:"A recent audit that a new network was created in Your GCP project. In this network, a GCE instance has an SSH port open the world. You want to discover this network's origin. What should you do?",options:{A:"Search for Create VM entry in the Stackdriver alerting console.",B:"Navigate to the Activity page in the Home sectio Set category to Data Access and search for Create VM entry.",C:"In the logging section of the console, specify GCE Network as the logging sectio Search for the Create Insert entry.",D:"Connect to the GCE instance using project SSH Key Identify previous logins in system logs, and match these with the project owners list."},correctAnswer:["C"],explanation:{correct:"To find out who created a GCP resource, you need to look at the Admin Activity audit logs, which are available in Cloud Logging. You can filter the logs by the resource type (GCE Network) and the API method for creation (`networks.insert`). The log entry will show the identity (user or service account) that performed the action.",incorrect:{A:"The alerting console is for alerts, not for auditing past activities.",B:"Data Access logs track reads and writes to data, not the creation of infrastructure resources like networks.",D:"System logs on the instance will show who has logged in to the instance, but not who created the instance or the network it resides in."}},conditions:["A new network and a GCE instance with an open SSH port were discovered","Need to discover the origin of the new network (who created it)"],caseStudyId:null},{id:224,topic:"Operations",question:"You want to allow your operations team to store logs from all the production projects in your Organization, without including logs from other projects. All of the production projects are contained in a folder. You want to ensure that all logs for existing and new production projects are captured automatically. What should you do?",options:{A:"Create an aggregated export on the Production folder. Set the log sink to be a Cloud Storage bucket in an operations project.",B:"Create an aggregated export on the Organization resource. Set the log sink to be a Cloud Storage bucket in an operations project.",C:"Create log exports in the production projects. Set the log sinks to be a Cloud Storage bucket in an operations project.",D:"Create log exports in the production projects. Set the log sinks to be BigQuery datasets in the production projects, and grant IAM access to the operations team to run queries on the datasets."},correctAnswer:["A"],explanation:{correct:"Cloud Logging's aggregated sinks are designed for this. By creating a sink at the 'Production' folder level and enabling the `includeChildren` option, you ensure that logs from all current and future projects within that folder are automatically exported to a single, centralized destination (like a GCS bucket in an operations project).",incorrect:{B:"Creating the sink at the Organization level would include logs from non-production projects, violating the requirement.",C:"Creating sinks in each individual project is not scalable and does not automatically capture logs from new projects.",D:"This is also not scalable and stores the logs in the production projects, whereas a centralized location is usually preferred for an operations team."}},conditions:["Store logs from all production projects only","All production projects are contained in a single folder","Automatically capture logs from new production projects","Store logs in a centralized location for the operations team"],caseStudyId:null},{id:225,topic:"Operations",question:"Your company has an application running on multiple instances of Compute Engine. It generates 1 TB per day of logs. For compliance reasons, the logs need to be kept for at least two years. The logs need to be available for active query for 30 days. After that, they just need to be retained for audit purposes. You want to implement a storage solution that is compliant, minimizes costs, and follows Google-recommended practices. What should you do?",options:{A:"1. Install a Cloud Logging agent on all instances. 2. Create a sink to export logs into a regional Cloud Storage bucket. 3. Create an Object Lifecycle rule to move files into a Coldline Cloud Storage bucket after one month. 4. Configure a retention policy at the bucket level using bucket lock.",B:"1. Write a daily cron job, running on all instances, that uploads logs into a Cloud Storage bucket. 2. Create a sink to export logs into a regional Cloud Storage bucket. 3. Create an Object Lifecycle rule to move files into a Coldline Cloud Storage bucket after one month.",C:"1. Install a Cloud Logging agent on all instances. 2. Create a sink to export logs into a partitioned BigQuery table. 3. Set a time_partitioning_expiration of 30 days.",D:"1. Create a daily cron job, running on all instances, that uploads logs into a partitioned BigQuery table. 2. Set a time_partitioning_expiration of 30 days."},correctAnswer:["A"],explanation:{correct:"This solution addresses all requirements. The Cloud Logging agent reliably collects logs. The default 30-day retention in Cloud Logging meets the active query requirement. The sink to GCS provides long-term storage. The lifecycle rule to Coldline optimizes cost for archival. The retention policy with bucket lock ensures the 2-year compliance requirement is met and data is not prematurely deleted.",incorrect:{B:"A cron job for log collection is less reliable than the Cloud Logging agent.",C:"Setting a partition expiration of 30 days would delete the logs, violating the 2-year retention requirement.",D:"This is incorrect for the same reasons as B and C."}},conditions:["Collect 1 TB/day of logs from Compute Engine instances","Keep logs for at least two years for compliance","Logs must be available for active query for 30 days","Minimize costs and follow Google-recommended practices"],caseStudyId:null},{id:226,topic:"Operations",question:"Your company has an application running on multiple instances of Compute Engine. It generates 1 TB per day of logs. For compliance reasons, the logs need to be kept for at least two years. The logs need to be available for active query for 30 days. After that, they just need to be retained for audit purposes. You want to implement a storage solution that is compliant, minimizes costs, and follows Google-recommended practices. What should you do?",options:{A:"1. Install a Cloud Logging agent on all instances. 2. Create a sink to export logs into a regional Cloud Storage bucket. 3. Create an Object Lifecycle rule to move files into a Coldline Cloud Storage bucket after one month. 4. Configure a retention policy at the bucket level using bucket lock.",B:"1. Write a daily cron job, running on all instances, that uploads logs into a Cloud Storage bucket. 2. Create a sink to export logs into a regional Cloud Storage bucket. 3. Create an Object Lifecycle rule to move files into a Coldline Cloud Storage bucket after one month.",C:"1. Install a Cloud Logging agent on all instances. 2. Create a sink to export logs into a partitioned BigQuery table. 3. Set a time_partitioning_expiration of 30 days.",D:"1. Create a daily cron job, running on all instances, that uploads logs into a partitioned BigQuery table. 2. Set a time_partitioning_expiration of 30 days."},correctAnswer:["A"],explanation:{correct:"This solution addresses all requirements. The Cloud Logging agent reliably collects logs. The default 30-day retention in Cloud Logging meets the active query requirement. The sink to GCS provides long-term storage. The lifecycle rule to Coldline optimizes cost for archival. The retention policy with bucket lock ensures the 2-year compliance requirement is met and data is not prematurely deleted.",incorrect:{B:"A cron job for log collection is less reliable than the Cloud Logging agent.",C:"Setting a partition expiration of 30 days would delete the logs, violating the 2-year retention requirement.",D:"This is incorrect for the same reasons as B and C."}},conditions:["Collect 1 TB/day of logs from GCE instances","Retain >= 2 years (compliance)","Active query for 30 days","Retain for audit after 30 days","Compliant, cost-minimized, recommended solution"],caseStudyId:null},{id:227,topic:"Operations",question:"You are managing several internal applications that are deployed on Compute Engine. Business users inform you that an application has become very slow over the past few days. You want to find the underlying cause in order to solve the problem. What should you do first?",options:{A:"Inspect the logs and metrics from the instances in Cloud Logging and Cloud Monitoring.",B:"Change the Compute Engine Instances behind the application to a machine type with more CPU and memory.",C:"Restore a backup of the application database from a time before the application became slow.",D:"Deploy the applications on a managed instance group with autoscaling enabled. Add a load balancer in front of the managed instance group, and have the users connect to the IP of the load balancer."},correctAnswer:["A"],explanation:{correct:"The first and most critical step in troubleshooting any performance issue is to gather data. Inspecting logs (for errors or unusual activity) and metrics (CPU, memory, disk I/O, network) in Cloud Logging and Monitoring will provide visibility into what the system is doing and help identify the bottleneck or root cause. Only after a proper diagnosis should you take corrective action.",incorrect:{B:"This is a potential solution, but making this change without diagnosis is just guessing. The problem might not be CPU or memory.",C:"This is a drastic action that could cause data loss and should only be considered if database corruption is identified as the cause.",D:"Re-architecting the deployment is a significant change that should only be done after understanding the root cause of the current performance issue."}},conditions:["Internal GCE app became slow","Find the cause"],caseStudyId:null},{id:228,topic:"Operations",question:"JencoMart has built a version of their application on Google Cloud that serves traffic to Asia. You want to measure success against the business and technical goals. Which metrics should you track?",options:{A:"Error rates for requests from Asia",B:"Latency difference between US and Asia",C:"Total visits, error rates, and latency from Asia",D:"Total visits and average latency for users from Asia",E:"The number of character sets present in the database"},correctAnswer:["D"],explanation:{correct:"The case study's business goal is to 'Expand services into Asia,' which is directly measured by `Total visits`. The technical goal is to 'Decrease latency in Asia,' which is directly measured by `average latency`. This option tracks the two primary key performance indicators (KPIs) for the stated goals.",incorrect:{A:"Error rates are an important operational metric but don't directly measure the success of market expansion.",B:"Latency difference is an analytical point but not the core success metric itself. The goal is to ensure low latency in Asia, not just compare it to the US.",C:"This is a good set of metrics, but D is more focused on the two specific goals mentioned in the case study.",E:"This is irrelevant to the business or technical goals of the expansion."}},conditions:["Measure success of a new deployment in Asia","Business Goal: Expand services into Asia","Technical Goal: Decrease latency in Asia"],caseStudyId:"cs_jencostore"},{id:229,topic:"Operations",question:"TerramEarth has a legacy web application that you cannot migrate to cloud. However, you still want to build a cloud-native way to monitor the application. If the application goes down, you want the URL to point to a 'Site is unavailable' page as soon as possible. You also want your Ops team to receive a notification for the issue. You need to build a reliable solution for minimum cost. What should you do?",options:{A:"Create a scheduled job in Cloud Run to invoke a container every minute. The container will check the application URL. If the application is down, switch the URL to the 'Site is unavailable page, and notify the Ops team.",B:"Create a cron job on a Compute Engine VM that runs every minute. The cron job invokes a Python program to check the application URL. If the application is down, switch the URL to the 'Site is unavailable page, and notify the Ops team.",C:"Create a Cloud Monitoring uptime check to validate the application URL. If it fails, put a message in a Pub/Sub queue that triggers a Cloud Function to switch the URL to the 'Site is unavailable page, and notify the Ops team.",D:"Use Cloud Error Reporting to check the application URL. If the application is down, switch the URL to the 'Site is unavailable page, and notify the Ops team."},correctAnswer:["C"],explanation:{correct:"This is a robust, serverless, and event-driven architecture. Cloud Monitoring Uptime Checks are the native service for external endpoint monitoring. Tying an alert to a Pub/Sub notification provides a reliable and decoupled trigger. A Cloud Function is a cost-effective, serverless way to execute the automated failover logic (like updating a DNS record) and send notifications.",incorrect:{A:"This is a custom polling solution. Using the native Cloud Monitoring Uptime Check is more reliable and integrated.",B:"This is also a custom polling solution that requires a continuously running VM, which is not cost-effective.",D:"Cloud Error Reporting is for tracking application code errors, not for monitoring uptime of an external URL."}},conditions:["Monitor a legacy, non-migratable web application","On failure, redirect URL to a static page and notify the Ops team","Build a reliable and cost-effective solution"],caseStudyId:"cs_terram_main"},{id:230,topic:"Operations",question:"Dress4Win has configured a new uptime check with Google Stackdriver for several of their legacy services. The Stackdriver dashboard is not reporting the services as healthy. What should they do?",options:{A:"Install the Stackdriver agent on all of the legacy web servers.",B:"In the Cloud Platform Console download the list of the uptime servers' IP addresses and create an inbound firewall rule",C:"Configure their load balancer to pass through the User-Agent HTTP header when the value matches GoogleStackdriverMonitoring-UptimeChecks (https:// cloud.google.com/monitoring)",D:"Configure their legacy web servers to allow requests that contain user-Agent HTTP header when the value matches GoogleStackdriverMonitoring-UptimeChecks (https://cloud.google.com/monitoring)"},correctAnswer:["B"],explanation:{correct:"Cloud Monitoring Uptime Checks probe endpoints from Google's global infrastructure. A common reason for these checks to fail when monitoring on-premises or other non-GCP services is that a firewall is blocking the incoming requests from Google's uptime check servers. The correct procedure is to get the official list of uptime checker IP addresses and add a firewall rule to allow traffic from these source IPs.",incorrect:{A:"The Cloud Monitoring agent is for collecting metrics *from* a server; it does not enable inbound traffic *to* the server for uptime checks.",C:"While User-Agent filtering can be a secondary issue, the most common problem is network-level blocking, which must be solved first.",D:"This is also a secondary, application-level concern. The primary suspect is the network firewall."}},conditions:["Configured a Cloud Monitoring (Stackdriver) Uptime Check for legacy services","The services are reported as unhealthy","Need to troubleshoot the issue"],caseStudyId:"cs_dress4win_v1"},{id:231,topic:"Operations",question:"To be legally compliant during an audit, Dress4Win must be able to give insights in all administrative actions that modify the configuration or metadata of resources on Google Cloud. What should you do?",options:{A:"Use Stackdriver Trace to create a Trace list analysis.",B:"Use Stackdriver Monitoring to create a dashboard on the project's activity.",C:"Enable Cloud Identity-Aware Proxy in all projects, and add the group of Administrators as a member.",D:"Use the Activity page in the GCP Console and Stackdriver Logging to provide the required insight."},correctAnswer:["D"],explanation:{correct:"Admin Activity audit logs are automatically collected by Google Cloud and record all administrative changes. These logs are surfaced in both the GCP Console's Activity page (for a quick, high-level view) and in Cloud Logging (formerly Stackdriver Logging) for detailed querying, analysis, and long-term retention. These are the primary sources for auditing administrative actions.",incorrect:{A:"Cloud Trace is for application performance and latency tracing, not for auditing administrative actions.",B:"Cloud Monitoring is for time-series metrics, not for detailed audit logs of who did what.",C:"Cloud IAP is an access control service for applications; it does not log administrative changes to GCP resources."}},conditions:["Legal compliance/audit requirement","Provide insight into all admin actions that modify GCP resource configuration or metadata"],caseStudyId:"cs_dress4win_v1"},{id:232,topic:"Operations",question:"You want to establish procedures for testing the resilience of the delivery-by-drone solution. How would you simulate a scalability issue?",options:{A:"Block access to storage assets in one of your zones.",B:"Inject a bad health check for one or more of your resources.",C:"Load test your application to see how it responds.",D:"Block access to all resources in a zone."},correctAnswer:["C"],explanation:{correct:"Scalability is about how a system handles increasing load. Therefore, the most direct way to test resilience to a scalability issue is to perform a load test. By pushing the application to its limits, you can observe how it responds, whether autoscaling triggers correctly, where the bottlenecks are, and at what point it starts to fail. This directly simulates a scalability challenge.",incorrect:{A:"This simulates a specific storage failure, not a scalability issue.",B:"This simulates an instance health issue, which tests autohealing, but not necessarily how the system performs under high load.",D:"This simulates a zonal failure, which tests for high availability, not scalability."}},conditions:["Establish procedures for resilience testing","Need to simulate a scalability issue"],caseStudyId:"CS001"},{id:233,topic:"Operations",question:"Your environment has multiple projects used for development and testing. Each project has a budget, and each developer has a budget. A personal budget overrun can cause a project budget overrun. Several developers are creating resources for testing as part of their CI/CD pipeline but are not deleting these resources after their tests are complete. If the compute resource fails during testing, the test can be run again. You want to reduce costs and notify the developer when a personal budget overrun causes a project budget overrun. What should you do?",options:{A:"Configure billing export to BigQuery. Create a Google Cloud budget for each project. Create a group for the developers in each project, and add them to the appropriate group. Create a notification channel for each group. Configure a billing alert to notify the group when their budget is exceeded. Modify the build scripts/pipeline to label all resources with the label 'creator' set to the developer's email address. Use spot (preemptible) instances wherever possible.",B:"Configure billing export to BigQuery. Create a Google Cloud budget for each project. Configure a billing alert to notify billing admins and users when their budget is exceeded. Modify the build scripts/pipeline to label all resources with the label 'creator' set to the developer's email address. Use spot (preemptible) instances wherever possible.",C:"Configure billing export to BigQuery. Create a Google Cloud budget for each project. Create a Pub/Sub topic for developer-budget-notifications. Create a Cloud Run function to notify the developer based on the labels. Modify the build scripts/pipeline to label all resources with the label 'creator' set to the developer's email address. Use spot (preemptible) instances wherever possible.",D:"Configure billing export to BigQuery. Create a Google Cloud budget for each project. Create a Pub/Sub topic for developer-budget-notifications. Create a Cloud Run function to notify the developer based on the labels. Modify the build scripts/pipeline to label all resources with the label 'creator' set to the developer's email address. Use spot (preemptible) instances wherever possible. Use Cloud Scheduler to delete resources older than 24 hours in each project."},correctAnswer:["D"],explanation:{correct:"This is the most comprehensive solution. It uses labeling to track costs per developer via BigQuery. It uses a programmatic notification system (Pub/Sub + Cloud Run) to alert the specific developer responsible. It uses Spot VMs to reduce the cost of the test resources. Crucially, it also includes an automated cleanup mechanism (Cloud Scheduler) to address the root problem of developers not deleting resources, which provides a strong preventative measure against cost overruns.",incorrect:{A:"This notifies the entire group, not the specific developer responsible for the cost overrun.",B:"This notifies all users and admins, which can be noisy and doesn't target the specific developer.",C:"This is a very good solution for tracking and notification but lacks the automated cleanup step, which is a key part of solving the stated problem."}},conditions:["Developers creating test resources via CI/CD and not deleting them","Need to reduce costs","Need to notify the specific developer responsible for a budget overrun"],caseStudyId:null},{id:234,topic:"Operations",question:"Cymbal Direct's warehouse and inventory system was written in Java. The system uses a microservices architecture in GKE and is instrumented with Zipkin. Seemingly at random, a request will be 5-10 times slower than others. The development team tried to reproduce the problem in testing, but failed to determine the cause of the issue. What should you do?",options:{A:"Create metrics in Cloud Monitoring for your microservices to test whether they are intermittently unavailable or slow to respond to HTTPS requests. Use Cloud Profiler to determine which functions/methods in your application's code use the most system resources. Use Cloud Trace to identify slow requests and determine which microservices/calls take the most time to respond.",B:"Create metrics in Cloud Monitoring for your microservices to test whether they are intermittently unavailable or slow to respond to HTTPS requests. Use Cloud Trace to determine which functions/methods in your application's code use the most system resources. Use Cloud Profiler to identify slow requests and determine which microservices/calls take the most time to respond.",C:"Use Error Reporting to test whether your microservices are intermittently unavailable or slow to respond to HTTPS requests. Use Cloud Profiler to determine which functions/methods in your application's code use the most system resources. Use Cloud Trace to identify slow requests and determine which microservices/calls take the most time to respond.",D:"Use Error Reporting to test whether your microservices are intermittently unavailable or slow to respond to HTTPS requests. Use Cloud Trace to determine which functions/methods in your application's code Use the most system resources. Use Cloud Profiler to identify slow requests and determine which microservices/calls take the most time to respond."},correctAnswer:["A"],explanation:{correct:"This option correctly assigns the right tool to the right job for diagnosing intermittent latency. Cloud Trace (which can ingest Zipkin data) is the primary tool for distributed tracing to see the end-to-end latency of a request and identify which specific microservice call is slow. Cloud Profiler is then used to dig into the code of that slow microservice to find the specific function or method causing high CPU or resource consumption. Cloud Monitoring provides the high-level metrics and alerts to detect the issue in the first place.",incorrect:{B:"This option incorrectly swaps the roles of Cloud Trace and Cloud Profiler.",C:"Error Reporting is for tracking exceptions and crashes, not for diagnosing latency issues.",D:"This is incorrect for the same reasons as B and C."}},conditions:["Java microservices application on GKE","Instrumented with Zipkin (distributed tracing)","Experiencing intermittent slow requests","Unable to reproduce in testing"],caseStudyId:"CS001"},{id:235,topic:"Operations",question:"Cymbal Direct has a new social media integration service that pulls images of its products from social media sites and displays them in a gallery of customer images on your online store. You receive an alert from Cloud Monitoring at 3:34 AM on Saturday. The store is still online, but the gallery does not appear. The CPU utilization is 30% higher than expected on the VMs running the service, which causes the managed instance group (MIG) to scale to the maximum number of instances. You verify that the issue is real by checking the site and by checking the incidents timeline. What should you do to resolve the issue?",options:{A:"Increase the maximum number of instances in the MIG and verify that this resolves the issue. Ensure that the ticket is annotated with your solution. Create a normal work ticket for the application developer with a link to the incident. Mark the incident as closed.",B:"Check the incident documentation or labels to determine the on-call contact. Appoint an incident commander, and open a chat channel, or conference call for emergency response. Investigate and resolve the issue by increasing the maximum number of instances in the MIG, and verify that this resolves the issue. Mark the incident as closed.",C:"Increase the maximum number of instances in the MIG and verify that this resolves the issue. Check the incident documentation or labels to determine the on-call contact. Appoint an incident commander, and open a chat channel, or conference call for emergency response. Investigate and resolve the root cause of the issue. Write a blameless post-mortem and identify steps to prevent the issue, to ensure a culture of continuous improvement.",D:"Verify the high CPU is not user impacting, increase the maximum number of instances in the MIG and verify that this resolves the issue."},correctAnswer:["C"],explanation:{correct:"This option describes a full, mature incident response process. It includes immediate mitigation (increasing MIG size), followed by a proper investigation involving the right people (on-call, incident commander), communication (chat/call), a focus on finding and fixing the root cause (not just the symptom), and finally, a blameless post-mortem to learn from the incident and prevent recurrence. This focus on continuous improvement is a key SRE principle.",incorrect:{A:"This only addresses the symptom (mitigation) and then closes the incident without a proper root cause analysis or post-mortem.",B:"This is better as it involves a proper incident response team, but it also stops after mitigation and doesn't include the critical root cause analysis and post-mortem steps.",D:"The problem is already user-impacting (the gallery does not appear). This option also only focuses on mitigation."}},conditions:["Alert received: high CPU, MIG at max, feature (gallery) is down","Need to follow a proper incident resolution process"],caseStudyId:"CS001"},{id:236,topic:"SRE and Monitoring",question:"Your company runs a critical application on a GKE cluster. You want to implement Site Reliability Engineering (SRE) practices. You need to define a Service Level Objective (SLO) for request availability based on the proportion of successful requests (HTTP 200 responses) versus total requests over a 28-day rolling window. Which service should you primarily use to define and monitor this SLO?",options:{A:"Cloud Logging with log-based metrics",B:"Cloud Trace",C:"Cloud Profiler",D:"Cloud Monitoring"},correctAnswer:["D"],explanation:{correct:"Cloud Monitoring is the GCP service purpose-built for SRE practices like defining, measuring, and alerting on SLOs. It has a dedicated SLO monitoring interface where you can configure availability or latency SLOs based on metrics (Service Level Indicators or SLIs). It automatically calculates the error budget and allows you to create alerting policies based on the SLO's health.",incorrect:{A:"Cloud Logging is the source of the data if you use log-based metrics as your SLI, but Cloud Monitoring is the service where you actually define and manage the SLO itself.",B:"Cloud Trace is for analyzing the latency of distributed requests, not for defining availability SLOs.",C:"Cloud Profiler is for analyzing code-level CPU and memory performance, not for tracking service availability."}},conditions:["Implement SRE practices for a critical application on GKE","Need to define and monitor an SLO for request availability","SLO is based on the proportion of successful requests over a 28-day window"],caseStudyId:null},{id:237,topic:"SRE and Monitoring",question:"Customers need to have a good experience when accessing your web application so they will continue to use your service. You want to define key performance indicators (KPIs) to establish a service level objective (SLO). Which KPI could you use?",options:{A:"Eighty-five percent of customers are satisfied users",B:"Eighty-five percent of requests succeed when aggregated over 1 minute",C:"Low latency for > 85% of requests when aggregated over 1 minute",D:"Eighty-five percent of requests are successful"},correctAnswer:["D"],explanation:{correct:"A Key Performance Indicator (KPI) should be a quantifiable measure of performance. 'Eighty-five percent of requests are successful' is a specific, measurable metric (an availability or success rate) that can be directly used to define a Service Level Objective (SLO). It's a clear indicator of the service's reliability from a user's perspective.",incorrect:{A:"'Satisfied users' is a subjective business outcome, not a directly measurable technical KPI.",B:"This is a good Service Level Indicator (SLI), which is the actual measurement. The KPI is the concept (success rate), and the SLO is the target (e.g., 99.9% success rate over a month). D is the closest to a KPI that can become an SLO.",C:"This also describes an SLI/SLO for latency, which is another valid KPI, but D is a more fundamental KPI for availability."}},conditions:["Define Key Performance Indicators (KPIs)","Use KPIs to establish a Service Level Objective (SLO)"],caseStudyId:null},{id:238,topic:"SRE and Monitoring",question:"Your client has adopted a multi-cloud strategy that uses a virtual machine-based infrastructure. The client's website serves users across the globe. The client needs a single dashboard view to monitor performance in their AWS and Google Cloud environments. Your client previously experienced an extended outage and wants to establish a monthly service level objective (SLO) of no outage longer than an hour. What should you do?",options:{A:"In Cloud Monitoring, create an uptime check for the URL your clients will access. Configure it to check from multiple regions. Use the Cloud Monitoring dashboard to view the uptime metrics over time and ensure that the SLO is met. Recommend an SLO of 97% uptime per month.",B:"In Cloud Monitoring, create an uptime check for the URL your clients will access. Configure it to check from multiple regions. Use the Cloud Monitoring dashboard to view the uptime metrics over time and ensure that the SLO is met. Recommend an SLO of 97% uptime per day.",C:"Authorize access to your Google Cloud project from AWS with a service account. Install the monitoring agent on AWS EC2 (virtual machines) and Compute Engine instances. Use Cloud Monitoring to create dashboards that use the performance metrics from virtual machines to ensure that the SLO is met.",D:"Create a new project to use as an AWS connector project. Authorize access to the project from AWS with a service account. Install the monitoring agent on AWS EC2 (virtual machines) and Compute Engine instances. Use Cloud Monitoring to create dashboards that use the performance metrics from virtual machines to ensure that the SLO is met."},correctAnswer:["D"],explanation:{correct:"Cloud Monitoring supports multi-cloud monitoring. The recommended practice is to create a dedicated GCP project to act as a metrics scope (an 'AWS connector project') and configure it to pull metrics from your AWS account. By also installing the Cloud Monitoring agent on both your AWS EC2 and GCP Compute Engine instances, you can collect detailed system and application metrics from all VMs and view them together in a single set of dashboards within Cloud Monitoring. This provides the required single dashboard view.",incorrect:{A:"Uptime checks only monitor external availability. They do not provide the detailed performance metrics from within the VMs in both clouds, which is required for a comprehensive single dashboard view.",B:"This is incorrect for the same reason as A.",C:"This is very close, but creating a dedicated 'AWS connector project' (as in D) is the specific best practice for organizing multi-cloud monitoring in Cloud Monitoring to keep the metrics scope clean."}},conditions:["Multi-cloud strategy (AWS and GCP) with VMs","Need a single dashboard view to monitor performance in both environments","Establish a monthly SLO of no outage longer than one hour"],caseStudyId:null},{id:239,topic:"Security",question:"Your agricultural division is experimenting with fully autonomous vehicles. You want your architecture to promote strong security during vehicle operation. Which two architecture should you consider?",options:{A:"Treat every micro service call between modules on the vehicle as untrusted.",B:"Require IPv6 for connectivity to ensure a secure address space.",C:"Use a trusted platform module (TPM) and verify firmware and binaries on boot.",D:"Use a functional programming language to isolate code execution cycles.",E:"Use multiple connectivity subsystems for redundancy.",F:"Enclose the vehicle's drive electronics in a Faraday cage to isolate chips."},correctAnswer:["A","C"],explanation:{correct:"A describes a zero-trust architecture within the vehicle, which is a modern security best practice. It contains breaches by requiring authentication and authorization for all internal communications. C describes using a hardware root of trust (TPM) to ensure system integrity through secure boot, which prevents tampering with the vehicle's core software.",incorrect:{B:"IPv6 provides a larger address space but is not inherently more secure than a properly configured IPv4 network.",D:"While functional programming can have security benefits, it is a language choice, not a primary system architecture for security.",E:"This provides reliability and redundancy, not security against cyber threats.",F:"A Faraday cage protects against electromagnetic interference, not software or network-based attacks."}},conditions:["Designing a security architecture for autonomous vehicles","Promote strong security during vehicle operation"],caseStudyId:"cs_terram_main"},{id:240,topic:"Security",question:"You need to establish a secure, high-throughput connection between your data center and your Google Cloud VPC. You need to encrypt the traffic in transit. The current VPN connection does not meet the throughput requirements. What should you do?",options:{A:"Set up Carrier Peering for the connection.",B:"Set up Direct Peering for the connection.",C:"Set up MACsec for Cloud Interconnect for the connection.",D:"Set up HA VPN over Cloud Interconnect for the connection."},correctAnswer:["D"],explanation:{correct:"This is the standard solution for high-throughput, encrypted hybrid connectivity. Cloud Interconnect provides the high-throughput private link, and running an HA VPN tunnel over that link provides strong, end-to-end IPsec encryption. This combines the bandwidth of Interconnect with the security of VPN.",incorrect:{A:"Carrier Peering is for connecting to Google's public network edge, not for private VPC connectivity.",B:"Direct Peering is also for connecting to Google's public network edge.",C:"MACsec provides Layer 2 encryption for Dedicated Interconnect, which is a valid option, but VPN over Interconnect provides more flexible Layer 3 encryption and is a more commonly referenced solution for this requirement."}},conditions:["Need a secure, high-throughput connection between a data center and a GCP VPC","Traffic must be encrypted in transit","Current VPN throughput is insufficient"],caseStudyId:null},{id:241,topic:"Security",question:"Your company has an application running as a Deployment in a Google Kubernetes Engine (GKE) cluster. You have separate clusters for development, staging, and production. You have discovered that the team is able to deploy a Docker image to the production cluster without first testing the deployment in development and then staging. You want to allow the team to have autonomy but want to prevent this from happening. You want a Google Cloud solution that can be implemented quickly with minimal effort. What should you do?",options:{A:"Configure a Kubernetes lifecycle hook to prevent the container from starting if it is not approved for usage in the given environment.",B:"Implement a corporate policy to prevent teams from deploying Docker images to an environment unless the Docker image was tested in a earlier environment.",C:"Configure binary authorization policies for the development, staging, and production clusters. Create attestations as part of the continuous integration pipeline.",D:"Create a Kubernetes admissions controller to prevent the container from starting if it is not approved for usage in the given environment."},correctAnswer:["C"],explanation:{correct:"Binary Authorization is the Google Cloud-native service designed to solve this exact software supply chain security problem. By creating policies that require specific attestations (digital signatures) for each environment, you can enforce a promotion workflow. For example, the production cluster's policy can require an attestation that is only created after the image has been successfully tested in staging. This provides a strong, preventative technical control.",incorrect:{A:"Lifecycle hooks are for running scripts within a container's lifecycle (e.g., post-start), not for enforcing deployment policies.",B:"A corporate policy is an administrative control, not a technical preventative measure. It can be easily ignored or bypassed.",D:"While you could build a custom admissions controller for this, Binary Authorization is the managed, out-of-the-box solution that requires minimal effort."}},conditions:["GKE application with dev, staging, and production clusters","Problem: Developers can deploy directly to production without testing","Goal: Enforce a dev -> staging -> production promotion workflow","Solution should be quick to implement with minimal effort"],caseStudyId:null},{id:242,topic:"Security",question:"One of your primary business objectives is being able to trust the data stored in your application. You want to log all changes to the application data. How can you design your logging system to verify authenticity of your logs?",options:{A:"Write the log concurrently in the cloud and on premises",B:"Use a SQL database and limit who can modify the log table",C:"Digitally sign each timestamp and log entry and store the signature",D:"Create a JSON dump of each log entry and store it in Google Cloud Storage"},correctAnswer:["C"],explanation:{correct:"Digital signatures provide a cryptographic guarantee of authenticity and integrity. By signing each log entry (or a hash of a batch of entries) with a private key, you can later use the corresponding public key to verify that the log came from the expected source and has not been tampered with. This is the standard method for ensuring log authenticity.",incorrect:{A:"This provides redundancy but does not guarantee authenticity. An attacker could potentially modify logs in both locations.",B:"Limiting access is a good security practice for integrity but does not provide the cryptographic proof of authenticity that a digital signature does.",D:"This describes a storage format and location, not a mechanism for verifying authenticity."}},conditions:["Log all changes to application data","Need to verify the authenticity of the logs"],caseStudyId:null},{id:243,topic:"Security",question:"Your team is in charge of creating a payment card data vault for card numbers used to bill tens of thousands of viewers, merchandise consumers, and season ticket holders. You need to implement a custom card tokenization service that meets the following requirements: It must provide low latency at minimal cost. It must be able to identify duplicate credit cards and must not store plaintext card numbers. It should support annual key rotation. Which storage approach should you adopt for your tokenization service?",options:{A:"Store the card data in Secret Manager after running a query to identify duplicates.",B:"Encrypt the card data with a deterministic algorithm stored in Firestore using Datastore mode.",C:"Encrypt the card data with a deterministic algorithm and shard it across multiple Memorystore instances.",D:"Use column-level encryption to store the data in Cloud SQL."},correctAnswer:["B"],explanation:{correct:"This is a strong solution. Using a deterministic encryption algorithm (where the same plaintext always produces the same ciphertext for a given key) allows you to identify duplicate card numbers by comparing the ciphertexts. Storing this encrypted data in Firestore (Datastore mode) provides a scalable, low-latency, and persistent storage solution. The encryption key for the deterministic algorithm should be managed in Cloud KMS to support the annual key rotation requirement.",incorrect:{A:"Secret Manager is for storing a small number of secrets like API keys, not for a large dataset of tokenized card data. You also cannot query it to find duplicates.",C:"Memorystore is an in-memory cache and is not suitable for the primary, durable storage of a payment card data vault.",D:"While possible, Firestore is often a more scalable and cost-effective choice for a simple tokenization mapping compared to a relational database like Cloud SQL."}},conditions:["Create a payment card tokenization service","Provide low latency at minimal cost","Identify duplicate credit cards without storing plaintext card numbers","Support annual key rotation"],caseStudyId:"cs_hrl"},{id:244,topic:"Security",question:"You are responsible for ensuring that EHR's use of Google Cloud will pass an upcoming privacy compliance audit. What should you do?",options:{A:"Verify EHR's product usage against the list of compliant products on the Google Cloud compliance page.",B:"Advise EHR to execute a Business Associate Agreement (BAA) with Google Cloud.",C:"Use Firebase Authentication for EHRs user facing applications.",D:"Implement Prometheus to detect and prevent security breaches on EHR's web-based applications.",E:"Use GKE private clusters for all Kubernetes workloads."},correctAnswer:["A","B"],explanation:{correct:"For a healthcare company, compliance with regulations like HIPAA is paramount. A privacy audit will focus on two key areas: 1) Ensuring that the specific cloud services used to handle Protected Health Information (PHI) are covered under Google's compliance program (e.g., are HIPAA-eligible). 2) Ensuring the necessary legal agreements, specifically a Business Associate Agreement (BAA), are in place between the covered entity (EHR) and the business associate (Google Cloud). These are fundamental administrative and legal prerequisites for compliance.",incorrect:{C:"Using Firebase Authentication is a technical choice for identity management. While it can be part of a secure solution, it is not a direct requirement for passing the overall privacy compliance audit itself.",D:"Implementing Prometheus is a technical choice for monitoring. It contributes to security but is not a primary compliance document or agreement that an auditor would check for first.",E:"Using GKE private clusters is a security best practice to reduce the attack surface. It's a strong technical control, but like C and D, it's an implementation detail, not a foundational compliance requirement like the BAA or service eligibility."}},conditions:["Ensure GCP usage passes privacy compliance audit (e.g., HIPAA)"],caseStudyId:"cs_ehr_healthcare"},{id:245,topic:"Security",question:"You need to define the technical architecture for securely deploying workloads to Google Cloud. You also need to ensure that only verified containers are deployed using Google Cloud services. What should you do?",options:{A:"Enable Binary Authorization on GKE, and sign containers as part of a CI/CD pipeline.",B:"Configure Jenkins to utilize Kritis to cryptographically sign a container as part of a CI/CD pipeline.",C:"Configure Container Registry to only allow trusted service accounts to create and deploy containers from the registry.",D:"Configure Container Registry to use vulnerability scanning to confirm that there are no vulnerabilities before deploying the workload."},correctAnswer:["A","D"],explanation:{correct:"A and D together form a comprehensive software supply chain security strategy. Vulnerability scanning (D) identifies known security issues in container images. Binary Authorization (A) then acts as an enforcement gate, allowing you to create policies that require images to be free of critical vulnerabilities (as determined by the scanner) and to be cryptographically signed (attested) before they can be deployed to GKE.",incorrect:{B:"Kritis is an open-source project; Binary Authorization is the managed and integrated Google Cloud service for this purpose.",C:"IAM controls on the registry restrict *who* can push/pull images but do not verify the *content* or security posture of the images themselves."}},conditions:["Secure workload deployment","Ensure only verified containers deployed","Use GCP services"],caseStudyId:"cs_ehr_healthcare"},{id:246,topic:"Security",question:"Mountkirk Games wants you to secure the connectivity from the new gaming application platform to Google Cloud. You want to streamline the process and follow Google-recommended practices. What should you do?",options:{A:"Configure Workload Identity and service accounts to be used by the application platform.",B:"Use Kubernetes Secrets, which are obfuscated by default. Configure these Secrets to be used by the application platform.",C:"Configure Kubernetes Secrets to store the secret, enable Application-Layer Secrets Encryption, and use Cloud Key Management Service (Cloud KMS) to manage the encryption keys. Configure these Secrets to be used by the application platform.",D:"Configure HashiCorp Vault on Compute Engine, and use customer managed encryption keys and Cloud Key Management Service (Cloud KMS) to manage the encryption keys. Configure these Secrets to be used by the application platform."},correctAnswer:["A"],explanation:{correct:"Workload Identity is the Google-recommended and most secure method for applications running in GKE to authenticate to Google Cloud services. It allows you to map a Kubernetes service account to a Google service account, enabling pods to get short-lived GCP credentials automatically without managing or storing service account key files. This is streamlined, secure, and follows the principle of least privilege.",incorrect:{B:"Kubernetes Secrets are for application-level secrets (e.g., database passwords), not for authenticating the workload itself to GCP APIs. They are only base64 encoded, not truly encrypted by default.",C:"This describes how to secure Kubernetes secrets, but not how to authenticate the application to GCP services.",D:"While Vault is a valid secrets management tool, Workload Identity is the native, simpler, and recommended solution for authenticating GKE workloads to GCP."}},conditions:["Secure GKE application platform connectivity to GCP services","Streamline the process","Follow Google recommended practices"],caseStudyId:"cs_mountkirk_main"},{id:247,topic:"Security",question:"You want to prevent accidental deletion of a critical Compute Engine instance that runs a legacy workload. While the instance data is backed up, recreating and configuring the instance is a time-consuming process you want to avoid. What is the most effective way to protect this specific instance from accidental deletion?",options:{A:"Apply the compute.instances.delete IAM permission deny policy directly to the instance.",B:"Remove the Project Editor/Owner roles from users who might accidentally delete it.",C:"Enable deletion protection on the Compute Engine instance.",D:"Store the instance configuration in Terraform and use state locking."},correctAnswer:["C"],explanation:{correct:"Compute Engine provides a specific 'deletion protection' flag for VM instances. When this flag is enabled, any attempt to delete the instance via the API, gcloud CLI, or console will fail. To delete the instance, an authorized user must first explicitly disable this protection. This provides a direct and effective safeguard against accidental deletion.",incorrect:{A:"IAM Deny policies are a more complex feature and may not be as straightforward to apply for this specific use case as the built-in deletion protection flag.",B:"Removing broad roles is good practice for least privilege but doesn't specifically protect one instance from a user who still has legitimate (but perhaps more granular) deletion permissions.",D:"Terraform state locking prevents concurrent modifications but does not prevent an authorized user from intentionally or unintentionally running `terraform destroy` or from deleting the instance via other means."}},conditions:["Prevent accidental deletion of a specific critical GCE instance","Recreating the instance is time-consuming"],caseStudyId:null},{id:248,topic:"Security",question:"You have several Compute Engine instances running NGINX and Tomcat for a web application. In your web server logs, many login failures come from a single IP address, which looks like a brute force attack. How can you block this traffic?",options:{A:"Edit the Compute Engine instances running your web application, and enable Google Cloud Armor. Create a Google Cloud Armor policy with a default rule action of 'Allow.' Add a new rule that specifies the IP address causing the login failures as the Condition, with an action of 'Deny' and a deny status of '403,' and accept the default priority (1000).",B:"Ensure that an Application Load Balancer is configured to send traffic to the backend Compute Engine instances running your web server. Create a Google Cloud Armor policy with a default rule action of 'Deny.' Add a new rule that specifies the IP address causing the login failures as the Condition, with an action of 'Deny' and a deny status of '403,' and accept the default priority (1000). Add the load balancer backend service's HTTP-backend as the target.",C:"Ensure that an Application Load Balancer is configured to send traffic to the backend Compute Engine instances running your web server. Create a Google Cloud Armor policy with a default rule action of 'Allow.' Add a new rule that specifies the IP address causing the login failures as the Condition, with an action of 'Deny' and a deny status of '403,' and accept the default priority (1000). Add the load balancer backend service's HTTP-backend as the target.",D:"Ensure that an Application Load Balancer is configured to send traffic to your backend Compute Engine instances running your web server. Create a Google Cloud Armor policy using the instance's local firewall with a default rule action of 'Allow.' Add a new local firewall rule that specifies the IP address causing the login failures as the Condition, with an action of 'Deny' and a deny status of '403,' and accept the default priority (1000)."},correctAnswer:["C"],explanation:{correct:"This is the correct procedure for using Cloud Armor. Cloud Armor is a WAF that integrates with the external Application Load Balancer. To block a specific IP, you create a security policy, set the default rule to allow all other traffic, add a higher-priority rule to deny the malicious IP, and then attach this policy to the backend service of your load balancer.",incorrect:{A:"Cloud Armor cannot be enabled directly on Compute Engine instances; it must be attached to a load balancer's backend service.",B:"Setting the default rule to 'Deny' would block all traffic to your application except for what is explicitly allowed, which is not what's intended here. The goal is to block one bad IP, not all IPs.",D:"Cloud Armor policies are configured at the cloud level and attached to load balancers, not through an 'instance's local firewall'."}},conditions:["Block traffic from a single IP address","Traffic is suspected to be a brute force attack","Application is running on Compute Engine"],caseStudyId:null},{id:249,topic:"Security",question:"Your client is legally required to comply with the Payment Card Industry Data Security Standard (PCI-DSS). The client has formal audits already, but the audits are only done periodically. The client needs to monitor for common violations to meet those requirements more easily. The client does not want to replace audits but wants to engage in continuous compliance and catch violations early. What would you recommend that this client do?",options:{A:"Enable the Security Command Center (SCC) dashboard, asset discovery, and Security Health Analytics in the Premium tier. Export or view the PCI-DSS Report from the SCC dashboard's Compliance tab.",B:"Enable the Security Command Center (SCC) dashboard, asset discovery, and Security Health Analytics in the Standard tier. Export or view the PCI-DSS Report from the SCC dashboard's Compliance tab.",C:"Enable the Security Command Center (SCC) dashboard, asset discovery, and Security Health Analytics in the Premium tier. Export or view the PCI-DSS Report from the SCC dashboard's Vulnerabilities tab.",D:"Enable the Security Command Center (SCC) dashboard, asset discovery, and Security Health Analytics in the Standard tier. Export or view the PCI-DSS Report from the SCC dashboard's Vulnerabilities tab."},correctAnswer:["A"],explanation:{correct:"Security Command Center (SCC) Premium tier includes advanced features like compliance monitoring. Security Health Analytics scans for misconfigurations and generates findings that can be mapped to compliance standards like PCI-DSS. The 'Compliance' tab in the SCC dashboard provides specific reports against these standards, enabling continuous monitoring and early detection of violations.",incorrect:{B:"The Standard tier of SCC has limited features and does not include the advanced compliance reporting capabilities needed for PCI-DSS.",C:"The 'Vulnerabilities' tab shows specific findings, but the 'Compliance' tab provides the aggregated report against the specific PCI-DSS standard.",D:"This is incorrect because the Standard tier is insufficient and the report is in the Compliance tab, not the Vulnerabilities tab."}},conditions:["Required to comply with PCI-DSS","Need to monitor for common violations continuously","Want to catch violations early between formal audits"],caseStudyId:null},{id:250,topic:"Security",question:"You need to adopt Site Reliability Engineering principles and increase visibility into your environment. You want to minimize management overhead and reduce noise generated by the information being collected. You also want to streamline the process of reacting to analyzing and improving your environment, and to ensure that only trusted container images are deployed to production. What should you do?",options:{A:"Adopt Google Cloud Observability to gain visibility into the environment. Use Cloud Trace for distributed tracing, Cloud Logging for logging, and Cloud Monitoring for monitoring, alerting, and dashboards. Only page the on-call contact about novel issues or events that haven't been seen before. Use GNU Privacy Guard (GPG) to check container image signatures and ensure that only signed containers are deployed.",B:"Adopt Google Cloud Observability to gain visibility into the environment. Use Cloud Trace for distributed tracing, Cloud Logging for logging, and Cloud Monitoring for monitoring, alerting, and dashboards. Page the on-call contact when issues that affect resources in the environment are detected. Use GPG to check container image signatures and ensure that only signed containers are deployed.",C:"Adopt Google Cloud Observability to gain visibility into the environment. Use Cloud Trace for distributed tracing, Cloud Logging for logging, and Cloud Monitoring for monitoring, alerting, and dashboards. Only page the on-call contact about novel issues that violate a SLO or events that haven't been seen before. Use Binary Authorization to ensure that only signed container images are deployed.",D:"Adopt Google Cloud Observability to gain visibility into the environment. Use Cloud Trace for distributed tracing, Cloud Logging for logging, and Cloud Monitoring for monitoring, alerting, and dashboards. Page the on-call contact when issues that affect resources in the environment are detected. Use Binary Authorization to ensure that only signed container images are deployed."},correctAnswer:["C"],explanation:{correct:"This option combines key SRE and security best practices. It correctly identifies the Cloud Observability suite for visibility. It advocates for intelligent alerting based on SLO violations to reduce noise, a core SRE principle. For container security, it correctly identifies Binary Authorization as the GCP-native service for enforcing that only trusted, signed images are deployed.",incorrect:{A:"Using GPG is a generic way to sign things, but Binary Authorization is the specific, integrated enforcement mechanism for GKE/Cloud Run. Also, the alerting strategy is less defined than SLO-based alerting.",B:"Paging on all issues is too noisy and against SRE principles. It also incorrectly suggests GPG instead of Binary Authorization.",D:"Paging on all issues is too noisy."}},conditions:["Adopt SRE principles","Increase visibility and reduce alert noise","Streamline incident response and improvement processes","Ensure only trusted container images are deployed to production"],caseStudyId:null},{id:251,topic:"Serverless",question:"An application development team has come to you for advice. They are planning to write and deploy an HTTP(S) API using Go 1.12. The API will have a very unpredictable workload and must remain reliable during peaks in traffic. They want to minimize operational overhead for this application. Which approach should you recommend?",options:{A:"Develop the application with containers, and deploy to Google Kubernetes Engine.",B:"Develop the application for App Engine standard environment.",C:"Use a Managed Instance Group when deploying to Compute Engine.",D:"Develop the application for App Engine flexible environment, using a custom runtime."},correctAnswer:["B"],explanation:{correct:"App Engine standard environment is the ideal choice for this scenario. It is a fully managed, serverless platform that automatically and rapidly scales to handle unpredictable workloads, including scaling down to zero to save costs. It minimizes operational overhead as Google manages the infrastructure, and it supports the Go runtime.",incorrect:{A:"GKE provides more control but also requires more operational overhead to manage the cluster and deployments.",C:"A MIG on Compute Engine is an IaaS solution that requires the most operational overhead (managing VMs, OS, patching).",D:"App Engine flexible environment is also a good option, but the standard environment typically scales faster and can scale to zero more readily, making it a better fit for a 'very unpredictable workload' where cost during idle times is a concern."}},conditions:["Deploy an HTTP(S) API written in Go","Workload is very unpredictable","Must be reliable during traffic peaks","Minimize operational overhead"],caseStudyId:null},{id:252,topic:"Serverless",question:"The HRL development team releases a new version of their predictive capability application every Tuesday evening at 3 a.m. UTC to a repository. The security team at HRL has developed an in-house penetration testing Cloud Function called Airwolf. The security team wants to run Airwolf against the predictive capability application as soon as it is released every Tuesday. You need to set up Airwolf to run at the recurring weekly cadence. What should you do?",options:{A:"Set up Cloud Tasks and a Cloud Storage bucket that triggers a Cloud Function.",B:"Set up a Cloud Logging sink and a Cloud Storage bucket that triggers a Cloud Function.",C:"Configure the deployment job to notify a Pub/Sub queue that triggers a Cloud Function.",D:"Set up Identity and Access Management (IAM) and Confidential Computing to trigger a Cloud Function."},correctAnswer:["C"],explanation:{correct:"This describes an event-driven architecture, which is the most robust way to trigger an action *immediately* after a preceding event. By modifying the deployment job to publish a message to a Pub/Sub topic upon successful completion, you create a reliable signal. The 'Airwolf' Cloud Function can then be configured with a Pub/Sub trigger to subscribe to this topic, ensuring it runs as soon as the deployment is finished, regardless of minor variations in the release time.",incorrect:{A:"Cloud Tasks is for queueing and executing tasks, not for triggering events based on a deployment. A Cloud Storage trigger is not relevant here.",B:"A logging sink is for reacting to log entries, which is a less direct and potentially less reliable signal than a direct notification from the deployment job itself.",D:"IAM and Confidential Computing are security features and are not used for triggering Cloud Functions."}},conditions:["A new application version is released weekly","A security test (Cloud Function) needs to run immediately after each release","Need to set up the trigger mechanism"],caseStudyId:"cs_hrl"},{id:253,topic:"Serverless",question:"You start to build a new application that uses a few Cloud Functions for the backend. One use case requires a Cloud Function func_display to invoke another Cloud Function func_query. You want func_query only to accept invocations from func_display. You also want to follow Google's recommended best practices. What should you do?",options:{A:"Create a token and pass it in as an environment variable to func_display. When invoking func_query, include the token in the request. Pass the same token to func_query and reject the invocation if the tokens are different.",B:"Make func_query Require authentication. Create a unique service account and associate it to func_display. Grant the service account invoker role for func_query. Create an id token in func_display and include the token to the request when invoking func_query.",C:"Make func_query Require authentication and only accept internal traffic. Create those two functions in the same VPC. Create an ingress firewall rule for func_query to only allow traffic from func_display.",D:"Create those two functions in the same project and VPC. Make func_query only accept internal traffic. Create an ingress firewall for func_query to only allow traffic from func_display. Also, make sure both functions use the same service account."},correctAnswer:["B"],explanation:{correct:"This describes the standard and most secure, identity-based method for function-to-function authentication. By giving the calling function (`func_display`) a unique identity (its service account) and granting that identity the specific `Cloud Functions Invoker` role on the target function (`func_query`), you use GCP's built-in IAM to enforce access control. The calling function then authenticates its requests using a short-lived OIDC token, which is a secure and managed process.",incorrect:{A:"Using a shared secret (token) stored in an environment variable is less secure than identity-based authentication. The secret could be compromised and is hard to rotate.",C:"Relying only on network controls (internal traffic, firewalls) is not sufficient. You should always have service-level authentication and authorization as well. A function in the same VPC could still call `func_query` if not for IAM controls.",D:"This also relies on network controls and incorrectly suggests using the same service account, which might violate the principle of least privilege if the functions have different needs."}},conditions:["Cloud Function `func_display` needs to invoke `func_query`","`func_query` must only accept invocations from `func_display`","Follow Google-recommended best practices for security"],caseStudyId:"cs_terram_main"},{id:254,topic:"Serverless",question:"You have broken down a legacy monolithic application into a few containerized RESTful microservices. You want to run those microservices on Cloud Run. You also want to make sure the services are highly available with low latency to your customers. What should you do?",options:{A:"Deploy Cloud Run services to multiple availability zones. Create Cloud Endpoints that point to the services. Create a global HTTP(S) Load Balancing instance and attach the Cloud Endpoints to its backend.",B:"Deploy Cloud Run services to multiple regions. Create serverless network endpoint groups pointing to the services. Add the serverless NEGs to a backend service that is used by a global HTTP(S) Load Balancing instance.",C:"Deploy Cloud Run services to multiple regions. In Cloud DNS, create a latency-based DNS name that points to the services.",D:"Deploy Cloud Run services to multiple availability zones. Create a TCP/IP global load balancer. Add the Cloud Run Endpoints to its backend service."},correctAnswer:["B"],explanation:{correct:"This is the standard and recommended architecture for global, highly available Cloud Run services. You deploy the service to multiple regions for redundancy and to be closer to users. A Global HTTP(S) Load Balancer provides a single global IP address. Serverless Network Endpoint Groups (NEGs) are the mechanism used to make serverless services like Cloud Run backends for the load balancer. The load balancer will then automatically route users to the closest healthy Cloud Run region.",incorrect:{A:"Cloud Run is a regional service; you achieve high availability by deploying to multiple regions, not just zones. Cloud Endpoints is an API gateway, not the primary tool for multi-region load balancing.",C:"While DNS-based routing can work, a global load balancer provides more sophisticated health checking and traffic management, making it the more reliable solution.",D:"A TCP/IP load balancer (Layer 4) is not suitable for HTTP-based Cloud Run services; an HTTP(S) load balancer (Layer 7) is required. Also, Cloud Run is regional, not zonal."}},conditions:["Deploy containerized RESTful microservices on Cloud Run","Ensure services are highly available with low latency for global customers"],caseStudyId:"cs_terram_main"},{id:255,topic:"Storage",question:"TerramEarth has equipped unconnected trucks with servers and sensors to collet telemetry data. Next year they want to use the data to train machine learning models. They want to store this data in the cloud while reducing costs. What should they do?",options:{A:"Have the vehicles computer compress the data in hourly snapshots, and store it in a Google Cloud storage (GCS) Nearline bucket.",B:"Push the telemetry data in Real-time to a streaming dataflow job that compresses the data, and store it in Google BigQuery.",C:"Push the telemetry data in real-time to a streaming dataflow job that compresses the data, and store it in Cloud Bigtable.",D:"Have the vehicle's computer compress the data in hourly snapshots, a Store it in a GCS Coldline bucket."},correctAnswer:["D"],explanation:{correct:"The data is for archival purposes, as it will not be accessed until 'next year'. Cloud Storage Coldline offers the lowest storage cost for data accessed less than once a year. Having the vehicle's computer compress the data before upload further reduces storage costs and transfer time, making this the most cost-effective solution.",incorrect:{A:"Nearline storage is for data accessed less than once a month and is more expensive than Coldline for long-term archival.",B:"A real-time streaming solution to BigQuery is significantly more expensive and unnecessarily complex for data that will not be accessed for a long time.",C:"A real-time streaming solution to Bigtable is also more expensive and complex than needed for this archival use case."}},conditions:["Store telemetry data from unconnected trucks","Data will be used next year for ML training (infrequent access)","Need to reduce storage costs"],caseStudyId:"cs_terram_main"},{id:256,topic:"Storage",question:"At Dress4Win, an operations engineer wants to create a tow-cost solution to remotely archive copies of database backup files. The database files are compressed tar files stored in their current data center. How should he proceed?",options:{A:"Create a cron script using gsutil to copy the files to a Coldline Storage bucket.",B:"Create a cron script using gsutil to copy the files to a Regional Storage bucket.",C:"Create a Cloud Storage Transfer Service Job to copy the files to a Coldline Storage bucket.",D:"Create a Cloud Storage Transfer Service job to copy the files to a Regional Storage bucket."},correctAnswer:["A"],explanation:{correct:"For archiving data with a focus on low cost, Cloud Storage Coldline is the appropriate storage class. Since the files originate from an on-premises data center, using the `gsutil` command-line tool is the standard method for uploading. Automating this upload with a cron script is a simple and effective solution.",incorrect:{B:"Regional storage is more expensive than Coldline for long-term archival.",C:"Storage Transfer Service is typically for large-scale online migrations from other cloud providers or HTTP sources, not for scheduled file uploads from an on-premises server.",D:"This is incorrect for the same reasons as B and C."}},conditions:["Create a low-cost solution for archiving","Archive database backup files (compressed tar files)","Files are located in an on-premises data center"],caseStudyId:"cs_dress4win_v1"},{id:257,topic:"Storage",question:"Your company has been storing 100 TB of data in a multi-regional bucket for 2 years. Your CFO wants to reduce costs because the data is not being accessed frequently. You need to ensure that this data is archived and follows Google-recommended practices. What should you do?",options:{A:"Create a lifecycle rule to migrate the data to Coldline class storage.",B:"Move the data to another bucket using the gsutil tool with the regional class specified.",C:"Use Cloud Dataflow to migrate the data to BigQuery tables and configure the tables to use long-term storage.",D:"Export the data to tape storage using the Google Transfer appliance and send it to an offsite location."},correctAnswer:["A"],explanation:{correct:"Object Lifecycle Management is the recommended and most efficient way to manage the storage class of existing objects. For data that is 2 years old and accessed infrequently, transitioning it to a cheaper archival class like Coldline or Archive will significantly reduce storage costs. A lifecycle rule automates this process without needing to move or copy data.",incorrect:{B:"Manually moving data to another bucket with `gsutil` would incur operational overhead and potentially data transfer costs.",C:"Migrating raw files to BigQuery is not an appropriate archival strategy for this type of data.",D:"Transfer Appliance is for moving data into or out of GCP, not for managing data that is already within Cloud Storage."}},conditions:["100 TB of data stored in a Multi-Regional bucket for 2 years","Data is not accessed frequently","Need to reduce storage costs by archiving the data","Follow Google-recommended practices"],caseStudyId:null},{id:258,topic:"Storage",question:"Your company is planning to upload several important files to Cloud Storage. After the upload is completed, they want to verify that the uploaded content is identical to what they have on-premises. You want to minimize the cost and effort of performing this check. What should you do?",options:{A:"1. Use Linux shasum to compute a digest of files you want to upload. 2. Use gsutil -m to upload all the files to Cloud Storage. 3. Use gsutil cp to download the uploaded files. 4. Use Linux shasum to compute a digest of the downloaded files. 5. Compare the hashes.",B:"1. Use gsutil -m to upload the files to Cloud Storage. 2. Develop a custom Java application that computes CRC32C hashes. 3. Use gsutil ls -L gs://[YOUR_BUCKET_NAME] to collect CRC32C hashes of the uploaded files. 4. Compare the hashes.",C:"1. Use gsutil -m to upload all the files to Cloud Storage. 2. Use gsutil cp to download the uploaded files. 3. Use Linux diff to compare the content of the files.",D:"1. Use gsutil -m to upload the files to Cloud Storage. 2. Use gsutil hash -c FILELNAME to generate CRC32C hashes of all on-premises files. 3. Use gsutil ls -L gs://[YOUR_BUCKET_NAME] to collect CRC32C hashes of the uploaded files. 4. Compare the hashes."},correctAnswer:["D"],explanation:{correct:"This is the most efficient method. Cloud Storage automatically computes a CRC32C hash for uploaded objects. By computing the same hash on the local files using `gsutil hash -c` and comparing it to the hash stored in GCS metadata (retrieved via `gsutil ls -L`), you can verify integrity without re-downloading any data, thus minimizing cost and effort.",incorrect:{A:"This involves re-downloading the files, which is costly and time-consuming.",B:"There is no need to develop a custom application; `gsutil` can compute the CRC32C hash locally.",C:"This involves re-downloading the files, which is costly and time-consuming."}},conditions:["Upload important files from on-premises to Cloud Storage","Need to verify that the uploaded content is identical to the original","Minimize cost and effort"],caseStudyId:null},{id:259,topic:"Storage",question:"You need to deploy a stateful workload on Google Cloud. The workload can scale horizontally, but each instance needs to read and write to the same POSIX filesystem. At high load, the stateful workload needs to support up to 100 MB/s of writes. What should you do?",options:{A:"Use a persistent disk for each instance.",B:"Use a regional persistent disk for each instance.",C:"Create a Cloud Filestore instance and mount it in each instance.",D:"Create a Cloud Storage bucket and mount it in each instance using gcsfuse."},correctAnswer:["C"],explanation:{correct:"Cloud Filestore is a managed NFS file service. It provides a POSIX-compliant filesystem that can be mounted and accessed in read-write mode by multiple Compute Engine instances simultaneously. This meets the 'read and write to the same POSIX filesystem' requirement. Filestore also offers performance tiers that can easily support the 100 MB/s write throughput requirement.",incorrect:{A:"Persistent disks are block storage and can only be attached in read-write mode to a single instance at a time.",B:"Regional persistent disks are also block storage and cannot be attached in read-write mode to multiple instances simultaneously.",D:"gcsfuse does not provide full POSIX compliance or the consistent low-latency write performance required by many stateful applications."}},conditions:["Deploy a stateful, horizontally scalable workload","Each instance needs to read and write to the same POSIX filesystem","Support up to 100 MB/s of writes at high load"],caseStudyId:null},{id:260,topic:"Storage",question:"You want to store critical business information in Cloud Storage buckets. The information is regularly changed but previous versions need to be referenced on a regular basis. You want to ensure that there is a record of all changes to any information in these buckets. You want to ensure that accidental edits or deletions can be easily rolled back. Which feature should you enable?",options:{A:"Bucket Lock",B:"Object Versioning",C:"Object change notification",D:"Object Lifecycle Management"},correctAnswer:["B"],explanation:{correct:"Object Versioning is designed for this exact use case. It keeps a history of all versions of an object. When an object is overwritten or deleted, the previous version becomes a noncurrent version instead of being permanently removed. This allows you to reference, restore, or roll back to any previous version, protecting against accidental changes.",incorrect:{A:"Bucket Lock is for WORM (Write-Once-Read-Many) compliance. It prevents deletion or modification for a set retention period but does not keep a history of intermediate versions.",C:"Object change notification sends alerts when objects change but does not store the previous versions.",D:"Object Lifecycle Management is used to automate actions like changing storage class or deleting objects based on rules, not for keeping a history of all changes."}},conditions:["Store critical information in Cloud Storage","Information is regularly changed","Previous versions need to be referenced","Need a record of all changes","Need to be able to easily roll back accidental edits or deletions"],caseStudyId:null},{id:261,topic:"Storage",question:"You have developed an application using Cloud ML Engine that recognizes famous paintings from uploaded images. You want to test the application and allow specific people to upload images for the next 24 hours. Not all users have a Google Account. How should you have users upload images?",options:{A:"Have users upload the images to Cloud Storage. Protect the bucket with a password that expires after 24 hours.",B:"Have users upload the images to Cloud Storage using a signed URL that expires after 24 hours.",C:"Create an App Engine web application where users can upload images. Configure App Engine to disable the application after 24 hours. Authenticate users via Cloud Identity.",D:"Create an App Engine web application where users can upload images for the next 24 hours. Authenticate users via Cloud Identity."},correctAnswer:["B"],explanation:{correct:"Signed URLs are the perfect solution for providing temporary, limited access to a Cloud Storage bucket for users who do not have Google accounts. Your backend can generate a short-lived URL (e.g., expiring in 24 hours) that grants the bearer permission to perform a specific action (like uploading a file) on a specific object. This is secure and does not require managing user accounts.",incorrect:{A:"Cloud Storage buckets cannot be protected with a password.",C:"This is overly complex and requires users to have Google accounts for Cloud Identity authentication, which violates the requirement.",D:"This is also incorrect for the same reason as C."}},conditions:["Allow specific people to upload images for testing","Upload window is 24 hours","Not all users have a Google Account"],caseStudyId:null},{id:262,topic:"Storage",question:"You need to upload files from your on-premises environment to Cloud Storage. You want the files to be encrypted on Cloud Storage using customer-supplied encryption keys. What should you do?",options:{A:"Supply the encryption key in a .boto configuration file. Use gsutil to upload the files.",B:"Supply the encryption key using gcloud config. Use gsutil to upload the files to that bucket.",C:"Use gsutil to upload the files, and use the flag --encryption-key to supply the encryption key.",D:"Use gsutil to create a bucket, and use the flag --encryption-key to supply the encryption key. Use gsutil to upload the files to that bucket."},correctAnswer:["A"],explanation:{correct:"When using the `gsutil` command-line tool, Customer-Supplied Encryption Keys (CSEK) are configured in the `.boto` configuration file. You add a section that specifies the base64-encoded AES-256 key. `gsutil` will then automatically use this key to encrypt objects on upload and will require it for decryption on download.",incorrect:{B:"`gcloud config` is not used to manage CSEK keys for `gsutil`.",C:"The `gsutil cp` command does not have a flag to supply the key directly on the command line.",D:"The `gsutil mb` command does not have a flag to supply an encryption key for the bucket; CSEK is applied on a per-object basis."}},conditions:["Upload files from on-premises to Cloud Storage","Encrypt files using Customer-Supplied Encryption Keys (CSEK)"],caseStudyId:null},{id:263,topic:"Storage",question:"You have been engaged by your client to lead the migration of their application infrastructure to GCP. One of their current problems is that the on-premises high performance SAN is requiring frequent and expensive upgrades to keep up with the variety of workloads that are identified as follows: 20TB of log archives retained for legal reasons; 500 GB of VM boot/data volumes and templates; 500 GB of image thumbnails; 200 GB customer session state data that allows customers to restart sessions even if off-line for several days. Which of the following best reflects your recommendations for a cost-effective storage allocation?",options:{A:"Local SSD for customer session state dat Lifecycle-managed Cloud Storage for log archives, thumbnails, and VM boot/data volumes.",B:"Memcache backed by Cloud Datastore for the customer session state dat Lifecycle-managed Cloud Storage for log archives, thumbnails and VM boot/data volumes.",C:"Memcache backed by Cloud SQL for customer session state dat Assorted local SSD- backed instances for VM boot/data volume Cloud Storage for log archives and thumbnails.",D:"Memcache backed by Persistent Disk SSD storage for customer session state dat Assorted local SSD-backed instances for VM boot/data volume Cloud Storage for log archives and thumbnails."},correctAnswer:["B"],explanation:{correct:"This option correctly maps most workloads. A cache like Memorystore (Memcache) backed by a durable NoSQL database like Cloud Datastore is a great pattern for persistent session state. Cloud Storage with lifecycle management is ideal for cost-effective archival of logs and for serving static assets like thumbnails. While VM boot/data volumes should be on Persistent Disk, this option correctly identifies the best solutions for the other three, more distinct workloads.",incorrect:{A:"Local SSD is ephemeral and not suitable for persistent session state. VM boot/data volumes should be on Persistent Disk, not Cloud Storage.",C:"Local SSD is ephemeral and not suitable for VM boot/data volumes. Cloud SQL could work for session state but Datastore is often a better fit.",D:"Local SSD is ephemeral. Using Persistent Disk as a direct backing for Memcache is not a standard pattern."}},conditions:["Migrate a variety of workloads from an on-premises SAN","Workloads: 20TB log archives, 500GB VM boot/data, 500GB thumbnails, 200GB persistent session state","Need a cost-effective storage allocation"],caseStudyId:null},{id:264,topic:"Storage",question:"You have an application that makes HTTP requests to Cloud Storage. Occasionally the requests fail with HTTP status codes of 5xx and 429. How should you handle these types of errors?",options:{A:"Use gRPC instead of HTTP for better performance.",B:"Implement retry logic using a truncated exponential backoff strategy.",C:"Make sure the Cloud Storage bucket is multi-regional for geo-redundancy.",D:"Monitor https://status.cloud.google.com/feed.atom and only make requests if Cloud Storage is not reporting an incident."},correctAnswer:["B"],explanation:{correct:"HTTP 5xx (server errors) and 429 (rate limiting) errors are often transient. The recommended practice for handling them is to implement a retry mechanism with exponential backoff and jitter. This strategy waits for an increasing amount of time between retries, preventing a thundering herd of clients from overwhelming a temporarily struggling service.",incorrect:{A:"Changing the protocol does not solve the underlying issue of transient server errors or rate limiting.",C:"Multi-regional buckets improve availability against regional outages but do not prevent transient 5xx or 429 errors.",D:"Checking the status page is reactive and doesn't handle brief, intermittent issues. A robust client should always have retry logic."}},conditions:["Application makes HTTP requests to Cloud Storage","Occasionally receives 5xx (server error) and 429 (too many requests) errors"],caseStudyId:null},{id:265,topic:"Storage",question:"Your company creates rendering software which users can download from the company website. Your company has customers all over the world. You want to minimize latency for all your customers. You want to follow Google-recommended practices. How should you store the files?",options:{A:"Save the files in a Multi-Regional Cloud Storage bucket.",B:"Save the files in a Regional Cloud Storage bucket, one bucket per zone of the region.",C:"Save the files in multiple Regional Cloud Storage buckets, one bucket per zone per region.",D:"Save the files in multiple Multi-Regional Cloud Storage buckets, one bucket per multi-region."},correctAnswer:["A"],explanation:{correct:"For serving content to a global audience with low latency, the best practice is to store the files in a Multi-Regional Cloud Storage bucket and enable Cloud CDN on the load balancer that fronts the bucket. The multi-regional bucket itself provides low-latency access within a large geographic area (e.g., the US or Europe), and Cloud CDN caches the content at Google's edge locations worldwide for the lowest possible latency to all users.",incorrect:{B:"Buckets are regional or multi-regional, not zonal. This configuration is not possible.",C:"Managing multiple regional buckets is more complex than using a single multi-regional bucket with a CDN.",D:"Managing multiple multi-regional buckets is also more complex and generally unnecessary."}},conditions:["Store downloadable software files for a global customer base","Minimize latency for all customers","Follow Google-recommended practices"],caseStudyId:null},{id:266,topic:"Storage",question:"Your company acquired a healthcare startup and must retain its customers medical information for up to 4 more years, depending on when it was created. Your corporate policy is to securely retain this data, and then delete it as soon as regulations allow. Which approach should you take?",options:{A:"Store the data in Google Drive and manually delete records as they expire.",B:"Anonymize the data using the Cloud Data Loss Prevention API and store it indefinitely.",C:"Store the data in Cloud Storage and use lifecycle management to delete files when they expire.",D:"Store the data in Cloud Storage and run a nightly batch script that deletes all expired data."},correctAnswer:["C"],explanation:{correct:"Cloud Storage is a suitable and secure location for archival data. Object Lifecycle Management is the perfect tool for this use case, as it can be configured with an `Age`-based rule to automatically and reliably delete objects after their required retention period has passed. This is an automated, 'set it and forget it' solution that ensures compliance.",incorrect:{A:"Google Drive is not designed for this type of archival, and manual deletion is error-prone and not scalable.",B:"Anonymizing the data is not the same as retaining the original medical information, which is likely what is required for compliance.",D:"A custom batch script is more complex to manage and less robust than the native Cloud Storage lifecycle management feature."}},conditions:["Retain medical information for up to 4 years","Securely retain the data","Delete the data as soon as regulations allow"],caseStudyId:null},{id:267,topic:"Storage",question:"Your company is building a new architecture to support its data-centric business focus. You are responsible for setting up the network. Your companys mobile and web-facing applications will be deployed on-premises, and all data analysis will be conducted in GCP. The plan is to process and load 7 years of archived .csv files totaling 900 TB of data and then continue loading 10 TB of data daily. You currently have an existing 100-MB internet connection. What actions will meet your companys needs?",options:{A:"Compress and upload both achieved files and files uploaded daily using the qsutil m option.",B:"Lease a Transfer Appliance, upload archived files to it, and send it, and send it to Google to transfer archived data to Cloud Storage. Establish a connection with Google using a Dedicated Interconnect or Direct Peering connection and use it to upload files daily.",C:"Lease a Transfer Appliance, upload archived files to it, and send it, and send it to Google to transfer archived data to Cloud Storage. Establish one Cloud VPN Tunnel to VPC networks over the public internet, and compares and upload files daily using the gsutil m option.",D:"Lease a Transfer Appliance, upload archived files to it, and send it to Google to transfer archived data to Cloud Storage. Establish a Cloud VPN Tunnel to VPC networks over the public internet, and compress and upload files daily."},correctAnswer:["B"],explanation:{correct:"This scenario requires a two-pronged approach. For the massive 900 TB initial load, an online transfer over a 100 Mbps connection is not feasible (it would take decades). The Transfer Appliance is the correct solution for this offline transfer. For the ongoing 10 TB daily load, the 100 Mbps connection is also insufficient (it would take over a week to upload one day's worth of data). A high-bandwidth, dedicated link like Dedicated Interconnect is required for these daily transfers.",incorrect:{A:"The 100 Mbps connection is far too slow for either the initial or the daily data transfer volumes.",C:"A Cloud VPN over the existing 100 Mbps connection would still be too slow for the 10 TB daily load.",D:"This is incorrect for the same reason as C."}},conditions:["Initial data load of 900 TB of archived files","Ongoing daily load of 10 TB of data","Existing network connection is 100 Mbps"],caseStudyId:null},{id:268,topic:"Storage",question:"You are working at a financial institution that stores mortgage loan approval documents on Cloud Storage. Any change to these approval documents must be uploaded as a separate approval file, so you want to ensure that these documents cannot be deleted or overwritten for the next 5 years. What should you do?",options:{A:"Create a retention policy on the bucket for the duration of 5 years. Create a lock on the retention policy.",B:"Create the bucket with uniform bucket-level access, and grant a service account the role of Object Writer. Use the service account to upload new files.",C:"Use a customer-managed key for the encryption of the bucket. Rotate the key after 5 years.",D:"Create the bucket with fine-grained access control, and grant a service account the role of Object Writer. Use the service account to upload new files."},correctAnswer:["A"],explanation:{correct:"This is the correct approach for WORM (Write-Once-Read-Many) compliance. A bucket retention policy enforces that objects cannot be deleted or overwritten for the specified duration. Locking the policy makes the policy itself irreversible, providing the strongest guarantee of immutability, which is critical for financial and legal document retention.",incorrect:{B:"IAM controls manage *who* can access the bucket, but they do not prevent an authorized user from deleting or overwriting objects.",C:"Encryption keys protect data confidentiality, not its integrity or immutability. An authorized user can still delete an encrypted object.",D:"This is incorrect for the same reason as B."}},conditions:["Store mortgage loan documents in Cloud Storage","Documents must not be deleted or overwritten for 5 years","Changes are uploaded as new files (implies original is immutable)"],caseStudyId:null},{id:269,topic:"Storage",question:"Your company wants to migrate their 10-TB on-premises database export into Cloud Storage. You want to minimize the time it takes to complete this activity, the overall cost, and database load. The bandwidth between the on-premises environment and Google Cloud is 1 Gbps. You want to follow Google-recommended practices. What should you do?",options:{A:"Develop a Dataflow job to read data directly from the database and write it into Cloud Storage.",B:"Use the Data Transfer appliance to perform an offline migration.",C:"Use a commercial partner ETL solution to extract the data from the on-premises database and upload it into Cloud Storage.",D:"Compress the data and upload it with gsutil -m to enable multi-threaded copy."},correctAnswer:["D"],explanation:{correct:"With a 1 Gbps link, a 10 TB online transfer is feasible (approx. 24-30 hours). To minimize time, compressing the data and using `gsutil -m` for parallel uploads is the most effective strategy. This uses standard, free tools and maximizes the use of the available bandwidth. Since it uses an existing export file, it adds no load to the source database.",incorrect:{A:"Dataflow is for data processing, not a simple file transfer. This would be overly complex and costly.",B:"Transfer Appliance is for much larger datasets or when network bandwidth is very limited. The turnaround time for the appliance would be longer than the online transfer.",C:"A commercial ETL solution is unnecessary and costly for a simple file transfer."}},conditions:["Migrate a 10 TB database export file from on-premises to Cloud Storage","Minimize transfer time, overall cost, and database load","Available bandwidth is 1 Gbps","Follow Google-recommended practices"],caseStudyId:null},{id:270,topic:"Storage",question:"The application reliability team at your company this added a debug feature to their backend service to send all server events to Google Cloud Storage for eventual analysis. The event records are at least 50 KB and at most 15 MB and are expected to peak at 3,000 events per second. You want to minimize data loss. Which process should you implement?",options:{A:"Append metadata to file body Compress individual files Name files with serverName - Timestamp Create a new bucket bucket is older than 1 hour and save individual files to the new bucket. Otherwise, save files to existing bucket.",B:"Batch every 10,000 events with a single manifest file for metadata Compress event files and manifest file into a single archive file Name files using serverName - EventSequence Create a new bucket if bucket is older than 1 day and save the single archive file to the new bucket. Otherwise, save the single archive file to existing bucket.",C:"Compress individual files Name files with serverName - EventSequence Save files to one bucket Set custom metadata headers for each object after saving",D:"Append metadata to file body Compress individual files Name files with a random prefix pattern Save files to one bucket"},correctAnswer:["D"],explanation:{correct:"At a high write rate (3,000 objects/sec), the object naming convention is critical to avoid 'hotspotting' on Cloud Storage backend servers. Naming files sequentially can cause all writes to target a single server, leading to throttling and data loss. Using a random prefix pattern for object names ensures that writes are distributed across many different servers, maximizing ingestion throughput and minimizing data loss.",incorrect:{A:"Naming files with a timestamp creates a sequential pattern, which can lead to hotspotting.",B:"Naming files with a sequence number also creates a sequential pattern. Creating new buckets frequently is also an anti-pattern.",C:"Naming files with a sequence number is a sequential pattern that should be avoided at high write rates."}},conditions:["Send server events to Cloud Storage","High peak write rate (3,000 events/sec)","Goal is to minimize data loss (maximize ingestion performance)"],caseStudyId:null},{id:271,topic:"Storage",question:"Your company runs several databases on a single MySQL instance. They need to take backups of a specific database at regular intervals. The backup activity needs to complete as quickly as possible and cannot be allowed to impact disk performance. How should you configure the storage?",options:{A:"Configure a cron job to use the gcloud tool to take regular backups using persistent disk snapshots.",B:"Mount a Local SSD volume as the backup locatio After the backup is complete, use gsutil to move the backup to Google Cloud Storage.",C:"Use gcsfuse to mount a Google Cloud Storage bucket as a volume directly on the instance and write backups to the mounted location using mysqldump",D:"Mount additional persistent disk volumes onto each virtual machine (VM) instance in a RAID10 array and use LVM to create snapshots to send to Cloud Storage."},correctAnswer:["A"],explanation:{correct:"Persistent disk snapshots are the fastest way to back up a disk with the least performance impact. They are block-level, incremental, and the I/O operations for the snapshot are handled by the hypervisor layer, minimizing the impact on the running instance. This process can be easily automated with a cron job calling the `gcloud` command.",incorrect:{B:"Local SSD is ephemeral; if the instance stops before the backup is copied to GCS, the backup is lost. Writing a large backup file can also impact performance.",C:"Writing large backup files via gcsfuse can have performance limitations and is generally not recommended for this use case.",D:"This is an overly complex solution. Native persistent disk snapshots are much simpler and more efficient."}},conditions:["Take regular backups of a specific database on a MySQL instance","Backup must complete as quickly as possible","Backup activity must not impact disk performance"],caseStudyId:null},{id:272,topic:"Storage",question:"Your operations team currently stores 10 TB of data m an object storage service from a third-party provider. They want to move this data to a Cloud Storage bucket as quickly as possible, following Google-recommended practices. They want to minimize the cost of this data migration. When approach should they use?",options:{A:"Use the gsutil mv command lo move the data",B:"Use the Storage Transfer Service to move the data",C:"Download the data to a Transfer Appliance and ship it to Google",D:"Download the data to the on-premises data center and upload it to the Cloud Storage bucket"},correctAnswer:["B"],explanation:{correct:"Storage Transfer Service is the purpose-built, managed service for large-scale online data transfers *into* Cloud Storage from other cloud providers. It is fast, reliable, and cost-effective as it manages the transfer process without you needing to provision intermediate infrastructure. Data transfer into GCP is generally free.",incorrect:{A:"`gsutil` does not support direct cloud-to-cloud transfers.",C:"Transfer Appliance is for transfers from an on-premises location, not from another cloud provider.",D:"This would incur egress costs from the source cloud provider and would be slower and more complex than a direct cloud-to-cloud transfer."}},conditions:["10 TB data in a third-party object storage service","Move data to a Cloud Storage bucket","As quickly as possible","Follow Google-recommended practices and minimize cost"],caseStudyId:null},{id:273,topic:"Storage",question:"Your company wants to migrate their 10-TB on-premises database export into Cloud Storage You want to minimize the time it takes to complete this activity and the overall cost. The bandwidth between the on-premises environment and Google Cloud is 1 Gbps. You want to follow Google-recommended practices. What should you do?",options:{A:"Develop a Dataflow job to read data directly from the database and write it into Cloud Storage.",B:"Use the Data Transfer appliance to perform an offline migration.",C:"Use a commercial partner ETL solution to extract the data from the on-premises database and upload it into Cloud Storage.",D:"Upload the data with gcloud storage cp."},correctAnswer:["D"],explanation:{correct:"With a 1 Gbps link, a 10 TB online transfer is feasible (approx. 24-30 hours). The `gcloud storage cp` command (and its predecessor `gsutil cp`) is the standard, free, and recommended tool for this task. It supports parallel uploads to maximize the use of the available bandwidth, thus minimizing time. Since it uses an existing export file, it adds no load to the source database.",incorrect:{A:"Dataflow is for data processing, not a simple file transfer. This would be overly complex and costly.",B:"Transfer Appliance is for much larger datasets or when network bandwidth is very limited. The turnaround time for the appliance would likely be longer than the online transfer.",C:"A commercial ETL solution is unnecessary and costly for a simple file transfer."}},conditions:["Migrate 10 TB on-prem DB export to GCS","Minimize time and cost","1 Gbps bandwidth","Follow Google recommended practices"],caseStudyId:null},{id:274,topic:"Storage",question:"You need to optimize batch file transfers into Cloud Storage for Mountkirk Games new Google Cloud solution. The batch files contain game statistics that need to be staged in Cloud Storage and be processed by an extract transform load (ETL) tool. What should you do?",options:{A:"Use gsutil to batch move files in sequence.",B:"Use gsutil to batch copy the files in parallel.",C:"Use gsutil to extract the files as the first part of ETL.",D:"Use gsutil to load the files as the last part of ETL."},correctAnswer:["B"],explanation:{correct:"To optimize the transfer of multiple files, using the `-m` flag with `gsutil cp` enables parallel (multi-threaded/multi-process) copying. This significantly speeds up the upload of a batch of files compared to copying them one by one in sequence.",incorrect:{A:"Copying files in sequence is slow and does not optimize the transfer.",C:"`gsutil` is a tool for interacting with Cloud Storage (copying, moving, listing files), not an ETL tool for extracting data from files.",D:"This is incorrect for the same reason as C."}},conditions:["Optimize batch file transfers into Cloud Storage","Files contain game statistics to be staged for an ETL tool"],caseStudyId:"cs_mountkirk_main"},{id:275,topic:"Storage",question:"TerramEarth has equipped all connected trucks with servers and sensors to collect telemetry data. Next year they want to use the data to train machine learning models. They want to store this data in the cloud while reducing costs. What should they do?",options:{A:"Have the vehicles computer compress the data in hourly snapshots, and store it in a Google Cloud Storage (GCS) Nearline bucket",B:"Push the telemetry data in real-time to a streaming dataflow job that compresses the data, and store it in Google BigQuery",C:"Push the telemetry data in real-time to a streaming dataflow job that compresses the data, and store it in Cloud Bigtable",D:"Have the vehicles computer compress the data in hourly snapshots, and store it in a GCS Coldline bucket"},correctAnswer:["D"],explanation:{correct:"The data is for archival purposes, as it will not be accessed until 'next year'. Cloud Storage Coldline offers the lowest storage cost for data accessed less than once a year. Having the vehicle's computer compress the data before upload further reduces storage costs and transfer time, making this the most cost-effective solution.",incorrect:{A:"Nearline storage is for data accessed less than once a month and is more expensive than Coldline for long-term archival.",B:"A real-time streaming solution to BigQuery is significantly more expensive and unnecessarily complex for data that will not be accessed for a long time.",C:"A real-time streaming solution to Bigtable is also more expensive and complex than needed for this archival use case."}},conditions:["Store telemetry data from connected trucks","Data used for ML training next year","Reduce storage costs"],caseStudyId:"cs_terram_main"},{id:276,topic:"Storage",question:"To be compliant with European GDPR regulation, TerramEarth is required to delete data generated from its European customers after a period of 36 months when it contains personal data. In the new architecture, this data will be stored in both Cloud Storage and BigQuery. What should you do?",options:{A:"Create a BigQuery table for the European data, and set the table retention period to 36 months. For Cloud Storage, use gsutil to enable lifecycle management using a DELETE action with an Age condition of 36 months.",B:"Create a BigQuery table for the European data, and set the table retention period to 36 months. For Cloud Storage, use gsutil to create a SetStorageClass to NONE action when with an Age condition of 36 months.",C:"Create a BigQuery time-partitioned table for the European data, and set the partition expiration period to 36 months. For Cloud Storage, use gsutil to enable lifecycle management using a DELETE action with an Age condition of 36 months.",D:"Create a BigQuery time-partitioned table for the European data, and set the partition expiration period to 36 months. For Cloud Storage, use gsutil to create a SetStorageClass to NONE action with an Age condition of 36 months."},correctAnswer:["C"],explanation:{correct:"This option uses the correct, most efficient features for both services. For BigQuery, using a time-partitioned table and setting a partition expiration is the best practice for automatically deleting old data. For Cloud Storage, Object Lifecycle Management with a `DELETE` action based on `Age` is the standard way to enforce data retention policies.",incorrect:{A:"Setting a table retention period in BigQuery would delete the entire table, not just the old partitions.",B:"`SetStorageClass to NONE` is not a valid lifecycle action.",D:"`SetStorageClass to NONE` is not a valid lifecycle action."}},conditions:["GDPR compliance: Delete EU customer personal data after 36 months","Data is stored in both Cloud Storage and BigQuery"],caseStudyId:"cs_terram_main"},{id:277,topic:"Storage",question:"TerramEarth has decided to store data files in Cloud Storage. You need to configure Cloud Storage lifecycle rules to store 1 year of data and minimize file storage cost. Which two actions should you take?",options:{A:"Create a Cloud Storage lifecycle rule with Age: 30, Storage Class: Standard, and Action: Set to Coldline, and create a second GCS life-cycle rule with Age: 365, Storage Class: Coldline, and Action: Delete.",B:"Create a Cloud Storage lifecycle rule with Age: 30, Storage Class: Coldline, and Action: Set to Nearline, and create a second GCS life-cycle rule with Age: 91, Storage Class: Coldline, and Action: Set to Nearline.",C:"Create a Cloud Storage lifecycle rule with Age: 90, Storage Class: Standard, and Action: Set to Nearline, and create a second GCS life-cycle rule with Age: 91, Storage Class: Nearline, and Action: Set to Coldline.",D:"Create a Cloud Storage lifecycle rule with Age: 30, Storage Class: Standard, and Action: Set to Coldline, and create a second GCS life-cycle rule with Age: 365, Storage Class: Nearline, and Action: Delete."},correctAnswer:["A"],explanation:{correct:"This strategy effectively minimizes cost. It transitions data to the very-low-cost Coldline storage after just 30 days (assuming it becomes infrequently accessed) and then deletes it at the 365-day mark as required. This is a common and cost-effective lifecycle pattern.",incorrect:{B:"Transitioning from a colder class (Coldline) to a warmer one (Nearline) does not make sense and would increase costs.",C:"While this is a valid tiered storage strategy, moving to Coldline earlier (as in A) would be even more cost-effective if the access patterns allow.",D:"The second rule is flawed. The delete action should be on the storage class where the object resides at 365 days, which would be Coldline, not Nearline."}},conditions:["Store files in GCS for 1 year, then delete","Minimize file storage cost using lifecycle rules"],caseStudyId:"cs_terram_main"},{id:278,topic:"Storage",question:"TerramEarth has about 1 petabyte (PB) of vehicle testing data in a private data center. You want to move the data to Cloud Storage for your machine learning team. Currently, a 1-Gbps interconnect link is available for you. The machine learning team wants to start using the data in a month. What should you do?",options:{A:"Request Transfer Appliances from Google Cloud, export the data to appliances, and return the appliances to Google Cloud.",B:"Configure the Storage Transfer service from Google Cloud to send the data from your data center to Cloud Storage.",C:"Make sure there are no other users consuming the 1Gbps link, and use multi-thread transfer to upload the data to Cloud Storage.",D:"Export files to an encrypted USB device, send the device to Google Cloud, and request an import of the data to Cloud Storage."},correctAnswer:["A"],explanation:{correct:"An online transfer of 1 PB over a 1 Gbps link would take approximately 95 days, which is far too long to meet the one-month deadline. Transfer Appliance is the Google-recommended solution for large-scale (petabyte) offline data transfers from an on-premises location when network bandwidth is insufficient.",incorrect:{B:"Storage Transfer Service is for online transfers and would be limited by the 1 Gbps link, making it too slow.",C:"Even with exclusive use of the link and multi-threading, the transfer would take over three months, failing to meet the deadline.",D:"Using generic USB devices is not a supported, secure, or scalable Google Cloud data import method."}},conditions:["Move 1 PB of data from a private data center to Cloud Storage","A 1-Gbps interconnect link is available","Data must be available within one month"],caseStudyId:"cs_terram_main"},{id:279,topic:"Storage",question:"As part of their new application experience, Dress4Win allows customers to upload images of themselves. The customer has exclusive control over who may view these images. Customers should be able to upload images with minimal latency and also be shown their images quickly on the main application page when they log in. Which configuration should Dress4Win use?",options:{A:"Store image files in a Google Cloud Storage bucket. Use Google Cloud Datastore to maintain metadata that maps each customer's ID and their image files.",B:"Store image files in a Google Cloud Storage bucket. Add custom metadata to the uploaded images in Cloud Storage that contains the customer's unique ID.",C:"Use a distributed file system to store customers images. As storage needs increase, add more persistent disks and/or nodes. Assign each customer a unique ID, which sets each file's owner attribute, ensuring privacy of images.",D:"Use a distributed file system to store customers images. As storage needs increase, add more persistent disks and/or nodes. Use a Google Cloud SQL database to maintain metadata that maps each customer's ID to their image files."},correctAnswer:["A"],explanation:{correct:"This is a standard and highly scalable pattern. Cloud Storage is the ideal place to store the image files (objects). Cloud Datastore (or Firestore) is a scalable NoSQL database perfect for storing the metadata mapping a customer's ID to their list of images. This allows for very fast lookups to retrieve the list of images for a given user upon login.",incorrect:{B:"Querying or listing Cloud Storage objects based on custom metadata is much less efficient and slower than a dedicated database query, especially as the number of images grows.",C:"A distributed file system (like Filestore) is overly complex and not the right tool for storing and serving image objects. Cloud Storage is purpose-built for this.",D:"This is incorrect for the same reason as C."}},conditions:["Customers upload images","Customer has exclusive control over viewing","Minimal latency for uploads","Quickly display a user's images when they log in"],caseStudyId:"cs_dress4win_v1"},{id:280,topic:"Storage",question:"You want to ensure Dress4Win's sales and tax records remain available for infrequent viewing by auditors for at least 10 years. Cost optimization is your top priority. Which cloud services should you choose?",options:{A:"Google Cloud Storage Coldline to store the data, and gsutil to access the data.",B:"Google Cloud Storage Nearline to store the data, and gsutil to access the data.",C:"Google Bigtable with US or EU as location to store the data, and gcloud to access the data.",D:"BigQuery to store the data, and a web server cluster in a managed instance group to access the data. Google Cloud SQL mirrored across two distinct regions to store the data, and a Redis cluster in a managed instance group to access the data."},correctAnswer:["A"],explanation:{correct:"For long-term (10 years) archival with infrequent access and a primary goal of cost optimization, Cloud Storage Coldline is the most appropriate storage class. It offers very low at-rest storage costs. While retrieval costs are higher, the overall cost for this access pattern will be the lowest. `gsutil` is the standard tool for accessing the data.",incorrect:{B:"Nearline is for data accessed more frequently (less than monthly) and has higher storage costs than Coldline.",C:"Bigtable is a high-performance NoSQL database and is not cost-optimized for long-term, infrequent-access archival.",D:"This is an overly complex and expensive architecture. BigQuery and Cloud SQL are not the primary tools for cost-optimized archival of records."}},conditions:["Store sales and tax records for at least 10 years","Data will be viewed infrequently by auditors","Cost optimization is the top priority"],caseStudyId:"cs_dress4win_v1"},{id:281,topic:"Storage",question:"Your company wants to try out the cloud with low risk. They want to archive approximately 100 TB of their log data to the cloud and test the analytics features available to them there, while also retaining that data as a long-term disaster recovery backup. Which two steps should they take?",options:{A:"Load logs into Google BigQuery.",B:"Load logs into Google Cloud SQL.",C:"Import logs into Google Stackdriver.",D:"Insert logs into Google Cloud Bigtable.",E:"Upload log files into Google Cloud Storage."},correctAnswer:["A","E"],explanation:{correct:"This requires two solutions for two goals. E) Cloud Storage is the ideal, cost-effective service for long-term archival and DR backup of raw log files. A) BigQuery is the premier service for testing cloud analytics. You can load the data from Cloud Storage into BigQuery to perform analysis and explore its features.",incorrect:{B:"Cloud SQL is a relational database and not suitable for storing and analyzing 100 TB of log data.",C:"Cloud Logging (Stackdriver) is for real-time log ingestion and monitoring, not for long-term archival of 100 TB of historical data.",D:"While Bigtable can handle large data volumes, BigQuery is the purpose-built service for testing ad-hoc analytics on log data."}},conditions:["Archive 100 TB of log data to the cloud","Test cloud analytics features on the data","Retain the data as a long-term DR backup"],caseStudyId:null},{id:282,topic:"Storage",question:"You are designing storage for a new application that needs to store large unstructured blobs up to 10 TB in size. Your CISO requires that the encryption keys for this data are stored on-premises in your company's HSM cluster. Which storage solution should you choose?",options:{A:"Cloud Storage with Customer-Managed Encryption Keys (CMEK)",B:"Cloud Storage with Customer-Supplied Encryption Keys (CSEK)",C:"Persistent Disk with Customer-Managed Encryption Keys (CMEK)",D:"Persistent Disk with Customer-Supplied Encryption Keys (CSEK)"},correctAnswer:["B"],explanation:{correct:"The requirement that encryption keys are stored *on-premises* and not in the cloud points directly to Customer-Supplied Encryption Keys (CSEK). With CSEK, you provide the key with each request to Cloud Storage, and Google uses it to encrypt/decrypt the data but never stores the key. Cloud Storage is the correct service for large, unstructured blobs.",incorrect:{A:"With CMEK, the key is managed within Google's Cloud KMS. Even if you import the key material from your HSM, the key itself then resides within GCP, which violates the requirement.",C:"Persistent Disk is block storage for VMs, not ideal for storing large, standalone unstructured blobs. Also uses CMEK, which is incorrect.",D:"Persistent Disk is not the right storage type for this use case."}},conditions:["Store large unstructured blobs","Encryption keys must be stored on-premises in an HSM"],caseStudyId:null},{id:283,topic:"Storage",question:"You need to store application checkpoint data that is frequently overwritten. The data must be accessible with the lowest possible latency from Compute Engine instances within a single zone. Durability is not a primary concern, as the data can be regenerated if lost. Which storage option provides the best performance for this use case?",options:{A:"Local SSD",B:"Standard Persistent Disk",C:"SSD Persistent Disk",D:"Cloud Storage Standard"},correctAnswer:["A"],explanation:{correct:"Local SSDs are physically attached to the host VM and offer the absolute lowest latency and highest IOPS. Since durability is not a concern and the data can be lost if the instance stops (which is the nature of Local SSDs), it is the perfect choice for a high-performance scratch disk or checkpointing where speed is the primary requirement.",incorrect:{B:"Standard Persistent Disks are HDD-based and have the highest latency of the block storage options.",C:"SSD Persistent Disks are fast but are network-attached, so they will always have higher latency than physically attached Local SSDs.",D:"Cloud Storage is object storage and has much higher latency than any block storage option, making it completely unsuitable for this use case."}},conditions:["Store frequently overwritten application checkpoint data","Requires the lowest possible latency","Accessed from GCE instances in a single zone","Durability is not a primary concern"],caseStudyId:null},{id:284,topic:"Storage",question:"You are working with a client who has built a secure messaging application. The application is open source and consists of two components. The first component is a web app, written in Go, which is used to register an account and authorize the user's IP address. The second is an encrypted chat protocol that uses TCP to talk to the backend chat servers running Debian. If the client's IP address doesn't match the registered IP address, the application is designed to terminate their session. The number of clients using the service varies greatly based on time of day, and the client wants to be able to easily scale as needed. What should you do?",options:{A:"Deploy the web application using the App Engine standard environment using a global external Application Load Balancer and a network endpoint group. Use an unmanaged instance group for the backend chat servers. Use an external network load balancer to load-balance traffic across the backend chat servers.",B:"Deploy the web application using the App Engine flexible environment using a global external Application Load Balancer and a network endpoint group. Use an unmanaged instance group for the backend chat servers. Use an external passthrough Network Load Balancer to load-balance traffic across the backend chat servers.",C:"Deploy the web application using the App Engine standard environment using a global external Application Load Balancer and a network endpoint group. Use a managed instance group for the backend chat servers. Use a global external proxy Network Load Balancer to load-balance traffic across the backend chat servers.",D:"Deploy the web application using the App Engine standard environment with a global external Application Load Balancer. Use a managed instance group for the backend chat servers. Use an external passthrough Network Load Balancer to load-balance traffic across the backend chat servers."},correctAnswer:["D"],explanation:{correct:"This architecture correctly separates the components. App Engine Standard is great for the scalable Go web app. A Managed Instance Group is perfect for the scalable Debian TCP servers. Crucially, an external *passthrough* Network Load Balancer (Layer 4) is required for the TCP backend because it preserves the client's original source IP address, which is essential for the application's IP-based authorization logic.",incorrect:{A:"An unmanaged instance group doesn't provide autoscaling, which is a requirement.",B:"An unmanaged instance group is incorrect. App Engine Standard is a better fit than Flexible for this simple web app.",C:"A proxy Network Load Balancer terminates the client's TCP connection and initiates a new one to the backend, which would hide the original client IP address. The passthrough load balancer is required."}},conditions:["Two-component application: Go web app and Debian TCP chat backend","Application logic relies on the client's original source IP address","Needs to scale easily to handle variable load"],caseStudyId:null},{id:285,topic:"IAM & Security",question:"You need to deploy resources from your laptop to Google Cloud using Terraform. Resources in your Google Cloud environment must be created using a service account. Your Cloud Identity has the roles/iam.serviceAccountTokenCreator Identity and Access Management (IAM) role and the necessary permissions to deploy the resources using Terraform. You want to set up your development environment to deploy the desired resources following Google-recommended best practices. What should you do?",options:{A:"1. Download the service accounts key file in JSON format, and store it locally on your laptop. 2. Set the GOOGLE_APPLICATION_CREDENTIALS environment variable to the path of your downloaded key file.",B:"1. Run the following command from a command line: gcloud config set auth/impersonate_service_account service-account-name@project.iam.gserviceacccount.com. 2. Set the GOOGLE_OAUTH_ACCESS_TOKEN environment variable to the value that is returned by the gcloud auth print-access-token command.",C:"1. Run the following command from a command line: gcloud auth application-default login. 2. In the browser window that opens, authenticate using your personal credentials.",D:"1. Store the service accounts key file in JSON format in Hashicorp Vault. 2. Integrate Terraform with Vault to retrieve the key file dynamically, and authenticate to Vault using a short-lived access token."},correctAnswer:["B"],explanation:{correct:"This is the recommended best practice for using a service account from a local development environment without downloading long-lived key files. Your user identity, which has the `Service Account Token Creator` role, can impersonate the target service account. The `gcloud` command generates short-lived credentials that Terraform can then use. This method, often called service account impersonation, is significantly more secure than storing static JSON keys on your laptop.",incorrect:{A:"Downloading a service account key file creates a long-lived credential that is a significant security risk if compromised. This is not a recommended best practice.",C:"This authenticates the `gcloud` CLI as your user identity, but it does not configure Terraform to act *as the service account*, which is the requirement.",D:"While using Vault is a good practice for secrets management, it still relies on an initial service account key file being stored somewhere, which impersonation (Option B) avoids entirely."}},conditions:["Deploy resources using Terraform from a laptop","Deployment must use a service account's identity","User has `serviceAccountTokenCreator` role","Follow security best practices"],caseStudyId:null},{id:286,topic:"Operations",question:"Your companys Google Cloud organization has about 200 projects and 1,500 virtual machines. There is no uniform strategy for logs and events management, which reduces visibility for your security operations team. You need to design a logs management solution that provides visibility and allows the security team to view the environments configuration. What should you do?",options:{A:"1. Create a dedicated log sink for each project that is in scope. 2. Use a BigQuery dataset with time partitioning enabled as a destination of the log sinks. 3. Deploy alerts based on log metrics in every project. 4. Grant the role Monitoring Viewer to the security operations team in each project.",B:"1. Create one log sink at the organization level that includes all the child resources. 2. Use as destination a Pub/Sub topic to ingest the logs into the security information and event management (SIEM) on-premises, and ensure that the right team can access the SIEM. 3. Grant the Viewer role at organization level to the security operations team.",C:"1. Enable network logs and data access logs for all resources in the Production folder. 2. Do not create log sinks to avoid unnecessary costs and latency. 3. Grant the roles Logs Viewer and Browser at project level to the security operations team.",D:"1. Create one sink for the Production folder that includes child resources and one sink for the logs ingested at the organization level that excludes child resources. 2. As destination, use a log bucket with a minimum retention period of 90 days in a project that can be accessed by the security team. 3. Grant the security operations team the role of Security Reviewer at organization level."},correctAnswer:["B"],explanation:{correct:"This provides the most scalable and centralized solution. Creating a single log sink at the organization level automatically captures logs from all 200 projects (including any new ones) without manual configuration per project. Exporting to Pub/Sub allows for real-time ingestion into an existing on-prem SIEM, a common enterprise pattern. Granting the `Viewer` role at the organization level provides the required read-only visibility into the environment's configuration.",incorrect:{A:"Creating a log sink for each of the 200 projects is not a scalable or manageable strategy.",C:"Not creating log sinks means logs will expire after the default retention period. Accessing logs on a per-project basis does not provide centralized visibility.",D:"The `Security Reviewer` role is very broad. Granting the `Viewer` role (as in B) is more aligned with the principle of least privilege for configuration visibility. Also, creating two separate sinks as described is more complex than a single organizational sink."}},conditions:["Large environment (200 projects, 1500 VMs)","No uniform log management strategy currently","Provide centralized visibility for the security team","Allow security team to view environment configuration"],caseStudyId:null},{id:287,topic:"Security",question:"Your Google Cloud organization allows for administrative capabilities to be distributed to each team through provision of a Google Cloud project with Owner role (roles/owner). The organization contains thousands of Google Cloud projects. Security Command Center Premium has surfaced multiple OPEN_MYSQL_PORT findings. You are enforcing the guardrails and need to prevent these types of common misconfigurations. What should you do?",options:{A:"Create a hierarchical firewall policy configured at the organization to deny all connections from 0.0.0.0/0.",B:"Create a hierarchical firewall policy configured at the organization to allow connections only from internal IP ranges.",C:"Create a Google Cloud Armor security policy to deny traffic from 0.0.0.0/0.",D:"Create a firewall rule for each virtual private cloud (VPC) to deny traffic from 0.0.0.0/0 with priority 0."},correctAnswer:["B"],explanation:{correct:"Hierarchical firewall policies, applied at the organization or folder level, are the ideal way to enforce baseline network security guardrails across many projects. A policy that allows connections *only* from internal IP ranges effectively creates a default-deny for all external traffic to internal resources like databases, preventing the `OPEN_MYSQL_PORT` misconfiguration while still permitting legitimate internal communication.",incorrect:{A:"Denying *all* connections from `0.0.0.0/0` would also block legitimate public-facing web traffic. The policy in B is more specific and appropriate.",C:"Cloud Armor is a WAF that protects external Application Load Balancers; it does not control direct VM port access.",D:"Creating firewall rules in thousands of individual VPCs is unmanageable. A hierarchical policy is the scalable solution."}},conditions:["Thousands of projects with delegated Owner roles","Need to prevent common misconfigurations like open database ports","Enforce preventative security guardrails at scale"],caseStudyId:null},{id:288,topic:"Security",question:"Your organization must comply with the regulation to keep instance logging data within Europe. Your workloads will be hosted in the Netherlands in region europe-west4 in a new project. You must configure Cloud Logging to keep your data in the country. What should you do?",options:{A:"Configure the organization policy constraint gcp.resourceLocations to europe-west4.",B:"Configure log sink to export all logs into a Cloud Storage bucket in europe-west4.",C:"Create a new log bucket in europe-west4, and redirect the _Default bucket to the new bucket.",D:"Set the logging storage region to europe-west4 by using the gcloud CLI logging settings update."},correctAnswer:["C"],explanation:{correct:"Cloud Logging allows you to control the storage location of logs through Log Buckets. To meet data residency requirements, you should create a new Log Bucket in the desired region (europe-west4). Then, you must configure the `_Default` log sink for the project (or folder/org) to route logs to this new regionalized bucket instead of the default global `_Default` bucket. This ensures logs are stored only in the specified region.",incorrect:{A:"The `gcp.resourceLocations` constraint restricts where resources like VMs can be created, but it does not control the storage location of the logs they generate.",B:"Exporting logs via a sink to a regional bucket is a good practice, but the logs are still initially processed and stored in the default global log bucket before being exported. The requirement is to keep the data within Europe from the start.",D:"This gcloud command does not exist in this form. Data residency for logging is controlled via regionalized Log Buckets."}},conditions:["Comply with regulation to keep logging data within Europe","Workloads hosted in europe-west4","Configure Cloud Logging for data residency"],caseStudyId:null},{id:289,topic:"Security",question:"You are using Security Command Center (SCC) to protect your workloads and receive alerts for suspected security breaches at your company. You need to detect cryptocurrency mining software. Which SCC service should you use?",options:{A:"Virtual Machine Threat Detection",B:"Container Threat Detection",C:"Rapid Vulnerability Detection",D:"Web Security Scanner"},correctAnswer:["A"],explanation:{correct:"Virtual Machine Threat Detection (VMTD) is a built-in service of Security Command Center Premium that specifically scans Compute Engine instances to detect potentially malicious applications, including cryptocurrency mining software and kernel-mode rootkits. This is the purpose-built tool for this detection.",incorrect:{B:"Container Threat Detection is for detecting threats within GKE clusters and containers, not on standalone Compute Engine VMs.",C:"Rapid Vulnerability Detection scans for network-exposed vulnerabilities, not for malware running inside a VM.",D:"Web Security Scanner checks for common vulnerabilities in App Engine, GKE, and Compute Engine web applications, not for malware."}},conditions:["Use Security Command Center for threat detection","Need to detect cryptocurrency mining software"],caseStudyId:null},{id:290,topic:"Security",question:"You are running applications outside Google Cloud that need access to Google Cloud resources. You are using workload identity federation to grant external identities Identity and Access Management (IAM) roles to eliminate the maintenance and security burden associated with service account keys. You must protect against attempts to spoof another users identity and gain unauthorized access to Google Cloud resources. What should you do? (Choose two.)",options:{A:"Enable data access logs for IAM APIs.",B:"Limit the number of external identities that can impersonate a service account.",C:"Use a dedicated project to manage workload identity pools and providers.",D:"Use immutable attributes in attribute mappings.",E:"Limit the resources that a service account can access."},correctAnswer:["C","D"],explanation:{correct:"C and D are two key best practices for securing workload identity federation. C) Using a dedicated project for managing pools and providers centralizes control and allows you to apply stricter security policies to that project. D) Using immutable attributes (attributes that cannot be reused or easily changed by a user, like a unique user ID from the IdP) in your attribute mappings is crucial. If you map a mutable attribute like an email address, an attacker could potentially take over a deleted email address and use it to spoof an identity and gain access.",incorrect:{A:"This is a good practice for auditing but is a detective control, not a preventative one against spoofing.",B:"While limiting impersonation is good, it doesn't protect against a valid but spoofed identity.",E:"This is the principle of least privilege and is always a good practice, but it doesn't specifically prevent identity spoofing."}},conditions:["Using workload identity federation for external applications","Need to protect against identity spoofing attempts"],caseStudyId:null},{id:291,topic:"Operations",question:"You recently deployed a Go application on Google Kubernetes Engine (GKE). The operations team has noticed that the applications CPU usage is high even when there is low production traffic. The operations team has asked you to optimize your applications CPU resource consumption. You want to determine which Go functions consume the largest amount of CPU. What should you do?",options:{A:"Deploy a Fluent Bit daemonset on the GKE cluster to log data in Cloud Logging. Analyze the logs to get insights into your application codes performance.",B:"Create a custom dashboard in Cloud Monitoring to evaluate the CPU performance metrics of your application.",C:"Connect to your GKE nodes using SSH. Run the top command on the shell to extract the CPU utilization of your application.",D:"Modify your Go application to capture profiling data. Analyze the CPU metrics of your application in flame graphs in Profiler."},correctAnswer:["D"],explanation:{correct:"Cloud Profiler is the tool specifically designed for this purpose. By instrumenting your Go application to send profiling data to Cloud Profiler, you can analyze its CPU usage at the function level. The flame graphs provided by Profiler give a clear visual representation of which functions are consuming the most CPU, allowing you to pinpoint the exact areas of the code that need optimization.",incorrect:{A:"Logs typically don't provide function-level CPU performance data.",B:"Cloud Monitoring can show you that the application's *overall* CPU usage is high, but it can't tell you *which function inside the code* is responsible.",C:"The `top` command shows process-level CPU usage, not function-level usage within your application's code."}},conditions:["Go application on GKE has unexpectedly high CPU usage","Need to optimize CPU consumption","Goal is to identify which specific Go functions are the cause"],caseStudyId:null},{id:292,topic:"IAM & Security",question:"Your team manages a Google Kubernetes Engine (GKE) cluster where an application is running. A different team is planning to integrate with this application. Before they start the integration, you need to ensure that the other team cannot make changes to your application, but they can deploy the integration on GKE. What should you do?",options:{A:"Using Identity and Access Management (IAM), grant the Viewer IAM role on the cluster project to the other team.",B:"Create a new GKE cluster. Using Identity and Access Management (IAM), grant the Editor role on the cluster project to the other team.",C:"Create a new namespace in the existing cluster. Using Identity and Access Management (IAM), grant the Editor role on the cluster project to the other team.",D:"Create a new namespace in the existing cluster. Using Kubernetes role-based access control (RBAC), grant the Admin role on the new namespace to the other team."},correctAnswer:["D"],explanation:{correct:"This is the standard Kubernetes-native way to provide scoped access. Creating a new namespace provides a logical isolation boundary for the other team's resources. Using Kubernetes RBAC to create a Role and RoleBinding that grants them `Admin` permissions *only within that new namespace* allows them to deploy and manage their integration components without being able to see or modify your application in its separate namespace.",incorrect:{A:"The Viewer IAM role is too restrictive; they wouldn't be able to deploy anything.",B:"Creating a new cluster might be unnecessary overhead. Granting the Editor role is too permissive as it gives them broad control over the entire project.",C:"Granting the Editor IAM role on the project is too permissive and would allow them to modify your application and other project resources."}},conditions:["Two teams sharing a single GKE cluster","One team needs to deploy their own integration application","The integration team must not be able to modify the existing application"],caseStudyId:null},{id:293,topic:"Networking",question:"Your company uses the Firewall Insights feature in the Google Network Intelligence Center. You have several firewall rules applied to Compute Engine instances. You need to evaluate the efficiency of the applied firewall ruleset. When you bring up the Firewall Insights page in the Google Cloud Console, you notice that there are no log rows to display. What should you do to troubleshoot the issue?",options:{A:"Enable Virtual Private Cloud (VPC) flow logging.",B:"Enable Firewall Rules Logging for the firewall rules you want to monitor.",C:"Verify that your user account is assigned the compute.networkAdmin Identity and Access Management (IAM) role.",D:"Install the Google Cloud SDK, and verify that there are no Firewall logs in the command line output."},correctAnswer:["B"],explanation:{correct:"Firewall Insights relies on data from Firewall Rules Logging to generate its insights (e.g., identifying overly permissive `allow` rules or `deny` rules that are never hit). If Firewall Rules Logging is not enabled for the specific rules you want to analyze, Firewall Insights will have no data to process and display. Therefore, the first troubleshooting step is to enable logging on those rules.",incorrect:{A:"VPC Flow Logs record a sample of network flows but are distinct from Firewall Rules Logging, which specifically logs when a firewall rule is evaluated and matched. Firewall Insights primarily uses Firewall Rules Logging.",C:"While you need appropriate IAM permissions to view the page, a lack of permission would typically result in an access denied error, not a 'no log rows to display' message.",D:"The Google Cloud SDK is a command-line tool; installing it won't generate the necessary logs. The issue is in the cloud configuration, not the client-side tooling."}},conditions:["Using Firewall Insights to evaluate firewall rules","The Firewall Insights page shows no data","Need to troubleshoot the issue"],caseStudyId:null},{id:294,topic:"Security",question:"Your organization has decided to restrict the use of external IP addresses on instances to only approved instances. You want to enforce this requirement across all of your Virtual Private Clouds (VPCs). What should you do?",options:{A:"Remove the default route on all VPCs. Move all approved instances into a new subnet that has a default route to an internet gateway.",B:"Create a new VPC in custom mode. Create a new subnet for the approved instances, and set a default route to the internet gateway on this new subnet.",C:"Implement a Cloud NAT solution to remove the need for external IP addresses entirely.",D:"Set an Organization Policy with a constraint on constraints/compute.vmExternalIpAccess. List the approved instances in the allowedValues list."},correctAnswer:["D"],explanation:{correct:"Organization Policies provide centralized, preventative control over resource configurations. The `constraints/compute.vmExternalIpAccess` constraint is specifically designed to control which VMs are allowed to have external IP addresses. By setting this policy at the organization level and specifying the list of approved instances, you can enforce this requirement across all projects and VPCs.",incorrect:{A:"Modifying routes controls traffic flow but doesn't prevent an external IP from being assigned to an instance.",B:"This approach is not scalable and doesn't enforce the policy across all existing VPCs.",C:"Cloud NAT provides outbound internet access for instances *without* external IPs, but it doesn't prevent an instance from being configured *with* an external IP."}},conditions:["Restrict the use of external IP addresses to only approved instances","Enforce this requirement across all VPCs in the organization"],caseStudyId:null},{id:295,topic:"Migration",question:"You are moving an application that uses MySQL from on-premises to Google Cloud. The application will run on Compute Engine and will use Cloud SQL. You want to cut over to the Compute Engine deployment of the application with minimal downtime and no data loss to your customers. You want to migrate the application with minimal modification. You also need to determine the cutover strategy. What should you do?",options:{A:"1. Set up Cloud VPN to provide private network connectivity between the Compute Engine application and the on-premises MySQL server. 2. Stop the on-premises application. 3. Create a mysqldump of the on-premises MySQL server. 4. Upload the dump to a Cloud Storage bucket. 5. Import the dump into Cloud SQL. 6. Modify the source code of the application to write queries to both databases and read from its local database. 7. Start the Compute Engine application. 8. Stop the on-premises application.",B:"1. Set up Cloud SQL proxy and MySQL proxy. 2. Create a mysqldump of the on-premises MySQL server. 3. Upload the dump to a Cloud Storage bucket. 4. Import the dump into Cloud SQL. 5. Stop the on-premises application. 6. Start the Compute Engine application.",C:"1. Set up Cloud VPN to provide private network connectivity between the Compute Engine application and the on-premises MySQL server. 2. Stop the on-premises application. 3. Start the Compute Engine application, configured to read and write to the on-premises MySQL server. 4. Create the replication configuration in Cloud SQL. 5. Configure the source database server to accept connections from the Cloud SQL replica. 6. Finalize the Cloud SQL replica configuration. 7. When replication has been completed, stop the Compute Engine application. 8. Promote the Cloud SQL replica to a standalone instance. 9. Restart the Compute Engine application, configured to read and write to the Cloud SQL standalone instance.",D:"1. Stop the on-premises application. 2. Create a mysqldump of the on-premises MySQL server. 3. Upload the dump to a Cloud Storage bucket. 4. Import the dump into Cloud SQL. 5. Start the application on Compute Engine."},correctAnswer:["C"],explanation:{correct:"This option, while complex, describes a valid, phased cutover strategy for minimal downtime using database replication. It first migrates the application to GCE while still pointing to the on-prem database, then sets up replication to Cloud SQL. The final cutover involves a brief application stop to promote the Cloud SQL replica and repoint the application. This ensures data is synchronized up to the last moment, minimizing data loss and downtime.",incorrect:{A:"This describes an offline migration with a complex and risky dual-write strategy.",B:"This describes an offline migration that would incur significant downtime.",D:"This is also an offline migration with significant downtime."}},conditions:["Migrate an on-premises MySQL application to GCE and Cloud SQL","Minimize downtime and data loss","Minimize application modification"],caseStudyId:null},{id:296,topic:"Compute",question:"Your company is running its application workloads on Compute Engine. The applications have been deployed in production, acceptance, and development environments. The production environment is business-critical and is used 24/7, while the acceptance and development environments are only critical during office hours. Your CFO has asked you to optimize these environments to achieve cost savings during idle times. What should you do?",options:{A:"Create a shell script that uses the gcloud command to change the machine type of the development and acceptance instances to a smaller machine type outside of office hours. Schedule the shell script on one of the production instances to automate the task.",B:"Use Cloud Scheduler to trigger a Cloud Function that will stop the development and acceptance environments after office hours and start them just before office hours.",C:"Deploy the development and acceptance applications on a managed instance group and enable autoscaling.",D:"Use regular Compute Engine instances for the production environment, and use preemptible VMs for the acceptance and development environments."},correctAnswer:["B"],explanation:{correct:"This is the most effective and cloud-native way to achieve cost savings. By stopping the non-production instances entirely outside of office hours, you eliminate all compute costs for them during idle times. Using Cloud Scheduler and a Cloud Function to automate this start/stop process is a reliable and serverless approach.",incorrect:{A:"Resizing instances still incurs costs for the smaller machine type and attached disks. Running the script from a production instance is not a good practice for managing other environments.",C:"Autoscaling will scale down based on load but may not scale to zero, thus still incurring costs.",D:"Preemptible (Spot) VMs are not suitable for development/acceptance environments that need to be reliably available during office hours, as they can be shut down at any time."}},conditions:["Production environment runs 24/7","Development and acceptance environments are used only during office hours","Need to optimize costs during idle times for non-production environments"],caseStudyId:null},{id:297,topic:"Data Analytics",question:"You are working at a sports association whose members range in age from 8 to 30. The association collects a large amount of health data, such as sustained injuries. You are storing this data in BigQuery. Current legislation requires you to delete such information upon request of the subject. You want to design a solution that can accommodate such a request. What should you do?",options:{A:"Use a unique identifier for each individual. Upon a deletion request, delete all rows from BigQuery with this identifier.",B:"When ingesting new data in BigQuery, run the data through the Data Loss Prevention (DLP) API to identify any personal information. As part of the DLP scan, save the result to Data Catalog. Upon a deletion request, query Data Catalog to find the column with personal information.",C:"Create a BigQuery view over the table that contains all the data. Upon a deletion request, exclude the rows that affect the subjects data from this view. Use this view instead of the source table for all analysis tasks.",D:"Use a unique identifier for each individual. Upon a deletion request, overwrite the column with the unique identifier with a salted SHA256 of its value."},correctAnswer:["A"],explanation:{correct:"Using a `DELETE` DML statement in BigQuery is the direct and compliant way to handle 'right to be forgotten' requests. By having a unique identifier for each individual, you can accurately target and delete all rows associated with that person when they request it, permanently removing their data from the table.",incorrect:{B:"DLP and Data Catalog are for identifying and classifying data, not for deleting it.",C:"Using a view to hide the data does not delete it. The original data remains in the underlying table, which would not be compliant with the deletion request.",D:"Overwriting the identifier with a hash is a form of pseudonymization, not deletion. The health data would still exist in the table, just without a direct link to the original identifier, which may not be sufficient for legal compliance."}},conditions:["Storing health data in BigQuery","Legislation requires deletion of information upon subject's request","Need a solution to handle deletion requests"],caseStudyId:null},{id:298,topic:"Databases",question:"You are implementing a single Cloud SQL MySQL second-generation database that contains business-critical transaction data. You want to ensure that the minimum amount of data is lost in case of catastrophic failure. Which two features should you implement? (Choose two.)",options:{A:"Sharding",B:"Read replicas",C:"Binary logging",D:"Automated backups",E:"Semisynchronous replication"},correctAnswer:["C","D"],explanation:{correct:"D) Automated backups are essential for point-in-time recovery (PITR). They provide a baseline for restoring the database to a recent state. C) Binary logging must be enabled for PITR to work. The binary logs record all data changes made after a backup was taken, allowing you to restore the backup and then 'replay' the logs to recover the database to a specific moment just before the failure, thus minimizing data loss.",incorrect:{A:"Sharding is a technique for horizontal scaling, not for minimizing data loss in a single instance.",B:"Read replicas are for scaling read traffic. While they contain a copy of the data, they use asynchronous replication and are not the primary mechanism for point-in-time recovery.",E:"Semisynchronous replication is a feature used in High Availability (HA) configurations to reduce the chance of data loss during a failover, but automated backups and binary logging are the core features for point-in-time recovery from a catastrophic failure."}},conditions:["Single Cloud SQL MySQL instance with business-critical data","Need to minimize data loss in case of a catastrophic failure"],caseStudyId:null},{id:299,topic:"Operations",question:"You are monitoring Google Kubernetes Engine (GKE) clusters in a Cloud Monitoring workspace. As a Site Reliability Engineer (SRE), you need to triage incidents quickly. What should you do?",options:{A:"Navigate the predefined dashboards in the Cloud Monitoring workspace, and then add metrics and create alert policies.",B:"Navigate the predefined dashboards in the Cloud Monitoring workspace, create custom metrics, and install alerting software on a Compute Engine instance.",C:"Write a shell script that gathers metrics from GKE nodes, publish these metrics to a Pub/Sub topic, export the data to BigQuery, and make a Data Studio dashboard.",D:"Create a custom dashboard in the Cloud Monitoring workspace for each incident, and then add metrics and create alert policies."},correctAnswer:["A"],explanation:{correct:"Cloud Monitoring provides rich, predefined dashboards specifically for GKE that show key metrics for clusters, nodes, workloads, and services. These dashboards are the ideal starting point for an SRE to quickly triage an incident, understand the state of the system, and identify potential issues. From these dashboards, you can drill down into specific metrics and create alerts for future incidents.",incorrect:{B:"There is no need to install separate alerting software; Cloud Monitoring has a built-in, powerful alerting system.",C:"This describes a complex, custom monitoring solution. The native, predefined GKE dashboards in Cloud Monitoring are much more efficient for quick incident triage.",D:"You should have proactive dashboards and alerts set up *before* an incident occurs. Creating a new dashboard during an incident is inefficient."}},conditions:["Monitoring GKE clusters in Cloud Monitoring","Role is a Site Reliability Engineer (SRE)","Need to triage incidents quickly"],caseStudyId:null},{id:300,topic:"Serverless",question:"You are managing an application deployed on Cloud Run for Anthos, and you need to define a strategy for deploying new versions of the application. You want to evaluate the new code with a subset of production traffic to decide whether to proceed with the rollout. What should you do?",options:{A:"Deploy a new revision to Cloud Run with the new version. Configure traffic percentage between revisions.",B:"Deploy a new service to Cloud Run with the new version. Add a Cloud Load Balancing instance in front of both services.",C:"In the Google Cloud Console page for Cloud Run, set up continuous deployment using Cloud Build for the development branch. As part of the Cloud Build trigger, configure the substitution variable TRAFFIC_PERCENTAGE with the percentage of traffic you want directed to a new version.",D:"In the Google Cloud Console, configure Traffic Director with a new Service that points to the new version of the application on Cloud Run. Configure Traffic Director to send a small percentage of traffic to the new version of the application."},correctAnswer:["A"],explanation:{correct:"Cloud Run has built-in support for revisions and traffic splitting. When you deploy a new version of your code, it creates a new revision. You can then configure the Cloud Run service to split traffic between the old and new revisions (e.g., send 90% to the stable revision and 10% to the new one). This is a native canary deployment strategy that allows you to evaluate the new version with a subset of production traffic.",incorrect:{B:"Deploying a completely new service and using an external load balancer is unnecessarily complex when Cloud Run provides this functionality natively.",C:"While CI/CD is good, this describes a configuration variable for a build trigger, not the actual runtime traffic splitting mechanism within Cloud Run.",D:"Traffic Director is a service mesh control plane, which is overly complex for a simple canary deployment on a single Cloud Run service that has this feature built-in."}},conditions:["Application deployed on Cloud Run for Anthos","Need a deployment strategy for new versions","Want to evaluate new code with a subset of production traffic (canary release)"],caseStudyId:null},{id:301,topic:"Networking",question:"Your company has an application running on multiple Compute Engine instances. You need to ensure that the application can communicate with an on-premises service that requires high throughput via internal IPs, while minimizing latency. What should you do?",options:{A:"Use OpenVPN to configure a VPN tunnel between the on-premises environment and Google Cloud.",B:"Configure a direct peering connection between the on-premises environment and Google Cloud.",C:"Use Cloud VPN to configure a VPN tunnel between the on-premises environment and Google Cloud.",D:"Configure a Cloud Dedicated Interconnect connection between the on-premises environment and Google Cloud."},correctAnswer:["D"],explanation:{correct:"Cloud Dedicated Interconnect provides a direct, private, physical connection between your on-premises network and Google's network. It offers the highest throughput and lowest latency of all hybrid connectivity options, making it the ideal choice for high-throughput, latency-sensitive applications.",incorrect:{A:"A self-managed OpenVPN solution is more complex and likely less performant than the managed GCP options.",B:"Direct Peering is for connecting to Google's public network edge to access public services, not for private communication with your VPC.",C:"Cloud VPN runs over the public internet and typically has higher latency and less consistent throughput than a Dedicated Interconnect connection."}},conditions:["Application on Compute Engine","Need to communicate with an on-premises service","Requires high throughput and minimal latency","Communication must be via internal IPs"],caseStudyId:null},{id:302,topic:"Compute",question:"Your company is designing its application landscape on Compute Engine. Whenever a zonal outage occurs, the application should be restored in another zone as quickly as possible with the latest application data. You need to design the solution to meet this requirement. What should you do?",options:{A:"Create a snapshot schedule for the disk containing the application data. Whenever a zonal outage occurs, use the latest snapshot to restore the disk in the same zone.",B:"Configure the Compute Engine instances with an instance template for the application, and use a regional persistent disk for the application data. Whenever a zonal outage occurs, use the instance template to spin up the application in another zone in the same region. Use the regional persistent disk for the application data.",C:"Create a snapshot schedule for the disk containing the application data. Whenever a zonal outage occurs, use the latest snapshot to restore the disk in another zone within the same region.",D:"Configure the Compute Engine instances with an instance template for the application, and use a regional persistent disk for the application data. Whenever a zonal outage occurs, use the instance template to spin up the application in another region. Use the regional persistent disk for the application data."},correctAnswer:["B"],explanation:{correct:"This describes a robust high-availability architecture. Regional persistent disks synchronously replicate data across two zones in a region. If a zonal outage occurs, you can quickly launch a new instance from an instance template in the other zone and attach the same regional persistent disk, which contains the latest data. This provides a very low Recovery Point Objective (RPO) and a fast Recovery Time Objective (RTO).",incorrect:{A:"Restoring in the *same* zone is not possible if that zone has an outage.",C:"Restoring from a snapshot would result in data loss up to the time of the last snapshot, and the recovery process would be slower than simply attaching an already-replicated regional disk.",D:"The question asks for recovery in another *zone*, not another *region*. Regional disks do not replicate across regions."}},conditions:["Application on Compute Engine","In case of a zonal outage, application must be restored in another zone","Recovery must be as quick as possible with the latest data"],caseStudyId:null},{id:303,topic:"Compute",question:"You have developed a non-critical update to your application that is running in a managed instance group, and have created a new instance template with the update that you want to release. To prevent any possible impact to the application, you dont want to update any running instances. You want any new instances that are created by the managed instance group to contain the new update. What should you do?",options:{A:"Start a new rolling restart operation.",B:"Start a new rolling replace operation.",C:"Start a new rolling update. Select the Proactive update mode.",D:"Start a new rolling update. Select the Opportunistic update mode."},correctAnswer:["D"],explanation:{correct:"The 'Opportunistic' update mode for a Managed Instance Group (MIG) does exactly this. It applies the new instance template to the MIG but does not actively replace any existing instances. The new template will only be used when new instances are created, either through manual resizing or by the autoscaler scaling out. This prevents any disruption to currently running instances.",incorrect:{A:"A rolling restart would restart existing instances, which you want to avoid.",B:"A rolling replace would replace existing instances, which you want to avoid.",C:"The 'Proactive' update mode actively replaces old instances with new ones, which is the opposite of the desired behavior."}},conditions:["Application running in a Managed Instance Group (MIG)","A new instance template with a non-critical update is ready","Do not want to update any currently running instances","Want new instances (from scale-out) to use the new template"],caseStudyId:null},{id:304,topic:"Security",question:"Your company has sensitive data in Cloud Storage buckets. Data analysts have Identity Access Management (IAM) permission to read the buckets. You want to prevent data analysts from retrieving the data in buckets from outside the office network. What should you do?",options:{A:"1. Create a VPC Service Controls perimeter that includes the projects with the buckets. 2. Create an access level with the CIDR of the office network.",B:"1. Create a firewall rule for all instances in the Virtual Private Cloud (VPC) network for source range. 2. Use the Classless Inter-domain Routing (CIDR) of the office network.",C:"1. Create a Cloud Function to remove IAM permissions from the buckets, and another Cloud Function to add IAM permissions to the buckets. 2. Schedule the Cloud Functions with Cloud Scheduler to add permissions at the start of business and remove permissions at the end of business.",D:"1. Create a Cloud VPN for the office network. 2. Configure Private Google Access for on-premises hosts."},correctAnswer:["A"],explanation:{correct:"VPC Service Controls are designed to prevent data exfiltration by creating a service perimeter. By placing the projects containing the Cloud Storage buckets inside this perimeter and defining an access level that only allows requests from the office network's IP range, you can enforce that even users with valid IAM permissions can only access the data when they are on the office network.",incorrect:{B:"VPC firewall rules apply to Compute Engine instances, not directly to access to Cloud Storage APIs.",C:"This is a brittle, time-based solution that doesn't restrict access based on network location.",D:"A VPN provides a secure path from the office network but does not *prevent* access from other locations over the public internet."}},conditions:["Data analysts have IAM permission to read sensitive data in Cloud Storage","Need to prevent them from accessing this data from outside the office network"],caseStudyId:null},{id:305,topic:"Compute",question:"You need to migrate Hadoop jobs for your companys Data Science team without modifying the underlying infrastructure. You want to minimize costs and infrastructure management effort. What should you do?",options:{A:"Create a Dataproc cluster using standard worker instances.",B:"Create a Dataproc cluster using preemptible worker instances.",C:"Manually deploy a Hadoop cluster on Compute Engine using standard instances.",D:"Manually deploy a Hadoop cluster on Compute Engine using preemptible instances."},correctAnswer:["B"],explanation:{correct:"Cloud Dataproc is the managed service for Hadoop/Spark, which minimizes infrastructure management. Hadoop jobs are often fault-tolerant batch processes. Using preemptible (now Spot) VMs for worker nodes in a Dataproc cluster is a key best practice for minimizing costs for such fault-tolerant workloads, as they offer significant discounts over standard instances.",incorrect:{A:"Using standard worker instances is more expensive than using preemptible instances.",C:"Manually deploying Hadoop on Compute Engine requires significant infrastructure management effort, violating a key requirement.",D:"This is also incorrect for the same reason as C."}},conditions:["Migrate Hadoop jobs","Without modifying the jobs themselves","Minimize costs and infrastructure management effort"],caseStudyId:null},{id:306,topic:"Networking",question:"Your company has just acquired another company, and you have been asked to integrate their existing Google Cloud environment into your companys data center. Upon investigation, you discover that some of the RFC 1918 IP ranges being used in the new companys Virtual Private Cloud (VPC) overlap with your data center IP space. What should you do to enable connectivity and make sure that there are no routing conflicts when connectivity is established?",options:{A:"Create a Cloud VPN connection from the new VPC to the data center, create a Cloud Router, and apply new IP addresses so there is no overlapping IP space.",B:"Create a Cloud VPN connection from the new VPC to the data center, and create a Cloud NAT instance to perform NAT on the overlapping IP space.",C:"Create a Cloud VPN connection from the new VPC to the data center, create a Cloud Router, and apply a custom route advertisement to block the overlapping IP space.",D:"Create a Cloud VPN connection from the new VPC to the data center, and apply a firewall rule that blocks the overlapping IP space."},correctAnswer:["A"],explanation:{correct:"The only way to resolve IP address range overlaps between two networks that need to be connected is to re-IP one of the networks. You cannot establish proper routing via VPN or Interconnect if the CIDR ranges conflict. The correct, though potentially difficult, solution is to plan and execute a re-addressing of the overlapping subnets in the acquired company's VPC before establishing the connection.",incorrect:{B:"Cloud NAT is for outbound internet access for instances without public IPs; it is not a solution for resolving IP conflicts in a hybrid connection.",C:"Blocking the advertisement of the overlapping routes would prevent connectivity for those subnets, not resolve the conflict to enable connectivity.",D:"A firewall rule operates at Layer 4 and cannot resolve a Layer 3 routing conflict caused by overlapping IP ranges."}},conditions:["Integrate an acquired company's GCP environment with an on-premises data center","Some RFC 1918 IP ranges in the GCP VPC overlap with the on-premises IP space","Need to enable connectivity without routing conflicts"],caseStudyId:null},{id:307,topic:"Compute",question:"Your customer is moving their corporate applications to Google Cloud Platform. The security team wants detailed visibility of all projects in the organization. You provision the Google Cloud Resource Manager and set up yourself as the org admin. What Google Cloud Identity and Access Management (Cloud IAM) roles should you give to the security team'?",options:{A:"Org viewer, project owner",B:"Org viewer, project viewer",C:"Org admin, project browser",D:"Project owner, network admin"},correctAnswer:["B"],explanation:{correct:"To provide read-only visibility across the entire organization, you should grant roles at the organization level. The `Organization Viewer` role allows the team to see the resource hierarchy (folders, projects). The `Project Viewer` role, when granted at the organization level, is inherited by all projects and allows the team to view the resources within each project. This combination provides comprehensive visibility without granting any modification rights, adhering to the principle of least privilege.",incorrect:{A:"Project Owner is an administrative role and grants far too much permission for a visibility requirement.",C:"Org Admin is the highest-level administrative role and is highly privileged. Project Browser only allows listing projects, not viewing their contents.",D:"Project Owner and Network Admin are both administrative roles that are inappropriate for a read-only visibility requirement."}},conditions:["Security team needs detailed visibility of all projects in the organization"],caseStudyId:null},{id:308,topic:"App Engine",question:"Your customer is receiving reports that their recently updated Google App Engine application is taking approximately 30 seconds to load for some of their users. This behavior was not reported before the update. What strategy should you take?",options:{A:"Work with your ISP to diagnose the problem.",B:"Open a support ticket to ask for network capture and flow data to diagnose the problem, then roll back your application.",C:"Roll back to an earlier known good release initially, then use Stackdriver Trace and logging to diagnose the problem in a development/test/staging environment.",D:"Roll back to an earlier known good release, then push the release again at a quieter period to investigate. Then use Stackdriver Trace and logging to diagnose the problem."},correctAnswer:["C"],explanation:{correct:"This follows the best practices for incident response. The immediate priority is to restore service for users, which is achieved by rolling back to the previous stable version. After service is restored, the problematic version can be safely investigated in a non-production environment using tools like Cloud Trace (for latency analysis) and Cloud Logging (for errors) to find the root cause without impacting users.",incorrect:{A:"Since the issue started immediately after a code update, it is highly likely an application issue, not an ISP problem.",B:"Rolling back should be the first step to mitigate user impact, not the last. A support ticket is a good step, but initial diagnosis should be performed by the team.",D:"Pushing a known-bad release back to production, even during a quiet period, is risky and unnecessary. The issue should be diagnosed in a proper staging/test environment."}},conditions:["App Engine application recently updated","New version has high load times (~30s)","Need to resolve the issue"],caseStudyId:null},{id:309,topic:"Data Analytics",question:"Your company has a Google Cloud project that uses BlgQuery for data warehousing There are some tables that contain personally identifiable information (PI!) Only the compliance team may access the PH. The other information in the tables must be available to the data science team. You want to minimize cost and the time it takes to assign appropriate access to the tables What should you do?",options:{A:"1. From the dataset where you have the source data, create views of tables that you want to share, excluding PII. 2. Assign an appropriate project-level IAM role to the members of the data science team. 3. Assign access controls to the dataset that contains the view",B:"1. From the dataset where you have the source data, create materialized views of tables that you want to share, excluding PII. 2. Assign an appropriate project-level IAM role to the members of the data science team. 3. Assign access controls to the dataset that contains the view.",C:"1. Create a dataset for the data science team. 2. Create views of tables that you want to share, excluding PII. 3. Assign an appropriate project-level IAM role to the members of the data science team. 4. Assign access controls to the dataset that contains the view. 5. Authorize the view to access the source dataset",D:"1. Create a dataset for the data science team. 2. Create materialized views of tables that you want to share, excluding PII. 3. Assign an appropriate project-level IAM role to the members of the data science team. 4. Assign access controls to the dataset that contains the view. 5. Authorize the view to access the source dataset"},correctAnswer:["C"],explanation:{correct:"This describes the best practice for column-level security using authorized views. Creating a separate dataset for the data science team provides a clean separation of concerns. The views in this dataset select only the non-PII columns. Authorizing the view (not the users) to access the source data means the data science team can query the view without needing any permissions on the sensitive source dataset, perfectly enforcing the security requirement. Since views don't store data, this is the most cost-effective solution.",incorrect:{A:"This approach is less secure because granting a project-level IAM role might inadvertently grant access to the source dataset. The authorized view pattern is more explicit and secure.",B:"Materialized views store data, which would duplicate the non-PII data and incur unnecessary storage costs, violating the cost-minimization requirement.",D:"This is incorrect for the same reason as B; materialized views incur extra storage costs."}},conditions:["BQ tables with PII","Compliance team: full access","Data science team: non-PII access only","Minimize cost and setup time"],caseStudyId:null},{id:310,topic:"IAM & Security",question:"How can you enable users defined in Microsoft Active Directory to access GCP resources using their on-premises usernames and passwords?",options:{A:"Use the Password Sync tool to replicate the passwords into GCP.",B:"Use Google Cloud Directory Sync (GCDS) to synchronize users with Cloud Identity and configure single sign-on (SSO) via SAML 2.0.",C:"Provision users in Cloud Identity and require users to set their passwords to match their on-premises ones.",D:"Configure Cloud Identity to authenticate directly against Active Directory using the LDAP protocol."},correctAnswer:["B"],explanation:{correct:"This describes the standard and secure identity federation pattern. GCDS provisions user identities (without passwords) into Cloud Identity. SAML 2.0 SSO is then configured to redirect authentication requests to the on-premises Active Directory (via ADFS or another IdP). This keeps AD as the single source of truth for authentication and avoids storing passwords in the cloud.",incorrect:{A:"Password Sync is generally considered less secure than federation because it involves replicating password hashes to the cloud.",C:"This is a manual, insecure, and unmanageable process that defeats the purpose of single sign-on.",D:"Cloud Identity does not use LDAP to directly authenticate users against an on-premises AD for GCP resource access; SAML is the standard protocol for this federation."}},conditions:["Enable AD users to access GCP resources","Use on-premises usernames/passwords"],caseStudyId:null},{id:311,topic:"Data Analytics",question:"For this question, refer to the Mountkirk Games case study. You need to optimize batch file transfers into Cloud Storage for Mountkirk Games new Google Cloud solution. The batch files contain game statistics that need to be staged in Cloud Storage and be processed by an extract transform load (ETL) tool. What should you do?",options:{A:"Use gsutil to batch move files in sequence.",B:"Use gsutil to batch copy the files in parallel.",C:"Use gsutil to extract the files as the first part of ETL.",D:"Use gsutil to load the files as the last part of ETL."},correctAnswer:["B"],explanation:{correct:"To optimize the transfer of multiple files, using the `-m` flag with `gsutil cp` enables parallel (multi-threaded/multi-process) copying. This significantly speeds up the upload of a batch of files compared to copying them one by one in sequence.",incorrect:{A:"Copying files in sequence is slow and does not optimize the transfer.",C:"`gsutil` is a tool for interacting with Cloud Storage (copying, moving, listing files), not an ETL tool for extracting data from files.",D:"This is incorrect for the same reason as C."}},conditions:["Optimize batch file transfers into Cloud Storage","Files contain game statistics to be staged for an ETL tool"],caseStudyId:"cs_mountkirk_main"},{id:312,topic:"DevOps",question:"Your organization is planning a disaster recovery (DR) strategy. Your stakeholders require a recovery time objective (RTO) of 0 and a recovery point objective (RPO) of 0 for zone outage. They require an RTO of 4 hours and an RPO of 1 hour for a regional outage. Your application consists of a web application and a backend MySQL database. You need the most efficient solution to meet your recovery KPIs. What should you do?",options:{A:"Use a global external Application Load Balancer. Deploy the web application as Compute Engine managed instance groups (MIG) in two regions, us-west and us-east. Configure the load balancer to use both backends. Use Cloud SQL with high availability (HA) enabled in us-east and a cross-region replica in us-west.",B:"Use a global external Application Load Balancer. Deploy the web application as Compute Engine managed instance groups (MIG) in two regions, us-west and us-east. Configure the load balancer to the us-east backend. Use Cloud SQL with high availability (HA) enabled in us-east and a cross-region replica in us-west. Manually promote the us-west Cloud SQL instance and change the load balancer backend to us-west.",C:"Use a global external Application Load Balancer. Deploy the web application as Compute Engine managed instance groups (MIG) in two regions, us-west and us-east. Configure the load balancer to use both backends. Use Cloud SQL with high availability (HA) enabled in us-east and back up the database every hour to a multi-region Cloud Storage bucket. Restore the data to a Cloud SQL database in us-west if there is a failure.",D:"Use a global external Application Load Balancer. Deploy the web application as Compute Engine managed instance groups (MIG) in two regions, us-west and us-east. Configure the load balancer to use both backends. Use Cloud SQL with high availability (HA) enabled in us-east and back up the database every hour to a multi-region Cloud Storage bucket. Restore the data to a Cloud SQL database in us-west if there is a failure and change the load balancer backend to us-west."},correctAnswer:["A"],explanation:{correct:"This architecture correctly addresses both zonal and regional failure requirements. For zonal failure (RTO/RPO=0), the regional MIGs (which span zones) and the Cloud SQL HA configuration (with a standby in a different zone) provide immediate, automatic failover. For regional failure (RTO 4h/RPO 1h), the multi-region MIGs behind a global load balancer handle the web tier, and the Cloud SQL cross-region replica can be promoted to become the new primary, meeting the RPO/RTO targets.",incorrect:{B:"Manually promoting the database and changing the load balancer backend would likely exceed the 4-hour RTO.",C:"Restoring a database from a Cloud Storage backup is a much slower process than promoting a replica, and it would likely not meet the 4-hour RTO. The RPO would also be up to 1 hour.",D:"This has the same RTO/RPO issues as option C."}},conditions:["RTO 0 / RPO 0 for a zonal outage","RTO 4 hours / RPO 1 hour for a regional outage","Application is a web app with a MySQL backend"],caseStudyId:null},{id:313,topic:"Application Modernization",question:"Cymbal Direct has created a proof of concept for a social integration service that highlights images of its products from social media. The proof of concept is a monolithic application running on a single SuSE Linux virtual machine (VM). The current version requires increasing the VM's CPU and RAM in order to scale. You would like to refactor the VM so that you can scale out instead of scaling up. What should you do?",options:{A:"Move the existing codebase and VM provisioning scripts to git, and attach external persistent volumes to the VMs.",B:"Make sure that the application declares any dependent requirements in a requirements.txt or equivalent statement so that they can be referenced in a startup script. Specify the startup script in a managed instance group template, and use an autoscaling policy.",C:"Make sure that the application declares any dependent requirements in a requirements.txt or equivalent statement so that they can be referenced in a startup script, and attach external persistent volumes to the VMs.",D:"Use containers instead of VMs, and use a GKE autoscaling deployment."},correctAnswer:["D"],explanation:{correct:"Refactoring to 'scale out' (horizontal scaling) instead of 'scaling up' (vertical scaling) is a primary benefit of containerization and orchestration. By containerizing the monolithic application and deploying it to Google Kubernetes Engine (GKE) as an autoscaling Deployment, GKE can automatically add or remove application instances (Pods) based on load, which is the definition of scaling out.",incorrect:{A:"This describes version control and using persistent disks but doesn't fundamentally change the scaling model from scaling up.",B:"This describes scaling out VMs using a Managed Instance Group, which is a valid approach. However, containerizing for GKE (Option D) is a more modern and often more efficient refactoring strategy for scaling out microservices or monolithic applications.",C:"This option is incomplete and doesn't describe a full scaling solution."}},conditions:["Monolithic application on a single VM","Currently scales by increasing VM size (scaling up)","Need to refactor to enable horizontal scaling (scaling out)"],caseStudyId:"CS001"},{id:314,topic:"Migration",question:"You are working in a mixed environment of VMs and Kubernetes. Some of your resources are on-premises, and some are in Google Cloud. Using containers as a part of your CI/CD pipeline has sped up releases significantly. You want to start migrating some of those VMs to containers so you can get similar benefits. You want to automate the migration process where possible. What should you do?",options:{A:"Manually create a GKE cluster, and then use Migrate to Containers to set up the cluster, import VMs, and convert them to containers.",B:"Use Migrate to Containers to automate the creation of Compute Engine instances to import VMs and convert them to containers.",C:"Manually create a GKE cluster. Use Cloud Build to import VMs and convert them to containers.",D:"Use Migrate for Compute Engine to import VMs and convert them to containers."},correctAnswer:["A"],explanation:{correct:"Migrate to Containers (now part of Google Cloud Migrate) is the purpose-built tool for modernizing existing VM-based applications into containers that can run on GKE. The process involves setting up a GKE cluster (often a dedicated 'processing' cluster) and then using the Migrate to Containers tools to analyze the source VM and automate its conversion into a container image and Kubernetes deployment artifacts.",incorrect:{B:"Migrate to Containers targets GKE or Cloud Run, not just creating new Compute Engine instances.",C:"Cloud Build is a CI/CD service for building artifacts from source code; it is not a tool for migrating VMs to containers.",D:"Migrate for Compute Engine is for migrating VMs to other VMs (a lift-and-shift), not for converting VMs into containers."}},conditions:["Mixed environment of VMs and Kubernetes","Goal is to migrate VMs to containers","Want to automate the migration process"],caseStudyId:null},{id:315,topic:"API Management",question:"Cymbal Direct wants to allow partners to make orders programmatically, without having to speak on the phone with an agent. What should you consider when designing the API?",options:{A:"The API backend should be loosely coupled. Clients should not be required to know too many details of the services they use. REST APIs using gRPC should be used for all external APIs.",B:"The API backend should be tightly coupled. Clients should know a significant amount about the services they use. REST APIs using gRPC should be used for all external APIs.",C:"The API backend should be loosely coupled. Clients should not be required to know too many details of the services they use. For REST APIs, HTTP(S) is the most common protocol.",D:"The API backend should be tightly coupled. Clients should know a significant amount about the services they use. For REST APIs, HTTP(S) is the most common protocol used."},correctAnswer:["C"],explanation:{correct:"This option lists key principles of good API design. An API should be loosely coupled so that the backend implementation can evolve without breaking client applications. It should abstract away internal complexity. For external partner-facing APIs, REST over HTTP(S) is the most common, widely understood, and interoperable standard.",incorrect:{A:"The phrase 'REST APIs using gRPC' is a contradiction. REST typically uses HTTP(S) as its transport, while gRPC uses HTTP/2 and is a different RPC framework.",B:"Tightly coupled APIs are brittle and difficult to maintain. This option also has the REST/gRPC contradiction.",D:"Tightly coupled APIs are a poor design choice."}},conditions:["Designing a programmatic API for partners to place orders"],caseStudyId:"CS001"},{id:316,topic:"IAM & Security",question:"Your client created an Identity and Access Management (IAM) resource hierarchy with Google Cloud when the company was a startup. Your client has grown and now has multiple departments and teams. You want to recommend a resource hierarchy that follows Google-recommended practices. What should you do?",options:{A:"Keep all resources in one project, and use a flat resource hierarchy to reduce complexity and simplify management.",B:"Keep all resources in one project, but change the resource hierarchy to reflect company organization.",C:"Use a flat resource hierarchy and multiple projects with established trust boundaries.",D:"Use multiple projects with established trust boundaries, and change the resource hierarchy to reflect company organization."},correctAnswer:["D"],explanation:{correct:"As a company grows, a flat hierarchy becomes unmanageable. The Google-recommended best practice is to use multiple projects to establish clear boundaries for billing, quotas, and IAM. These projects should then be organized under Folders that reflect the company's organizational structure (e.g., departments, teams). This allows for scalable and clear management of policies and resources.",incorrect:{A:"A single project for a grown company is not scalable and makes it difficult to manage permissions and costs.",B:"You cannot change the resource hierarchy *within* a single project. The hierarchy is Org -> Folders -> Projects.",C:"A flat hierarchy (no folders) is less organized and makes it harder to apply policies to groups of related projects."}},conditions:["Company has grown from a startup to having multiple departments","Need to recommend a resource hierarchy that follows Google best practices"],caseStudyId:null},{id:317,topic:"IAM & Security",question:"Cymbal Direct's social media app must run in a separate project from its APIs and web store. You want to use Identity and Access Management (IAM) to ensure a secure environment. How should you set up IAM?",options:{A:"Use separate service accounts for each component (social media app, APIs, and web store) with basic roles to grant access.",B:"Use one service account for all components (social media app, APIs, and web store) with basic roles to grant access.",C:"Use separate service accounts for each component (social media app, APIs, and web store) with predefined or custom roles to grant access.",D:"Use one service account for all components (social media app, APIs, and web store) with predefined or custom roles to grant access."},correctAnswer:["C"],explanation:{correct:"This follows the principle of least privilege. Each component should have its own identity (a separate service account) with the minimum set of permissions needed for its function. Using granular predefined or custom roles ensures that you are not granting excessive permissions, which is what happens when using broad basic roles like Owner or Editor.",incorrect:{A:"Using basic roles (Owner, Editor, Viewer) violates the principle of least privilege.",B:"Using a single service account and basic roles is the worst option from a security perspective.",D:"Using one service account for all components is also a violation of least privilege, as a compromise of one component would grant an attacker the permissions of all components."}},conditions:["Application components are in separate projects","Need to set up a secure IAM environment"],caseStudyId:"CS001"},{id:318,topic:"Case Study Analysis",question:"Michael is the owner/operator of 'Zneeks,' a retail shoe store that caters to sneaker aficionados. He regularly works with customers who order small batches of custom shoes. Michael is interested in using Cymbal Direct to manufacture and ship custom batches of shoes to these customers. Reasonably tech-savvy but not a developer, Michael likes using Cymbal Direct's partner purchase portal but wants the process to be easy. What is an example of a user story that could describe Michael's persona?",options:{A:"As a shoe retailer, Michael wants to send Cymbal Direct custom purchase orders so that batches of custom shoes are sent to his customers.",B:"Michael is a tech-savvy owner/operator of a small business.",C:"Zneeks is a retail shoe store that caters to sneaker aficionados.",D:"Michael is reasonably tech-savvy but needs Cymbal Direct's partner purchase portal to be easy."},correctAnswer:["A"],explanation:{correct:"A user story follows the format: 'As a [role], I want [goal] so that [benefit].' This option perfectly fits that structure, clearly defining Michael's role, what he wants to achieve with the system, and the desired outcome.",incorrect:{B:"This describes a characteristic of the persona, not a user story.",C:"This describes the business, not a user story.",D:"This describes a need or constraint of the persona, but it is not framed as an actionable user story."}},conditions:["Describe a persona (Michael, the shoe retailer) as a user story"],caseStudyId:"CS001"},{id:319,topic:"IAM & Security",question:"Cymbal Direct has an application running on a Compute Engine instance. You need to give the application access to several Google Cloud services. You do not want to keep any credentials on the VM instance itself. What should you do?",options:{A:"Create a service account for each of the services the VM needs to access. Associate the service accounts with the Compute Engine instance.",B:"Create a service account and assign it the project owner role, which enables access to any needed service.",C:"Create a service account for the instance. Use Access scopes to enable access to the required services.",D:"Create a service account with one or more predefined or custom roles, which give access to the required services. Associate this service account with the Compute Engine instance."},correctAnswer:["D"],explanation:{correct:"This is the best practice. You create a single identity (the service account) for the application/instance. You then grant that identity the specific, granular permissions (roles) it needs to access other services. By associating this service account with the instance, the application can get credentials from the metadata server without storing keys on the VM.",incorrect:{A:"You only need one service account for the instance's identity. Granting it multiple roles is the correct pattern. An instance can only be associated with one service account at a time.",B:"Assigning the project owner role is a major violation of the principle of least privilege.",C:"Access scopes are a legacy mechanism that provides broad, service-level permissions. IAM roles are preferred for providing more granular, resource-level permissions."}},conditions:["Application on a Compute Engine instance needs access to other GCP services","Do not want to store credentials on the VM"],caseStudyId:"CS001"},{id:320,topic:"IAM & Security",question:"Cymbal Direct wants to use Identity and Access Management (IAM) to allow employees to have access to Google Cloud resources and services based on their job roles. Several employees are project managers and want to have some level of access to see what has been deployed. The security team wants to ensure that securing the environment and managing resources is simple so that it will scale. What approach should you use?",options:{A:"Grant access by assigning custom roles to groups. Use multiple groups for better control. Give access as low in the hierarchy as possible to prevent the inheritance of too many abilities from a higher level.",B:"Grant access by assigning predefined roles to groups. Use multiple groups for better control. Give access as low in the hierarchy as possible to prevent the inheritance of too many abilities from a higher level.",C:"Give access directly to each individual for more granular control. Give access as low in the hierarchy as possible to prevent the inheritance of too many abilities from a higher level.",D:"Grant access by assigning predefined roles to groups. Use multiple groups for better control. Make sure you give out access to all the children in a hierarchy under the level needed, because child resources will not automatically inherit abilities."},correctAnswer:["B"],explanation:{correct:"This option combines several IAM best practices. Using groups for role-based access control is scalable and easy to manage. Using predefined roles is preferred for simplicity unless a specific set of permissions is not available. Granting permissions at the lowest possible level in the resource hierarchy adheres to the principle of least privilege.",incorrect:{A:"You should prefer predefined roles over custom roles for simplicity and maintainability, only creating custom roles when necessary.",C:"Granting access to individuals is not scalable and becomes difficult to manage.",D:"This statement is factually incorrect. IAM permissions *are* inherited down the resource hierarchy."}},conditions:["Implement role-based access control for employees","Solution must be simple to manage and scalable"],caseStudyId:"CS001"},{id:321,topic:"Serverless",question:"Cymbal Direct is experiencing success using Google Cloud and you want to leverage tools to make your solutions more efficient. Erik, one of the original web developers, currently adds new products to your application manually. Erik has many responsibilities and requires a long lead time to add new products. You need to create a Cloud Run functions application to let Cymbal Direct employees add new products instead of waiting for Erik. However, you want to make sure that only authorized employees can use the application. What should you do?",options:{A:"Set up Cloud VPN between the corporate network and the Google Cloud project's VPC network. Allow users to connect to the Cloud Run functions instance.",B:"Use Google Cloud Armor to restrict access to the corporate network's external IP address. Configure firewall rules to allow only HTTP(S) access.",C:"Create a Google group and add authorized employees to it. Configure Identity-Aware Proxy (IAP) to the Cloud Run functions application as an HTTP-resource. Add the group as a principle with the role 'Project Owner.'",D:"Create a Google group and add authorized employees to it. Configure Identity-Aware Proxy (IAP) to the Cloud Run functions application as an HTTP-resource. Add the group as a principle with the role 'IAP-secured Web App User.'"},correctAnswer:["D"],explanation:{correct:"Identity-Aware Proxy (IAP) is the ideal service for securing web applications and services like Cloud Run and controlling access based on user identity. By creating a Google Group for authorized employees and granting that group the `IAP-secured Web App User` role, you ensure that only authenticated and authorized users can access the application, without needing a VPN.",incorrect:{A:"A VPN provides network-level access, but IAP provides a more granular, identity-based, zero-trust approach at the application level.",B:"Cloud Armor is primarily a WAF for protecting against web attacks and DDoS; it is not the primary tool for user authentication and authorization.",C:"The `Project Owner` role is extremely permissive and completely unnecessary for simply accessing an IAP-protected application. This violates the principle of least privilege."}},conditions:["Create a Cloud Run application for internal employees","Ensure only authorized employees can use the application"],caseStudyId:"CS001"},{id:322,topic:"IAM & Security",question:"You've recently created an internal Cloud Run application for developers in your organization. The application lets developers clone production Cloud SQL databases into a project specifically created to test code and deployments. Your previous process was to export a database to a Cloud Storage bucket, and then import the SQL dump into a legacy on-premises testing environment with connectivity to Google Cloud via Cloud VPN. Management wants to incentivize using the new process with Cloud SQL for rapid testing and track how frequently rapid testing occurs. How can you ensure that the developers use the new process?",options:{A:"Use an ACL on the Cloud Storage bucket. Create a read-only group that only has viewer privileges, and ensure that the developers are in that group.",B:"Leave the ACLs on the Cloud Storage bucket as-is. Disable Cloud VPN, and have developers use Identity-Aware Proxy (IAP) to connect. Create an organization policy to enforce public access protection.",C:"Use predefined roles to restrict access to what the developers are allowed to do. Create a group for the developers, and associate the group with the Cloud SQL Viewer role. Remove the 'cloudsql.instances.export' ability from the role.",D:"Create a custom role to restrict access to what developers are allowed to do. Create a group for the developers, and associate the group with your custom role. Ensure that the custom role does not have 'cloudsql.instances.export.'"},correctAnswer:["D"],explanation:{correct:"The best way to enforce a new process is to technically prevent the old one. The old process relies on developers being able to export Cloud SQL instances. By creating a custom IAM role that grants them the permissions they need for their daily work *but explicitly omits* the `cloudsql.instances.export` permission, you can prevent them from performing the old workflow while still allowing them to use the new Cloud Run application.",incorrect:{A:"Controlling access to the GCS bucket for the *dump file* doesn't prevent them from creating the dump in the first place.",B:"Disabling the VPN might have unintended consequences and doesn't directly address the export permission.",C:"You cannot modify predefined IAM roles. To create a specific set of permissions, you must create a custom role."}},conditions:["Incentivize a new process (using a Cloud Run app for DB cloning)","Discourage the old process (manual DB export)","Track usage of the new process"],caseStudyId:null},{id:323,topic:"Migration",question:"You are implementing a disaster recovery plan for the cloud version of your drone solution. Sending videos to the pilots is crucial from an operational perspective. What design pattern should you choose for this part of your architecture?",options:{A:"Hot with a low recovery time objective (RTO)",B:"Warm with a high recovery time objective (RTO)",C:"Cold with a low recovery time objective (RTO)",D:"Hot with a high recovery time objective (RTO)"},correctAnswer:["A"],explanation:{correct:"A 'Hot' disaster recovery pattern means having a fully scaled, active-active or active-standby environment ready to take over traffic immediately. This pattern is chosen for critical services because it provides the lowest possible Recovery Time Objective (RTO), meaning the service can be recovered very quickly after a disaster. Since video delivery to pilots is crucial, a low RTO is essential.",incorrect:{B:"Warm DR has a higher RTO than Hot, and the combination 'Warm with a high RTO' is consistent but not suitable for a critical service. 'Hot' implies low RTO.",C:"This is a contradictory statement. A 'Cold' pattern (where infrastructure must be provisioned from scratch) inherently has a very high RTO, not a low one.",D:"This is a contradictory statement. A 'Hot' pattern is designed to achieve a low RTO, not a high one."}},conditions:["Implement a disaster recovery plan","The service (video delivery to pilots) is crucial","Need to choose a DR design pattern"],caseStudyId:"CS001"},{id:324,topic:"Operations",question:"The number of requests received by your application is nearing the maximum specified in your design. You want to limit the number of incoming requests until the system can handle the workload. What design pattern does this situation describe?",options:{A:"Applying a circuit breaker",B:"Applying exponential backoff",C:"Increasing jitter",D:"Applying graceful degradation"},correctAnswer:["D"],explanation:{correct:"Graceful degradation is the practice of maintaining essential application functionality even when parts of the system are failing or overloaded. Actively limiting or throttling incoming requests (load shedding) to protect the core system from being completely overwhelmed is a form of graceful degradation. The system remains partially available instead of failing entirely.",incorrect:{A:"A circuit breaker is a pattern where a client stops making requests to a downstream service that it detects is failing, preventing cascading failures. It's a client-side pattern, whereas this question describes a server-side action.",B:"Exponential backoff is a client-side retry strategy used after a request fails.",C:"Jitter is a randomization technique added to exponential backoff to prevent clients from retrying in synchronized waves."}},conditions:["Application is nearing its maximum capacity","Need to limit incoming requests to prevent total failure"],caseStudyId:null},{id:325,topic:"Operations",question:"Cymbal Direct's warehouse and inventory system was written in Java. The system uses a microservices architecture in GKE and is instrumented with Zipkin. Seemingly at random, a request will be 5-10 times slower than others. The development team tried to reproduce the problem in testing, but failed to determine the cause of the issue. What should you do?",options:{A:"Create metrics in Cloud Monitoring for your microservices to test whether they are intermittently unavailable or slow to respond to HTTPS requests. Use Cloud Profiler to determine which functions/methods in your application's code use the most system resources. Use Cloud Trace to identify slow requests and determine which microservices/calls take the most time to respond.",B:"Create metrics in Cloud Monitoring for your microservices to test whether they are intermittently unavailable or slow to respond to HTTPS requests. Use Cloud Trace to determine which functions/methods in your application's code use the most system resources. Use Cloud Profiler to identify slow requests and determine which microservices/calls take the most time to respond.",C:"Use Error Reporting to test whether your microservices are intermittently unavailable or slow to respond to HTTPS requests. Use Cloud Profiler to determine which functions/methods in your application's code use the most system resources. Use Cloud Trace to identify slow requests and determine which microservices/calls take the most time to respond.",D:"Use Error Reporting to test whether your microservices are intermittently unavailable or slow to respond to HTTPS requests. Use Cloud Trace to determine which functions/methods in your application's code Use the most system resources. Use Cloud Profiler to identify slow requests and determine which microservices/calls take the most time to respond."},correctAnswer:["A"],explanation:{correct:"This option correctly assigns the right tool to the right job for diagnosing intermittent latency. Cloud Trace (which can ingest Zipkin data) is the primary tool for distributed tracing to see the end-to-end latency of a request and identify which specific microservice call is slow. Cloud Profiler is then used to dig into the code of that slow microservice to find the specific function or method causing high CPU or resource consumption. Cloud Monitoring provides the high-level metrics and alerts to detect the issue in the first place.",incorrect:{B:"This option incorrectly swaps the roles of Cloud Trace and Cloud Profiler.",C:"Error Reporting is for tracking exceptions and crashes, not for diagnosing latency issues.",D:"This is incorrect for the same reasons as B and C."}},conditions:["Java microservices application on GKE","Instrumented with Zipkin (distributed tracing)","Experiencing intermittent slow requests","Unable to reproduce in testing"],caseStudyId:"CS001"},{id:326,topic:"Operations",question:"Cymbal Direct has a new social media integration service that pulls images of its products from social media sites and displays them in a gallery of customer images on your online store. You receive an alert from Cloud Monitoring at 3:34 AM on Saturday. The store is still online, but the gallery does not appear. The CPU utilization is 30% higher than expected on the VMs running the service, which causes the managed instance group (MIG) to scale to the maximum number of instances. You verify that the issue is real by checking the site and by checking the incidents timeline. What should you do to resolve the issue?",options:{A:"Increase the maximum number of instances in the MIG and verify that this resolves the issue. Ensure that the ticket is annotated with your solution. Create a normal work ticket for the application developer with a link to the incident. Mark the incident as closed.",B:"Check the incident documentation or labels to determine the on-call contact. Appoint an incident commander, and open a chat channel, or conference call for emergency response. Investigate and resolve the issue by increasing the maximum number of instances in the MIG, and verify that this resolves the issue. Mark the incident as closed.",C:"Increase the maximum number of instances in the MIG and verify that this resolves the issue. Check the incident documentation or labels to determine the on-call contact. Appoint an incident commander, and open a chat channel, or conference call for emergency response. Investigate and resolve the root cause of the issue. Write a blameless post-mortem and identify steps to prevent the issue, to ensure a culture of continuous improvement.",D:"Verify the high CPU is not user impacting, increase the maximum number of instances in the MIG and verify that this resolves the issue."},correctAnswer:["C"],explanation:{correct:"This option describes a full, mature incident response process. It includes immediate mitigation (increasing MIG size), followed by a proper investigation involving the right people (on-call, incident commander), communication (chat/call), a focus on finding and fixing the root cause (not just the symptom), and finally, a blameless post-mortem to learn from the incident and prevent recurrence. This focus on continuous improvement is a key SRE principle.",incorrect:{A:"This only addresses the symptom (mitigation) and then closes the incident without a proper root cause analysis or post-mortem.",B:"This is better as it involves a proper incident response team, but it also stops after mitigation and doesn't include the critical root cause analysis and post-mortem steps.",D:"The problem is already user-impacting (the gallery does not appear). This option also only focuses on mitigation."}},conditions:["Alert received: high CPU, MIG at max, feature (gallery) is down","Need to follow a proper incident resolution process"],caseStudyId:"CS001"},{id:327,topic:"Security",question:"You need to adopt Site Reliability Engineering principles and increase visibility into your environment. You want to minimize management overhead and reduce noise generated by the information being collected. You also want to streamline the process of reacting to analyzing and improving your environment, and to ensure that only trusted container images are deployed to production. What should you do?",options:{A:"Adopt Google Cloud Observability to gain visibility into the environment. Use Cloud Trace for distributed tracing, Cloud Logging for logging, and Cloud Monitoring for monitoring, alerting, and dashboards. Only page the on-call contact about novel issues or events that haven't been seen before. Use GNU Privacy Guard (GPG) to check container image signatures and ensure that only signed containers are deployed.",B:"Adopt Google Cloud Observability to gain visibility into the environment. Use Cloud Trace for distributed tracing, Cloud Logging for logging, and Cloud Monitoring for monitoring, alerting, and dashboards. Page the on-call contact when issues that affect resources in the environment are detected. Use GPG to check container image signatures and ensure that only signed containers are deployed.",C:"Adopt Google Cloud Observability to gain visibility into the environment. Use Cloud Trace for distributed tracing, Cloud Logging for logging, and Cloud Monitoring for monitoring, alerting, and dashboards. Only page the on-call contact about novel issues that violate a SLO or events that haven't been seen before. Use Binary Authorization to ensure that only signed container images are deployed.",D:"Adopt Google Cloud Observability to gain visibility into the environment. Use Cloud Trace for distributed tracing, Cloud Logging for logging, and Cloud Monitoring for monitoring, alerting, and dashboards. Page the on-call contact when issues that affect resources in the environment are detected. Use Binary Authorization to ensure that only signed container images are deployed."},correctAnswer:["C"],explanation:{correct:"This option combines key SRE and security best practices. It correctly identifies the Cloud Observability suite for visibility. It advocates for intelligent alerting based on SLO violations to reduce noise, a core SRE principle. For container security, it correctly identifies Binary Authorization as the GCP-native service for enforcing that only trusted, signed images are deployed.",incorrect:{A:"Using GPG is a generic way to sign things, but Binary Authorization is the specific, integrated enforcement mechanism for GKE/Cloud Run. Also, the alerting strategy is less defined than SLO-based alerting.",B:"Paging on all issues is too noisy and against SRE principles. It also incorrectly suggests GPG instead of Binary Authorization.",D:"Paging on all issues is too noisy."}},conditions:["Adopt SRE principles","Increase visibility and reduce alert noise","Streamline incident response and improvement processes","Ensure only trusted container images are deployed to production"],caseStudyId:null},{id:328,topic:"Data Analytics",question:"For this question, refer to the Mountkirk Games case study. You need to optimize batch file transfers into Cloud Storage for Mountkirk Games new Google Cloud solution. The batch files contain game statistics that need to be staged in Cloud Storage and be processed by an extract transform load (ETL) tool. What should you do?",options:{A:"Use gsutil to batch move files in sequence.",B:"Use gsutil to batch copy the files in parallel.",C:"Use gsutil to extract the files as the first part of ETL.",D:"Use gsutil to load the files as the last part of ETL."},correctAnswer:["B"],explanation:{correct:"To optimize the transfer of multiple files, using the `-m` flag with `gsutil cp` enables parallel (multi-threaded/multi-process) copying. This significantly speeds up the upload of a batch of files compared to copying them one by one in sequence.",incorrect:{A:"Copying files in sequence is slow and does not optimize the transfer.",C:"`gsutil` is a tool for interacting with Cloud Storage (copying, moving, listing files), not an ETL tool for extracting data from files.",D:"This is incorrect for the same reason as C."}},conditions:["Optimize batch file transfers into Cloud Storage","Files contain game statistics to be staged for an ETL tool"],caseStudyId:"cs_mountkirk_main"}],Wg={quizTitle:ed,caseStudies:td,quizItems:od},Hg=Object.freeze(Object.defineProperty({__proto__:null,caseStudies:td,default:Wg,quizItems:od,quizTitle:ed},Symbol.toStringTag,{value:"Module"})),nd="Google Cloud Professional Developer Practice Questions",id=[{id:1,topic:"Storage",question:`You want to upload files from an on-premises virtual machine to Google Cloud Storage as part of a data migration. These files will be consumed by Cloud DataProc Hadoop cluster in a GCP environment.
Which command should you use?`,options:{A:"gsutil cp [LOCAL_OBJECT] gs://[DESTINATION_BUCKET_NAME]/",B:"gcloud cp [LOCAL_OBJECT] gs://[DESTINATION_BUCKET_NAME]/",C:"hadoop fs cp [LOCAL_OBJECT] gs://[DESTINATION_BUCKET_NAME]/",D:"gcloud dataproc cp [LOCAL_OBJECT] gs://[DESTINATION_BUCKET_NAME]/"},correctAnswer:["A"],explanation:"The `gsutil cp` command is the standard and recommended tool for copying files between a local machine and Google Cloud Storage. `gcloud compute scp` is used for copying files to/from Compute Engine instances, not Cloud Storage directly. `hadoop fs cp` is used within a Hadoop environment, typically with HDFS or compatible filesystems, although the Cloud Storage connector allows its use, `gsutil` is more direct for uploads from on-premises. `gcloud dataproc` commands manage Dataproc clusters and jobs, not direct file transfers to GCS.",conditions:["Upload files from on-premises VM to GCS for Dataproc consumption."],caseStudyContext:null},{id:2,topic:"Monitoring & Logging",question:`You migrated your applications to Google Cloud Platform and kept your existing monitoring platform. You now find that your notification system is too slow for time critical problems.
What should you do?`,options:{A:"Replace your entire monitoring platform with Stackdriver.",B:"Install the Stackdriver agents on your Compute Engine instances.",C:"Use Stackdriver to capture and alert on logs, then ship them to your existing platform.",D:"Migrate some traffic back to your old platform and perform AB testing on the two platforms concurrently."},correctAnswer:["C"],explanation:"The core issue is slow notifications for critical problems from the existing on-premises system, even though apps are now on GCP. Replacing the entire system (A) might be excessive if only notifications are slow. Installing agents (B) is necessary to get metrics/logs into Cloud Monitoring (formerly Stackdriver) but doesn't directly solve the notification latency issue. Migrating traffic back (D) is irrelevant to fixing monitoring notifications. Using Cloud Monitoring/Logging for fast alerting on GCP-based logs (C) addresses the critical problem directly while allowing the continued use of the existing platform for other purposes by shipping logs to it. This is the least disruptive and most targeted solution.",conditions:["Apps migrated to GCP, existing on-prem monitoring platform retained, notification system is too slow for critical issues."],caseStudyContext:null},{id:3,topic:"Databases",question:`You are planning to migrate a MySQL database to the managed Cloud SQL database for Google Cloud. You have Compute Engine virtual machine instances that will connect with this Cloud SQL instance. You do not want to whitelist IPs for the Compute Engine instances to be able to access Cloud SQL.
What should you do?`,options:{A:"Enable private IP for the Cloud SQL instance.",B:"Whitelist a project to access Cloud SQL, and add Compute Engine instances in the whitelisted project.",C:"Create a role in Cloud SQL that allows access to the database from external instances, and assign the Compute Engine instances to that role.",D:"Create a CloudSQL instance on one project. Create Compute engine instances in a different project. Create a VPN between these two projects to allow internal access to CloudSQL."},correctAnswer:["A"],explanation:"Enabling private IP for the Cloud SQL instance allows Compute Engine instances within the same VPC network to connect directly using internal IP addresses, bypassing the need for public IP whitelisting. Project whitelisting (B) isn't a standard Cloud SQL access method. Cloud SQL roles (C) control database-level permissions, not network connectivity. Setting up a VPN between projects (D) adds unnecessary complexity when private IP within the same VPC can achieve the goal.",conditions:["Migrating MySQL to Cloud SQL, GCE instances need access, avoid IP whitelisting."],caseStudyContext:null},{id:4,topic:"Networking",question:`You have deployed an HTTP(s) Load Balancer with the gcloud commands shown below. 

Health checks to port 80 on the Compute Engine virtual machine instance are failing and no traffic is sent to your instances. You want to resolve the problem.
Which commands should you run?`,options:{A:"gcloud compute instances add-access-config ${NAME}-backend-instance-1",B:"gcloud compute instances add-tags ${NAME}-backend-instance-1 --tags http-server",C:"gcloud compute firewall-rules create allow-lb --network load-balancer --allow tcp --source-ranges 130.211.0.0/22,35.191.0.0/16 --direction INGRESS",D:"gcloud compute firewall-rules create allow-lb --network load-balancer --allow tcp --destination-ranges 130.211.0.0/22,35.191.0.0/16 -direction EGRESS"},correctAnswer:["C"],explanation:"Google Cloud HTTP(S) Load Balancer health checks originate from specific IP ranges (130.211.0.0/22 and 35.191.0.0/16). Health checks are incoming (INGRESS) connections to the backend instances. Therefore, an INGRESS firewall rule is required to allow TCP traffic from these source ranges to the instances on the appropriate port (implicitly port 80 here, based on the scenario). Option A adds an external IP, not relevant. Option B adds a tag, but doesn't create the rule. Option D creates an EGRESS rule, which is incorrect.",conditions:["HTTP(s) Load Balancer deployed, health checks to port 80 failing, no traffic reaching instances."],caseStudyContext:null},{id:5,topic:"Compute",question:`Your website is deployed on Compute Engine. Your marketing team wants to test conversion rates between 3 different website designs.
Which approach should you use?`,options:{A:"Deploy the website on App Engine and use traffic splitting.",B:"Deploy the website on App Engine as three separate services.",C:"Deploy the website on Cloud Functions and use traffic splitting.",D:"Deploy the website on Cloud Functions as three separate functions."},correctAnswer:["A"],explanation:"App Engine's traffic splitting feature is specifically designed for scenarios like A/B testing or gradual rollouts. It allows routing percentages of traffic to different versions within the same service, making it easy to compare performance (like conversion rates) between designs under the same URL. Deploying as separate services (B) would typically involve different URLs. Cloud Functions (C, D) are generally for event-driven code or APIs, not typically for hosting full websites, although possible, App Engine is a better fit.",conditions:["Website currently on GCE, need A/B testing for 3 designs."],caseStudyContext:null},{id:6,topic:"Compute",question:`You need to copy directory local-scripts and all of its contents from your local workstation to a Compute Engine virtual machine instance.
Which command should you use?`,options:{A:"gsutil cp --project my-gcp-project -r ~/local-scripts/ gcp-instance-name:~/server-scripts/ --zone us-east1-b",B:"gsutil cp --project my-gcp-project -R ~/local-scripts/ gcp-instance-name:~/server-scripts/ --zone us-east1-b",C:"gcloud compute scp --project my-gcp-project --recurse ~/local-scripts/ gcp-instance-name:~/server-scripts/ --zone us-east1-b",D:"gcloud compute mv --project my-gcp-project --recurse ~/local-scripts/ gcp-instance-name:~/server-scripts/ --zone us-east1-b"},correctAnswer:["C"],explanation:"`gcloud compute scp` is the command used to securely copy files and directories between a local machine and a Compute Engine instance. The `--recurse` flag is used to copy directories and their contents. `gsutil cp` is used for Cloud Storage operations. `gcloud compute mv` does not exist; file moving on the instance would use standard Linux commands via SSH.",conditions:["Copy directory from local workstation to GCE instance."],caseStudyContext:null},{id:7,topic:"Monitoring & Logging",question:`You are deploying your application to a Compute Engine virtual machine instance with the Stackdriver Monitoring Agent installed. Your application is a unix process on the instance. You want to be alerted if the unix process has not run for at least 5 minutes. You are not able to change the application to generate metrics or logs.
Which alert condition should you configure?`,options:{A:"Uptime check",B:"Process health",C:"Metric absence",D:"Metric threshold"},correctAnswer:["B"],explanation:"Cloud Monitoring (formerly Stackdriver) provides process health monitoring via its agent. This allows you to define alerting policies based on whether a process matching a certain pattern is running or not, or if the count of matching processes crosses a threshold. Since the application cannot be modified, relying on its presence monitored by the agent (Process health) is the direct approach. Uptime checks monitor external endpoints. Metric absence/threshold require specific metrics, which the application isn't generating.",conditions:["GCE instance with Monitoring Agent, monitor specific Unix process, alert if not running for 5 mins, cannot modify app."],caseStudyContext:null},{id:8,topic:"Databases",question:`You have two tables in an ANSI-SQL compliant database with identical columns that you need to quickly combine into a single table, removing duplicate rows from the result set.
What should you do?`,options:{A:"Use the JOIN operator in SQL to combine the tables.",B:"Use nested WITH statements to combine the tables.",C:"Use the UNION operator in SQL to combine the tables.",D:"Use the UNION ALL operator in SQL to combine the tables."},correctAnswer:["C"],explanation:"The SQL `UNION` operator combines the result sets of two or more SELECT statements and removes duplicate rows. `UNION ALL` also combines result sets but includes all rows, including duplicates. `JOIN` combines rows from tables based on a related column. `WITH` statements (Common Table Expressions) help organize complex queries but don't inherently combine tables like UNION.",conditions:["Combine two SQL tables with identical columns, remove duplicates."],caseStudyContext:null},{id:9,topic:"Compute",question:`You have an application deployed in production. When a new version is deployed, some issues don't arise until the application receives traffic from users in production. You want to reduce both the impact and the number of users affected.
Which deployment strategy should you use?`,options:{A:"Blue/green deployment",B:"Canary deployment",C:"Rolling deployment",D:"Recreate deployment"},correctAnswer:["B"],explanation:"Canary deployment involves rolling out the new version to a small subset of users first. This allows monitoring for issues with minimal impact before exposing the version to the entire user base. Blue/green swaps all traffic at once after testing the new environment, affecting all users simultaneously if issues occur post-switch. Rolling updates gradually replace old instances, potentially affecting more users than an initial canary release. Recreate involves downtime.",conditions:["Deploy new version, issues appear only in production, need to reduce impact and number of users affected."],caseStudyContext:null},{id:10,topic:"Databases",question:`Your company wants to expand their users outside the United States for their popular application. The company wants to ensure 99.999% availability of the database for their application and also wants to minimize the read latency for their users across the globe.
Which two actions should they take? (Choose two.)`,options:{A:'Create a multi-regional Cloud Spanner instance with "nam-asia-eur1" configuration.',B:'Create a multi-regional Cloud Spanner instance with "nam3" configuration.',C:"Create a cluster with at least 3 Spanner nodes.",D:"Create a cluster with at least 1 Spanner node.",E:"Create a minimum of two Cloud Spanner instances in separate regions with at least one node.",F:"Create a Cloud Dataflow pipeline to replicate data across different databases."},correctAnswer:["A","C"],explanation:"Cloud Spanner multi-region configurations provide 99.999% availability. The `nam-eur-asia1` (likely intended by \"nam-asia-eur1\") configuration places replicas across North America, Europe, and Asia, minimizing read latency for global users (A). Spanner instances require compute capacity (nodes) for performance and availability; a minimum of 3 nodes is generally recommended for production multi-region instances to handle replication and load (C). 'nam3' (B) is a regional configuration. 1 node (D) is insufficient for HA/performance. Separate instances (E) don't provide the strong consistency guarantees of a single multi-region instance. Dataflow (F) is not needed for Spanner's internal replication.",conditions:["Global user base, 99.999% database availability needed, minimize global read latency."],caseStudyContext:null},{id:11,topic:"Storage",question:`You need to migrate an internal file upload API with an enforced 500-MB file size limit to App Engine.
What should you do?`,options:{A:"Use FTP to upload files.",B:"Use CPanel to upload files.",C:"Use signed URLs to upload files.",D:"Change the API to be a multipart file upload API."},correctAnswer:["C"],explanation:"App Engine has request size limitations (typically 32MB, though Flex can handle larger requests, large uploads are still problematic). Using signed URLs allows the client application to upload large files directly to Google Cloud Storage, bypassing App Engine's request limits. The App Engine application only needs to generate the signed URL. FTP/CPanel (A, B) are not relevant App Engine mechanisms. Multipart upload via the API (D) would still likely hit App Engine limits.",conditions:["Migrate file upload API (500MB limit) to App Engine."],caseStudyContext:null},{id:12,topic:"Compute",question:`You are planning to deploy your application in a Google Kubernetes Engine (GKE) cluster. The application exposes an HTTP-based health check at /healthz. You want to use this health check endpoint to determine whether traffic should be routed to the pod by the load balancer.
Which code snippet should you include in your Pod configuration?
A.
livenessProbe:
  httpGet:
    path: /healthz
    port: 80
B.
readinessProbe:
  httpGet:
    path: /healthz
    port: 80
C.
loadbalancerHealthCheck:
  httpGet:
    path: /healthz
    port: 80
D.
healthCheck:
  httpGet:
    path: /healthz
    port: 80`,options:{A:`livenessProbe:
  httpGet:
    path: /healthz
    port: 80`,B:`readinessProbe:
  httpGet:
    path: /healthz
    port: 80`,C:`loadbalancerHealthCheck:
  httpGet:
    path: /healthz
    port: 80`,D:`healthCheck:
  httpGet:
    path: /healthz
    port: 80`},correctAnswer:["B"],explanation:"Kubernetes uses readiness probes to determine if a Pod is ready to receive traffic. If the readiness probe fails, the Pod's IP address is removed from the corresponding Service's endpoints, effectively taking it out of the load balancer's rotation. Liveness probes determine if a container needs to be restarted. `loadbalancerHealthCheck` and `healthCheck` are not standard Kubernetes Pod spec fields for this purpose.",conditions:["GKE deployment, use /healthz endpoint to control LB traffic routing."],caseStudyContext:null},{id:13,topic:"Databases",question:`You are writing a single-page web application with a user-interface that communicates with a third-party API for content using XMLHttpRequest. The data displayed on the UI by the API results is less critical than other data displayed on the same web page, so it is acceptable for some requests to not have the API data displayed in the UI. However, calls made to the API should not delay rendering of other parts of the user interface. You want your application to perform well when the API response is an error or a timeout.
What should you do?`,options:{A:"Set the asynchronous option for your requests to the API to false and omit the widget displaying the API results when a timeout or error is encountered.",B:"Set the asynchronous option for your request to the API to true and omit the widget displaying the API results when a timeout or error is encountered.",C:"Catch timeout or error exceptions from the API call and keep trying with exponential backoff until the API response is successful.",D:"Catch timeout or error exceptions from the API call and display the error response in the UI widget."},correctAnswer:["B"],explanation:"To prevent the API call from blocking the rendering of the main UI, the XMLHttpRequest should be asynchronous (true). Since the API data is less critical and it's acceptable for it not to display, omitting the widget gracefully handles timeout or error scenarios without disrupting the user experience for the rest of the page. Retrying indefinitely (C) isn't necessary for non-critical data. Displaying the error (D) might not be the desired user experience.",conditions:["Single-page web app, third-party API call via XMLHttpRequest, API data less critical, call must not block UI, handle API errors/timeouts gracefully."],caseStudyContext:null},{id:14,topic:"Compute",question:`You are developing a JPEG image-resizing API hosted on Google Kubernetes Engine (GKE). Callers of the service will exist within the same GKE cluster. You want clients to be able to get the IP address of the service.
What should you do?`,options:{A:"Define a GKE Service. Clients should use the name of the A record in Cloud DNS to find the service's cluster IP address.",B:"Define a GKE Service. Clients should use the service name in the URL to connect to the service.",C:"Define a GKE Endpoint. Clients should get the endpoint name from the appropriate environment variable in the client container.",D:"Define a GKE Endpoint. Clients should get the endpoint name from Cloud DNS."},correctAnswer:["B"],explanation:"Within a GKE cluster, Kubernetes provides internal DNS for service discovery. When you define a Service (typically of type ClusterIP for internal services), Kubernetes automatically creates a DNS record `<service-name>.<namespace-name>.svc.cluster.local`. Pods within the cluster can resolve this service name to the Service's ClusterIP. Using just the service name (e.g., `http://my-service`) often works if the client and service are in the same namespace due to DNS search path configuration. This is the standard Kubernetes way for intra-cluster communication. Endpoints are usually managed by Services.",conditions:["GKE hosted API, clients are within the same cluster, need service discovery."],caseStudyContext:null},{id:15,topic:"Compute",question:`You are using Cloud Build to build and test application source code stored in Cloud Source Repositories. The build process requires a build tool not available in the Cloud Build environment.
What should you do?`,options:{A:"Download the binary from the internet during the build process.",B:"Build a custom cloud builder image and reference the image in your build steps.",C:"Include the binary in your Cloud Source Repositories repository and reference it in your build scripts.",D:"Ask to have the binary added to the Cloud Build environment by filing a feature request against the Cloud Build public Issue Tracker."},correctAnswer:["B"],explanation:"If a required tool isn't available in the standard Cloud Build builders, the recommended approach is to create a custom builder image (a Docker image containing the necessary tool) and push it to a registry (like Artifact Registry). Then, reference this custom builder image in your `cloudbuild.yaml` configuration file for the relevant build steps. Downloading during build (A) can be slow and unreliable. Including binaries in source control (C) is generally bad practice. Filing a feature request (D) won't solve the immediate need.",conditions:["Cloud Build pipeline needs a tool not available in default builders."],caseStudyContext:null},{id:16,topic:"Monitoring & Logging",question:`You are deploying your application to a Compute Engine virtual machine instance. Your application is configured to write its log files to disk. You want to view the logs in Stackdriver Logging without changing the application code.
What should you do?`,options:{A:"Install the Stackdriver Logging Agent and configure it to send the application logs.",B:"Use a Stackdriver Logging Library to log directly from the application to Stackdriver Logging.",C:"Provide the log file folder path in the metadata of the instance to configure it to send the application logs.",D:"Change the application to log to /var/log so that its logs are automatically sent to Stackdriver Logging."},correctAnswer:["A"],explanation:"The Cloud Logging Agent (or the newer Ops Agent) is designed specifically to collect logs from files on Compute Engine instances (and other sources) and forward them to Cloud Logging. Option A requires installing and configuring the agent to monitor the specific log file paths, achieving the goal without application code changes. Using a library (B) requires code changes. Instance metadata (C) doesn't directly configure log shipping. Logging to /var/log (D) might work for some standard system logs, but custom application logs usually require explicit agent configuration.",conditions:["GCE application logs to disk files, need logs in Cloud Logging, cannot change app code."],caseStudyContext:null},{id:17,topic:"Storage",question:`Your service adds text to images that it reads from Cloud Storage. During busy times of the year, requests to Cloud Storage fail with an HTTP 429 "Too Many Requests" status code.
How should you handle this error?`,options:{A:"Add a cache-control header to the objects.",B:"Request a quota increase from the GCP Console.",C:"Retry the request with a truncated exponential backoff strategy.",D:"Change the storage class of the Cloud Storage bucket to Multi-regional."},correctAnswer:["C"],explanation:"HTTP 429 errors indicate rate limiting. The standard and recommended way to handle rate limiting and transient server-side errors (like 5xx) is to implement retries with exponential backoff. This involves waiting progressively longer periods between retries, up to a maximum limit, to avoid overwhelming the service. Cache-control (A) affects client-side caching. Quota increases (B) might be possible but aren't the immediate error handling mechanism. Storage class (D) affects cost/availability/latency, not rate limits directly.",conditions:["Reading from GCS, encountering HTTP 429 errors during busy times."],caseStudyContext:null},{id:18,topic:"Compute",question:`You are building an API that will be used by Android and iOS apps. The API must:
* Support HTTPs
* Minimize bandwidth cost
* Integrate easily with mobile apps
Which API architecture should you use?`,options:{A:"RESTful APIs",B:"MQTT for APIs",C:"gRPC-based APIs",D:"SOAP-based APIs"},correctAnswer:["C"],explanation:"gRPC uses HTTP/2 and Protocol Buffers for serialization, which is significantly more bandwidth-efficient than text-based formats like JSON used in typical REST APIs. It supports HTTPS and has good library support for mobile platforms (Android/iOS). While REST (A) offers easy integration, gRPC (C) better meets the 'minimize bandwidth cost' requirement. MQTT (B) is primarily for messaging, not request/response APIs. SOAP (D) is generally verbose and less suitable for mobile.",conditions:["API for mobile apps, requires HTTPS, minimize bandwidth, easy integration."],caseStudyContext:null},{id:19,topic:"Databases",question:`Your application takes an input from a user and publishes it to the user's contacts. This input is stored in a table in Cloud Spanner. Your application is more sensitive to latency and less sensitive to consistency.
How should you perform reads from Cloud Spanner for this application?`,options:{A:"Perform Read-Only transactions.",B:"Perform stale reads using single-read methods.",C:"Perform strong reads using single-read methods.",D:"Perform stale reads using read-write transactions."},correctAnswer:["B"],explanation:"When latency is more critical than having the absolute latest data (strong consistency), Cloud Spanner's stale reads are the optimal choice. They can often be served faster by any available replica that has data within the specified staleness bound, reducing potential contention or coordination overhead. Single-read methods are generally preferred over read-only transactions for simple reads due to lower overhead. Read-write transactions (D) are for writes and don't support stale reads.",conditions:["Reading from Cloud Spanner, latency sensitive, consistency tolerant."],caseStudyContext:null},{id:20,topic:"Compute",question:`Your application is deployed in a Google Kubernetes Engine (GKE) cluster. When a new version of your application is released, your CI/CD tool updates the spec.template.spec.containers[0].image value to reference the Docker image of your new application version. When the Deployment object applies the change, you want to deploy at least 1 replica of the new version and maintain the previous replicas until the new replica is healthy.
Which change should you make to the GKE Deployment object shown below?`,options:{A:"Set the Deployment strategy to RollingUpdate with maxSurge set to 0, maxUnavailable set to 1.",B:"Set the Deployment strategy to RollingUpdate with maxSurge set to 1, maxUnavailable set to 0.",C:"Set the Deployment strategy to Recreate with maxSurge set to 0, maxUnavailable set to 1.",D:"Set the Deployment strategy to Recreate with maxSurge set to 1, maxUnavailable set to 0."},correctAnswer:["B"],explanation:"The RollingUpdate strategy allows zero-downtime updates. `maxSurge` defines how many pods can be created *above* the desired count during an update. `maxUnavailable` defines how many pods can be *unavailable* during the update. To deploy a new replica (`maxSurge=1`) while keeping all previous replicas running until the new one is healthy (`maxUnavailable=0`), option B is the correct configuration. Recreate strategy (C, D) involves downtime.",conditions:["GKE Deployment update, deploy 1 new replica while keeping all old replicas until new is healthy."],caseStudyContext:null},{id:21,topic:"Storage",question:`You plan to make a simple HTML application available on the internet. This site keeps information about FAQs for your application. The application is static and contains images, HTML, CSS, and Javascript. You want to make this application available on the internet with as few steps as possible.
What should you do?`,options:{A:"Upload your application to Cloud Storage.",B:"Upload your application to an App Engine environment.",C:"Create a Compute Engine instance with Apache web server installed. Configure Apache web server to host the application.",D:"Containerize your application first. Deploy this container to Google Kubernetes Engine (GKE) and assign an external IP address to the GKE pod hosting the application."},correctAnswer:["A"],explanation:"Google Cloud Storage provides the simplest and most cost-effective way to host static website content (HTML, CSS, JavaScript, images). You simply upload the files to a bucket, configure public access (or use a Load Balancer/CDN), and optionally set up a custom domain. App Engine (B), GCE (C), and GKE (D) are overkill and involve significantly more setup and management for purely static content.",conditions:["Host static website (HTML, CSS, JS, images), minimal steps required."],caseStudyContext:null},{id:22,topic:"Monitoring & Logging",question:`Your company has deployed a new API to App Engine Standard environment. During testing, the API is not behaving as expected. You want to monitor the application over time to diagnose the problem within the application code without redeploying the application.
Which tool should you use?`,options:{A:"Stackdriver Trace",B:"Stackdriver Monitoring",C:"Stackdriver Debug Snapshots",D:"Stackdriver Debug Logpoints"},correctAnswer:["D"],explanation:"Cloud Debugger Logpoints (part of the former Stackdriver suite, now Operations Suite) allow developers to inject logging statements into running applications (including App Engine) without stopping them or redeploying code. This lets you gather diagnostic information over time from specific code locations to understand unexpected behavior. Snapshots (C) capture state at one point. Trace (A) focuses on latency across requests. Monitoring (B) tracks metrics and overall health.",conditions:["App Engine app misbehaving, need to diagnose code issue over time without redeploying."],caseStudyContext:null},{id:23,topic:"Monitoring & Logging",question:"You want to use the Stackdriver Logging Agent to send an application's log file to Stackdriver from a Compute Engine virtual machine instance. After installing the Stackdriver Logging Agent, what should you do first?",options:{A:"Enable the Error Reporting API on the project.",B:"Grant the instance full access to all Cloud APIs.",C:"Configure the application log file as a custom source.",D:"Create a Stackdriver Logs Export Sink with a filter that matches the application's log entries."},correctAnswer:["C"],explanation:"After installing the Logging Agent (or Ops Agent), you must configure it to tell it *which* files to monitor and how to parse them. This involves defining the application's log file path as a custom log source in the agent's configuration file (e.g., fluentd or otel config). Enabling Error Reporting (A) is separate. Granting full access (B) violates least privilege (appropriate scopes/roles are needed, usually assigned during install or via instance service account). Creating sinks (D) is for routing logs *after* they are in Cloud Logging.",conditions:["Use Logging Agent on GCE to send app logs from file."],caseStudyContext:null},{id:24,topic:"Databases",question:`Your company has a BigQuery data mart that provides analytics information to hundreds of employees. One user of wants to run jobs without interrupting important workloads. This user isn't concerned about the time it takes to run these jobs. You want to fulfill this request while minimizing cost to the company and the effort required on your part.
What should you do?`,options:{A:"Ask the user to run the jobs as batch jobs.",B:"Create a separate project for the user to run jobs.",C:"Add the user as a job.user role in the existing project.",D:"Allow the user to run jobs when important workloads are not running."},correctAnswer:["A"],explanation:"BigQuery batch jobs are queued and run when idle resources are available, ensuring they don't interfere with interactive or high-priority queries. They are ideal for non-time-sensitive tasks and offer a lower cost (often free tier applicable). This directly addresses the user's requirements with minimal effort. A separate project (B) adds overhead. The `job.user` role (C) grants permission to run jobs but doesn't control priority. Running off-hours (D) requires manual coordination and isn't guaranteed not to conflict.",conditions:["BigQuery data mart, user needs to run low-priority, non-time-sensitive jobs without interrupting others, minimize cost/effort."],caseStudyContext:null},{id:25,topic:"Monitoring & Logging",question:`You want to notify on-call engineers about a service degradation in production while minimizing development time.
What should you do?`,options:{A:"Use Cloud Function to monitor resources and raise alerts.",B:"Use Cloud Pub/Sub to monitor resources and raise alerts.",C:"Use Stackdriver Error Reporting to capture errors and raise alerts.",D:"Use Stackdriver Monitoring to monitor resources and raise alerts."},correctAnswer:["D"],explanation:"Cloud Monitoring (formerly Stackdriver) is Google Cloud's native monitoring solution. It allows you to create alerting policies based on metrics (e.g., latency, error rates, resource utilization) that indicate service degradation. Setting up these alerts requires configuration through the console or API, minimizing development time compared to building custom monitoring with Cloud Functions (A) or Pub/Sub (B). Error Reporting (C) focuses specifically on application errors, while Monitoring covers broader service health.",conditions:["Need notifications for service degradation, minimize development time."],caseStudyContext:null},{id:26,topic:"Compute",question:`You are writing a single-page web application with a user-interface that communicates with a third-party API for content using XMLHttpRequest. The data displayed on the UI by the API results is less critical than other data displayed on the same web page, so it is acceptable for some requests to not have the API data displayed in the UI. However, calls made to the API should not delay rendering of other parts of the user interface. You want your application to perform well when the API response is an error or a timeout.
What should you do?`,options:{A:"Set the asynchronous option for your requests to the API to false and omit the widget displaying the API results when a timeout or error is encountered.",B:"Set the asynchronous option for your request to the API to true and omit the widget displaying the API results when a timeout or error is encountered.",C:"Catch timeout or error exceptions from the API call and keep trying with exponential backoff until the API response is successful.",D:"Catch timeout or error exceptions from the API call and display the error response in the UI widget."},correctAnswer:["B"],explanation:"Setting the asynchronous option to `true` ensures the API request doesn't block the main browser thread, allowing the rest of the UI to render without delay. Since the data is less critical and it's acceptable for it not to appear, the best way to handle timeouts or errors is to simply omit the widget or relevant UI section, providing a smooth experience for the user regarding the rest of the page. Setting async to `false` (A) would block rendering. Retrying indefinitely (C) isn't suitable for non-critical data. Displaying the raw error (D) might not be ideal UI.",conditions:["Single-page app, non-critical API call via XMLHttpRequest, must not block UI, handle errors/timeouts gracefully."],caseStudyContext:null},{id:27,topic:"IAM & Security",question:"You are creating a web application that runs in a Compute Engine instance and writes a file to any user's Google Drive. You need to configure the application to authenticate to the Google Drive API. What should you do?",options:{A:"Use an OAuth Client ID that uses the https://www.googleapis.com/auth/drive.file scope to obtain an access token for each user.",B:"Use an OAuth Client ID with delegated domain-wide authority.",C:"Use the App Engine service account and https://www.googleapis.com/auth/drive.file scope to generate a signed JSON Web Token (JWT).",D:"Use the App Engine service account with delegated domain-wide authority."},correctAnswer:["A"],explanation:"To act on behalf of individual users and access their private data (like Google Drive), the application must obtain user consent via the OAuth 2.0 protocol. This involves registering the application to get an OAuth Client ID, requesting the appropriate scope (`drive.file` allows per-file access created or opened by the app), and guiding the user through the consent flow to obtain an access token specific to that user. Domain-wide delegation (B, D) allows a service account to impersonate users within a G Suite domain, but requires admin setup and is less suitable for accessing *any* user's Drive. Service accounts (C, D) represent the application itself, not individual users.",conditions:["GCE web application needs to write files to any user's Google Drive."],caseStudyContext:null},{id:28,topic:"Compute",question:`You are creating a Google Kubernetes Engine (GKE) cluster and run this command:

The command fails with the error:

ERROR: (gcloud.container.clusters.create) ResponseError: code=403, message=Insufficient regional quota to satisfy request: resource "CPUS": request requires 'XX' and is short 'YY'.. Need more quota? http://...

You want to resolve the issue. What should you do?`,options:{A:"Request additional GKE quota in the GCP Console.",B:"Request additional Compute Engine quota in the GCP Console.",C:"Open a support case to request additional GKE quota.",D:"Decouple services in the cluster, and rewrite new clusters to function with fewer cores."},correctAnswer:["B"],explanation:`The error message explicitly mentions "Insufficient regional quota... resource \\"CPUS\\"". GKE nodes are Compute Engine instances. Therefore, CPU quota limits are part of Compute Engine quotas, not a separate GKE quota. You need to request an increase in the Compute Engine CPU quota for the specific region in the GCP Console. Decoupling services (D) doesn't address the quota issue directly.`,conditions:["GKE cluster creation command fails with CPU quota error."],caseStudyContext:null},{id:29,topic:"Compute",question:`You are parsing a log file that contains three columns: a timestamp, an account number (a string), and a transaction amount (a number). You want to calculate the sum of all transaction amounts for each unique account number efficiently.
Which data structure should you use?`,options:{A:"A linked list",B:"A hash table",C:"A two-dimensional array",D:"A comma-delimited string"},correctAnswer:["B"],explanation:"A hash table (also known as a dictionary or map) provides efficient (average O(1)) lookup, insertion, and update operations based on a key. In this case, the account number can be used as the key, and the transaction amount sum can be the value. As you parse the log, you can quickly look up the account number in the hash table and update its corresponding sum. Linked lists, arrays, and strings would require less efficient searching (O(n)) to find and update the sum for each account.",conditions:["Parse log file (timestamp, account_number, amount), efficiently sum amounts per unique account number."],caseStudyContext:null},{id:30,topic:"Databases",question:`Your company has a BigQuery dataset named "Master" that keeps information about employee travel and expenses. This information is organized by employee department. That means employees should only be able to view information for their department. You want to apply a security framework to enforce this requirement with the minimum number of steps.
What should you do?`,options:{A:"Create a separate dataset for each department. Create a view with an appropriate WHERE clause to select records from a particular dataset for the specific department. Authorize this view to access records from your Master dataset. Give employees the permission to this department-specific dataset.",B:"Create a separate dataset for each department. Create a data pipeline for each department to copy appropriate information from the Master dataset to the specific dataset for the department. Give employees the permission to this department-specific dataset.",C:"Create a dataset named Master dataset. Create a separate view for each department in the Master dataset. Give employees access to the specific view for their department.",D:"Create a dataset named Master dataset. Create a separate table for each department in the Master dataset. Give employees access to the specific table for their department."},correctAnswer:["C"],explanation:"Authorized views are the standard and simplest method in BigQuery for implementing row-level or column-level security. You create a view within the same dataset (`Master`) for each department, filtering the data using a `WHERE` clause (e.g., `WHERE department = 'Sales'`). You then grant employees access only to their department's specific view, not the underlying table. This avoids data duplication (unlike B or D) and the complexity of managing multiple datasets (unlike A).",conditions:["BigQuery dataset 'Master', row-level security based on employee department, minimum steps."],caseStudyContext:null},{id:31,topic:"Monitoring & Logging",question:`You have an application in production. It is deployed on Compute Engine virtual machine instances controlled by a managed instance group. Traffic is routed to the instances via a HTTP(s) load balancer. Your users are unable to access your application. You want to implement a monitoring technique to alert you when the application is unavailable.
Which technique should you choose?`,options:{A:"Smoke tests",B:"Stackdriver uptime checks",C:"Cloud Load Balancing - heath checks",D:"Managed instance group - heath checks"},correctAnswer:["B"],explanation:"Cloud Monitoring Uptime Checks are specifically designed to monitor the external availability of applications (web servers, VMs, etc.) from various locations around the world. They can be configured to check HTTP(S) endpoints and trigger alerts if the application becomes unavailable. Load balancer and MIG health checks (C, D) are used internally to manage traffic routing and instance health within GCP, but uptime checks provide the external perspective on availability and integrated alerting needed here. Smoke tests (A) are a type of test, not a specific monitoring technique in this context.",conditions:["GCE app in MIG behind HTTP(S) LB, users cannot access, need alerting for unavailability."],caseStudyContext:null},{id:32,topic:"Storage",question:`You are load testing your server application. During the first 30 seconds, you observe that a previously inactive Cloud Storage bucket is now servicing 2000 write requests per second and 7500 read requests per second. Your application is now receiving intermittent 5xx and 429 HTTP responses from the Cloud Storage JSON API as the demand escalates. You want to decrease the failed responses from the Cloud Storage API.
What should you do?`,options:{A:"Distribute the uploads across a large number of individual storage buckets.",B:"Use the XML API instead of the JSON API for interfacing with Cloud Storage.",C:"Pass the HTTP response codes back to clients that are invoking the uploads from your application.",D:"Limit the upload rate from your application clients so that the dormant bucket's peak request rate is reached more gradually."},correctAnswer:["D"],explanation:"Cloud Storage buckets automatically scale their I/O capacity based on sustained traffic. However, a sudden, massive increase in requests to a previously inactive or low-traffic bucket can exceed the initial capacity, leading to 5xx or 429 errors. The recommended practice is to gradually ramp up the request rate (D) to allow the bucket time to scale. Distributing across many buckets (A) can help but adds complexity. Changing the API (B) is unlikely to solve scaling issues. Passing errors back (C) doesn't prevent them.",conditions:["Load testing GCS bucket, high initial request rate on inactive bucket, receiving 5xx/429 errors."],caseStudyContext:null},{id:33,topic:"Storage",question:`Your application is controlled by a managed instance group. You want to share a large read-only data set between all the instances in the managed instance group. You want to ensure that each instance can start quickly and can access the data set via its filesystem with very low latency. You also want to minimize the total cost of the solution.
What should you do?`,options:{A:"Move the data to a Cloud Storage bucket, and mount the bucket on the filesystem using Cloud Storage FUSE.",B:"Move the data to a Cloud Storage bucket, and copy the data to the boot disk of the instance via a startup script.",C:"Move the data to a Compute Engine persistent disk, and attach the disk in read-only mode to multiple Compute Engine virtual machine instances.",D:"Move the data to a Compute Engine persistent disk, take a snapshot, create multiple disks from the snapshot, and attach each disk to its own instance."},correctAnswer:["C"],explanation:"Compute Engine allows attaching a standard or SSD persistent disk in read-only mode to multiple VM instances simultaneously within the same zone. This provides low-latency, block-level filesystem access (C). GCS FUSE (A) provides file system access to GCS but generally has higher latency than local PDs. Copying data on startup (B) slows down instance start time. Creating disks from snapshots (D) duplicates data, increasing costs compared to sharing one disk.",conditions:["Share large read-only dataset across MIG instances, fast start, low latency filesystem access, minimize cost."],caseStudyContext:null},{id:34,topic:"Networking",question:`You are developing an HTTP API hosted on a Compute Engine virtual machine instance that needs to be invoked by multiple clients within the same Virtual Private Cloud (VPC). You want clients to be able to get the IP address of the service.
What should you do?`,options:{A:"Reserve a static external IP address and assign it to an HTTP(S) load balancing service's forwarding rule. Clients should use this IP address to connect to the service.",B:"Reserve a static external IP address and assign it to an HTTP(S) load balancing service's forwarding rule. Then, define an A record in Cloud DNS. Clients should use the name of the A record to connect to the service.",C:"Ensure that clients use Compute Engine internal DNS by connecting to the instance name with the url https://[INSTANCE_NAME].[ZONE].c.[PROJECT_ID].internal/.",D:"Ensure that clients use Compute Engine internal DNS by connecting to the instance name with the url https://[API_NAME]/[API_VERSION]/."},correctAnswer:["C"],explanation:"For communication *within* the same VPC, using external IPs and load balancers (A, B) is unnecessary and less efficient. Compute Engine provides an internal DNS service that allows instances to resolve each other's internal IP addresses using their fully qualified internal DNS names (`[INSTANCE_NAME].[ZONE].c.[PROJECT_ID].internal`). This is the standard and recommended way for internal service discovery (C). Option D uses an invalid URL format.",conditions:["GCE hosted HTTP API, clients within same VPC need to discover/connect."],caseStudyContext:null},{id:35,topic:"Monitoring & Logging",question:`Your application is logging to Stackdriver. You want to get the count of all requests on all /api/alpha/* endpoints.
What should you do?`,options:{A:"Add a Stackdriver counter metric for path:/api/alpha/.",B:"Add a Stackdriver counter metric for endpoint:/api/alpha/*.",C:"Export the logs to Cloud Storage and count lines matching /api/alpha.",D:"Export the logs to Cloud Pub/Sub and count lines matching /api/alpha."},correctAnswer:["B"],explanation:"Cloud Logging allows creating log-based metrics to count log entries matching specific filters. You can define a counter metric based on log entries where the request path or endpoint matches the pattern `/api/alpha/*`. Option B uses a plausible filter syntax (`endpoint:/api/alpha/*`) which is suitable for matching URL paths with wildcards. Option A (`path:/api/alpha/`) likely wouldn't match sub-paths correctly. Exporting logs (C, D) requires external processing and is less efficient for simple counting.",conditions:["Count log entries for requests matching path /api/alpha/* in Cloud Logging."],caseStudyContext:null},{id:36,topic:"Compute",question:`You want to re-architect a monolithic application so that it follows a microservices model. You want to accomplish this efficiently while minimizing the impact of this change to the business.
Which approach should you take?`,options:{A:"Deploy the application to Compute Engine and turn on autoscaling.",B:"Replace the application's features with appropriate microservices in phases.",C:"Refactor the monolithic application with appropriate microservices in a single effort and deploy it.",D:"Build a new application with the appropriate microservices separate from the monolith and replace it when it is complete."},correctAnswer:["B"],explanation:"Migrating a monolith to microservices carries risk. The Strangler Fig pattern, which involves replacing features or functionalities incrementally with microservices (B), is the recommended approach to minimize risk and business impact. This allows gradual rollout, testing, and learning. A big-bang refactor (C) or replacement (D) is much riskier. Simply deploying to GCE with autoscaling (A) doesn't change the monolithic architecture.",conditions:["Re-architect monolith to microservices, minimize business impact."],caseStudyContext:null},{id:37,topic:"Databases",question:`Your existing application keeps user state information in a single MySQL database. This state information is very user-specific and depends heavily on how long a user has been using an application. The MySQL database is causing challenges to maintain and enhance the schema for various users.
Which storage option should you choose?`,options:{A:"Cloud SQL",B:"Cloud Storage",C:"Cloud Spanner",D:"Cloud Datastore/Firestore"},correctAnswer:["D"],explanation:"The key challenges are maintaining and enhancing the schema due to user-specific variations. This points towards needing a flexible schema, which NoSQL databases excel at. Firestore (or Datastore mode) is a managed, scalable NoSQL document database suitable for user profiles and semi-structured data, allowing schema flexibility. Cloud SQL (A) and Spanner (C) are relational and have stricter schemas. Cloud Storage (B) is object storage, not ideal for structured state data.",conditions:["Storing user state, currently MySQL, schema maintenance/enhancement challenges due to user variations."],caseStudyContext:null},{id:38,topic:"Networking",question:`You are building a new API. You want to minimize the cost of storing and reduce the latency of serving images.
Which architecture should you use?`,options:{A:"App Engine backed by Cloud Storage",B:"Compute Engine backed by Persistent Disk",C:"Transfer Appliance backed by Cloud Filestore",D:"Cloud Content Delivery Network (CDN) backed by Cloud Storage"},correctAnswer:["D"],explanation:"For serving static assets like images globally with low latency and cost, Cloud Storage combined with Cloud CDN is the standard best practice. Cloud Storage offers low-cost, scalable object storage. Cloud CDN caches the images at edge locations closer to users, significantly reducing latency and egress costs. App Engine (A) or Compute Engine (B) serving images directly is less efficient and more costly than using a CDN. Transfer Appliance/Filestore (C) are not relevant for serving web content.",conditions:["API needs to store and serve images, minimize storage cost, reduce serving latency."],caseStudyContext:null},{id:39,topic:"IAM & Security",question:`Your company's development teams want to use Cloud Build in their projects to build and push Docker images to Container Registry. The operations team requires all Docker images to be published to a centralized, securely managed Docker registry that the operations team manages.
What should you do?`,options:{A:"Use Container Registry to create a registry in each development team's project. Configure the Cloud Build build to push the Docker image to the project's registry. Grant the operations team access to each development team's registry.",B:"Create a separate project for the operations team that has Container Registry configured. Assign appropriate permissions to the Cloud Build service account in each developer team's project to allow access to the operation team's registry.",C:"Create a separate project for the operations team that has Container Registry configured. Create a Service Account for each development team and assign the appropriate permissions to allow it access to the operations team's registry. Store the service account key file in the source code repository and use it to authenticate against the operations team's registry.",D:"Create a separate project for the operations team that has the open source Docker Registry deployed on a Compute Engine virtual machine instance. Create a username and password for each development team. Store the username and password in the source code repository and use it to authenticate against the operations team's Docker registry."},correctAnswer:["B"],explanation:"To centralize registry management under the Ops team while allowing Dev teams' Cloud Build pipelines to push images, create a dedicated Ops project with Container Registry (or Artifact Registry). Then, grant the Cloud Build service account from each Dev project the necessary IAM permissions (e.g., Storage Object Creator/Admin on the underlying GCS bucket, or Artifact Registry Writer role) on the Ops team's registry. This avoids managing registries per project (A) and insecure practices like storing keys in source code (C, D) or running unmanaged registries (D).",conditions:["Dev teams use Cloud Build, Ops manages a centralized/secure container registry, Cloud Build needs push access."],caseStudyContext:null},{id:40,topic:"Compute",question:`You are planning to deploy your application in a Google Kubernetes Engine (GKE) cluster. Your application can scale horizontally, and each instance of your application needs to have a stable network identity and its own persistent disk.
Which GKE object should you use?`,options:{A:"Deployment",B:"StatefulSet",C:"ReplicaSet",D:"ReplicaController"},correctAnswer:["B"],explanation:"StatefulSets are designed for applications that require stable, unique network identifiers (e.g., predictable hostnames like `my-app-0`, `my-app-1`), stable persistent storage per replica, and ordered, graceful deployment and scaling. Deployments are for stateless applications where replicas are interchangeable. ReplicaSets/ReplicaControllers manage pod replicas but don't provide stable identities or storage.",conditions:["GKE application, needs horizontal scaling, stable network identity per instance, persistent disk per instance."],caseStudyContext:null},{id:41,topic:"Compute",question:`You are using Cloud Build to build a Docker image. You need to modify the build to execute unit and run integration tests. When there is a failure, you want the build history to clearly display the stage at which the build failed.
What should you do?`,options:{A:"Add RUN commands in the Dockerfile to execute unit and integration tests.",B:"Create a Cloud Build build config file with a single build step to compile unit and integration tests.",C:"Create a Cloud Build build config file that will spawn a separate cloud build pipeline for unit and integration tests.",D:"Create a Cloud Build build config file with separate cloud builder steps to compile and execute unit and integration tests."},correctAnswer:["D"],explanation:"Cloud Build configuration files (`cloudbuild.yaml`) allow defining multiple build steps. Defining separate steps for compilation, unit tests, and integration tests (D) provides clear separation in the build history. If a step fails, Cloud Build marks that specific step as failed, making it easy to identify the stage of failure. Running tests inside the Dockerfile (A) makes the image larger and couples testing with image building. A single step (B) obscures which part failed. Spawning separate pipelines (C) adds unnecessary complexity.",conditions:["Cloud Build Docker build, need to add unit/integration tests, clearly show failed stage."],caseStudyContext:null},{id:42,topic:"IAM & Security",question:`Your code is running on Cloud Functions in project A. It is supposed to write an object in a Cloud Storage bucket owned by project B. However, the write call is failing with the error "403 Forbidden".
What should you do to correct the problem?`,options:{A:"Grant your user account the roles/storage.objectCreator role for the Cloud Storage bucket.",B:"Grant your user account the roles/iam.serviceAccountUser role for the service-PROJECTA@gcf-admin-robot.iam.gserviceaccount.com service account.",C:"Grant the service-PROJECTA@gcf-admin-robot.iam.gserviceaccount.com service account the roles/storage.objectCreator role for the Cloud Storage bucket.",D:"Enable the Cloud Storage API in project B."},correctAnswer:["C"],explanation:"Cloud Functions execute using an identity, which by default is the project-specific Cloud Functions service agent (`service-<PROJECT_NUMBER>@gcf-admin-robot.iam.gserviceaccount.com`) or a specified runtime service account. A 403 Forbidden error indicates the identity making the call lacks permission. To allow the function in Project A to write to a bucket in Project B, the function's service account identity (from Project A) must be granted the appropriate role (like `roles/storage.objectCreator`) on the target bucket in Project B.",conditions:["Cloud Function in Project A writing to GCS bucket in Project B fails with 403."],caseStudyContext:null},{id:43,topic:"Case Study Analysis",question:"Case study - HipLocal: HipLocal's .net-based auth service fails under intermittent load. What should they do?",options:{A:"Use App Engine for autoscaling.",B:"Use Cloud Functions for autoscaling.",C:"Use a Compute Engine cluster for the service.",D:"Use a dedicated Compute Engine virtual machine instance for the service."},correctAnswer:["A"],explanation:"The case study's technical requirements state a need to 'Move to serverless architecture to facilitate elastic scaling.' The auth service is .NET based and fails under load. App Engine (specifically Flexible environment) supports .NET, is serverless (or server-managed), and provides automatic scaling, addressing both the technical requirement and the load issue. Cloud Functions (B) are less suitable for full services. Compute Engine options (C, D) are not serverless and require more management.",conditions:["HipLocal case study context, .NET auth service fails under load, need autoscaling, prefer serverless."],caseStudyContext:`<strong>Company overview</strong>
HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.

<strong>Executive statement</strong>
We are the number one local community app; its time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.

<strong><strong>Solution Concept </strong><br></strong>
HipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.

<strong><strong>Existing Technical Environment </strong><br></strong>
HipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:
<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>

<strong><strong>Business Requirements </strong><br></strong>
HipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:
<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>

<strong>Technical requirements</strong>
<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>`},{id:44,topic:"Case Study Analysis",question:"Case study - HipLocal: HipLocal's APIs are having occasional application failures. They want to collect application information specifically to troubleshoot the issue. What should they do?",options:{A:"Take frequent snapshots of the virtual machines.",B:"Install the Cloud Logging agent on the virtual machines.",C:"Install the Cloud Monitoring agent on the virtual machines.",D:"Use Cloud Trace to look for performance bottlenecks."},correctAnswer:["B"],explanation:"The case study explicitly states 'The application has no logging.' To troubleshoot application failures, collecting logs is essential. Installing the Cloud Logging agent on the Compute Engine VMs where the APIs run will enable log collection without modifying the application code itself. Snapshots (A) are for backups. Monitoring agent (C) collects metrics, not detailed application logs for failure analysis. Trace (D) analyzes latency but requires application instrumentation and might not directly reveal the cause of failures without logs.",conditions:["HipLocal case study context, APIs on GCE failing occasionally, need troubleshooting info, application has no logging currently."],caseStudyContext:`<strong>Company overview</strong>
HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.

<strong>Executive statement</strong>
We are the number one local community app; its time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.

<strong><strong>Solution Concept </strong><br></strong>
HipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.

<strong><strong>Existing Technical Environment </strong><br></strong>
HipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:
<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>

<strong><strong>Business Requirements </strong><br></strong>
HipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:
<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>

<strong>Technical requirements</strong>
<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>`},{id:45,topic:"Case Study Analysis",question:"Case study - HipLocal: HipLocal has connected their Hadoop infrastructure to GCP using Cloud Interconnect in order to query data stored on persistent disks. Which IP strategy should they use?",options:{A:"Create manual subnets.",B:"Create an auto mode subnet.",C:"Create multiple peered VPCs.",D:"Provision a single instance for NAT."},correctAnswer:["A"],explanation:"Connecting an on-premises network (like the one hosting Hadoop) to GCP via Cloud Interconnect requires careful IP address planning to avoid conflicts between on-prem ranges and VPC subnet ranges. Using custom mode VPC with manual subnets (A) provides full control over VPC subnet IP ranges, allowing HipLocal to ensure they don't overlap with their existing on-premises IP space. Auto mode VPC (B) assigns predefined CIDR ranges per region, which might conflict with on-prem ranges.",conditions:["HipLocal case study context, connecting on-prem Hadoop to GCP via Cloud Interconnect, need VPC IP strategy."],caseStudyContext:`<strong>Company overview</strong>
HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.

<strong>Executive statement</strong>
We are the number one local community app; its time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.

<strong><strong>Solution Concept </strong><br></strong>
HipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.

<strong><strong>Existing Technical Environment </strong><br></strong>
HipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:
<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>

<strong><strong>Business Requirements </strong><br></strong>
HipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:
<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>

<strong>Technical requirements</strong>
<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>`},{id:46,topic:"Case Study Analysis",question:"Case study - HipLocal: Which service should HipLocal use to enable access to internal apps?",options:{A:"Cloud VPN",B:"Cloud Armor",C:"Virtual Private Cloud",D:"Cloud Identity-Aware Proxy"},correctAnswer:["D"],explanation:"The technical requirements specifically state: 'Provide authorized access to internal apps in a secure manner.' Cloud Identity-Aware Proxy (IAP) is designed for this exact purpose. It provides secure, identity-based access control to applications hosted on GCP (or even on-prem with connectors) without requiring a traditional VPN. Cloud VPN (A) provides network connectivity. Cloud Armor (B) provides DDoS/WAF protection. VPC (C) is the network itself.",conditions:["HipLocal case study context, need secure access to internal apps."],caseStudyContext:`<strong>Company overview</strong>
HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.

<strong>Executive statement</strong>
We are the number one local community app; its time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.

<strong><strong>Solution Concept </strong><br></strong>
HipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.

<strong><strong>Existing Technical Environment </strong><br></strong>
HipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:
<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>

<strong><strong>Business Requirements </strong><br></strong>
HipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:
<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>

<strong>Technical requirements</strong>
<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>`},{id:47,topic:"Case Study Analysis",question:"Case study - HipLocal: HipLocal wants to reduce the number of on-call engineers and eliminate manual scaling. Which two services should they choose? (Choose two.)",options:{A:"Use Google App Engine services.",B:"Use serverless Google Cloud Functions.",C:"Use Knative to build and deploy serverless applications.",D:"Use Google Kubernetes Engine for automated deployments.",E:"Use a large Google Compute Engine cluster for deployments."},correctAnswer:["A","B"],explanation:"The technical requirements include 'Move to serverless architecture to facilitate elastic scaling' and a business requirement is 'Reduce infrastructure management time and cost'. Google App Engine (A) and Cloud Functions (B) are fully managed, serverless platforms that automatically handle scaling based on load, eliminating manual scaling and reducing operational overhead for on-call engineers. Knative (C) provides serverless capabilities but often runs on GKE (D), which still requires some cluster management. GCE (E) requires manual scaling configuration.",conditions:["HipLocal case study context, reduce on-call engineers, eliminate manual scaling, prefer serverless."],caseStudyContext:`<strong>Company overview</strong>
HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.

<strong>Executive statement</strong>
We are the number one local community app; its time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.

<strong><strong>Solution Concept </strong><br></strong>
HipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.

<strong><strong>Existing Technical Environment </strong><br></strong>
HipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:
<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>

<strong><strong>Business Requirements </strong><br></strong>
HipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:
<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>

<strong>Technical requirements</strong>
<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>`},{id:48,topic:"Case Study Analysis",question:"Case study - HipLocal: In order to meet their <strong>Business Requirements </strong><br>, how should HipLocal store their application state?",options:{A:"Use local SSDs to store state.",B:"Put a memcache layer in front of MySQL.",C:"Move the state storage to Cloud Spanner.",D:"Replace the MySQL instance with Cloud SQL."},correctAnswer:["C"],explanation:"<strong>Business Requirements </strong><br> include expanding globally, supporting 10x users, and ensuring a consistent experience across regions. The current state is stored in a single MySQL instance. This won't scale globally or provide a consistent low-latency experience. Cloud Spanner (C) is a globally distributed, strongly consistent, horizontally scalable relational database designed for these requirements. Cloud SQL (D) is regional. Memcache (B) helps with read latency but doesn't solve the global scaling/consistency issue for writes. Local SSDs (A) are ephemeral or local to a single VM.",conditions:["HipLocal case study context, store application state, meet requirements of global expansion, high concurrency, consistency."],caseStudyContext:`<strong>Company overview</strong>
HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.

<strong>Executive statement</strong>
We are the number one local community app; its time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.

<strong><strong>Solution Concept </strong><br></strong>
HipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.

<strong><strong>Existing Technical Environment </strong><br></strong>
HipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:
<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>

<strong><strong>Business Requirements </strong><br></strong>
HipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:
<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>

<strong>Technical requirements</strong>
<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>`},{id:49,topic:"Case Study Analysis",question:"Case study - HipLocal: Which service should HipLocal use for their public APIs?",options:{A:"Cloud Armor",B:"Cloud Functions",C:"Cloud Endpoints",D:"Shielded Virtual Machines"},correctAnswer:["C"],explanation:"The technical requirements state 'APIs require strong authentication and authorization.' Cloud Endpoints (or Apigee) provides a managed API gateway solution that handles authentication, authorization, monitoring, quotas, and other API management tasks. Cloud Armor (A) is for WAF/DDoS. Cloud Functions (B) can host APIs but lack built-in management features. Shielded VMs (D) relate to infrastructure security, not API management.",conditions:["HipLocal case study context, need service for public APIs, requires auth/authz."],caseStudyContext:`<strong>Company overview</strong>
HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.

<strong>Executive statement</strong>
We are the number one local community app; its time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.

<strong><strong>Solution Concept </strong><br></strong>
HipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.

<strong><strong>Existing Technical Environment </strong><br></strong>
HipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:
<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>

<strong><strong>Business Requirements </strong><br></strong>
HipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:
<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>

<strong>Technical requirements</strong>
<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>`},{id:50,topic:"Case Study Analysis",question:"Case study - HipLocal: HipLocal wants to improve the resilience of their MySQL deployment, while also meeting their business and technical requirements. Which configuration should they choose?",options:{A:"Use the current single instance MySQL on Compute Engine and several read-only MySQL servers on Compute Engine.",B:"Use the current single instance MySQL on Compute Engine, and replicate the data to Cloud SQL in an external master configuration.",C:"Replace the current single instance MySQL instance with Cloud SQL, and configure high availability.",D:"Replace the current single instance MySQL instance with Cloud SQL, and Google provides redundancy without further configuration."},correctAnswer:["C"],explanation:"HipLocal currently uses a single MySQL instance on GCE. To improve resilience and reduce management ('Reduce infrastructure management time and cost'), migrating to the managed Cloud SQL service is recommended. Configuring Cloud SQL for High Availability (HA) (C) provides automatic failover to a standby instance in a different zone within the same region, significantly improving resilience over a single instance. Adding read replicas (A) improves read scalability but not write resilience. External master setup (B) is complex. Cloud SQL requires explicit HA configuration (D is incorrect).",conditions:["HipLocal case study context, improve resilience of MySQL deployment (currently single GCE instance), meet business/tech reqs."],caseStudyContext:`<strong>Company overview</strong>
HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.

<strong>Executive statement</strong>
We are the number one local community app; its time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.

<strong><strong>Solution Concept </strong><br></strong>
HipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.

<strong><strong>Existing Technical Environment </strong><br></strong>
HipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:
<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>

<strong><strong>Business Requirements </strong><br></strong>
HipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:
<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>

<strong>Technical requirements</strong>
<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>`},{id:51,topic:"Monitoring & Logging",question:`Your application is running in multiple Google Kubernetes Engine clusters. It is managed by a Deployment in each cluster. The Deployment has created multiple replicas of your Pod in each cluster. You want to view the logs sent to stdout for all of the replicas in your Deployment in all clusters.
Which command should you use?`,options:{A:"kubectl logs [PARAM]",B:"gcloud logging read [PARAM]",C:'kubectl exec "it [PARAM] journalctl',D:'gcloud compute ssh [PARAM] "-command= sudo journalctl'},correctAnswer:["B"],explanation:"GKE integrates with Cloud Logging, which aggregates logs from all nodes and pods across clusters within a project (or configured scope). `gcloud logging read` allows querying these aggregated logs using filters based on resource type (k8s_container, k8s_pod, k8s_cluster), labels (like app name), etc., enabling viewing logs from all replicas across multiple clusters. `kubectl logs` targets pods within a specific cluster context. `kubectl exec` runs commands inside a pod. `gcloud compute ssh` connects to GCE nodes.",conditions:["View stdout logs for all replicas of a Deployment across multiple GKE clusters."],caseStudyContext:null},{id:52,topic:"Compute",question:`You are using Cloud Build to create a new Docker image on each source code commit to a Cloud Source Repositories repository. Your application is built on every commit to the master branch. You want to release specific commits made to the master branch in an automated method.
What should you do?`,options:{A:"Manually trigger the build for new releases.",B:"Create a build trigger on a Git tag pattern. Use a Git tag convention for new releases.",C:"Create a build trigger on a Git branch name pattern. Use a Git branch naming convention for new releases.",D:"Commit your source code to a second Cloud Source Repositories repository with a second Cloud Build trigger. Use this repository for new releases only."},correctAnswer:["B"],explanation:"While builds happen on every commit to master, releases usually correspond to specific, tagged commits. Cloud Build triggers can be configured to activate based on Git tags matching a specific pattern (e.g., `v*.*.*` or `release-*`). This allows developers to tag a specific commit on the master branch when it's ready for release, automatically triggering the release build/deployment process. Triggering on branch name (C) doesn't target specific commits. Manual triggers (A) aren't automated. A second repo (D) adds unnecessary complexity.",conditions:["Cloud Build builds on every master commit, need automated release for specific commits."],caseStudyContext:null},{id:53,topic:"Databases",question:`You are designing a schema for a table that will be moved from MySQL to Cloud Bigtable. The MySQL table is as follows:

CREATE TABLE events (
  Account_id VARCHAR(32),
  Event_timestamp TIMESTAMP,
  Event_type VARCHAR(16),
  Event_details VARCHAR(1024),
  PRIMARY KEY (Account_id, Event_timestamp)
);

How should you design a row key for Cloud Bigtable for this table?`,options:{A:"Set Account_id as a key.",B:"Set Account_id_Event_timestamp as a key.",C:"Set Event_timestamp_Account_id as a key.",D:"Set Event_timestamp as a key."},correctAnswer:["B"],explanation:"The goal is to uniquely identify each event row and design for efficient querying and avoiding hotspots. The MySQL primary key is composite (Account_id, Event_timestamp). A common Bigtable pattern is to concatenate primary key components. Starting the row key with a high-cardinality field like `Account_id` helps distribute writes across nodes, avoiding hotspots that can occur if keys start with a monotonically increasing value like a timestamp. Therefore, `Account_id#Event_timestamp` (or similar concatenation represented by B) is generally the best practice.",conditions:["Migrating MySQL table with composite PK (Account_id, Event_timestamp) to Bigtable, design row key."],caseStudyContext:null},{id:54,topic:"Monitoring & Logging",question:`You want to view the memory usage of your application deployed on Compute Engine.
What should you do?`,options:{A:"Install the Stackdriver Client Library.",B:"Install the Stackdriver Monitoring Agent.",C:"Use the Stackdriver Metrics Explorer.",D:"Use the Google Cloud Platform Console."},correctAnswer:["B"],explanation:"By default, Compute Engine instances send basic metrics (like CPU usage, disk I/O, network traffic) to Cloud Monitoring. However, memory utilization and disk space utilization are not included by default. To collect these metrics, you must install the Cloud Monitoring Agent (or the newer Ops Agent) on the instance. Once the agent is installed and sending data, you can view memory usage in Metrics Explorer (C) or the Cloud Console (D), but installation (B) is the prerequisite.",conditions:["Need to view memory usage of GCE application."],caseStudyContext:null},{id:55,topic:"Monitoring & Logging",question:`You have an analytics application that runs hundreds of queries on BigQuery every few minutes using BigQuery API. You want to find out how much time these queries take to execute.
What should you do?`,options:{A:"Use Stackdriver Monitoring to plot slot usage.",B:"Use Stackdriver Trace to plot API execution time.",C:"Use Stackdriver Trace to plot query execution time.",D:"Use Stackdriver Monitoring to plot query execution times."},correctAnswer:["D"],explanation:"BigQuery automatically sends metrics about job execution, including query execution time, to Cloud Monitoring. You can view these metrics (like `job/query_execution_times`) in Metrics Explorer or create dashboards and alerts based on them. Plotting slot usage (A) shows resource consumption, not execution time. Cloud Trace (B, C) is typically used for tracing requests across distributed services, not for monitoring the duration of individual BigQuery jobs submitted via the API.",conditions:["Need to find execution time for BigQuery queries run via API."],caseStudyContext:null},{id:56,topic:"Databases",question:`You are designing a schema for a Cloud Spanner customer database. You want to store a phone number array field in a customer table. You also want to allow users to search customers by phone number.
How should you design this schema?`,options:{A:"Create a table named Customers. Add an Array field in a table that will hold phone numbers for the customer.",B:"Create a table named Customers. Create a table named Phones. Add a CustomerId field in the Phones table to find the CustomerId from a phone number.",C:"Create a table named Customers. Add an Array field in a table that will hold phone numbers for the customer. Create a secondary index on the Array field.",D:"Create a table named Customers as a parent table. Create a table named Phones, and interleave this table into the Customer table. Create an index on the phone number field in the Phones table."},correctAnswer:["D"],explanation:"Cloud Spanner does not support secondary indexes directly on ARRAY type columns (eliminating C). To efficiently search by phone number when a customer can have multiple numbers, a separate, related table is needed. Interleaving the `Phones` table within the `Customers` table (D) physically co-locates a customer's phones with their main record, which can improve performance for queries fetching a customer and their phones. Creating a secondary index on the phone number column in the `Phones` table enables efficient lookups by phone number. A non-interleaved table (B) is also possible but interleaving is often preferred for 1-to-many relationships frequently accessed together.",conditions:["Spanner DB, store multiple phone numbers per customer, search by phone number."],caseStudyContext:null},{id:57,topic:"Compute",question:`You are deploying a single website on App Engine that needs to be accessible via the URL http://www.altostrat.com/.
What should you do?`,options:{A:"Verify domain ownership with Webmaster Central. Create a DNS CNAME record to point to the App Engine canonical name ghs.googlehosted.com.",B:"Verify domain ownership with Webmaster Central. Define an A record pointing to the single global App Engine IP address.",C:"Define a mapping in dispatch.yaml to point the domain www.altostrat.com to your App Engine service. Create a DNS CNAME record to point to the App Engine canonical name ghs.googlehosted.com.",D:"Define a mapping in dispatch.yaml to point the domain www.altostrat.com to your App Engine service. Define an A record pointing to the single global App Engine IP address."},correctAnswer:["A"],explanation:"To map a custom domain to an App Engine application, you first need to verify ownership of the domain (e.g., via Google Search Console/Webmaster Central). Then, you configure the domain mapping within the GCP project settings for App Engine. Finally, you update your domain's DNS records. For a `www` subdomain (or other subdomains), the standard practice is to create a CNAME record pointing to `ghs.googlehosted.com`. A records (B, D) point to specific IP addresses, which isn't the recommended method for App Engine's managed infrastructure. `dispatch.yaml` (C, D) is used for routing requests to different services within an App Engine app, not for mapping external domains.",conditions:["Deploy App Engine website, make accessible via custom domain www.altostrat.com."],caseStudyContext:null},{id:58,topic:"IAM & Security",question:`You are running an application on App Engine that you inherited. You want to find out whether the application is using insecure binaries or is vulnerable to XSS attacks.
Which service should you use?`,options:{A:"Cloud Amor",B:"Stackdriver Debugger",C:"Cloud Security Scanner",D:"Stackdriver Error Reporting"},correctAnswer:["C"],explanation:"Cloud Security Scanner is specifically designed to scan App Engine (Standard and Flex), GKE, and Compute Engine web applications for common vulnerabilities, including cross-site scripting (XSS), outdated libraries (which might include insecure binaries), mixed content, and others. Cloud Armor (A) is for WAF/DDoS protection. Debugger (B) is for inspecting running code. Error Reporting (D) aggregates application errors.",conditions:["App Engine application, check for insecure binaries and XSS vulnerabilities."],caseStudyContext:null},{id:59,topic:"Storage",question:`You are working on a social media application. You plan to add a feature that allows users to upload images. These images will be 2 MB " 1 GB in size. You want to minimize their infrastructure operations overhead for this feature.
What should you do?`,options:{A:"Change the application to accept images directly and store them in the database that stores other user information.",B:"Change the application to create signed URLs for Cloud Storage. Transfer these signed URLs to the client application to upload images to Cloud Storage.",C:"Set up a web server on GCP to accept user images and create a file store to keep uploaded files. Change the application to retrieve images from the file store.",D:"Create a separate bucket for each user in Cloud Storage. Assign a separate service account to allow write access on each bucket. Transfer service account credentials to the client application based on user information. The application uses this service account to upload images to Cloud Storage."},correctAnswer:["B"],explanation:"Storing large binary objects like images/videos directly in databases (A) is inefficient. Setting up dedicated web servers and file stores (C) increases operational overhead. Managing buckets and service accounts per user (D) is extremely complex and insecure (transferring SA creds to client). The best practice for minimizing operational overhead and handling large uploads securely is to use Signed URLs (B). The application backend generates a short-lived URL granting the client permission to upload directly to a specific Cloud Storage location, offloading the transfer from the application servers.",conditions:["Social media app feature: user image uploads (2MB-1GB). Minimize infrastructure operations overhead."],caseStudyContext:null},{id:60,topic:"Compute",question:`Your application is built as a custom machine image. You have multiple unique deployments of the machine image. Each deployment is a separate managed instance group with its own template. Each deployment requires a unique set of configuration values. You want to provide these unique values to each deployment but use the same custom machine image in all deployments. You want to use out-of-the-box features of Compute Engine.
What should you do?`,options:{A:"Place the unique configuration values in the persistent disk.",B:"Place the unique configuration values in a Cloud Bigtable table.",C:"Place the unique configuration values in the instance template startup script.",D:"Place the unique configuration values in the instance template instance metadata."},correctAnswer:["D"],explanation:"Compute Engine instance metadata allows you to define key-value pairs specific to an instance or, more relevantly here, within an instance template used by a managed instance group. Applications running on the instances can query the metadata server to retrieve these values at runtime. This allows you to use the same base image but provide different configurations (e.g., database endpoints, API keys) to different deployments by setting unique metadata in each instance template (D). Storing config on PD (A) is possible but less standard. Bigtable (B) is overkill. Embedding in startup scripts (C) makes updates harder.",conditions:["Deploy multiple unique MIGs from same custom image, need unique config per deployment, use GCE features."],caseStudyContext:null},{id:61,topic:"Monitoring & Logging",question:"Your application performs well when tested locally, but it runs significantly slower after you deploy it to a Compute Engine instance. You need to diagnose the problem. What should you do?",options:{A:"File a ticket with Cloud Support indicating that the application performs faster locally.",B:"Use Cloud Debugger snapshots to look at a point-in-time execution of the application.",C:"Use Cloud Profiler to determine which functions within the application take the longest amount of time.",D:"Add logging commands to the application and use Cloud Logging to check where the latency problem occurs."},correctAnswer:["C"],explanation:"When an application runs slower in a different environment, performance profiling is needed to identify bottlenecks. Cloud Profiler continuously collects CPU usage and memory allocation data from production applications with low overhead. It attributes this information to the source code, helping pinpoint which functions or code paths are consuming the most resources or time (C). Filing a ticket (A) isn't a diagnostic step. Debugger snapshots (B) show state at one point, not performance over time. Adding logging (D) can help but Profiler is specifically designed for performance analysis.",conditions:["Application slower on GCE than locally, need to diagnose performance issue."],caseStudyContext:null},{id:62,topic:"Compute",question:`You have an application running in App Engine. Your application is instrumented with Stackdriver Trace. The /product-details request reports details about four known unique products at /sku-details as shown below. You want to reduce the time it takes for the request to complete.
What should you do?`,options:{A:"Increase the size of the instance class.",B:"Change the Persistent Disk type to SSD.",C:"Change /product-details to perform the requests in parallel.",D:"Store the /sku-details information in a database, and replace the webservice call with a database query."},correctAnswer:["C"],explanation:"The trace likely shows four sequential calls to `/sku-details`, each taking some time, contributing to the total latency of `/product-details`. Since fetching details for one product likely doesn't depend on another, these calls can be made concurrently (in parallel). This significantly reduces the total time, as the calls execute largely simultaneously instead of one after another (C). Increasing instance size (A) or changing disk type (B - App Engine doesn't use user-managed PDs) might not help if the bottleneck is sequential network calls. Storing in a DB (D) might be an option but parallelizing existing calls is a more direct fix for the observed sequential bottleneck.",conditions:["App Engine app, Trace shows sequential calls to /sku-details within /product-details request, need to reduce latency."],caseStudyContext:null},{id:63,topic:"Databases",question:`Your company has a data warehouse that keeps your application information in BigQuery. The BigQuery data warehouse keeps 2 PBs of user data. Recently, your company expanded your user base to include EU users and needs to comply with these requirements:

 Your company must be able to delete all user account information upon user request.
 All EU user data must be stored in a single region specifically for EU users.
Which two actions should you take? (Choose two.)`,options:{A:"Use BigQuery federated queries to query data from Cloud Storage.",B:"Create a dataset in the EU region that will keep information about EU users only.",C:"Create a Cloud Storage bucket in the EU region to store information for EU users only.",D:"Re-upload your data using to a Cloud Dataflow pipeline by filtering your user records out.",E:"Use DML statements in BigQuery to update/delete user records based on their requests."},correctAnswer:["B","E"],explanation:"To comply with the EU data residency requirement, create a separate BigQuery dataset located in an EU region specifically for EU user data (B). This ensures their data stays within the EU. To comply with the right to erasure (delete user data on request), use BigQuery's Data Manipulation Language (DML) statements, specifically `DELETE`, to remove specific user records from the tables (E). Federated queries (A), GCS buckets (C), and Dataflow filtering during re-upload (D) don't directly address both requirements within the BigQuery warehouse context.",conditions:["BigQuery data warehouse (2PB), EU users added, GDPR compliance needed (data deletion, EU data residency)."],caseStudyContext:null},{id:64,topic:"Compute",question:`Your App Engine standard configuration is as follows:
service: production
instance_class: B1
You want to limit the application to 5 instances.
Which code snippet should you include in your configuration?`,options:{A:"manual_scaling: instances: 5 min_pending_latency: 30ms",B:"manual_scaling: max_instances: 5 idle_timeout: 10m",C:"basic_scaling: instances: 5 min_pending_latency: 30ms",D:"basic_scaling: max_instances: 5 idle_timeout: 10m"},correctAnswer:["D"],explanation:"App Engine Standard offers several scaling types. `manual_scaling` requires setting a fixed number of `instances`. `basic_scaling` scales based on request load up to a maximum. `automatic_scaling` (default) is more dynamic. To limit the *maximum* number of instances while still allowing scaling based on load (implied since no fixed instance count is desired), `basic_scaling` with the `max_instances` parameter set to 5 is the correct approach (D). Option A sets a fixed number. Option B uses `max_instances` with `manual_scaling`, which is contradictory. Option C uses `instances` with `basic_scaling`, which is incorrect syntax.",conditions:["App Engine Standard app, instance class B1, limit maximum instances to 5."],caseStudyContext:null},{id:65,topic:"Databases",question:`Your analytics system executes queries against a BigQuery dataset. The SQL query is executed in batch and passes the contents of a SQL file to the BigQuery CLI. Then it redirects the BigQuery CLI output to another process. However, you are getting a permission error from the BigQuery CLI when the queries are executed.
You want to resolve the issue. What should you do?`,options:{A:"Grant the service account BigQuery Data Viewer and BigQuery Job User roles.",B:"Grant the service account BigQuery Data Editor and BigQuery Data Viewer roles.",C:"Create a view in BigQuery from the SQL query and SELECT* from the view in the CLI.",D:"Create a new dataset in BigQuery, and copy the source table to the new dataset Query the new dataset and table from the CLI."},correctAnswer:["A"],explanation:"To execute a query using the BigQuery CLI (or API), the identity running the command (likely a service account in an automated system) needs two key permissions: permission to run jobs (specifically query jobs), provided by the `roles/bigquery.jobUser` role, and permission to read the data from the tables being queried, provided by the `roles/bigquery.dataViewer` role (or higher). Granting both roles (A) provides the necessary permissions following the principle of least privilege. Data Editor (B) is excessive if only reading is needed. Creating views (C) or copying data (D) doesn't fix the permission issue for running the query itself.",conditions:["BigQuery query executed via CLI (batch mode) fails with permission error."],caseStudyContext:null},{id:66,topic:"Compute",question:`Your application is running on Compute Engine and is showing sustained failures for a small number of requests. You have narrowed the cause down to a single Compute Engine instance, but the instance is unresponsive to SSH.
What should you do next?`,options:{A:"Reboot the machine.",B:"Enable and check the serial port output.",C:"Delete the machine and create a new one.",D:"Take a snapshot of the disk and attach it to a new machine."},correctAnswer:["B"],explanation:"When an instance is unresponsive via SSH, the next step is often to check the serial console output. The serial console logs kernel messages and system events, including boot processes and potential crash information, which can be accessed even if the network stack or SSH daemon is down. This can help diagnose the root cause of the unresponsiveness (B). Rebooting (A) might fix it temporarily but doesn't diagnose. Deleting (C) loses diagnostic data. Attaching the disk elsewhere (D) is a more involved diagnostic step, usually done after checking serial logs.",conditions:["Single GCE instance failing, unresponsive to SSH."],caseStudyContext:null},{id:67,topic:"Compute",question:`You configured your Compute Engine instance group to scale automatically according to overall CPU usage. However, your application's response latency increases sharply before the cluster has finished adding up instances. You want to provide a more consistent latency experience for your end users by changing the configuration of the instance group autoscaler.
Which two configuration changes should you make? (Choose two.)`,options:{A:"Add the label AUTOSCALE to the instance group template.",B:"Decrease the cool-down period for instances added to the group.",C:"Increase the target CPU usage for the instance group autoscaler.",D:"Decrease the target CPU usage for the instance group autoscaler.",E:"Remove the health-check for individual VMs in the instance group."},correctAnswer:["B","D"],explanation:"The latency increases before scaling completes, suggesting the autoscaler reacts too slowly or too late. To make it react sooner, decrease the target CPU utilization threshold (D)  this triggers scaling at a lower average CPU usage. To allow the autoscaler to add more instances more quickly after a scale-out event (reducing the delay before it can scale out again if needed), decrease the cool-down period (B). Increasing target CPU (C) would worsen the problem. Labels (A) and health checks (E) are not directly related to autoscaling thresholds or speed.",conditions:["MIG autoscaling on CPU usage, latency spikes before scaling completes."],caseStudyContext:null},{id:68,topic:"Compute",question:`You have an application controlled by a managed instance group. When you deploy a new version of the application, costs should be minimized and the number of instances should not increase. You want to ensure that, when each new instance is created, the deployment only continues if the new instance is healthy.
What should you do?`,options:{A:"Perform a rolling-action with maxSurge set to 1, maxUnavailable set to 0.",B:"Perform a rolling-action with maxSurge set to 0, maxUnavailable set to 1",C:"Perform a rolling-action with maxHealthy set to 1, maxUnhealthy set to 0.",D:"Perform a rolling-action with maxHealthy set to 0, maxUnhealthy set to 1."},correctAnswer:["B"],explanation:"The requirement is to minimize cost and *not increase* the instance count during the update. This means `maxSurge` (extra instances) must be 0. To allow the update to proceed by replacing instances one by one, `maxUnavailable` should be set to 1 (or more, but 1 minimizes disruption). This configuration (B) ensures one old instance is terminated, then one new instance is created and must become healthy (pass health checks) before the next old instance is terminated. Option A increases instance count. C and D use invalid parameters.",conditions:["MIG update, minimize cost, instance count must not increase, proceed only if new instance is healthy."],caseStudyContext:null},{id:69,topic:"IAM & Security",question:`Your application requires service accounts to be authenticated to GCP products via credentials stored on its host Compute Engine virtual machine instances. You want to distribute these credentials to the host instances as securely as possible.
What should you do?`,options:{A:"Use HTTP signed URLs to securely provide access to the required resources.",B:"Use the instance's service account Application Default Credentials to authenticate to the required resources.",C:"Generate a P12 file from the GCP Console after the instance is deployed, and copy the credentials to the host instance before starting the application.",D:"Commit the credential JSON file into your application's source repository, and have your CI/CD process package it with the software that is deployed to the instance."},correctAnswer:["B"],explanation:"The most secure and recommended way for applications running on Compute Engine to authenticate to Google Cloud APIs is by using the service account attached to the instance. Google Cloud client libraries automatically find these credentials via Application Default Credentials (ADC). This avoids the need to manually generate, distribute, and manage key files (like P12 or JSON keys) stored on the instance (C) or in source code (D), which is insecure. Signed URLs (A) are for granting temporary access to specific resources, not for general application authentication.",conditions:["GCE application needs SA credentials stored on host, distribute securely."],caseStudyContext:null},{id:70,topic:"Compute",question:`Your application is deployed in a Google Kubernetes Engine (GKE) cluster. You want to expose this application publicly behind a Cloud Load Balancing HTTP(S) load balancer.
What should you do?`,options:{A:"Configure a GKE Ingress resource.",B:"Configure a GKE Service resource.",C:"Configure a GKE Ingress resource with type: LoadBalancer.",D:"Configure a GKE Service resource with type: LoadBalancer."},correctAnswer:["A"],explanation:"In GKE, the standard way to expose HTTP(S) services publicly via a Google Cloud HTTP(S) Load Balancer is by creating an Ingress resource (A). The GKE Ingress controller automatically provisions and configures the necessary Cloud Load Balancer components based on the Ingress definition. A Service resource (B) primarily handles internal load balancing or node port exposure. A Service of `type: LoadBalancer` (D) provisions a Network Load Balancer (L4), not an HTTP(S) Load Balancer (L7). Ingress does not have a `type: LoadBalancer` field (C).",conditions:["Expose GKE application publicly via HTTP(S) Load Balancer."],caseStudyContext:null},{id:71,topic:"Migration",question:`Your company is planning to migrate their on-premises Hadoop environment to the cloud. Increasing storage cost and maintenance of data stored in HDFS is a major concern for your company. You also want to make minimal changes to existing data analytics jobs and existing architecture.
How should you proceed with the migration?`,options:{A:"Migrate your data stored in Hadoop to BigQuery. Change your jobs to source their information from BigQuery instead of the on-premises Hadoop environment.",B:"Create Compute Engine instances with HDD instead of SSD to save costs. Then perform a full migration of your existing environment into the new one in Compute Engine instances.",C:"Create a Cloud Dataproc cluster on Google Cloud Platform, and then migrate your Hadoop environment to the new Cloud Dataproc cluster. Move your HDFS data into larger HDD disks to save on storage costs.",D:"Create a Cloud Dataproc cluster on Google Cloud Platform, and then migrate your Hadoop code objects to the new cluster. Move your data to Cloud Storage and leverage the Cloud Dataproc connector to run jobs on that data."},correctAnswer:["D"],explanation:"To address HDFS cost/maintenance while minimizing job changes, the best approach is to decouple compute and storage. Use Cloud Dataproc (managed Hadoop/Spark) for running the existing jobs (minimal changes) and migrate the HDFS data to Google Cloud Storage (GCS), which is cost-effective and low-maintenance. Dataproc clusters can directly process data in GCS using the Cloud Storage connector (D). Migrating data to BigQuery (A) requires rewriting jobs. Migrating HDFS to GCE disks (B, C) doesn't fully address the cost/maintenance concerns compared to GCS.",conditions:["Migrate on-prem Hadoop to cloud, reduce HDFS cost/maintenance, minimal changes to jobs/architecture."],caseStudyContext:null},{id:72,topic:"Storage",question:`Your data is stored in Cloud Storage buckets. Fellow developers have reported that data downloaded from Cloud Storage is resulting in slow API performance.
You want to research the issue to provide details to the GCP support team.
Which command should you run?`,options:{A:'gsutil test "o output.json gs://my-bucket',B:'gsutil perfdiag "o output.json gs://my-bucket',C:'gcloud compute scp example-instance:~/test-data "o output.json gs://my-bucket',D:'gcloud services test "o output.json gs://my-bucket'},correctAnswer:["B"],explanation:"The `gsutil perfdiag` command is specifically designed to run diagnostic tests against a Cloud Storage bucket to measure performance for various operations (uploads, downloads, metadata requests) and collect detailed information useful for troubleshooting performance issues with GCP support. The output can be saved to a file (using `-o`) for sharing. `gsutil test` runs integration tests for gsutil itself. `gcloud compute scp` copies files to/from GCE. `gcloud services test` doesn't exist in this context.",conditions:["Slow GCS download performance reported, need to gather diagnostic data for support."],caseStudyContext:null},{id:73,topic:"Compute",question:`You are using Cloud Build build to promote a Docker image to Development, Test, and Production environments. You need to ensure that the same Docker image is deployed to each of these environments.
How should you identify the Docker image in your build?`,options:{A:"Use the latest Docker image tag.",B:"Use a unique Docker image name.",C:"Use the digest of the Docker image.",D:"Use a semantic version Docker image tag."},correctAnswer:["C"],explanation:"Docker image digests (e.g., sha256:...) are immutable content hashes of the image layers and configuration. Using the digest guarantees that the exact same image content is being referenced across environments, regardless of whether tags (like 'latest' or 'v1.2.0') have been moved or overwritten. Tags (A, D) are mutable pointers and don't guarantee immutability. Unique names (B) don't solve the versioning/content identity problem.",conditions:["Promote same Docker image across Dev, Test, Prod using Cloud Build."],caseStudyContext:null},{id:74,topic:"Compute",question:`Your company has created an application that uploads a report to a Cloud Storage bucket. When the report is uploaded to the bucket, you want to publish a message to a Cloud Pub/Sub topic. You want to implement a solution that will take a small amount to effort to implement.
What should you do?`,options:{A:"Configure the Cloud Storage bucket to trigger Cloud Pub/Sub notifications when objects are modified.",B:"Create an App Engine application to receive the file; when it is received, publish a message to the Cloud Pub/Sub topic.",C:"Create a Cloud Function that is triggered by the Cloud Storage bucket. In the Cloud Function, publish a message to the Cloud Pub/Sub topic.",D:"Create an application deployed in a Google Kubernetes Engine cluster to receive the file; when it is received, publish a message to the Cloud Pub/Sub topic."},correctAnswer:["A"],explanation:"Cloud Storage has a built-in feature to directly send notifications to a Pub/Sub topic upon specific object events (like creation/finalization). This requires minimal configuration via the console or `gsutil notification` command (A) and is the simplest, least-effort solution. Using a Cloud Function trigger (C) works but adds an extra service layer. Options B and D involve building custom receiving applications, which is significantly more effort.",conditions:["Publish Pub/Sub message when report uploaded to GCS bucket, minimal effort."],caseStudyContext:null},{id:75,topic:"Databases",question:`Your teammate has asked you to review the code below, which is adding a credit to an account balance in Cloud Datastore.

\`\`\`java
// Pseudo-code
func AddCredit(acctId, amount) {
  acct = datastore.Get(acctId)
  new_balance = acct.balance + amount
  acct.balance = new_balance
  datastore.Put(acct)
  return acct
}
\`\`\`

Which improvement should you suggest your teammate make?`,options:{A:"Get the entity with an ancestor query.",B:"Get and put the entity in a transaction.",C:"Use a strongly consistent transactional database.",D:"Don't return the account entity from the function."},correctAnswer:["B"],explanation:"The read-modify-write pattern shown (Get, calculate new balance, Put) is susceptible to race conditions if multiple requests try to update the same account concurrently. The final balance could be incorrect. Performing the Get and Put operations within a Datastore transaction (B) ensures atomicity. The transaction will retry if the entity was modified between the Get and Put, guaranteeing a consistent update. Ancestor queries (A) relate to consistency within entity groups. Suggesting a different database (C) isn't reviewing the code. Return value (D) is irrelevant to the core consistency problem.",conditions:["Code review for Datastore balance update, potential concurrency issue."],caseStudyContext:null},{id:76,topic:"Compute",question:`Your company stores their source code in a Cloud Source Repositories repository. Your company wants to build and test their code on each source code commit to the repository and requires a solution that is managed and has minimal operations overhead.
Which method should they use?`,options:{A:"Use Cloud Build with a trigger configured for each source code commit.",B:"Use Jenkins deployed via the Google Cloud Platform Marketplace, configured to watch for source code commits.",C:"Use a Compute Engine virtual machine instance with an open source continuous integration tool, configured to watch for source code commits.",D:"Use a source code commit trigger to push a message to a Cloud Pub/Sub topic that triggers an App Engine service to build the source code."},correctAnswer:["A"],explanation:"Cloud Build is Google Cloud's fully managed CI/CD platform. It integrates directly with Cloud Source Repositories and allows creating triggers that automatically start builds on events like code commits (A). This meets the requirements of being managed and having minimal operational overhead. Jenkins (B) or other CI tools on GCE (C) require managing the CI server infrastructure. Using Pub/Sub and App Engine (D) is overly complex for a standard CI build process.",conditions:["Build/test code on each commit to CSR, require managed solution, minimal ops overhead."],caseStudyContext:null},{id:77,topic:"IAM & Security",question:`You are writing a Compute Engine hosted application in project A that needs to securely authenticate to a Cloud Pub/Sub topic in project B.
What should you do?`,options:{A:"Configure the instances with a service account owned by project B. Add the service account as a Cloud Pub/Sub publisher to project A.",B:"Configure the instances with a service account owned by project A. Add the service account as a publisher on the topic.",C:"Configure Application Default Credentials to use the private key of a service account owned by project B. Add the service account as a Cloud Pub/Sub publisher to project A.",D:"Configure Application Default Credentials to use the private key of a service account owned by project A. Add the service account as a publisher on the topic"},correctAnswer:["B"],explanation:"The Compute Engine instance runs in Project A and should use a service account from Project A as its identity. To allow this service account (the actor) to publish to a topic (the resource) in Project B, you need to grant the service account from Project A the necessary IAM role (e.g., `roles/pubsub.publisher`) on the specific topic in Project B. This follows the principle of granting permissions on the resource being accessed to the identity accessing it (B). Option A incorrectly uses an SA from Project B on Project A's instance. Options C and D involve managing private keys, which is less secure than using the attached service account via ADC.",conditions:["GCE app in Project A needs to publish to Pub/Sub topic in Project B securely."],caseStudyContext:null},{id:78,topic:"IAM & Security",question:`You are developing a corporate tool on Compute Engine for the finance department, which needs to authenticate users and verify that they are in the finance department. All company employees use G Suite.
What should you do?`,options:{A:"Enable Cloud Identity-Aware Proxy on the HTTP(s) load balancer and restrict access to a Google Group containing users in the finance department. Verify the provided JSON Web Token within the application.",B:"Enable Cloud Identity-Aware Proxy on the HTTP(s) load balancer and restrict access to a Google Group containing users in the finance department. Issue client-side certificates to everybody in the finance team and verify the certificates in the application.",C:"Configure Cloud Armor Security Policies to restrict access to only corporate IP address ranges. Verify the provided JSON Web Token within the application.",D:"Configure Cloud Armor Security Policies to restrict access to only corporate IP address ranges. Issue client side certificates to everybody in the finance team and verify the certificates in the application."},correctAnswer:["A"],explanation:"Cloud Identity-Aware Proxy (IAP) integrates with Google Workspace (G Suite) identities. By enabling IAP on the Load Balancer fronting the GCE application, you can enforce authentication. You can then restrict access based on Google Group membership (e.g., a 'finance-dept' group). IAP verifies the user's identity and group membership. It also forwards a signed JWT header to the backend application, which the application can optionally verify to get user identity details (A). Client certificates (B, D) add complexity. Cloud Armor (C, D) restricts by IP, not user identity/group.",conditions:["Internal GCE tool for finance dept, authenticate G Suite users, verify finance dept membership."],caseStudyContext:null},{id:79,topic:"Hybrid & Multi-cloud",question:`Your API backend is running on multiple cloud providers. You want to generate reports for the network latency of your API.
Which two steps should you take? (Choose two.)`,options:{A:"Use Zipkin collector to gather data.",B:"Use Fluentd agent to gather data.",C:"Use Stackdriver Trace to generate reports.",D:"Use Stackdriver Debugger to generate report.",E:"Use Stackdriver Profiler to generate report."},correctAnswer:["A","C"],explanation:"To trace requests across multiple cloud providers, you need a distributed tracing system compatible with different environments. OpenTelemetry or Zipkin (A) are common open standards/tools for instrumenting applications to generate and collect trace data. This collected data can then be sent to a backend for analysis and reporting. Cloud Trace (formerly Stackdriver Trace) (C) can ingest trace data (including from Zipkin via adapters/collectors) and provides analysis, visualization, and reporting capabilities for latency. Fluentd (B) is for logs. Debugger (D) and Profiler (E) are for debugging code state and resource usage, respectively.",conditions:["API backend on multiple clouds, need network latency reports."],caseStudyContext:null},{id:80,topic:"Case Study Analysis",question:"Case study - HipLocal: Which database should HipLocal use for storing user activity?",options:{A:"BigQuery",B:"Cloud SQL",C:"Cloud Spanner",D:"Cloud Datastore"},correctAnswer:["A"],explanation:"The business requirement is 'Obtain user activity metrics to better understand how to monetize their product.' This implies analyzing potentially large volumes of user activity data (clicks, views, interactions). BigQuery (A) is Google Cloud's data warehouse service, specifically designed for large-scale analytics on structured and semi-structured data. While other databases (B, C, D) store data, BigQuery is the optimal choice for the analytical purpose stated in the requirement.",conditions:["HipLocal case study context, store user activity, need metrics for monetization."],caseStudyContext:`<strong>Company overview</strong>
HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.

<strong>Executive statement</strong>
We are the number one local community app; its time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.

<strong><strong>Solution Concept </strong><br></strong>
HipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.

<strong><strong>Existing Technical Environment </strong><br></strong>
HipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:
<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>

<strong><strong>Business Requirements </strong><br></strong>
HipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:
<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>

<strong>Technical requirements</strong>
<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>`},{id:81,topic:"Case Study Analysis",question:"Case study - HipLocal: HipLocal is configuring their access controls. Which firewall configuration should they implement?",options:{A:"Block all traffic on port 443.",B:"Allow all traffic into the network.",C:"Allow traffic on port 443 for a specific tag.",D:"Allow all traffic on port 443 into the network."},correctAnswer:["C"],explanation:"HipLocal needs to expose its application APIs (currently on GCE). Following security best practices, they should restrict ingress traffic as much as possible. Allowing traffic only on necessary ports (like 443 for HTTPS) and targeting specific instances using network tags (C) is the principle of least privilege applied to firewall rules. Blocking 443 (A) would deny access. Allowing all traffic (B) or all traffic on 443 to *all* instances (D) is too permissive.",conditions:["HipLocal case study context, configure firewall rules for application access."],caseStudyContext:`<strong>Company overview</strong>
HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.

<strong>Executive statement</strong>
We are the number one local community app; its time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.

<strong><strong>Solution Concept </strong><br></strong>
HipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.

<strong><strong>Existing Technical Environment </strong><br></strong>
HipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:
<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>

<strong><strong>Business Requirements </strong><br></strong>
HipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:
<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>

<strong>Technical requirements</strong>
<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>`},{id:82,topic:"Case Study Analysis",question:"Case study - HipLocal: HipLocal's data science team wants to analyze user reviews. How should they prepare the data?",options:{A:"Use the Cloud Data Loss Prevention API for redaction of the review dataset.",B:"Use the Cloud Data Loss Prevention API for de-identification of the review dataset.",C:"Use the Cloud Natural Language Processing API for redaction of the review dataset.",D:"Use the Cloud Natural Language Processing API for de-identification of the review dataset."},correctAnswer:["B"],explanation:"User reviews may contain Personally Identifiable Information (PII). To analyze reviews while complying with regulations (like GDPR mentioned in requirements), PII must be handled appropriately. The Cloud Data Loss Prevention (DLP) API is designed to discover, classify, and de-identify (e.g., redact, mask, tokenize) sensitive data (B). Redaction (A) is one specific form of de-identification. The Natural Language API (C, D) analyzes text content (sentiment, entities) but isn't the primary tool for PII de-identification.",conditions:["HipLocal case study context, analyze user reviews, prepare data appropriately."],caseStudyContext:`<strong>Company overview</strong>
HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.

<strong>Executive statement</strong>
We are the number one local community app; its time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.

<strong><strong>Solution Concept </strong><br></strong>
HipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.

<strong><strong>Existing Technical Environment </strong><br></strong>
HipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:
<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>

<strong><strong>Business Requirements </strong><br></strong>
HipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:
<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>

<strong>Technical requirements</strong>
<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>`},{id:83,topic:"Case Study Analysis",question:"Case study - HipLocal: In order for HipLocal to store application state and meet their stated <strong>Business Requirements </strong><br>, which database service should they migrate to?",options:{A:"Cloud Spanner",B:"Cloud Datastore",C:"Cloud Memorystore as a cache",D:"Separate Cloud SQL clusters for each region"},correctAnswer:["A"],explanation:"This question is similar to #48. <strong>Business Requirements </strong><br> include global expansion, 10x user support, and consistent experience across regions. The current single MySQL instance won't suffice. Cloud Spanner (A) is the globally distributed, strongly consistent, horizontally scalable relational database service best suited to meet these requirements. Datastore (B) is NoSQL. Memorystore (C) is a cache, not primary state storage. Separate Cloud SQL clusters (D) add management complexity and don't offer the same seamless global consistency as Spanner.",conditions:["HipLocal case study context, store application state, meet global scale/consistency requirements."],caseStudyContext:`<strong>Company overview</strong>
HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.

<strong>Executive statement</strong>
We are the number one local community app; its time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.

<strong><strong>Solution Concept </strong><br></strong>
HipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.

<strong><strong>Existing Technical Environment </strong><br></strong>
HipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:
<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>

<strong><strong>Business Requirements </strong><br></strong>
HipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:
<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>

<strong>Technical requirements</strong>
<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>`},{id:84,topic:"Compute",question:`You have an application deployed in production. When a new version is deployed, you want to ensure that all production traffic is routed to the new version of your application. You also want to keep the previous version deployed so that you can revert to it if there is an issue with the new version.
Which deployment strategy should you use?`,options:{A:"Blue/green deployment",B:"Canary deployment",C:"Rolling deployment",D:"Recreate deployment"},correctAnswer:["A"],explanation:"Blue/green deployment involves having two identical production environments (Blue and Green). The new version is deployed to the inactive environment (e.g., Green). Once tested, all traffic is switched (via load balancer/DNS) from the current environment (Blue) to the new one (Green). The old environment (Blue) is kept idle, allowing for instant rollback by simply switching traffic back if issues arise. This meets the requirements of routing all traffic to the new version while keeping the old one available for rollback (A). Canary (B) and Rolling (C) involve gradual traffic shifting. Recreate (D) involves downtime.",conditions:["Deploy new version, route all traffic to new version, keep old version for instant rollback."],caseStudyContext:null},{id:85,topic:"Compute",question:`You are porting an existing Apache/MySQL/PHP application stack from a single machine to Google Kubernetes Engine. You need to determine how to containerize the application. Your approach should follow Google-recommended best practices for availability.
What should you do?`,options:{A:"Package each component in a separate container. Implement readiness and liveness probes.",B:"Package the application in a single container. Use a process management tool to manage each component.",C:"Package each component in a separate container. Use a script to orchestrate the launch of the components.",D:"Package the application in a single container. Use a bash script as an entrypoint to the container, and then spawn each component as a background job."},correctAnswer:["A"],explanation:"The recommended best practice for containerizing multi-component applications like a LAMP stack is to package each major component (Apache, PHP-FPM, MySQL - though MySQL is often run outside the app cluster) into its own separate container (A). This allows independent scaling, updating, and resource management for each component. Using liveness and readiness probes ensures Kubernetes can manage the health and traffic routing for each container correctly. Running multiple processes in a single container (B, D) is an anti-pattern, making management and health checking difficult. Script orchestration (C) is less robust than Kubernetes' native mechanisms.",conditions:["Containerize Apache/MySQL/PHP stack for GKE, follow availability best practices."],caseStudyContext:null},{id:86,topic:"Compute",question:"You are developing an application that will be launched on Compute Engine instances into multiple distinct projects, each corresponding to the environments in your software development process (development, QA, staging, and production). The instances in each project have the same application code but a different configuration. During deployment, each instance should receive the application's configuration based on the environment it serves. You want to minimize the number of steps to configure this flow. What should you do?",options:{A:"When creating your instances, configure a startup script using the gcloud command to determine the project name that indicates the correct environment.",B:"In each project, configure a metadata key environment whose value is the environment it serves. Use your deployment tool to query the instance metadata and configure the application based on the environment value.",C:"Deploy your chosen deployment tool on an instance in each project. Use a deployment job to retrieve the appropriate configuration file from your version control system, and apply the configuration when deploying the application on each instance.",D:"During each instance launch, configure an instance custom-metadata key named environment whose value is the environment the instance serves. Use your deployment tool to query the instance metadata, and configure the application based on the environment value."},correctAnswer:["B"],explanation:"Project-level metadata allows defining key-value pairs that are accessible by all instances within that project. Setting an `environment` key in each project's metadata (B) provides a simple way for instances to identify their environment without needing instance-specific configuration during launch (unlike D). Startup scripts (A) could work but querying metadata is cleaner. Option C involves managing deployment tools per project.",conditions:["Deploy app to GCE in multiple projects (dev, QA, stage, prod), same code, different config per environment, minimize config steps."],caseStudyContext:null},{id:87,topic:"Databases",question:"You are developing an ecommerce application that stores customer, order, and inventory data as relational tables inside Cloud Spanner. During a recent load test, you discover that Spanner performance is not scaling linearly as expected. Which of the following is the cause?",options:{A:"The use of 64-bit numeric types for 32-bit numbers.",B:"The use of the STRING data type for arbitrary-precision values.",C:"The use of Version 1 UUIDs as primary keys that increase monotonically.",D:"The use of LIKE instead of STARTS_WITH keyword for parameterized SQL queries."},correctAnswer:["C"],explanation:"A common cause of non-linear scaling and performance issues in Cloud Spanner is hotspotting, where a disproportionate amount of traffic targets a single Spanner server/split. This frequently occurs when primary keys are chosen such that new inserts always go to the end of the key space (e.g., monotonically increasing timestamps or sequences). Version 1 UUIDs often contain a timestamp component, leading to monotonic increases and potential hotspotting (C). Data types (A, B) or query keywords (D) are less likely to cause fundamental scaling bottlenecks compared to poor key design.",conditions:["Spanner application, non-linear performance scaling under load."],caseStudyContext:null},{id:88,topic:"Compute",question:"You are developing an application that reads credit card data from a Pub/Sub subscription. You have written code and completed unit testing. You need to test the Pub/Sub integration before deploying to Google Cloud. What should you do?",options:{A:"Create a service to publish messages, and deploy the Pub/Sub emulator. Generate random content in the publishing service, and publish to the emulator.",B:"Create a service to publish messages to your application. Collect the messages from Pub/Sub in production, and replay them through the publishing service.",C:"Create a service to publish messages, and deploy the Pub/Sub emulator. Collect the messages from Pub/Sub in production, and publish them to the emulator.",D:"Create a service to publish messages, and deploy the Pub/Sub emulator. Publish a standard set of testing messages from the publishing service to the emulator."},correctAnswer:["D"],explanation:"The Pub/Sub emulator allows local development and testing of applications that interact with Pub/Sub without needing to connect to the actual Google Cloud service. To test the integration, you should deploy the emulator locally, configure your application to connect to it, and then publish a controlled set of *known* test messages (D) to the emulator topic. This allows verifying that your subscriber code correctly processes these specific test messages. Using random data (A) makes validation harder. Using production data (B, C) is risky, especially with credit card data.",conditions:["Test Pub/Sub integration locally before deployment, application reads credit card data."],caseStudyContext:null},{id:89,topic:"Compute",question:`You are designing an application that will subscribe to and receive messages from a single Pub/Sub topic and insert corresponding rows into a database. Your application runs on Linux and leverages preemptible virtual machines to reduce costs. You need to create a shutdown script that will initiate a graceful shutdown.
What should you do?`,options:{A:"Write a shutdown script that uses inter-process signals to notify the application process to disconnect from the database.",B:"Write a shutdown script that broadcasts a message to all signed-in users that the Compute Engine instance is going down and instructs them to save current work and sign out.",C:"Write a shutdown script that writes a file in a location that is being polled by the application once every five minutes. After the file is read, the application disconnects from the database.",D:"Write a shutdown script that publishes a message to the Pub/Sub topic announcing that a shutdown is in progress. After the application reads the message, it disconnects from the database."},correctAnswer:["A"],explanation:"Preemptible VMs receive a shutdown signal (ACPI G2 Soft Off) shortly before preemption. A shutdown script configured on the instance runs during this brief window. The most reliable way for the script to notify the running application process to shut down gracefully (finish processing, close DB connections) is via standard Linux inter-process signals like SIGTERM (A). Broadcasting to users (B) is irrelevant. Polling a file (C) is slow and unreliable. Publishing to the same Pub/Sub topic (D) adds delay and complexity.",conditions:["Application on preemptible VMs reads from Pub/Sub, writes to DB, needs graceful shutdown script."],caseStudyContext:null},{id:90,topic:"Compute",question:"You work for a web development team at a small startup. Your team is developing a Node.js application using Google Cloud services, including Cloud Storage and Cloud Build. The team uses a Git repository for version control. Your manager calls you over the weekend and instructs you to make an emergency update to one of the company's websites, and you're the only developer available. You need to access Google Cloud to make the update, but you don't have your work laptop. You are not allowed to store source code locally on a non-corporate computer. How should you set up your developer environment?",options:{A:"Use a text editor and the Git command line to send your source code updates as pull requests from a public computer.",B:"Use a text editor and the Git command line to send your source code updates as pull requests from a virtual machine running on a public computer.",C:"Use Cloud Shell and the built-in code editor for development. Send your source code updates as pull requests.",D:"Use a Cloud Storage bucket to store the source code that you need to edit. Mount the bucket to a public computer as a drive, and use a code editor to update the code. Turn on versioning for the bucket, and point it to the team's Git repository."},correctAnswer:["C"],explanation:"The requirement is to make changes without storing code locally on a non-corporate computer. Cloud Shell provides a browser-based Linux environment with pre-installed tools (gcloud, git, editors like Code-OSS) and persistent home directory storage. You can clone the repository within Cloud Shell, make changes using the built-in editor, and push commits/pull requests without any code residing on the local (public) computer (C). Options A and B involve storing code locally. Option D uses GCS inappropriately for code editing and version control.",conditions:["Emergency update needed, no work laptop, cannot store code locally on non-corporate machine."],caseStudyContext:null},{id:91,topic:"Monitoring & Logging",question:"Your team develops services that run on Google Kubernetes Engine. You need to standardize their log data using Google-recommended practices and make the data more useful in the fewest number of steps. What should you do? (Choose two.)",options:{A:"Create aggregated exports on application logs to BigQuery to facilitate log analytics.",B:"Create aggregated exports on application logs to Cloud Storage to facilitate log analytics.",C:"Write log output to standard output (stdout) as single-line JSON to be ingested into Cloud Logging as structured logs.",D:"Mandate the use of the Logging API in the application code to write structured logs to Cloud Logging.",E:"Mandate the use of the Pub/Sub API to write structured data to Pub/Sub and create a Dataflow streaming pipeline to normalize logs and write them to BigQuery for analytics."},correctAnswer:["A","C"],explanation:"The recommended practice for logging in GKE is to write logs to standard output (stdout) or standard error (stderr). Writing logs as structured JSON (C) allows Cloud Logging to automatically parse them, making them easily searchable and filterable. To make the data more useful for complex analysis, exporting logs from Cloud Logging to BigQuery (A) is the standard approach, enabling powerful SQL-based querying. Using the Logging API directly (D) works but stdout is often simpler. GCS export (B) is less useful for analytics. Pub/Sub/Dataflow (E) adds complexity.",conditions:["Standardize GKE service logs, make useful for analysis, fewest steps, follow best practices."],caseStudyContext:null},{id:92,topic:"Compute",question:"You are designing a deployment technique for your new applications on Google Cloud. As part of your deployment planning, you want to use live traffic to gather performance metrics for both new and existing applications. You need to test against the full production load prior to launch. What should you do?",options:{A:"Use canary deployment",B:"Use blue/green deployment",C:"Use rolling updates deployment",D:"Use A/B testing with traffic mirroring during deployment"},correctAnswer:["D"],explanation:"The requirement to test against the *full* production load *before* launch while gathering metrics points towards traffic mirroring (also known as shadowing). This involves deploying the new version alongside the old one and configuring the load balancer or service mesh to send a copy of the live production traffic to the new version without affecting user responses (which still come from the old version). This allows full load testing and metric comparison (D). Canary (A) and rolling updates (C) only expose a subset of users initially. Blue/green (B) tests the new environment but typically not with full live load *before* the switch.",conditions:["Deployment planning, gather metrics on new/old versions using live traffic, test against full production load before launch."],caseStudyContext:null},{id:93,topic:"Storage",question:`Your application uses the Cloud Storage API. You review the logs and discover multiple HTTP 503 Service Unavailable error responses from the API. Your application logs the error and does not take any further action. You want to implement Google-recommended retry logic to improve success rates.
Which approach should you take?`,options:{A:"Retry the failures in batch after a set number of failures is logged.",B:"Retry each failure at a set time interval up to a maximum number of times.",C:"Retry each failure at increasing time intervals up to a maximum number of tries.",D:"Retry each failure at decreasing time intervals up to a maximum number of tries."},correctAnswer:["C"],explanation:"HTTP 503 errors indicate temporary server-side unavailability. The Google-recommended practice for handling such transient errors (and others like 429s) is to retry with truncated exponential backoff (C). This means retrying the request after a short delay, and if it fails again, increasing the delay exponentially for subsequent retries, up to a maximum delay and/or maximum number of retries. This avoids overwhelming the potentially recovering service. Fixed intervals (B) or decreasing intervals (D) are less effective. Batch retries (A) are not standard.",conditions:["Application gets HTTP 503 errors from GCS API, need to implement retry logic."],caseStudyContext:null},{id:94,topic:"Compute",question:`You need to redesign the ingestion of audit events from your authentication service to allow it to handle a large increase in traffic. Currently, the audit service and the authentication system run in the same Compute Engine virtual machine. You plan to use the following Google Cloud tools in the new architecture:

 Multiple Compute Engine machines, each running an instance of the authentication service
 Multiple Compute Engine machines, each running an instance of the audit service
 Pub/Sub to send the events from the authentication services.
How should you set up the topics and subscriptions to ensure that the system can handle a large volume of messages and can scale efficiently?`,options:{A:"Create one Pub/Sub topic. Create one pull subscription to allow the audit services to share the messages.",B:"Create one Pub/Sub topic. Create one pull subscription per audit service instance to allow the services to share the messages.",C:"Create one Pub/Sub topic. Create one push subscription with the endpoint pointing to a load balancer in front of the audit services.",D:"Create one Pub/Sub topic per authentication service. Create one pull subscription per topic to be used by one audit service.",E:"Create one Pub/Sub topic per authentication service. Create one push subscription per topic, with the endpoint pointing to one audit service."},correctAnswer:["A"],explanation:"To decouple the authentication services (publishers) from the audit services (subscribers) and handle large, variable volume, a single Pub/Sub topic is appropriate for all auth services to publish events to. To allow multiple audit service instances to consume these messages in parallel and scale independently, a single shared *pull* subscription should be used (A). Each audit service instance pulls messages from this shared subscription, and Pub/Sub automatically distributes messages among the active pullers. Multiple subscriptions per instance (B) would duplicate messages. Push subscriptions (C, E) can be harder to scale reliably under high load compared to pull. Multiple topics (D, E) add unnecessary complexity.",conditions:["Redesign audit event ingestion, decouple auth (multiple GCE) and audit (multiple GCE) services using Pub/Sub, handle high volume, scale efficiently."],caseStudyContext:null},{id:95,topic:"Compute",question:"You are developing a marquee stateless web application that will run on Google Cloud. The rate of the incoming user traffic is expected to be unpredictable, with no traffic on some days and large spikes on other days. You need the application to automatically scale up and down, and you need to minimize the cost associated with running the application. What should you do?",options:{A:"Build the application in Python with Firestore as the database. Deploy the application to Cloud Run.",B:"Build the application in C# with Firestore as the database. Deploy the application to App Engine flexible environment.",C:"Build the application in Python with CloudSQL as the database. Deploy the application to App Engine standard environment.",D:"Build the application in Python with Firestore as the database. Deploy the application to a Compute Engine managed instance group with autoscaling."},correctAnswer:["A"],explanation:"The key requirements are unpredictable traffic, automatic scaling (including down to zero for no traffic), and cost minimization. Cloud Run (A) excels here: it's serverless, scales automatically based on requests (including to zero instances when idle, minimizing cost), and integrates well with serverless databases like Firestore. App Engine Standard (C) also scales to zero but Cloud SQL doesn't scale to zero cost. App Engine Flex (B) has a minimum instance count, increasing cost. GCE MIGs (D) also have minimum instances and more management overhead.",conditions:["Stateless web app, unpredictable traffic (spikes, zero traffic), auto scaling up/down, minimize cost."],caseStudyContext:null},{id:96,topic:"IAM & Security",question:"You have written a Cloud Function that accesses other Google Cloud resources. You want to secure the environment using the principle of least privilege. What should you do?",options:{A:"Create a new service account that has Editor authority to access the resources. The deployer is given permission to get the access token.",B:"Create a new service account that has a custom IAM role to access the resources. The deployer is given permission to get the access token.",C:"Create a new service account that has Editor authority to access the resources. The deployer is given permission to act as the new service account.",D:"Create a new service account that has a custom IAM role to access the resources. The deployer is given permission to act as the new service account."},correctAnswer:["D"],explanation:"The principle of least privilege dictates granting only the necessary permissions. Instead of broad roles like Editor (A, C), create a custom IAM role containing only the specific permissions the function needs to access other resources. Create a dedicated service account for the function and assign this custom role to it. When deploying the function, specify this user-managed service account as the runtime identity. The *deployer* (the user/SA performing the deployment) needs the `iam.serviceAccountUser` role (or the permission `iam.serviceAccounts.actAs`) on the function's service account to deploy the function *as* that service account (D). Options A/C use excessive permissions. Options A/B focus on access tokens, which isn't the primary mechanism here.",conditions:["Cloud Function needs access to other GCP resources, follow least privilege."],caseStudyContext:null},{id:97,topic:"Compute",question:"You are a SaaS provider deploying dedicated blogging software to customers in your Google Kubernetes Engine (GKE) cluster. You want to configure a secure multi-tenant platform to ensure that each customer has access to only their own blog and can't affect the workloads of other customers. What should you do?",options:{A:"Enable Application-layer Secrets on the GKE cluster to protect the cluster.",B:"Deploy a namespace per tenant and use Network Policies in each blog deployment.",C:"Use GKE Audit Logging to identify malicious containers and delete them on discovery.",D:"Build a custom image of the blogging software and use Binary Authorization to prevent untrusted image deployments."},correctAnswer:["B"],explanation:"Kubernetes namespaces provide logical isolation for resources within a cluster. Assigning each tenant (customer) their own namespace is a standard practice for multi-tenancy. Network Policies can then be applied within or between namespaces to restrict network traffic, ensuring tenants cannot access each other's workloads (B). Application-layer secrets (A) protect secrets at rest. Audit logging (C) is for monitoring, not prevention. Binary Authorization (D) controls image provenance, not runtime isolation.",conditions:["SaaS on GKE, multi-tenant, isolate tenants securely."],caseStudyContext:null},{id:98,topic:"Compute",question:"You have decided to migrate your Compute Engine application to Google Kubernetes Engine. You need to build a container image and push it to Artifact Registry using Cloud Build. What should you do? (Choose two.)",options:{A:"Run gcloud builds submit in the directory that contains the application source code.",B:"Run gcloud run deploy app-name --image gcr.io/$PROJECT_ID/app-name in the directory that contains the application source code.",C:"Run gcloud container images add-tag gcr.io/$PROJECT_ID/app-name gcr.io/$PROJECT_ID/app-name:latest in the directory that contains the application source code.",D:`In the application source directory, create a file named cloudbuild.yaml that contains the following contents:

steps:
- name: 'gcr.io/cloud-builders/docker'
  args: ['build', '-t', 'gcr.io/$PROJECT_ID/app-name', '.']
- name: 'gcr.io/cloud-builders/docker'
  args: ['push', 'gcr.io/$PROJECT_ID/app-name']`,E:`In the application source directory, create a file named cloudbuild.yaml that contains the following contents:

steps:
- name: 'gcr.io/cloud-builders/gcloud'
  args: ['app', 'deploy']`},correctAnswer:["A","D"],explanation:"To build and push a container image using Cloud Build, you need to provide build instructions and trigger the build. Creating a `cloudbuild.yaml` file (D) defines the steps: first `docker build` to create the image, then `docker push` to push it to a registry (Artifact Registry or Container Registry - gcr.io paths often work for AR via domain redirection). To initiate the build using this config file (or a Dockerfile if no config file is present), you run `gcloud builds submit` (A) from the directory containing the source code and configuration file. `gcloud run deploy` (B) deploys to Cloud Run. `gcloud container images add-tag` (C) tags existing images. `gcloud app deploy` (E) deploys to App Engine.",conditions:["Build container image from source using Cloud Build, push to Artifact Registry."],caseStudyContext:null},{id:99,topic:"IAM & Security",question:"You are developing an internal application that will allow employees to organize community events within your company. You deployed your application on a single Compute Engine instance. Your company uses Google Workspace (formerly G Suite), and you need to ensure that the company employees can authenticate to the application from anywhere. What should you do?",options:{A:"Add a public IP address to your instance, and restrict access to the instance using firewall rules. Allow your company's proxy as the only source IP address.",B:"Add an HTTP(S) load balancer in front of the instance, and set up Identity-Aware Proxy (IAP). Configure the IAP settings to allow your company domain to access the website.",C:"Set up a VPN tunnel between your company network and your instance's VPC location on Google Cloud. Configure the required firewall rules and routing information to both the on-premises and Google Cloud networks.",D:"Add a public IP address to your instance, and allow traffic from the internet. Generate a random hash, and create a subdomain that includes this hash and points to your instance. Distribute this DNS address to your company's employees."},correctAnswer:["B"],explanation:"The requirement is to allow authenticated employees access from anywhere, integrating with Google Workspace. Identity-Aware Proxy (IAP) provides identity-based access control for applications behind an HTTP(S) Load Balancer. It authenticates users via their Google Workspace account and checks IAM permissions (which can be granted to all users in the domain or specific groups) before allowing access (B). Restricting by corporate proxy IP (A) or VPN (C) limits access when employees are travelling. Obscurity via DNS (D) is not secure authentication.",conditions:["Internal GCE app, authenticate Google Workspace employees, access from anywhere."],caseStudyContext:null},{id:100,topic:"Compute",question:"Your development team is using Cloud Build to promote a Node.js application built on App Engine from your staging environment to production. The application relies on several directories of photos stored in a Cloud Storage bucket named webphotos-staging in the staging environment. After the promotion, these photos must be available in a Cloud Storage bucket named webphotos-prod in the production environment. You want to automate the process where possible. What should you do?",options:{A:"Manually copy the photos to webphotos-prod.",B:"Add a startup script in the application's app.yami file to move the photos from webphotos-staging to webphotos-prod.",C:`Add a build step in the cloudbuild.yaml file before the promotion step with the arguments:

- name: gcr.io/cloud-builders/gsutil
  args: ['cp','-r','gs://webphotos-staging','gs://webphotos-prod']
  waitFor: ['-']`,D:`Add a build step in the cloudbuild.yaml file before the promotion step with the arguments:

- name: gcr.io/cloud-builders/gsutil
  args: ['mv','-r','gs://webphotos-staging','gs://webphotos-prod']
  waitFor: ['-']`},correctAnswer:["C"],explanation:"The goal is to automate the copying of files between GCS buckets as part of a Cloud Build pipeline during promotion. Cloud Build steps can use standard builders like `gcr.io/cloud-builders/gsutil`. Adding a step that uses `gsutil cp -r` (recursive copy) (C) to copy the contents from the staging bucket to the production bucket before the App Engine deployment step is the correct automation approach. Manual copy (A) is not automated. Startup scripts (B) run on App Engine instances, not suitable for this type of data migration. Using `gsutil mv` (D) would delete the photos from staging, which might not be desirable.",conditions:["Cloud Build promotes App Engine app, need to copy photos from staging GCS bucket to prod GCS bucket automatically during promotion."],caseStudyContext:null},{id:101,topic:"Networking",question:"You are developing a web application that will be accessible over both HTTP and HTTPS and will run on Compute Engine instances. On occasion, you will need to SSH from your remote laptop into one of the Compute Engine instances to conduct maintenance on the app. How should you configure the instances while following Google-recommended best practices?",options:{A:"Set up a backend with Compute Engine web server instances with a private IP address behind a TCP proxy load balancer.",B:"Configure the firewall rules to allow all ingress traffic to connect to the Compute Engine web servers, with each server having a unique external IP address.",C:"Configure Cloud Identity-Aware Proxy API for SSH access. Then configure the Compute Engine servers with private IP addresses behind an HTTP(s) load balancer for the application web traffic.",D:"Set up a backend with Compute Engine web server instances with a private IP address behind an HTTP(S) load balancer. Set up a bastion host with a public IP address and open firewall ports. Connect to the web instances using the bastion host."},correctAnswer:["C"],explanation:"Best practice is to avoid exposing instances directly to the internet. Use an HTTP(S) Load Balancer for web traffic (handling HTTP/HTTPS) pointing to instances with private IPs only. For secure SSH access without public IPs or VPNs, use IAP TCP Forwarding. This allows authorized users to SSH to instances via an authenticated and authorized tunnel managed by Google (C). TCP Proxy (A) is for non-HTTP traffic. Allowing all ingress (B) is insecure. Bastion hosts (D) work but IAP is often preferred for its finer-grained access control and reduced infrastructure footprint.",conditions:["GCE web app accessible via HTTP/HTTPS, need occasional secure SSH access, follow best practices."],caseStudyContext:null},{id:102,topic:"Monitoring & Logging",question:"You have a mixture of packaged and internally developed applications hosted on a Compute Engine instance that is running Linux. These applications write log records as text in local files. You want the logs to be written to Cloud Logging. What should you do?",options:{A:"Pipe the content of the files to the Linux Syslog daemon.",B:"Install a Google version of fluentd on the Compute Engine instance.",C:"Install a Google version of collectd on the Compute Engine instance.",D:"Using cron, schedule a job to copy the log files to Cloud Storage once a day."},correctAnswer:["B"],explanation:"The standard Google Cloud Logging agent is based on fluentd (or Fluent Bit in the newer Ops Agent). Installing this agent (B) and configuring it to tail the application log files is the recommended way to stream logs from local files on a GCE instance to Cloud Logging. Piping to syslog (A) might work if the agent is configured to read syslog, but direct file monitoring is common. Collectd (C) is primarily for metrics collection. Copying to GCS (D) is batch, not real-time streaming, and doesn't integrate directly with Cloud Logging's features.",conditions:["Apps on Linux GCE instance write logs to local files, need logs in Cloud Logging."],caseStudyContext:null},{id:103,topic:"Compute",question:"You want to create `fully baked` or `golden` Compute Engine images for your application. You need to bootstrap your application to connect to the appropriate database according to the environment the application is running on (test, staging, production). What should you do?",options:{A:"Embed the appropriate database connection string in the image. Create a different image for each environment.",B:"When creating the Compute Engine instance, add a tag with the name of the database to be connected. In your application, query the Compute Engine API to pull the tags for the current instance, and use the tag to construct the appropriate database connection string.",C:"When creating the Compute Engine instance, create a metadata item with a key of DATABASE and a value for the appropriate database connection string. In your application, read the DATABASE environment variable, and use the value to connect to the appropriate database.",D:"When creating the Compute Engine instance, create a metadata item with a key of DATABASE and a value for the appropriate database connection string. In your application, query the metadata server for the DATABASE value, and use the value to connect to the appropriate database."},correctAnswer:["D"],explanation:"Golden images should be environment-agnostic. Configuration specific to an environment should be injected at deployment time. Instance metadata provides a mechanism for this. You can define a metadata key (e.g., `DATABASE`) in the instance template or during instance creation, holding the environment-specific connection string. The application inside the instance can then query the metadata server (a local endpoint) at startup to retrieve this value and connect to the correct database (D). Embedding config in the image (A) defeats the purpose of a golden image. Tags (B) are primarily for organization/filtering, not detailed config. Metadata isn't automatically exposed as environment variables (C).",conditions:["Use golden GCE images, bootstrap app with environment-specific DB connection string."],caseStudyContext:null},{id:104,topic:"IAM & Security",question:"You are developing a microservice-based application that will be deployed on a Google Kubernetes Engine cluster. The application needs to read and write to a Spanner database. You want to follow security best practices while minimizing code changes. How should you configure your application to retrieve Spanner credentials?",options:{A:"Configure the appropriate service accounts, and use Workload Identity to run the pods.",B:"Store the application credentials as Kubernetes Secrets, and expose them as environment variables.",C:"Configure the appropriate routing rules, and use a VPC-native cluster to directly connect to the database.",D:"Store the application credentials using Cloud Key Management Service, and retrieve them whenever a database connection is made."},correctAnswer:["A"],explanation:"Workload Identity is the recommended best practice for GKE applications to securely access Google Cloud services like Spanner. It allows you to bind a Kubernetes Service Account (KSA) used by your Pods to a Google Service Account (GSA) which has the necessary IAM permissions for Spanner. The application then uses the KSA token, which is automatically mounted, and Google Cloud client libraries transparently exchange it for short-lived GSA credentials via the metadata server. This avoids managing and distributing GSA keys (eliminating B, C, D variants that rely on stored keys).",conditions:["GKE microservice needs Spanner access, follow security best practices, minimize code changes."],caseStudyContext:null},{id:105,topic:"IAM & Security",question:"You are deploying your application on a Compute Engine instance that communicates with Cloud SQL. You will use Cloud SQL Proxy to allow your application to communicate to the database using the service account associated with the application's instance. You want to follow the Google-recommended best practice of providing minimum access for the role assigned to the service account. What should you do?",options:{A:"Assign the Project Editor role.",B:"Assign the Project Owner role.",C:"Assign the Cloud SQL Client role.",D:"Assign the Cloud SQL Editor role."},correctAnswer:["C"],explanation:"The principle of least privilege requires granting only the necessary permissions. For an application/service account to connect to a Cloud SQL instance (especially via the proxy), the only required role is `roles/cloudsql.client`. This role grants permissions to connect to instances but not to manage (create, delete, modify) them. Editor (A), Owner (B), and Cloud SQL Editor (D) roles grant excessive permissions beyond just connecting.",conditions:["GCE app uses Cloud SQL Proxy with instance SA, need minimum required IAM role for SA."],caseStudyContext:null},{id:106,topic:"Compute",question:"Your team develops stateless services that run on Google Kubernetes Engine (GKE). You need to deploy a new service that will only be accessed by other services running in the GKE cluster. The service will need to scale as quickly as possible to respond to changing load. What should you do?",options:{A:"Use a Vertical Pod Autoscaler to scale the containers, and expose them via a ClusterIP Service.",B:"Use a Vertical Pod Autoscaler to scale the containers, and expose them via a NodePort Service.",C:"Use a Horizontal Pod Autoscaler to scale the containers, and expose them via a ClusterIP Service.",D:"Use a Horizontal Pod Autoscaler to scale the containers, and expose them via a NodePort Service."},correctAnswer:["C"],explanation:"For stateless services needing rapid scaling based on load (like CPU or requests per second), the Horizontal Pod Autoscaler (HPA) is the standard Kubernetes mechanism. HPA adjusts the *number* of replicas (horizontal scaling). Vertical Pod Autoscaler (VPA) adjusts resources (CPU/memory) of existing pods. Since the service is only accessed internally within the cluster, a `ClusterIP` Service type is appropriate, as it provides a stable internal IP and DNS name without exposing the service externally. NodePort (B, D) exposes the service on each node's IP, which isn't necessary for purely internal communication.",conditions:["Stateless GKE service, internal access only, scale quickly with load."],caseStudyContext:null},{id:107,topic:"Compute",question:"You recently migrated a monolithic application to Google Cloud by breaking it down into microservices. One of the microservices is deployed using Cloud Functions. As you modernize the application, you make a change to the API of the service that is backward-incompatible. You need to support both existing callers who use the original API and new callers who use the new API. What should you do?",options:{A:"Leave the original Cloud Function as-is and deploy a second Cloud Function with the new API. Use a load balancer to distribute calls between the versions.",B:"Leave the original Cloud Function as-is and deploy a second Cloud Function that includes only the changed API. Calls are automatically routed to the correct function.",C:"Leave the original Cloud Function as-is and deploy a second Cloud Function with the new API. Use Cloud Endpoints to provide an API gateway that exposes a versioned API.",D:"Re-deploy the Cloud Function after making code changes to support the new API. Requests for both versions of the API are fulfilled based on a version identifier included in the call."},correctAnswer:["C"],explanation:"When introducing a backward-incompatible API change, you need to maintain the old version while deploying the new one. Deploying the new API as a separate Cloud Function is necessary. To manage both versions and route callers appropriately (e.g., via URL path like /v1/api and /v2/api), an API Gateway is needed. Cloud Endpoints (or Apigee) provides this capability, allowing you to define a versioned API specification that routes requests to the correct underlying Cloud Function based on the version requested (C). A simple load balancer (A) doesn't typically handle API version routing. Automatic routing (B) doesn't happen. Supporting both versions in one function (D) can become complex.",conditions:["Cloud Function microservice, backward-incompatible API change, need to support old and new API versions."],caseStudyContext:null},{id:108,topic:"Databases",question:"You are developing an application that will allow users to read and post comments on news articles. You want to configure your application to store and display user-submitted comments using Firestore. How should you design the schema to support an unknown number of comments and articles?",options:{A:"Store each comment in a subcollection of the article.",B:"Add each comment to an array property on the article.",C:"Store each comment in a document, and add the comment's key to an array property on the article.",D:"Store each comment in a document, and add the comment's key to an array property on the user profile."},correctAnswer:["A"],explanation:"Firestore's data model supports subcollections within documents. For a one-to-many relationship like articles-to-comments where the number of comments can be large and unknown, storing comments in a subcollection (`comments`) under the specific article document (`articles/{articleId}/comments/{commentId}`) is the recommended and scalable approach (A). This allows efficient querying of comments for a specific article. Storing comments in an array on the article document (B, C) is limited by the maximum document size (1 MiB) and becomes inefficient for large numbers of comments. Linking via user profile (D) doesn't model the article-comment relationship correctly.",conditions:["Firestore DB for comments on articles, unknown number of comments/articles."],caseStudyContext:null},{id:109,topic:"Networking",question:"You recently developed a new application that needs to call the Cloud Storage API from a Compute Engine instance that doesn't have a public IP address. What should you do?",options:{A:"Use Carrier Peering",B:"Use VPC Network Peering",C:"Use Shared VPC networks",D:"Use Private Google Access"},correctAnswer:["D"],explanation:"Private Google Access allows Compute Engine instances with only private IP addresses within a VPC network to reach the public IP addresses of Google APIs and services (like Cloud Storage) without needing an external IP or NAT gateway. Traffic stays within Google's network. VPC Peering (B) and Shared VPC (C) connect VPC networks. Carrier Peering (A) connects to Google's edge network via partners.",conditions:["GCE instance with no public IP needs to access Cloud Storage API."],caseStudyContext:null},{id:110,topic:"Compute",question:"You are a developer working with the CI/CD team to troubleshoot a new feature that your team introduced. The CI/CD team used HashiCorp Packer to create a new Compute Engine image from your development branch. The image was successfully built, but is not booting up. You need to investigate the issue with the CI/CD team. What should you do?",options:{A:"Create a new feature branch, and ask the build team to rebuild the image.",B:"Shut down the deployed virtual machine, export the disk, and then mount the disk locally to access the boot logs.",C:"Install Packer locally, build the Compute Engine image locally, and then run it in your personal Google Cloud project.",D:"Check Compute Engine OS logs using the serial port, and check the Cloud Logging logs to confirm access to the serial port."},correctAnswer:["D"],explanation:"If a GCE image fails to boot, the primary diagnostic tool is the serial console output, which captures kernel and boot messages. You can view this output via the Cloud Console or `gcloud compute instances get-serial-port-output`. Enabling serial port logging sends this output to Cloud Logging for easier access and retention (D). Rebuilding (A, C) doesn't diagnose the current failure. Exporting the disk (B) is a much more complex troubleshooting step, usually considered after checking serial logs.",conditions:["Packer-built GCE image fails to boot, need to investigate."],caseStudyContext:null},{id:111,topic:"Networking",question:"You manage an application that runs in a Compute Engine instance. You also have multiple backend services executing in stand-alone Docker containers running in Compute Engine instances. The Compute Engine instances supporting the backend services are scaled by managed instance groups in multiple regions. You want your calling application to be loosely coupled. You need to be able to invoke distinct service implementations that are chosen based on the value of an HTTP header found in the request. Which Google Cloud feature should you use to invoke the backend services?",options:{A:"Traffic Director",B:"Service Directory",C:"Anthos Service Mesh",D:"Internal HTTP(S) Load Balancing"},correctAnswer:["A"],explanation:"Traffic Director is a managed control plane for service mesh and proxy-based load balancing that provides advanced traffic management capabilities, including routing based on HTTP headers, paths, etc., across multiple regions and instance groups (including GCE and GKE). This allows selecting different backend services based on header values (A). Internal HTTP(S) LB (D) is regional and has less sophisticated routing rules. Service Directory (B) is for service registration/discovery. Anthos Service Mesh (C) provides similar capabilities but Traffic Director is the specific feature for advanced routing across environments.",conditions:["Invoke backend services (GCE MIGs, multi-region) from GCE app, route based on HTTP header value, loosely coupled."],caseStudyContext:null},{id:112,topic:"Databases",question:`You are developing an ecommerce application that uses App Engine standard environment and Memorystore for Redis. When a user logs into the app, the application caches the user's information (e.g., session, name, address, preferences), which is stored for quick retrieval during checkout.
While testing your application in a browser, you get a 502 Bad Gateway error. You have determined that the application is not connecting to Memorystore. What is the reason for this error?`,options:{A:"Your Memorystore for Redis instance was deployed without a public IP address.",B:"You configured your Serverless VPC Access connector in a different region than your App Engine instance.",C:"The firewall rule allowing a connection between App Engine and Memorystore was removed during an infrastructure update by the DevOps team.",D:"You configured your application to use a Serverless VPC Access connector on a different subnet in a different availability zone than your App Engine instance."},correctAnswer:["B"],explanation:"App Engine Standard needs a Serverless VPC Access connector to connect to resources (like Memorystore) on a VPC network via private IP. A critical requirement is that the App Engine service and the VPC Access connector must reside in the same region. If they are in different regions (B), the connection will fail, potentially leading to errors like 502 from App Engine if it cannot reach its required backend (Memorystore). Memorystore typically doesn't have public IPs (A). Firewall rules within the VPC usually allow traffic from the connector's IP range (C - less likely cause than region mismatch). Subnet/zone configuration for the connector (D) matters but region mismatch is a common failure point.",conditions:["App Engine Standard app uses Memorystore, gets 502 error, cannot connect to Memorystore."],caseStudyContext:null},{id:113,topic:"Compute",question:"You are designing a resource-sharing policy for applications used by different teams in a Google Kubernetes Engine cluster. You need to ensure that all applications can access the resources needed to run. What should you do? (Choose two.)",options:{A:"Specify the resource limits and requests in the object specifications.",B:"Create a namespace for each team, and attach resource quotas to each namespace.",C:"Create a LimitRange to specify the default compute resource requirements for each namespace.",D:"Create a Kubernetes service account (KSA) for each application, and assign each KSA to the namespace.",E:"Use the Anthos Policy Controller to enforce label annotations on all namespaces. Use taints and tolerations to allow resource sharing for namespaces."},correctAnswer:["B","C"],explanation:"To manage resource sharing fairly among teams in a shared GKE cluster, use namespaces for isolation (B). Attach ResourceQuotas to each namespace (B) to limit the total amount of resources (CPU, memory, storage, object counts) a team's namespace can consume, preventing one team from starving others. Additionally, use LimitRanges within each namespace (C) to set default resource requests/limits for containers and constrain the minimum/maximum resources individual pods/containers can request, ensuring sensible resource usage within the quota.",conditions:["Manage resource sharing for multiple teams in one GKE cluster, ensure all apps get needed resources."],caseStudyContext:null},{id:114,topic:"Compute",question:`You are developing a new application that has the following design requirements:

 Creation and changes to the application infrastructure are versioned and auditable.
 The application and deployment infrastructure uses Google-managed services as much as possible.
 The application runs on a serverless compute platform.
How should you design the application's architecture?`,options:{A:"1. Store the application and infrastructure source code in a Git repository. 2. Use Cloud Build to deploy the application infrastructure with Terraform. 3. Deploy the application to a Cloud Function as a pipeline step.",B:"1. Deploy Jenkins from the Google Cloud Marketplace, and define a continuous integration pipeline in Jenkins. 2. Configure a pipeline step to pull the application source code from a Git repository. 3. Deploy the application source code to App Engine as a pipeline step.",C:"1. Create a continuous integration pipeline on Cloud Build, and configure the pipeline to deploy the application infrastructure using Deployment Manager templates. 2. Configure a pipeline step to create a container with the latest application source code. 3. Deploy the container to a Compute Engine instance as a pipeline step.",D:"1. Deploy the application infrastructure using gcloud commands. 2. Use Cloud Build to define a continuous integration pipeline for changes to the application source code. 3. Configure a pipeline step to pull the application source code from a Git repository, and create a containerized application. 4. Deploy the new container on Cloud Run as a pipeline step."},correctAnswer:["A"],explanation:"Requirement 1 (versioned/auditable infra): Store infrastructure code (Terraform or Deployment Manager) in Git. Requirement 2 (managed services): Cloud Build, Cloud Functions/Cloud Run/App Engine. Requirement 3 (serverless compute): Cloud Functions, Cloud Run, App Engine Standard. Option A uses Git, Cloud Build (managed), Terraform (IaC for auditability), and Cloud Functions (serverless). Option B uses Jenkins (not fully managed). Option C uses Compute Engine (not serverless). Option D uses gcloud commands initially (less auditable than IaC) and Cloud Run (serverless), but A's use of IaC from the start is better for versioning/auditing infrastructure.",conditions:["New app architecture: versioned/auditable infra, managed services preferred, serverless compute."],caseStudyContext:null},{id:115,topic:"IAM & Security",question:"You are creating and running containers across different projects in Google Cloud. The application you are developing needs to access Google Cloud services from within Google Kubernetes Engine (GKE). What should you do?",options:{A:"Assign a Google service account to the GKE nodes.",B:"Use a Google service account to run the Pod with Workload Identity.",C:"Store the Google service account credentials as a Kubernetes Secret.",D:"Use a Google service account with GKE role-based access control (RBAC)."},correctAnswer:["B"],explanation:"Workload Identity is the recommended and most secure way for GKE workloads (Pods) to access Google Cloud services. It allows binding a Kubernetes Service Account (KSA) to a Google Service Account (GSA). Pods running with the KSA can then authenticate as the GSA to Google APIs without needing GSA keys (B). Assigning SA to nodes (A) grants permissions to all pods on the node. Storing keys as K8s secrets (C) requires key management and rotation. RBAC (D) controls access *within* the cluster, not to external Google Cloud services.",conditions:["GKE application needs to access Google Cloud services securely across different projects."],caseStudyContext:null},{id:116,topic:"Compute",question:"You are containerizing a legacy application that stores its configuration on an NFS share. You need to deploy this application to Google Kubernetes Engine (GKE) and do not want the application serving traffic until after the configuration has been retrieved. What should you do?",options:{A:"Use the gsutil utility to copy files from within the Docker container at startup, and start the service using an ENTRYPOINT script.",B:"Create a PersistentVolumeClaim on the GKE cluster. Access the configuration files from the volume, and start the service using an ENTRYPOINT script.",C:"Use the COPY statement in the Dockerfile to load the configuration into the container image. Verify that the configuration is available, and start the service using an ENTRYPOINT script.",D:"Add a startup script to the GKE instance group to mount the NFS share at node startup. Copy the configuration files into the container, and start the service using an ENTRYPOINT script."},correctAnswer:["B"],explanation:"Kubernetes PersistentVolumes (PV) and PersistentVolumeClaims (PVC) are the standard way to manage persistent storage for containers. You can configure a PV backed by an existing NFS share (e.g., hosted by Filestore or elsewhere). The container definition in the Deployment/Pod spec can then reference the PVC to mount the NFS volume into the container's filesystem. The application can then read its configuration directly from the mounted volume (B). Using gsutil (A) is for GCS. Copying into the image (C) makes config static. Mounting NFS on the node (D) couples the pod to the node configuration.",conditions:["Containerize legacy app for GKE, config stored on NFS, must retrieve config before serving traffic."],caseStudyContext:null},{id:117,topic:"Networking",question:"Your team is developing a new application using a PostgreSQL database and Cloud Run. You are responsible for ensuring that all traffic is kept private on Google Cloud. You want to use managed services and follow Google-recommended best practices. What should you do?",options:{A:"1. Enable Cloud SQL and Cloud Run in the same project. 2. Configure a private IP address for Cloud SQL. Enable private services access. 3. Create a Serverless VPC Access connector. 4. Configure Cloud Run to use the connector to connect to Cloud SQL.",B:"1. Install PostgreSQL on a Compute Engine virtual machine (VM), and enable Cloud Run in the same project. 2. Configure a private IP address for the VM. Enable private services access. 3. Create a Serverless VPC Access connector. 4. Configure Cloud Run to use the connector to connect to the VM hosting PostgreSQL.",C:"1. Use Cloud SQL and Cloud Run in different projects. 2. Configure a private IP address for Cloud SQL. Enable private services access. 3. Create a Serverless VPC Access connector. 4. Set up a VPN connection between the two projects. Configure Cloud Run to use the connector to connect to Cloud SQL.",D:"1. Install PostgreSQL on a Compute Engine VM, and enable Cloud Run in different projects. 2. Configure a private IP address for the VM. Enable private services access. 3. Create a Serverless VPC Access connector. 4. Set up a VPN connection between the two projects. Configure Cloud Run to use the connector to access the VM hosting PostgreSQL"},correctAnswer:["A"],explanation:"To keep traffic private between Cloud Run and a database, use managed services (Cloud SQL for PostgreSQL) and private networking. Configure Cloud SQL with a private IP (requires Private Services Access). Configure Cloud Run to use a Serverless VPC Access connector, which bridges Cloud Run's environment to your VPC network. This allows Cloud Run to connect to the Cloud SQL instance's private IP address via the connector, keeping traffic within Google's network (A). Installing PostgreSQL on GCE (B, D) adds management overhead. Using different projects and VPN (C, D) adds unnecessary complexity when resources can be in the same project/VPC.",conditions:["Cloud Run app needs PostgreSQL DB access, all traffic must be private, use managed services, follow best practices."],caseStudyContext:null},{id:118,topic:"Storage",question:"You are developing an application that will allow clients to download a file from your website for a specific period of time. How should you design the application to complete this task while following Google-recommended best practices?",options:{A:"Configure the application to send the file to the client as an email attachment.",B:"Generate and assign a Cloud Storage-signed URL for the file. Make the URL available for the client to download.",C:"Create a temporary Cloud Storage bucket with time expiration specified, and give download permissions to the bucket. Copy the file, and send it to the client.",D:"Generate the HTTP cookies with time expiration specified. If the time is valid, copy the file from the Cloud Storage bucket, and make the file available for the client to download."},correctAnswer:["B"],explanation:"Cloud Storage Signed URLs are designed for this exact use case: granting temporary, limited access (e.g., read-only for download) to a specific object without requiring the user to have Google credentials. The application generates a URL with an embedded signature and expiration time. Anyone with the URL can access the object until it expires (B). Email attachments (A) are impractical for large files and less secure. Temporary buckets (C) or cookies (D) are overly complex compared to signed URLs.",conditions:["Allow clients to download a file for a limited time."],caseStudyContext:null},{id:119,topic:"Compute",question:"Your development team has been asked to refactor an existing monolithic application into a set of composable microservices. Which design aspects should you implement for the new application? (Choose two.)",options:{A:"Develop the microservice code in the same programming language used by the microservice caller.",B:"Create an API contract agreement between the microservice implementation and microservice caller.",C:"Require asynchronous communications between all microservice implementations and microservice callers.",D:"Ensure that sufficient instances of the microservice are running to accommodate the performance requirements.",E:"Implement a versioning scheme to permit future changes that could be incompatible with the current interface."},correctAnswer:["B","E"],explanation:"Key principles of microservices include loose coupling and independent evolution. Defining clear API contracts (B) (e.g., using OpenAPI spec) decouples callers from implementation details. Implementing a versioning scheme (E) allows APIs to evolve, including making backward-incompatible changes, without breaking existing consumers, who can continue using older versions. Language choice (A) is independent per service. Asynchronous communication (C) is a pattern, not a universal requirement. Ensuring sufficient instances (D) is an operational concern, not a fundamental design aspect of the interfaces.",conditions:["Refactor monolith to microservices, implement key design aspects."],caseStudyContext:null},{id:120,topic:"Monitoring & Logging",question:"You deployed a new application to Google Kubernetes Engine and are experiencing some performance degradation. Your logs are being written to Cloud Logging, and you are using a Prometheus sidecar model for capturing metrics. You need to correlate the metrics and data from the logs to troubleshoot the performance issue and send real-time alerts while minimizing costs. What should you do?",options:{A:"Create custom metrics from the Cloud Logging logs, and use Prometheus to import the results using the Cloud Monitoring REST API.",B:"Export the Cloud Logging logs and the Prometheus metrics to Cloud Bigtable. Run a query to join the results, and analyze in Google Data Studio.",C:"Export the Cloud Logging logs and stream the Prometheus metrics to BigQuery. Run a recurring query to join the results, and send notifications using Cloud Tasks.",D:"Export the Prometheus metrics and use Cloud Monitoring to view them as external metrics. Configure Cloud Monitoring to create log-based metrics from the logs, and correlate them with the Prometheus data."},correctAnswer:["D"],explanation:"Cloud Monitoring can ingest metrics from various sources, including Prometheus (often via the Ops Agent or dedicated exporters), treating them as external metrics. It can also create metrics based on log entries (log-based metrics). By bringing both Prometheus metrics and log-based metrics into Cloud Monitoring (D), you can correlate them within Monitoring's dashboards, use Metrics Explorer, and configure alerting policies based on combinations of these metrics, all within a single managed service, minimizing cost and complexity compared to exporting to BigQuery/Bigtable (B, C) or complex cross-system imports (A).",conditions:["GKE app performance issue, have logs in Cloud Logging, metrics in Prometheus sidecar, need to correlate logs/metrics, alerts needed, minimize cost."],caseStudyContext:null},{id:121,topic:"Migration",question:"You have been tasked with planning the migration of your company's application from on-premises to Google Cloud. Your company's monolithic application is an ecommerce website. The application will be migrated to microservices deployed on Google Cloud in stages. The majority of your company's revenue is generated through online sales, so it is important to minimize risk during the migration. You need to prioritize features and select the first functionality to migrate. What should you do?",options:{A:"Migrate the Product catalog, which has integrations to the frontend and product database.",B:"Migrate Payment processing, which has integrations to the frontend, order database, and third-party payment vendor.",C:"Migrate Order fulfillment, which has integrations to the order database, inventory system, and third-party shipping vendor.",D:"Migrate the Shopping cart, which has integrations to the frontend, cart database, inventory system, and payment processing system."},correctAnswer:["A"],explanation:"When migrating a monolith to microservices incrementally (Strangler Fig pattern), it's generally recommended to start with functionality that has fewer dependencies and lower risk, while still providing value. The Product Catalog (A) often fits this description  it primarily serves read-only data to the frontend and might have simpler database interactions compared to transactional systems like payments (B), fulfillment (C), or the shopping cart (D), which often involve more complex logic and dependencies.",conditions:["Migrate on-prem ecommerce monolith to microservices on GCP in stages, minimize risk, select first functionality."],caseStudyContext:null},{id:122,topic:"Compute",question:`Your team develops services that run on Google Kubernetes Engine. Your team's code is stored in Cloud Source Repositories. You need to quickly identify bugs in the code before it is deployed to production. You want to invest in automation to improve developer feedback and make the process as efficient as possible.
What should you do?`,options:{A:"Use Spinnaker to automate building container images from code based on Git tags.",B:"Use Cloud Build to automate building container images from code based on Git tags.",C:"Use Spinnaker to automate deploying container images to the production environment.",D:"Use Cloud Build to automate building container images from code based on forked versions."},correctAnswer:["B"],explanation:"The goal is automated building and (implicitly) testing triggered by code changes (specifically tags, often used for releases or candidates) to provide quick feedback before production deployment. Cloud Build integrates natively with Cloud Source Repositories and can be triggered by Git tags (B). It's a managed service for building/testing. Spinnaker (A, C) is primarily a continuous delivery (deployment) tool, though it can integrate with build systems. Building based on forked versions (D) isn't the standard trigger for pre-production testing.",conditions:["GKE services, code in CSR, automate build/test on Git tags for early bug detection, efficient feedback."],caseStudyContext:null},{id:123,topic:"IAM & Security",question:"Your team is developing an application in Google Cloud that executes with user identities maintained by Cloud Identity. Each of your application's users will have an associated Pub/Sub topic to which messages are published, and a Pub/Sub subscription where the same user will retrieve published messages. You need to ensure that only authorized users can publish and subscribe to their own specific Pub/Sub topic and subscription. What should you do?",options:{A:"Bind the user identity to the pubsub.publisher and pubsub.subscriber roles at the resource level.",B:"Grant the user identity the pubsub.publisher and pubsub.subscriber roles at the project level.",C:"Grant the user identity a custom role that contains the pubsub.topics.create and pubsub.subscriptions.create permissions.",D:"Configure the application to run as a service account that has the pubsub.publisher and pubsub.subscriber roles."},correctAnswer:["A"],explanation:"To enforce that a user can only access *their own* specific topic and subscription, permissions must be granted at the resource level (the specific topic and subscription). Granting the `roles/pubsub.publisher` role on the user's topic and the `roles/pubsub.subscriber` role on the user's subscription directly to the user's Cloud Identity achieves this granular control (A). Project-level roles (B) grant access to *all* topics/subscriptions in the project. Create permissions (C) are irrelevant. Using a single service account (D) doesn't enforce per-user permissions.",conditions:["App uses Cloud Identity users, each user has own Pub/Sub topic/subscription, users must only access their own resources."],caseStudyContext:null},{id:124,topic:"Compute",question:"You are evaluating developer tools to help drive Google Kubernetes Engine adoption and integration with your development environment, which includes VS Code and IntelliJ. What should you do?",options:{A:"Use Cloud Code to develop applications.",B:"Use the Cloud Shell integrated Code Editor to edit code and configuration files.",C:"Use a Cloud Notebook instance to ingest and process data and deploy models.",D:"Use Cloud Shell to manage your infrastructure and applications from the command line."},correctAnswer:["A"],explanation:"Cloud Code is specifically designed as a set of IDE extensions (for VS Code, IntelliJ) to streamline Kubernetes (including GKE) and Cloud Run development. It provides features like cluster interaction, Skaffold integration for build/run/debug cycles, YAML linting/completion, and debugging directly within the IDE (A). Cloud Shell editor (B) is browser-based. Cloud Notebooks (C) are for data science. Cloud Shell CLI (D) is useful but doesn't integrate directly into the IDE workflow like Cloud Code.",conditions:["Need developer tools for GKE, integrate with VS Code/IntelliJ."],caseStudyContext:null},{id:125,topic:"Networking",question:`You are developing an ecommerce web application that uses App Engine standard environment and Memorystore for Redis. When a user logs into the app, the application caches the user's information (e.g., session, name, address, preferences), which is stored for quick retrieval during checkout.
While testing your application in a browser, you get a 502 Bad Gateway error. You have determined that the application is not connecting to Memorystore. What is the reason for this error?`,options:{A:"Your Memorystore for Redis instance was deployed without a public IP address.",B:"You configured your Serverless VPC Access connector in a different region than your App Engine instance.",C:"The firewall rule allowing a connection between App Engine and Memorystore was removed during an infrastructure update by the DevOps team.",D:"You configured your application to use a Serverless VPC Access connector on a different subnet in a different availability zone than your App Engine instance."},correctAnswer:["B"],explanation:"App Engine Standard requires a Serverless VPC Access connector to communicate with resources like Memorystore on a private VPC network. A fundamental requirement for the connector is that it must be in the *same region* as the App Engine service using it. If the connector and App Engine are in different regions (B), the connection will fail, leading to application errors (potentially manifesting as a 502 if App Engine can't reach its backend cache). Memorystore doesn't use public IPs (A). Firewall rules (C) could be an issue, but region mismatch for the connector is a specific and common configuration error. Subnet/zone issues (D) are less likely to cause complete connection failure than a region mismatch.",conditions:["App Engine Standard app, uses Memorystore, gets 502 error, cannot connect to Memorystore."],caseStudyContext:null},{id:126,topic:"IAM & Security",question:"Your team develops services that run on Google Cloud. You need to build a data processing service and will use Cloud Functions. The data to be processed by the function is sensitive. You need to ensure that invocations can only happen from authorized services and follow Google-recommended best practices for securing functions. What should you do?",options:{A:"Enable Identity-Aware Proxy in your project. Secure function access using its permissions.",B:"Create a service account with the Cloud Functions Viewer role. Use that service account to invoke the function.",C:"Create a service account with the Cloud Functions Invoker role. Use that service account to invoke the function.",D:"Create an OAuth 2.0 client ID for your calling service in the same project as the function you want to secure. Use those credentials to invoke the function."},correctAnswer:["C"],explanation:"To securely invoke an HTTP-triggered Cloud Function from another service (like another Cloud Function, Cloud Run, GCE, etc.), the recommended practice is to use IAM-based authentication. Make the function private (require authentication). Create a dedicated service account for the calling service. Grant this service account the `roles/cloudfunctions.invoker` role on the target function. The calling service then authenticates using its service account credentials (e.g., via ADC) and includes an OIDC identity token in the request header, which Cloud Functions verifies (C). IAP (A) is not typically used for function-to-function auth. Viewer role (B) is insufficient. OAuth Client IDs (D) are for user authentication flows.",conditions:["Secure Cloud Function invocation from authorized services only, sensitive data."],caseStudyContext:null},{id:127,topic:"Compute",question:"You are deploying your applications on Compute Engine. One of your Compute Engine instances failed to launch. What should you do? (Choose two.)",options:{A:"Determine whether your file system is corrupted.",B:"Access Compute Engine as a different SSH user.",C:"Troubleshoot firewall rules or routes on an instance.",D:"Check whether your instance boot disk is completely full.",E:"Check whether network traffic to or from your instance is being dropped."},correctAnswer:["A","D"],explanation:"Instance launch failures are often related to the boot disk. Common causes include the boot disk being full (D), preventing the OS from starting essential services, or the file system on the boot disk being corrupted (A), making it unreadable by the bootloader or kernel. Network issues (C, E) typically prevent connectivity *after* boot, not the launch itself. SSH user issues (B) are also irrelevant if the instance doesn't even reach a state where SSH is running.",conditions:["GCE instance failed to launch/boot."],caseStudyContext:null},{id:128,topic:"IAM & Security",question:"Your web application is deployed to the corporate intranet. You need to migrate the web application to Google Cloud. The web application must be available only to company employees and accessible to employees as they travel. You need to ensure the security and accessibility of the web application while minimizing application changes. What should you do?",options:{A:"Configure the application to check authentication credentials for each HTTP(S) request to the application.",B:"Configure Identity-Aware Proxy to allow employees to access the application through its public IP address.",C:"Configure a Compute Engine instance that requests users to log in to their corporate account. Change the web application DNS to point to the proxy Compute Engine instance. After authenticating, the Compute Engine instance forwards requests to and from the web application.",D:"Configure a Compute Engine instance that requests users to log in to their corporate account. Change the web application DNS to point to the proxy Compute Engine instance. After authenticating, the Compute Engine issues an HTTP redirect to a public IP address hosting the web application."},correctAnswer:["B"],explanation:"The requirements are: migrate to GCP, accessible only to employees, accessible while traveling, minimize app changes. Identity-Aware Proxy (IAP) meets these. By placing the application (e.g., on GCE or GKE) behind a Google Cloud HTTP(S) Load Balancer and enabling IAP, access is controlled based on user identity (e.g., Google Workspace accounts) and IAM permissions. Users can access it securely from anywhere after authenticating, without needing VPNs or complex application changes (B). Implementing app-level auth (A) requires code changes. A custom proxy (C, D) adds complexity and management overhead compared to managed IAP.",conditions:["Migrate internal web app to GCP, employees only access, accessible while traveling, minimize app changes."],caseStudyContext:null},{id:129,topic:"Networking",question:`You have an application that uses an HTTP Cloud Function to process user activity from both desktop browser and mobile application clients. This function will serve as the endpoint for all metric submissions using HTTP POST.
Due to legacy restrictions, the function must be mapped to a domain that is separate from the domain requested by users on web or mobile sessions. The domain for the Cloud Function is https://fn.example.com. Desktop and mobile clients use the domain https://www.example.com.
You need to add a header to the function's HTTP response so that only those browser and mobile sessions can submit metrics to the Cloud Function. Which response header should you add?`,options:{A:"Access-Control-Allow-Origin: *",B:"Access-Control-Allow-Origin: https://*.example.com",C:"Access-Control-Allow-Origin: https://fn.example.com",D:"Access-Control-Allow-origin: https://www.example.com"},correctAnswer:["D"],explanation:"This scenario describes a Cross-Origin Resource Sharing (CORS) requirement. Browsers enforce the same-origin policy, preventing JavaScript running on `https://www.example.com` from making requests to `https://fn.example.com` unless the server at `fn.example.com` explicitly allows it. The `Access-Control-Allow-Origin` response header from the Cloud Function (`fn.example.com`) must specify the origin (`https://www.example.com`) that is permitted to make the request (D). `*` (A) allows any origin (less secure). `*.example.com` (B) uses a wildcard which is sometimes supported but specific origin is better. Allowing its own origin (C) doesn't help the client.",conditions:["HTTP Cloud Function at fn.example.com called by clients from www.example.com, need to allow cross-origin requests."],caseStudyContext:null},{id:130,topic:"Databases",question:"You have an HTTP Cloud Function that is called via POST. Each submission's request body has a flat, unnested JSON structure containing numeric and text data. After the Cloud Function completes, the collected data should be immediately available for ongoing and complex analytics by many users in parallel. How should you persist the submissions?",options:{A:"Directly persist each POST request's JSON data into Datastore.",B:"Transform the POST request's JSON data, and stream it into BigQuery.",C:"Transform the POST request's JSON data, and store it in a regional Cloud SQL cluster.",D:"Persist each POST request's JSON data as an individual file within Cloud Storage, with the file name containing the request identifier."},correctAnswer:["B"],explanation:"The requirement for data to be immediately available for complex analytics by many users in parallel strongly points to BigQuery. BigQuery is designed for large-scale, SQL-based analytics. Data can be streamed directly into BigQuery tables from sources like Cloud Functions, making it available for querying almost instantly (B). Datastore (A) and Cloud SQL (C) are primarily transactional databases and less suited for complex, parallel analytics on large volumes. Storing individual files in Cloud Storage (D) requires an additional ETL step before data can be analyzed efficiently in bulk.",conditions:["Cloud Function receives JSON POST data, persist for immediate, complex, parallel analytics."],caseStudyContext:null},{id:131,topic:"IAM & Security",question:"Your security team is auditing all deployed applications running in Google Kubernetes Engine. After completing the audit, your team discovers that some of the applications send traffic within the cluster in clear text. You need to ensure that all application traffic is encrypted as quickly as possible while minimizing changes to your applications and maintaining support from Google. What should you do?",options:{A:"Use Network Policies to block traffic between applications.",B:"Install Istio, enable proxy injection on your application namespace, and then enable mTLS.",C:"Define Trusted Network ranges within the application, and configure the applications to allow traffic only from those networks.",D:"Use an automated process to request SSL Certificates for your applications from Let's Encrypt and add them to your applications."},correctAnswer:["B"],explanation:"Istio (or Anthos Service Mesh, which is managed Istio) provides automatic mutual TLS (mTLS) encryption for traffic *between* services within the mesh. By installing Istio, enabling automatic sidecar proxy injection for the relevant namespaces, and configuring mTLS (often enabled by default or with simple policy), all service-to-service traffic handled by the proxies will be encrypted without requiring application code changes (B). Network Policies (A) control *whether* traffic is allowed, not encryption. Application-level changes (C, D) require modifying applications.",conditions:["GKE applications send internal traffic in clear text, need to encrypt quickly, minimize app changes, use supported solution."],caseStudyContext:null},{id:132,topic:"Monitoring & Logging",question:"You migrated some of your applications to Google Cloud. You are using a legacy monitoring platform deployed on-premises for both on-premises and cloud- deployed applications. You discover that your notification system is responding slowly to time-critical problems in the cloud applications. What should you do?",options:{A:"Replace your monitoring platform with Cloud Monitoring.",B:"Install the Cloud Monitoring agent on your Compute Engine instances.",C:"Migrate some traffic back to your old platform. Perform A/B testing on the two platforms concurrently.",D:"Use Cloud Logging and Cloud Monitoring to capture logs, monitor, and send alerts. Send them to your existing platform."},correctAnswer:["D"],explanation:"This is very similar to Q2. The problem is slow notifications for cloud apps from the on-prem system. Since only *some* apps were migrated, replacing the entire system (A) might be premature. Installing agents (B) helps get data into Cloud Monitoring but doesn't fix notifications alone. Migrating traffic (C) is irrelevant. The best approach is to leverage Cloud Monitoring for its fast, cloud-native alerting capabilities for the cloud applications, while potentially still integrating with the existing platform by forwarding logs/metrics/alerts to maintain a unified view (D). This addresses the critical latency issue where it occurs.",conditions:["Some apps migrated to GCP, hybrid monitoring (on-prem + GCP), slow notifications for cloud apps from on-prem system."],caseStudyContext:null},{id:133,topic:"Compute",question:"You recently deployed your application in Google Kubernetes Engine, and now need to release a new version of your application. You need the ability to instantly roll back to the previous version in case there are issues with the new version. Which deployment model should you use?",options:{A:"Perform a rolling deployment, and test your new application after the deployment is complete.",B:"Perform A/B testing, and test your application periodically after the new tests are implemented.",C:"Perform a blue/green deployment, and test your new application after the deployment is. complete.",D:"Perform a canary deployment, and test your new application periodically after the new version is deployed."},correctAnswer:["C"],explanation:"Blue/green deployment maintains two identical environments. Traffic is directed to one (blue). The new version is deployed to the other (green). After testing green, traffic is switched instantly. If issues arise, traffic can be instantly switched back to the stable blue environment. This provides the required instant rollback capability (C). Rolling updates (A) gradually replace pods, making rollback slower. A/B testing (B) and Canary (D) involve splitting traffic and don't offer the same instant, full rollback mechanism as blue/green.",conditions:["GKE new version release, need instant rollback capability."],caseStudyContext:null},{id:134,topic:"IAM & Security",question:"You developed a JavaScript web application that needs to access Google Drive's API and obtain permission from users to store files in their Google Drives. You need to select an authorization approach for your application. What should you do?",options:{A:"Create an API key.",B:"Create a SAML token.",C:"Create a service account.",D:"Create an OAuth Client ID."},correctAnswer:["D"],explanation:"To access user data (like Google Drive files) on behalf of a user, the application must use OAuth 2.0 to obtain the user's consent and an access token. This requires registering the application (JavaScript web app in this case) with Google Cloud to obtain an OAuth Client ID (D). The application then initiates the OAuth flow, redirecting the user to Google for authentication and consent. API keys (A) are for accessing public APIs or identifying a project, not for user data access. SAML (B) is for federated identity SSO. Service accounts (C) represent the application itself, not the end-user.",conditions:["JavaScript web app needs to access users' Google Drive files with user permission."],caseStudyContext:null},{id:135,topic:"Compute",question:"You manage an ecommerce application that processes purchases from customers who can subsequently cancel or change those purchases. You discover that order volumes are highly variable and the backend order-processing system can only process one request at a time. You want to ensure seamless performance for customers regardless of usage volume. It is crucial that customers' order update requests are performed in the sequence in which they were generated. What should you do?",options:{A:"Send the purchase and change requests over WebSockets to the backend.",B:"Send the purchase and change requests as REST requests to the backend.",C:"Use a Pub/Sub subscriber in pull mode and use a data store to manage ordering.",D:"Use a Pub/Sub subscriber in push mode and use a data store to manage ordering."},correctAnswer:["C"],explanation:"The backend can only process one request at a time, and requests must be processed sequentially per customer. Pub/Sub provides a buffer to handle variable volume and decouples the frontend from the slow backend. Using Pub/Sub ordering keys (e.g., based on CustomerId) ensures messages for the same customer are delivered in order. A *pull* subscriber (C) allows the single-threaded backend to consume messages at its own pace, processing one message (and its associated updates from a data store if needed for sequence management within the message group) before pulling the next. Push mode (D) could overwhelm the backend. Direct requests (A, B) would fail or block during high volume.",conditions:["Ecommerce app, variable order volume, backend processes 1 request at a time, requests must be processed sequentially per customer."],caseStudyContext:null},{id:136,topic:"Databases",question:`Your company needs a database solution that stores customer purchase history and meets the following requirements:

 Customers can query their purchase immediately after submission.
 Purchases can be sorted on a variety of fields.
 Distinct record formats can be stored at the same time.
Which storage option satisfies these requirements?`,options:{A:"Firestore in Native mode",B:"Cloud Storage using an object read",C:"Cloud SQL using a SQL SELECT statement",D:"Firestore in Datastore mode using a global query"},correctAnswer:["A"],explanation:"Firestore in Native mode is a NoSQL document database that meets these requirements. It offers strong consistency for reads after writes ('query immediately'). As a document database, it naturally supports distinct record formats (flexible schema) within the same collection. It provides powerful indexing and querying capabilities, allowing sorting on various fields (A). Cloud Storage (B) isn't a queryable database. Cloud SQL (C) is relational and enforces a stricter schema, making distinct record formats difficult. Datastore mode (D) has eventual consistency for global queries by default, potentially violating the 'query immediately' requirement.",conditions:["Database for purchase history: query immediately after write, sortable on multiple fields, supports distinct record formats."],caseStudyContext:null},{id:137,topic:"Compute",question:"You recently developed a new service on Cloud Run. The new service authenticates using a custom service and then writes transactional information to a Cloud Spanner database. You need to verify that your application can support up to 5,000 read and 1,000 write transactions per second while identifying any bottlenecks that occur. Your test infrastructure must be able to autoscale. What should you do?",options:{A:"Build a test harness to generate requests and deploy it to Cloud Run. Analyze the VPC Flow Logs using Cloud Logging.",B:"Create a Google Kubernetes Engine cluster running the Locust or JMeter images to dynamically generate load tests. Analyze the results using Cloud Trace.",C:"Create a Cloud Task to generate a test load. Use Cloud Scheduler to run 60,000 Cloud Task transactions per minute for 10 minutes. Analyze the results using Cloud Monitoring.",D:"Create a Compute Engine instance that uses a LAMP stack image from the Marketplace, and use Apache Bench to generate load tests against the service. Analyze the results using Cloud Trace."},correctAnswer:["B"],explanation:"To generate significant, scalable load (5000+ TPS) and identify bottlenecks, a distributed load testing framework is needed. Deploying tools like Locust or JMeter on GKE (B) allows the test infrastructure itself to autoscale by adjusting the number of load-generating pods. GKE provides the necessary scalability. Analyzing results with Cloud Trace helps pinpoint latency bottlenecks within the application or its dependencies (like Spanner or the auth service). Cloud Run (A), Cloud Tasks (C), or a single GCE instance (D) are unlikely to generate the required scale of load efficiently or provide the necessary distributed testing infrastructure.",conditions:["Load test Cloud Run service (5k reads/s, 1k writes/s), identify bottlenecks, test infra must autoscale."],caseStudyContext:null},{id:138,topic:"Compute",question:"You are using Cloud Build for your CI/CD pipeline to complete several tasks, including copying certain files to Compute Engine virtual machines. Your pipeline requires a flat file that is generated in one builder in the pipeline to be accessible by subsequent builders in the same pipeline. How should you store the file so that all the builders in the pipeline can access it?",options:{A:"Store and retrieve the file contents using Compute Engine instance metadata.",B:"Output the file contents to a file in /workspace. Read from the same /workspace file in the subsequent build step.",C:"Use gsutil to output the file contents to a Cloud Storage object. Read from the same object in the subsequent build step.",D:"Add a build argument that runs an HTTP POST via curl to a separate web server to persist the value in one builder. Use an HTTP GET via curl from the subsequent build step to read the value."},correctAnswer:["B"],explanation:"Cloud Build mounts a persistent disk volume at `/workspace` which is shared across all build steps within a single build execution. Files written to `/workspace` by one step are available to subsequent steps in the same build. This is the standard and simplest mechanism for passing files or artifacts between build steps (B). Instance metadata (A) isn't directly accessible or suitable for files. Using GCS (C) works but adds overhead compared to the local workspace. Using an external web server (D) is overly complex.",conditions:["Pass a file generated in one Cloud Build step to subsequent steps in the same pipeline."],caseStudyContext:null},{id:139,topic:"IAM & Security",question:"Your companys development teams want to use various open source operating systems in their Docker builds. When images are created in published containers in your companys environment, you need to scan them for Common Vulnerabilities and Exposures (CVEs). The scanning process must not impact software development agility. You want to use managed services where possible. What should you do?",options:{A:"Enable the Vulnerability scanning setting in the Container Registry.",B:"Create a Cloud Function that is triggered on a code check-in and scan the code for CVEs.",C:"Disallow the use of non-commercially supported base images in your development environment.",D:"Use Cloud Monitoring to review the output of Cloud Build to determine whether a vulnerable version has been used."},correctAnswer:["A"],explanation:"Container Registry (and its successor, Artifact Registry) has a built-in, managed vulnerability scanning feature (powered by Container Analysis). Enabling this feature (A) automatically scans images upon push for known OS CVEs without requiring changes to the build process, thus not impacting agility. Scanning source code (B) doesn't scan the final image dependencies. Restricting base images (C) limits flexibility. Monitoring build output (D) doesn't perform the scan itself.",conditions:["Scan published container images for OS CVEs, managed service preferred, minimal impact on agility."],caseStudyContext:null},{id:140,topic:"Compute",question:`You are configuring a continuous integration pipeline using Cloud Build to automate the deployment of new container images to Google Kubernetes Engine (GKE). The pipeline builds the application from its source code, runs unit and integration tests in separate steps, and pushes the container to Container Registry. The application runs on a Python web server.

The Dockerfile is as follows:

\`\`\`dockerfile
FROM python:3.7-alpine

COPY . /app

WORKDIR /app RUN pip install -r requirements.txt
CMD [ "gunicorn", "-w 4", "main:app" ]
\`\`\`

You notice that Cloud Build runs are taking longer than expected to complete. You want to decrease the build time. What should you do? (Choose two.)`,options:{A:"Select a virtual machine (VM) size with higher CPU for Cloud Build runs.",B:"Deploy a Container Registry on a Compute Engine VM in a VPC, and use it to store the final images.",C:"Cache the Docker image for subsequent builds using the -- cache-from argument in your build config file.",D:"Change the base image in the Dockerfile to ubuntu:latest, and install Python 3.7 using a package manager utility.",E:"Store application source code on Cloud Storage, and configure the pipeline to use gsutil to download the source code."},correctAnswer:["A","C"],explanation:"To decrease Cloud Build time: 1) Increase compute resources: Cloud Build allows selecting higher CPU machine types for builds, which can significantly speed up CPU-intensive tasks like compilation or testing (A). 2) Leverage caching: Docker builds utilize layer caching. Cloud Build can leverage Kaniko caching or Docker layer caching (`--cache-from`) (C) to reuse unchanged layers from previous builds, especially speeding up dependency installation (`pip install`). Running your own registry (B) is counterproductive. Using a larger base image (D) increases build time. Storing source in GCS (E) doesn't inherently speed up the build itself.",conditions:["Cloud Build pipeline (build, test, push Python container) is slow, need to decrease build time."],caseStudyContext:null},{id:141,topic:"Compute",question:`You are building a CI/CD pipeline that consists of a version control system, Cloud Build, and Container Registry. Each time a new tag is pushed to the repository, a Cloud Build job is triggered, which runs unit tests on the new code builds a new Docker container image, and pushes it into Container Registry. The last step of your pipeline should deploy the new container to your production Google Kubernetes Engine (GKE) cluster. You need to select a tool and deployment strategy that meets the following requirements:
 Zero downtime is incurred
 Testing is fully automated
 Allows for testing before being rolled out to users
 Can quickly rollback if needed

What should you do?`,options:{A:"Trigger a Spinnaker pipeline configured as an A/B test of your new code and, if it is successful, deploy the container to production.",B:"Trigger a Spinnaker pipeline configured as a canary test of your new code and, if it is successful, deploy the container to production.",C:"Trigger another Cloud Build job that uses the Kubernetes CLI tools to deploy your new container to your GKE cluster, where you can perform a canary test.",D:"Trigger another Cloud Build job that uses the Kubernetes CLI tools to deploy your new container to your GKE cluster, where you can perform a shadow test."},correctAnswer:["D"],explanation:"Shadow testing (traffic mirroring) meets the requirements. It deploys the new version alongside the old, mirrors production traffic to the new version for testing under full load *without* affecting users (responses still come from the old version), allows quick rollback (stop mirroring, remove new version), and avoids downtime (D). Canary testing (B, C) exposes a subset of users to the new version before full rollout. A/B testing (A) is similar but often focuses on feature comparison. Triggering another Cloud Build job is a valid way to initiate the deployment part of the pipeline.",conditions:["CI/CD pipeline (Build -> Test -> Push to Registry -> Deploy to GKE), deploy on new tag, requirements: zero downtime, automated testing, pre-rollout testing, quick rollback."],caseStudyContext:null},{id:142,topic:"Compute",question:"Your operations team hasasked you to create a script that lists the Cloud Bigtable, Memorystore, and Cloud SQL databases running within a project. The script should allow users to submit a filter expression to limit the results presented. How should you retrieve the data?",options:{A:"Use the HBase API, Redis API, and MySQL connection to retrieve database lists. Combine the results, and then apply the filter to display the results",B:"Use the HBase API, Redis API, and MySQL connection to retrieve database lists. Filter the results individually, and then combine them to display the results",C:"Run gcloud bigtable instances list, gcloud redis instances list, and gcloud sql databases list. Use a filter within the application, and then display the results",D:"Run gcloud bigtable instances list, gcloud redis instances list, and gcloud sql databases list. Use --filter flag with each command, and then display the results"},correctAnswer:["D"],explanation:"The `gcloud` command-line tool provides subcommands to list resources for various GCP services, including Bigtable (`gcloud bigtable instances list`), Memorystore (`gcloud redis instances list` or `gcloud memcache instances list`), and Cloud SQL (`gcloud sql databases list` or `gcloud sql instances list`). Crucially, `gcloud` list commands support a powerful `--filter` flag that allows server-side filtering based on resource attributes using expressions. This is the most efficient way to retrieve only the desired data (D). Filtering client-side (C) retrieves all data first. Using native APIs/connections (A, B) is much more complex than using the unified `gcloud` tool.",conditions:["Script to list Bigtable, Memorystore, Cloud SQL resources in a project, allow user filtering."],caseStudyContext:null},{id:143,topic:"Compute",question:"You need to deploy a new European version of a website hosted on Google Kubernetes Engine. The current and new websites must be accessed via the same HTTP(S) load balancer's external IP address, but have different domain names. What should you do?",options:{A:"Define a new Ingress resource with a host rule matching the new domain",B:"Modify the existing Ingress resource with a host rule matching the new domain",C:"Create a new Service of type LoadBalancer specifying the existing IP address as the loadBalancerIP",D:"Generate a new Ingress resource and specify the existing IP address as the kubernetes.io/ingress.global-static-ip-name annotation value"},correctAnswer:["B"],explanation:"Kubernetes Ingress resources configured with GKE Ingress controller support host-based routing (name-based virtual hosting). To serve multiple domains (e.g., current.com and europe.current.com) from the same Load Balancer IP, you modify the *existing* Ingress resource to add rules for the new host (`host: europe.current.com`) that route traffic to the appropriate backend Service for the European version (B). Creating a new Ingress (A, D) would typically provision a *new* Load Balancer IP unless specific annotations for IP reuse are used (which D mentions but is usually applied when creating the *first* ingress for that IP). Service type LoadBalancer (C) creates an L4 LB.",conditions:["Deploy new version of GKE website for Europe, access via same LB IP as current site, use different domain name."],caseStudyContext:null},{id:144,topic:"Compute",question:"You are developing a single-player mobile game backend that has unpredictable traffic patterns as users interact with the game throughout the day and night. You want to optimize costs by ensuring that you have enough resources to handle requests, but minimize over-provisioning. You also want the system to handle traffic spikes efficiently. Which compute platform should you use?",options:{A:"Cloud Run",B:"Compute Engine with managed instance groups",C:"Compute Engine with unmanaged instance groups",D:"Google Kubernetes Engine using cluster autoscaling"},correctAnswer:["A"],explanation:"The requirements are unpredictable traffic, cost optimization (minimize over-provisioning, handle zero traffic), and efficient handling of spikes. Cloud Run (A) is ideal for this. It scales automatically based on requests, including scaling down to zero when there's no traffic (eliminating cost during idle periods), and scales up very quickly to handle spikes. GCE MIGs (B) and GKE (D) with autoscaling work but typically have a minimum number of instances running, incurring costs even when idle, and their scaling might be slower than Cloud Run's request-based scaling. Unmanaged groups (C) lack autoscaling.",conditions:["Mobile game backend, unpredictable traffic (spikes, zero traffic), optimize cost (minimize over-provisioning), handle spikes efficiently."],caseStudyContext:null},{id:145,topic:"IAM & Security",question:"The development teams in your company want to manage resources from their local environments. You have been asked to enable developer access to each teams Google Cloud projects. You want to maximize efficiency while following Google-recommended best practices. What should you do?",options:{A:"Add the users to their projects, assign the relevant roles to the users, and then provide the users with each relevant Project ID.",B:"Add the users to their projects, assign the relevant roles to the users, and then provide the users with each relevant Project Number.",C:"Create groups, add the users to their groups, assign the relevant roles to the groups, and then provide the users with each relevant Project ID.",D:"Create groups, add the users to their groups, assign the relevant roles to the groups, and then provide the users with each relevant Project Number."},correctAnswer:["C"],explanation:"The recommended best practice for managing IAM permissions, especially for teams, is to use Google Groups. Create a group for each team, add the developers to their respective groups, and then assign the necessary IAM roles to the *group* at the project level. This simplifies management  adding/removing users only requires group membership changes, not individual IAM policy updates (C, D). Granting roles directly to users (A, B) is less efficient to manage at scale. Developers typically interact with projects using the Project ID, which is human-readable, rather than the Project Number.",conditions:["Enable developer access to team projects from local environments, maximize efficiency, follow best practices."],caseStudyContext:null},{id:146,topic:"Compute",question:"Your companys product team has a new requirement based on customer demand to autoscale your stateless and distributed service running in a Google Kubernetes Engine (GKE) duster. You want to find a solution that minimizes changes because this feature will go live in two weeks. What should you do?",options:{A:"Deploy a Vertical Pod Autoscaler, and scale based on the CPU load.",B:"Deploy a Vertical Pod Autoscaler, and scale based on a custom metric.",C:"Deploy a Horizontal Pod Autoscaler, and scale based on the CPU toad.",D:"Deploy a Horizontal Pod Autoscaler, and scale based on a custom metric."},correctAnswer:["C"],explanation:"The application is stateless and distributed, needing autoscaling based on demand. Horizontal Pod Autoscaler (HPA) scales the *number* of replicas based on metrics, suitable for stateless distributed services. Vertical Pod Autoscaler (VPA) adjusts the resources (CPU/memory) *within* pods. Given the short timeline and need for minimal changes, scaling based on a standard metric like CPU utilization (C) is the simplest and quickest HPA configuration to implement, requiring no custom metric setup (unlike D). VPA (A, B) isn't the primary mechanism for scaling out replicas based on load.",conditions:["Stateless, distributed GKE service needs autoscaling, minimize changes, short timeline."],caseStudyContext:null},{id:147,topic:"Compute",question:"Your application is composed of a set of loosely coupled services orchestrated by code executed on Compute Engine. You want your application to easily bring up new Compute Engine instances that find and use a specific version of a service. How should this be configured?",options:{A:"Define your service endpoint information as metadata that is retrieved at runtime and used to connect to the desired service.",B:"Define your service endpoint information as label data that is retrieved at runtime and used to connect to the desired service.",C:"Define your service endpoint information to be retrieved from an environment variable at runtime and used to connect to the desired service.",D:"Define your service to use a fixed hostname and port to connect to the desired service. Replace the service at the endpoint with your new version."},correctAnswer:["A"],explanation:"For dynamic service discovery where new instances need to find specific versions of other services, instance or project metadata is a suitable mechanism. Store the endpoint information (e.g., IP/port or DNS name) for the specific service version required by the instance in its metadata (A). The instance can query the metadata server at startup or runtime to find the correct endpoint. Labels (B) are for organization. Environment variables (C) need to be set during instance creation or startup. Fixed hostnames (D) don't easily support versioning or dynamic discovery.",conditions:["GCE instances need to find/use specific versions of other services dynamically."],caseStudyContext:null},{id:148,topic:"IAM & Security",question:"You are developing a microservice-based application that will run on Google Kubernetes Engine (GKE). Some of the services need to access different Google Cloud APIs. How should you set up authentication of these services in the cluster following Google-recommended best practices? (Choose two.)",options:{A:"Use the service account attached to the GKE node.",B:"Enable Workload Identity in the cluster via the gcloud command-line tool.",C:"Access the Google service account keys from a secret management service.",D:"Store the Google service account keys in a central secret management service.",E:"Use gcloud to bind the Kubernetes service account and the Google service account using roles/iam.workloadIdentity."},correctAnswer:["B","E"],explanation:"Workload Identity is the recommended best practice. This involves enabling Workload Identity on the GKE cluster (B). Then, for each application/microservice needing specific Google Cloud access, you create a dedicated Kubernetes Service Account (KSA) and a Google Service Account (GSA) with the least privilege IAM roles. Finally, you create an IAM policy binding that allows the KSA to impersonate the GSA, using the `roles/iam.workloadIdentityUser` role (E describes this binding action, often done via `gcloud iam service-accounts add-iam-policy-binding`). This avoids using node service accounts (A) or managing GSA keys (C, D).",conditions:["GKE microservices need access to various Google Cloud APIs, follow security best practices."],caseStudyContext:null},{id:149,topic:"Compute",question:"Your development team has been tasked with maintaining a .NET legacy application. The application incurs occasional changes and was recently updated. Your goal is to ensure that the application provides consistent results while moving through the CI/CD pipeline from environment to environment. You want to minimize the cost of deployment while making sure that external factors and dependencies between hosting environments are not problematic. Containers are not yet approved in your organization. What should you do?",options:{A:"Rewrite the application using .NET Core, and deploy to Cloud Run. Use revisions to separate the environments.",B:"Use Cloud Build to deploy the application as a new Compute Engine image for each build. Use this image in each environment.",C:"Deploy the application using MS Web Deploy, and make sure to always use the latest, patched MS Windows Server base image in Compute Engine.",D:"Use Cloud Build to package the application, and deploy to a Google Kubernetes Engine cluster. Use namespaces to separate the environments."},correctAnswer:["B"],explanation:"The core requirement is consistency across environments, achieved by deploying the *same artifact* everywhere. Since containers are not approved, the artifact must be a VM image. Using Cloud Build to create a new GCE image ('golden image') containing the updated .NET application ensures that the exact same image (with the OS, dependencies, and application code) is deployed to each environment (Dev, Test, Prod) (B). This guarantees consistency and minimizes environment-specific issues. Rewriting (A) is major effort. Using latest base images (C) doesn't guarantee application consistency. GKE (D) uses containers.",conditions:[".NET legacy app, CI/CD deployment across environments, ensure consistency, minimize cost/dependencies, containers not approved."],caseStudyContext:null},{id:150,topic:"Compute",question:"The new version of your containerized application has been tested and is ready to deploy to production on Google Kubernetes Engine. You were not able to fully load-test the new version in pre-production environments, and you need to make sure that it does not have performance problems once deployed. Your deployment must be automated. What should you do?",options:{A:"Use Cloud Load Balancing to slowly ramp up traffic between versions. Use Cloud Monitoring to look for performance issues.",B:"Deploy the application via a continuous delivery pipeline using canary deployments. Use Cloud Monitoring to look for performance issues. and ramp up traffic as the metrics support it.",C:"Deploy the application via a continuous delivery pipeline using blue/green deployments. Use Cloud Monitoring to look for performance issues, and launch fully when the metrics support it.",D:"Deploy the application using kubectl and set the spec.updateStrategv.type to RollingUpdate. Use Cloud Monitoring to look for performance issues, and run the kubectl rollback command if there are any issues."},correctAnswer:["B"],explanation:"Given the inability to fully load-test pre-prod, a cautious rollout is needed. Canary deployment (B) addresses this by initially exposing only a small percentage of production traffic to the new version. This allows monitoring its performance (via Cloud Monitoring) under real load with minimal risk. If metrics are good, traffic can be gradually increased (ramped up). If issues arise, rollback is straightforward. Blue/green (C) switches all traffic at once after testing, which might be risky here. Rolling update (D) gradually replaces instances but affects users progressively. Option A describes the *mechanism* (LB traffic shifting) often used in canary/blue-green, not the overall strategy.",conditions:["Deploy new GKE app version to prod, couldn't fully load-test, ensure no performance issues, automated deployment."],caseStudyContext:null},{id:151,topic:"Compute",question:"Users are complaining that your Cloud Run-hosted website responds too slowly during traffic spikes. You want to provide a better user experience during traffic peaks. What should you do?",options:{A:"Read application configuration and static data from the database on application startup.",B:"Package application configuration and static data into the application image during build time.",C:"Perform as much work as possible in the background after the response has been returned to the user.",D:"Ensure that timeout exceptions and errors cause the Cloud Run instance to exit quickly so a replacement instance can be started."},correctAnswer:["B"],explanation:"Slow response during spikes often indicates slow startup or resource contention. Cloud Run performance can be improved by minimizing work done during startup or per-request. Reading config/static data from a DB on startup (A) adds latency to cold starts. Packaging static assets and configuration directly into the container image (B) allows the application to access them instantly from the local filesystem, significantly speeding up startup and request processing compared to external lookups. Background work (C) can help perceived latency but not necessarily throughput during spikes. Quick exits on errors (D) improve availability but not peak performance.",conditions:["Cloud Run website slow during traffic spikes, improve user experience."],caseStudyContext:null},{id:152,topic:"Compute",question:`You are a developer working on an internal application for payroll processing. You are building a component of the application that allows an employee to submit a timesheet, which then initiates several steps:

 An email is sent to the employee and manager, notifying them that the timesheet was submitted.
 A timesheet is sent to payroll processing for the vendor's API.
 A timesheet is sent to the data warehouse for headcount planning.

These steps are not dependent on each other and can be completed in any order. New steps are being considered and will be implemented by different development teams. Each development team will implement the error handling specific to their step. What should you do?`,options:{A:"Deploy a Cloud Function for each step that calls the corresponding downstream system to complete the required action.",B:"Create a Pub/Sub topic for each step. Create a subscription for each downstream development team to subscribe to their step's topic.",C:"Create a Pub/Sub topic for timesheet submissions. Create a subscription for each downstream development team to subscribe to the topic.",D:"Create a timesheet microservice deployed to Google Kubernetes Engine. The microservice calls each downstream step and waits for a successful response before calling the next step."},correctAnswer:["C"],explanation:"This scenario calls for decoupling and asynchronous processing. When a timesheet is submitted, the event should trigger multiple independent actions. Using a single Pub/Sub topic for 'timesheet submitted' events (C) is ideal. Each downstream system/team (email, payroll API, data warehouse) creates its own subscription to this topic. When a message is published, all subscriptions receive it, allowing each team to process the event independently and asynchronously. This decouples the services, allows adding new subscribers easily, and handles errors per subscriber. Cloud Functions (A) could implement the subscribers. Multiple topics (B) are unnecessary. Synchronous calls (D) create tight coupling and potential bottlenecks.",conditions:["Timesheet submission triggers multiple independent, asynchronous steps handled by different teams."],caseStudyContext:null},{id:153,topic:"Hybrid & Multi-cloud",question:"You are designing an application that uses a microservices architecture. You are planning to deploy the application in the cloud and on-premises. You want to make sure the application can scale up on demand and also use managed services as much as possible. What should you do?",options:{A:"Deploy open source Istio in a multi-cluster deployment on multiple Google Kubernetes Engine (GKE) clusters managed by Anthos.",B:"Create a GKE cluster in each environment with Anthos, and use Cloud Run for Anthos to deploy your application to each cluster.",C:"Install a GKE cluster in each environment with Anthos, and use Cloud Build to create a Deployment for your application in each cluster.",D:"Create a GKE cluster in the cloud and install open-source Kubernetes on-premises. Use an external load balancer service to distribute traffic across the two environments."},correctAnswer:["B"],explanation:"Anthos is Google's platform for modernizing applications across hybrid and multi-cloud environments, using Kubernetes as the foundation. To deploy microservices to both GCP and on-premises while using managed services and enabling scaling, Anthos is the key. Cloud Run for Anthos (B) allows deploying serverless, containerized applications onto Anthos clusters (both GKE on GCP and Anthos clusters on-prem). It provides automatic scaling and leverages the managed Anthos control plane. Using open source Istio (A) or K8s (D) requires more self-management. Cloud Build (C) is for CI/CD, not the runtime platform.",conditions:["Microservices app, deploy to cloud and on-prem, scale on demand, use managed services."],caseStudyContext:null},{id:154,topic:"Compute",question:"You want to migrate an on-premises container running in Knative to Google Cloud. You need to make sure that the migration doesn't affect your application's deployment strategy, and you want to use a fully managed service. Which Google Cloud service should you use to deploy your container?",options:{A:"Cloud Run",B:"Compute Engine",C:"Google Kubernetes Engine",D:"App Engine flexible environment"},correctAnswer:["A"],explanation:"Cloud Run is Google Cloud's fully managed serverless container platform built on Knative. Migrating a Knative container to Cloud Run (A) provides the most direct path, leveraging the same underlying serving primitives and concepts, thus minimizing impact on the deployment strategy. Cloud Run is fully managed, meeting that requirement. GKE (C) requires cluster management. Compute Engine (B) requires managing VMs. App Engine Flex (D) is another container platform but Cloud Run is the direct Knative equivalent.",conditions:["Migrate on-prem Knative container to GCP, maintain deployment strategy, use fully managed service."],caseStudyContext:null},{id:155,topic:"Databases",question:`This architectural diagram depicts a system that streams data from thousands of devices. You want to ingest data into a pipeline, store the data, and analyze the data using SQL statements. Which Google Cloud services should you use for steps 1, 2, 3, and 4?

[Diagram showing: Devices -> 1: Ingest -> 2: Pipeline -> 3: Transactional DB -> 4: Analytics DB]`,options:{A:`1. App Engine
2. Pub/Sub
3. BigQuery
4. Firestore`,B:`1. Dataflow
2. Pub/Sub
3. Firestore
4. BigQuery`,C:`1. Pub/Sub
2. Dataflow
3. BigQuery
4. Firestore`,D:`1. Pub/Sub
2. Dataflow
3. Firestore
4. BigQuery`},correctAnswer:["D"],explanation:"1. **Ingest:** Pub/Sub is the standard service for ingesting high-volume streaming data from many sources. 2. **Pipeline:** Dataflow is the managed service for building data processing pipelines (batch or stream). 3. **Transactional DB:** Firestore (or Cloud SQL/Spanner depending on exact needs, but Firestore fits general NoSQL transactional use) serves as a transactional database. 4. **Analytics DB:** BigQuery is the service for large-scale SQL-based analytics. Therefore, the sequence is Pub/Sub -> Dataflow -> Firestore -> BigQuery (D).",conditions:["Process streaming data from devices: ingest, pipeline, transactional storage, analytics storage (SQL)."],caseStudyContext:null},{id:156,topic:"Compute",question:"Your company just experienced a Google Kubernetes Engine (GKE) API outage due to a zone failure. You want to deploy a highly available GKE architecture that minimizes service interruption to users in the event of a future zone failure. What should you do?",options:{A:"Deploy Zonal clusters",B:"Deploy Regional clusters",C:"Deploy Multi-Zone clusters",D:"Deploy GKE on-premises clusters"},correctAnswer:["B"],explanation:"GKE Regional clusters provide the highest availability against zonal failures. They replicate the cluster control plane across multiple zones within a region. Node pools in a regional cluster can also span multiple zones. If one zone fails, the control plane remains available in other zones, and workloads can continue running on nodes in the surviving zones (B). Zonal (A) and Multi-Zonal (C - control plane in one zone, nodes can be multi-zone) clusters have a single point of failure for the control plane in their primary zone. On-prem (D) doesn't address cloud availability.",conditions:["Experienced GKE zonal outage, need highly available architecture resilient to zone failures."],caseStudyContext:null},{id:157,topic:"Compute",question:"Your team develops services that run on Google Cloud. You want to process messages sent to a Pub/Sub topic, and then store them. Each message must be processed exactly once to avoid duplication of data and any data conflicts. You need to use the cheapest and most simple solution. What should you do?",options:{A:"Process the messages with a Dataproc job, and write the output to storage.",B:"Process the messages with a Dataflow streaming pipeline using Apache Beam's PubSubIO package, and write the output to storage.",C:"Process the messages with a Cloud Function, and write the results to a BigQuery location where you can run a job to deduplicate the data.",D:"Retrieve the messages with a Dataflow streaming pipeline, store them in Cloud Bigtable, and use another Dataflow streaming pipeline to deduplicate messages."},correctAnswer:["B"],explanation:"Dataflow streaming pipelines using the native PubSubIO connector provide built-in support for exactly-once processing semantics by leveraging Pub/Sub message IDs for deduplication (B). This is generally the simplest and most cost-effective managed solution for robust, scalable, exactly-once stream processing from Pub/Sub. Dataproc (A) is for batch Hadoop/Spark jobs. Cloud Functions (C) don't inherently guarantee exactly-once processing without careful implementation, and require a separate deduplication step. Using two Dataflow pipelines and Bigtable (D) adds unnecessary complexity and cost.",conditions:["Process Pub/Sub messages exactly once, store results, cheapest and simplest solution."],caseStudyContext:null},{id:158,topic:"IAM & Security",question:"You are running a containerized application on Google Kubernetes Engine. Your container images are stored in Container Registry. Your team uses CI/CD practices. You need to prevent the deployment of containers with known critical vulnerabilities. What should you do?",options:{A:` Use Web Security Scanner to automatically crawl your application
 Review your application logs for scan results, and provide an attestation that the container is free of known critical vulnerabilities
 Use Binary Authorization to implement a policy that forces the attestation to be provided before the container is deployed`,B:` Use Web Security Scanner to automatically crawl your application
 Review the scan results in the scan details page in the Cloud Console, and provide an attestation that the container is free of known critical vulnerabilities
 Use Binary Authorization to implement a policy that forces the attestation to be provided before the container is deployed`,C:` Enable the Container Scanning API to perform vulnerability scanning
 Review vulnerability reporting in Container Registry in the Cloud Console, and provide an attestation that the container is free of known critical vulnerabilities
 Use Binary Authorization to implement a policy that forces the attestation to be provided before the container is deployed`,D:` Enable the Container Scanning API to perform vulnerability scanning
 Programmatically review vulnerability reporting through the Container Scanning API, and provide an attestation that the container is free of known critical vulnerabilities
 Use Binary Authorization to implement a policy that forces the attestation to be provided before the container is deployed`},correctAnswer:["D"],explanation:"The process involves: 1. Scanning: Enable Container Analysis vulnerability scanning (Container Scanning API) for images in Container Registry/Artifact Registry. 2. Verification/Attestation: Programmatically check the scan results via the API (e.g., in a CI/CD step) to ensure no critical vulnerabilities exist. If clean, create a Binary Authorization attestation. 3. Enforcement: Configure a Binary Authorization policy on the GKE cluster requiring a valid attestation from your attestor before allowing deployment. Option D correctly outlines this automated flow. Web Security Scanner (A, B) scans running web apps, not container images. Manual review (C) is less suitable for CI/CD.",conditions:["GKE app, images in Container Registry, CI/CD, prevent deployment of containers with critical CVEs."],caseStudyContext:null},{id:159,topic:"IAM & Security",question:"You have an on-premises application that authenticates to the Cloud Storage API using a user-managed service account with a user-managed key. The application connects to Cloud Storage using Private Google Access over a Dedicated Interconnect link. You discover that requests from the application to access objects in the Cloud Storage bucket are failing with a 403 Permission Denied error code. What is the likely cause of this issue?",options:{A:"The folder structure inside the bucket and object paths have changed.",B:"The permissions of the service accounts predefined role have changed.",C:"The service account key has been rotated but not updated on the application server.",D:"The Interconnect link from the on-premises data center to Google Cloud is experiencing a temporary outage."},correctAnswer:["C"],explanation:"A 403 error generally means authentication succeeded (the request reached the service and identity was recognized) but authorization failed (the identity lacks permission for the requested action). However, when using service account keys, an *invalid or expired key* can sometimes manifest as a 403, effectively an authentication failure misinterpreted layer up. Since the setup uses user-managed keys, rotation is a manual process. If the key was rotated in GCP but the application server still uses the old, invalid key file, authentication will fail, leading to the 403 error (C). Changed permissions (B) would also cause 403 but key rotation is a common issue with this setup. Path changes (A) or network outages (D) usually result in different errors (404, timeout/network error).",conditions:["On-prem app uses SA key for GCS access via Private Google Access/Interconnect, gets 403 error."],caseStudyContext:null},{id:160,topic:"Storage",question:"You are using the Cloud Client Library to upload an image in your application to Cloud Storage. Users of the application report that occasionally the upload does not complete and the client library reports an HTTP 504 Gateway Timeout error. You want to make the application more resilient to errors. What changes to the application should you make?",options:{A:"Write an exponential backoff process around the client library call.",B:"Write a one-second wait time backoff process around the client library call.",C:"Design a retry button in the application and ask users to click if the error occurs.",D:"Create a queue for the object and inform the users that the application will try again in 10 minutes."},correctAnswer:["A"],explanation:"HTTP 504 Gateway Timeout errors indicate a temporary issue, often network-related or a transient problem in the backend service. These are typically retryable errors. The recommended practice for handling such errors is to implement automatic retries with exponential backoff (A). This strategy attempts the upload again after increasing delays, improving the chance of success without overwhelming the system during temporary glitches. Fixed delays (B) are less adaptive. Manual user retries (C) provide poor user experience. Queuing (D) adds complexity and significant delay.",conditions:["Application uploading to GCS gets occasional HTTP 504 errors, need to make more resilient."],caseStudyContext:null},{id:161,topic:"Databases",question:"You are building a mobile application that will store hierarchical data structures in a database. The application will enable users working offline to sync changes when they are back online. A backend service will enrich the data in the database using a service account. The application is expected to be very popular and needs to scale seamlessly and securely. Which database and IAM role should you use?",options:{A:"Use Cloud SQL, and assign the roles/cloudsql.editor role to the service account.",B:"Use Bigtable, and assign the roles/bigtable.viewer role to the service account.",C:"Use Firestore in Native mode and assign the roles/datastore.user role to the service account.",D:"Use Firestore in Datastore mode and assign the roles/datastore.viewer role to the service account."},correctAnswer:["C"],explanation:"Firestore in Native mode (C) is well-suited: it handles hierarchical data (documents/collections), offers excellent offline support for mobile/web clients with automatic synchronization, scales seamlessly, and integrates with IAM. The `roles/datastore.user` role provides read/write access needed by the backend service. Cloud SQL (A) is relational, less natural for hierarchical data, and offline sync is complex. Bigtable (B) is for massive-scale NoSQL workloads, often overkill, and lacks mobile offline sync features. Datastore mode (D) is the older version, Native mode is generally preferred; viewer role is insufficient for writes.",conditions:["Mobile app DB: hierarchical data, offline sync, backend enrichment via SA, scalable, secure."],caseStudyContext:null},{id:162,topic:"Compute",question:"You have an application running on hundreds of Compute Engine instances in a managed instance group (MIG) in multiple zones. You need to deploy a new instance template to fix a critical vulnerability immediately but must avoid impact to your service. What setting should be made to the MIG after updating the instance template?",options:{A:"Set the Max Surge to 100%.",B:"Set the Update mode to Opportunistic.",C:"Set the Maximum Unavailable to 100%.",D:"Set the Minimum Wait time to 0 seconds."},correctAnswer:["B"],explanation:"The requirement is to deploy immediately *without impacting* the service. Proactive updates (the default) automatically roll out the change, which could cause disruption depending on maxUnavailable/maxSurge settings. Opportunistic update mode (B) applies the new template *only* when instances are manually recreated or when the MIG naturally scales up or down (e.g., due to autoscaler). This avoids forcing updates on running instances, minimizing impact, while ensuring new instances get the fix. Setting Max Surge (A) or Max Unavailable (C) relates to Proactive updates and could cause impact or cost. Min Wait Time (D) also applies to Proactive updates.",conditions:["Deploy new instance template to large MIG immediately (critical fix), avoid service impact."],caseStudyContext:null},{id:163,topic:"Compute",question:"You made a typo in a low-level Linux configuration file that prevents your Compute Engine instance from booting to a normal run level. You just created the Compute Engine instance today and have done no other maintenance on it, other than tweaking files. How should you correct this error?",options:{A:"Download the file using scp, change the file, and then upload the modified version",B:"Configure and log in to the Compute Engine instance through SSH, and change the file",C:"Configure and log in to the Compute Engine instance through the serial port, and change the file",D:"Configure and log in to the Compute Engine instance using a remote desktop client, and change the file"},correctAnswer:["C"],explanation:"If a configuration error prevents the instance from booting normally, SSH (B) or RDP (D) services might not start. SCP (A) also relies on SSH. The serial console provides low-level access, often available even during boot failures. By enabling interactive serial console access, you can log in (often to single-user mode or a recovery shell) and edit the problematic configuration file to fix the typo (C).",conditions:["Typo in Linux config file prevents GCE instance from booting normally, need to fix."],caseStudyContext:null},{id:164,topic:"Storage",question:"You are developing an application that needs to store files belonging to users in Cloud Storage. You want each user to have their own subdirectory in Cloud Storage. When a new user is created, the corresponding empty subdirectory should also be created. What should you do?",options:{A:"Create an object with the name of the subdirectory ending with a trailing slash ('/') that is zero bytes in length.",B:"Create an object with the name of the subdirectory, and then immediately delete the object within that subdirectory.",C:"Create an object with the name of the subdirectory that is zero bytes in length and has WRITER access control list permission.",D:"Create an object with the name of the subdirectory that is zero bytes in length. Set the Content-Type metadata to CLOUDSTORAGE_FOLDER."},correctAnswer:["A"],explanation:"Cloud Storage has a flat namespace, meaning folders don't truly exist as separate entities. However, tools like the Cloud Console and gsutil simulate folders based on object names containing '/'. To explicitly create an empty 'folder' placeholder that appears in listings, you create a zero-byte object whose name is the desired folder path ending with a '/' (A). Options B, C, and D describe incorrect or unnecessary procedures.",conditions:["Need to create empty user subdirectories in a GCS bucket when a user is created."],caseStudyContext:null},{id:165,topic:"Compute",question:"Your companys corporate policy states that there must be a copyright comment at the very beginning of all source files. You want to write a custom step in Cloud Build that is triggered by each source commit. You need the trigger to validate that the source contains a copyright and add one for subsequent steps if not there. What should you do?",options:{A:"Build a new Docker container that examines the files in /workspace and then checks and adds a copyright for each source file. Changed files are explicitly committed back to the source repository.",B:"Build a new Docker container that examines the files in /workspace and then checks and adds a copyright for each source file. Changed files do not need to be committed back to the source repository.",C:"Build a new Docker container that examines the files in a Cloud Storage bucket and then checks and adds a copyright for each source file. Changed files are written back to the Cloud Storage bucket.",D:"Build a new Docker container that examines the files in a Cloud Storage bucket and then checks and adds a copyright for each source file. Changed files are explicitly committed back to the source repository."},correctAnswer:["B"],explanation:"Cloud Build checks out the source code into the `/workspace` directory. A custom build step (running in a container) should operate on the files within `/workspace`. The step needs to check for the copyright and add it if missing. The modified files in `/workspace` will then be available to subsequent build steps (e.g., compilation, testing, packaging) within the *same build execution* without needing to be committed back to the repository (B). Committing back (A) within the check step is unusual and often discouraged in CI pipelines. Operating on files in GCS (C, D) is incorrect as the source is in `/workspace`.",conditions:["Cloud Build step triggered on commit, must validate/add copyright to source files for subsequent steps."],caseStudyContext:null},{id:166,topic:"Compute",question:"One of your deployed applications in Google Kubernetes Engine (GKE) is having intermittent performance issues. Your team uses a third-party logging solution. You want to install this solution on each node in your GKE cluster so you can view the logs. What should you do?",options:{A:"Deploy the third-party solution as a DaemonSet",B:"Modify your container image to include the monitoring software",C:"Use SSH to connect to the GKE node, and install the software manually",D:"Deploy the third-party solution using Terraform and deploy the logging Pod as a Kubernetes Deployment"},correctAnswer:["A"],explanation:"A Kubernetes DaemonSet ensures that a copy of a specific Pod runs on all (or a subset of) nodes in the cluster. This is the standard Kubernetes pattern for deploying node-level agents like logging collectors, monitoring agents, or CNI plugins (A). Modifying application images (B) is incorrect. Manual installation via SSH (C) bypasses Kubernetes management and won't persist. A Deployment (D) doesn't guarantee placement on every node like a DaemonSet does.",conditions:["Install third-party logging agent on every GKE node."],caseStudyContext:null},{id:167,topic:"Case Study Analysis",question:"Case study - HipLocal: How should HipLocal redesign their architecture to ensure that the application scales to support a large increase in users?",options:{A:"Use Google Kubernetes Engine (GKE) to run the application as a microservice. Run the MySQL database on a dedicated GKE node.",B:"Use multiple Compute Engine instances to run MySQL to store state information. Use a Google Cloud-managed load balancer to distribute the load between instances. Use managed instance groups for scaling.",C:"Use Memorystore to store session information and CloudSQL to store state information. Use a Google Cloud-managed load balancer to distribute the load between instances. Use managed instance groups for scaling.",D:"Use a Cloud Storage bucket to serve the application as a static website, and use another Cloud Storage bucket to store user state information."},correctAnswer:["C"],explanation:"The requirements include supporting 10x users, reducing management, and adopting best practices. The current setup uses GCE APIs and a single MySQL instance. Option C proposes a scalable architecture: using managed GCE instances (presumably in a MIG) behind a load balancer for the API/application layer, scaling horizontally. It replaces the single MySQL with managed Cloud SQL for state and adds Memorystore (managed Redis/Memcached) for caching session data, reducing load on Cloud SQL. Running MySQL on GKE (A) or GCE (B) increases management overhead compared to Cloud SQL. GCS (D) is unsuitable for application logic or primary state storage.",conditions:["HipLocal case study context, redesign architecture for 10x user scaling."],caseStudyContext:`<strong>Company overview</strong>
HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.

<strong>Executive statement</strong>
We are the number one local community app; its time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.

<strong><strong>Solution Concept </strong><br></strong>
HipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.

<strong><strong>Existing Technical Environment </strong><br></strong>
HipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:
<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>

<strong><strong>Business Requirements </strong><br></strong>
HipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:
<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>

<strong>Technical requirements</strong>
<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>`},{id:168,topic:"Case Study Analysis",question:"Case study - HipLocal: How should HipLocal increase their API development speed while continuing to provide the QA team with a stable testing environment that meets feature requirements?",options:{A:"Include unit tests in their code, and prevent deployments to QA until all tests have a passing status.",B:"Include performance tests in their code, and prevent deployments to QA until all tests have a passing status.",C:"Create health checks for the QA environment, and redeploy the APIs at a later time if the environment is unhealthy.",D:"Redeploy the APIs to App Engine using Traffic Splitting. Do not move QA traffic to the new versions if errors are found."},correctAnswer:["A"],explanation:"The goal is to speed up development while maintaining a stable QA environment. Development freezes are currently used. Implementing automated unit tests and gating deployments to QA based on their success (A) is a core CI/CD practice. It provides rapid feedback to developers and ensures only functionally correct code (at the unit level) reaches QA, improving QA stability and reducing the need for freezes. Performance tests (B) are important but come later. Health checks (C) monitor the environment but don't prevent bad code from entering. Traffic splitting (D) is a deployment strategy, not a pre-deployment quality gate.",conditions:["HipLocal case study context, increase dev speed, maintain stable QA environment."],caseStudyContext:`<strong>Company overview</strong>
HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.

<strong>Executive statement</strong>
We are the number one local community app; its time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.

<strong><strong>Solution Concept </strong><br></strong>
HipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.

<strong><strong>Existing Technical Environment </strong><br></strong>
HipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:
<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>

<strong><strong>Business Requirements </strong><br></strong>
HipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:
<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>

<strong>Technical requirements</strong>
<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>`},{id:169,topic:"Case Study Analysis",question:"Case study - HipLocal: HipLocal's application uses Cloud Client Libraries to interact with Google Cloud. HipLocal needs to configure authentication and authorization in the Cloud Client Libraries to implement least privileged access for the application. What should they do?",options:{A:"Create an API key. Use the API key to interact with Google Cloud.",B:"Use the default compute service account to interact with Google Cloud.",C:"Create a service account for the application. Export and deploy the private key for the application. Use the service account to interact with Google Cloud.",D:"Create a service account for the application and for each Google Cloud API used by the application. Export and deploy the private keys used by the application. Use the service account with one Google Cloud API to interact with Google Cloud."},correctAnswer:["C"],explanation:"Cloud Client Libraries use Application Default Credentials (ADC) to find credentials. When running on GCE, ADC defaults to using the attached service account. To implement least privilege, don't use the default compute service account (B) which often has broad permissions. Instead, create a *dedicated* service account for the application, grant it only the specific IAM roles needed, and attach this service account to the GCE instances running the application. Option C describes creating the dedicated SA, but incorrectly suggests exporting/deploying keys (best practice is to attach the SA to the instance). Option D is overly complex. API Keys (A) are not used for this type of authentication. Given the options, C is the closest to best practice by mentioning creating a dedicated SA, although the key export part is suboptimal compared to attaching the SA.",conditions:["HipLocal case study context, authenticate Cloud Client Libraries from GCE app, implement least privilege."],caseStudyContext:`<strong>Company overview</strong>
HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.

<strong>Executive statement</strong>
We are the number one local community app; its time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.

<strong><strong>Solution Concept </strong><br></strong>
HipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.

<strong><strong>Existing Technical Environment </strong><br></strong>
HipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:
<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>

<strong><strong>Business Requirements </strong><br></strong>
HipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:
<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>

<strong>Technical requirements</strong>
<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>`},{id:170,topic:"Compute",question:`You are in the final stage of migrating an on-premises data center to Google Cloud. You are quickly approaching your deadline, and discover that a web API is running on a server slated for decommissioning. You need to recommend a solution to modernize this API while migrating to Google Cloud. The modernized web API must meet the following requirements:

 Autoscales during high traffic periods at the end of each month
 Written in Python 3.x
 Developers must be able to rapidly deploy new versions in response to frequent code changes

You want to minimize cost, effort, and operational overhead of this migration. What should you do?`,options:{A:"Modernize and deploy the code on App Engine flexible environment.",B:"Modernize and deploy the code on App Engine standard environment.",C:"Deploy the modernized application to an n1-standard-1 Compute Engine instance.",D:"Ask the development team to re-write the application to run as a Docker container on Google Kubernetes Engine."},correctAnswer:["B"],explanation:"The requirements are: autoscaling, Python 3.x support, rapid deployment, minimize cost/effort/overhead. App Engine Standard (B) supports Python 3.x, provides excellent autoscaling (including scale-to-zero for cost savings), has very rapid deployment ('gcloud app deploy'), and is fully managed (minimal overhead). App Engine Flex (A) also works but usually has a minimum instance requirement, increasing cost. Single GCE instance (C) doesn't autoscale. GKE (D) involves containerization and cluster management, which adds effort/overhead compared to App Engine Standard.",conditions:["Migrate on-prem web API to GCP, needs autoscaling, Python 3.x, rapid deployment, minimize cost/effort/overhead."],caseStudyContext:null},{id:171,topic:"IAM & Security",question:"You are developing an application that consists of several microservices running in a Google Kubernetes Engine cluster. One microservice needs to connect to a third-party database running on-premises. You need to store credentials to the database and ensure that these credentials can be rotated while following security best practices. What should you do?",options:{A:"Store the credentials in a sidecar container proxy, and use it to connect to the third-party database.",B:"Configure a service mesh to allow or restrict traffic from the Pods in your microservice to the database.",C:"Store the credentials in an encrypted volume mount, and associate a Persistent Volume Claim with the client Pod.",D:"Store the credentials as a Kubernetes Secret, and use the Cloud Key Management Service plugin to handle encryption and decryption."},correctAnswer:["D"],explanation:"Kubernetes Secrets are designed for storing sensitive information like passwords or API keys. However, by default, they are only base64 encoded, not encrypted at rest within etcd. For enhanced security, especially for database credentials, use Application-layer Secrets Encryption. This involves encrypting the Kubernetes Secret data using a key managed in Cloud KMS. GKE integrates with Cloud KMS for this purpose (D). Storing credentials directly in a sidecar (A) or volume mount (C) without proper encryption/management is less secure. Service mesh (B) manages traffic, not credentials.",conditions:["Store/rotate on-prem DB credentials securely for GKE microservice."],caseStudyContext:null},{id:172,topic:"Monitoring & Logging",question:"You manage your company's ecommerce platform's payment system, which runs on Google Cloud. Your company must retain user logs for 1 year for internal auditing purposes and for 3 years to meet compliance requirements. You need to store new user logs on Google Cloud to minimize on-premises storage usage and ensure that they are easily searchable. You want to minimize effort while ensuring that the logs are stored correctly. What should you do?",options:{A:"Store the logs in a Cloud Storage bucket with bucket lock turned on.",B:"Store the logs in a Cloud Storage bucket with a 3-year retention period.",C:"Store the logs in Cloud Logging as custom logs with a custom retention period.",D:"Store the logs in a Cloud Storage bucket with a 1-year retention period. After 1 year, move the logs to another bucket with a 2-year retention period."},correctAnswer:["C"],explanation:"The requirements are: retain logs for 3 years (compliance), easily searchable for 1 year (auditing), minimize effort. Cloud Logging provides native log ingestion, storage, and powerful search capabilities. You can configure custom log retention periods for log buckets, setting it to 3 years (or more) to meet compliance (C). Logs are easily searchable within Cloud Logging for the entire retention period, satisfying the audit requirement. Storing in GCS (A, B, D) makes searching much harder and requires extra steps or tools. Bucket Lock (A) is for immutability, not just retention.",conditions:["Store user logs for 1yr (searchable audit) and 3yrs (compliance), minimize effort."],caseStudyContext:null},{id:173,topic:"IAM & Security",question:'Your company has a new security initiative that requires all data stored in Google Cloud to be encrypted by customer-managed encryption keys. You plan to use Cloud Key Management Service (KMS) to configure access to the keys. You need to follow the "separation of duties" principle and Google-recommended best practices. What should you do? (Choose two.)',options:{A:"Provision Cloud KMS in its own project.",B:"Do not assign an owner to the Cloud KMS project.",C:"Provision Cloud KMS in the project where the keys are being used.",D:"Grant the roles/cloudkms.admin role to the owner of the project where the keys from Cloud KMS are being used.",E:"Grant an owner role for the Cloud KMS project to a different user than the owner of the project where the keys from Cloud KMS are being used."},correctAnswer:["A","E"],explanation:"Separation of duties best practices for KMS recommend: 1. Hosting KMS resources (key rings, keys) in a separate, dedicated GCP project (A). This isolates key management from resource management. 2. Managing permissions carefully. Granting the owner role of the KMS project to a *different* individual or group than the owner(s) of the projects using the keys (E) ensures that those who manage resources cannot also manage the keys protecting them, and vice versa. Not assigning an owner (B) is possible but using an Org Admin might be better. C violates separation. D grants admin rights to the resource owner, also violating separation.",conditions:["Use KMS with CMEK, follow separation of duties best practices."],caseStudyContext:null},{id:174,topic:"Migration",question:"You need to migrate a standalone Java application running in an on-premises Linux virtual machine (VM) to Google Cloud in a cost-effective manner. You decide not to take the lift-and-shift approach, and instead you plan to modernize the application by converting it to a container. How should you accomplish this task?",options:{A:"Use Migrate for Anthos to migrate the VM to your Google Kubernetes Engine (GKE) cluster as a container.",B:"Export the VM as a raw disk and import it as an image. Create a Compute Engine instance from the Imported image.",C:"Use Migrate for Compute Engine to migrate the VM to a Compute Engine instance, and use Cloud Build to convert it to a container.",D:"Use Jib to build a Docker image from your source code, and upload it to Artifact Registry. Deploy the application in a GKE cluster, and test the application."},correctAnswer:["D"],explanation:"The goal is modernization by containerization, not lift-and-shift. Jib is a tool specifically designed to containerize Java applications directly from source code (Maven/Gradle projects) without needing a Dockerfile or Docker daemon, producing optimized images (D). This is often the most direct path for containerizing existing Java codebases. Migrate for Anthos (A) can containerize VMs but might be more complex. Options B and C involve lift-and-shift to GCE first, adding steps.",conditions:["Migrate on-prem Java app (Linux VM) to GCP container, modernize (no lift-and-shift), cost-effective."],caseStudyContext:null},{id:175,topic:"Compute",question:"Your organization has recently begun an initiative to replatform their legacy applications onto Google Kubernetes Engine. You need to decompose a monolithic application into microservices. Multiple instances have read and write access to a configuration file, which is stored on a shared file system. You want to minimize the effort required to manage this transition, and you want to avoid rewriting the application code. What should you do?",options:{A:"Create a new Cloud Storage bucket, and mount it via FUSE in the container.",B:"Create a new persistent disk, and mount the volume as a shared PersistentVolume.",C:"Create a new Filestore instance, and mount the volume as an NFS PersistentVolume.",D:"Create a new ConfigMap and volumeMount to store the contents of the configuration file."},correctAnswer:["C"],explanation:"The application requires multiple instances (Pods) to have simultaneous read and write access to the *same* configuration file, mimicking a shared file system. Kubernetes PersistentVolumes with ReadWriteMany (RWX) access mode are needed. Google Cloud Filestore provides managed NFS volumes that support RWX and can be mounted into GKE Pods via PersistentVolumes/PersistentVolumeClaims (C). GCE Persistent Disks (B) usually only support ReadWriteOnce. Cloud Storage FUSE (A) has limitations with concurrent writes and consistency. ConfigMaps (D) are read-only once mounted.",conditions:["Decompose monolith for GKE, multiple instances need read/write access to shared config file (currently on shared FS), minimize effort, avoid code rewrite."],caseStudyContext:null},{id:176,topic:"Compute",question:"Your team is developing unit tests for Cloud Function code. The code is stored in a Cloud Source Repositories repository. You are responsible for implementing the tests. Only a specific service account has the necessary permissions to deploy the code to Cloud Functions. You want to ensure that the code cannot be deployed without first passing the tests. How should you configure the unit testing process?",options:{A:"Configure Cloud Build to deploy the Cloud Function. If the code passes the tests, a deployment approval is sent to you.",B:"Configure Cloud Build to deploy the Cloud Function, using the specific service account as the build agent. Run the unit tests after successful deployment.",C:"Configure Cloud Build to run the unit tests. If the code passes the tests, the developer deploys the Cloud Function.",D:"Configure Cloud Build to run the unit tests, using the specific service account as the build agent. If the code passes the tests, Cloud Build deploys the Cloud Function."},correctAnswer:["D"],explanation:"The goal is an automated pipeline where deployment happens only after tests pass, using a specific service account for deployment. Cloud Build can orchestrate this. The pipeline should first run the unit tests. If the tests succeed, the next step should deploy the Cloud Function. Since a specific service account is required for deployment, Cloud Build needs to execute the deployment step using (or acting as) that service account (D). Running tests *after* deployment (B) defeats the purpose of gating. Manual deployment (C) or manual approval (A) doesn't meet the automation goal.",conditions:["Unit test Cloud Function code, deploy only if tests pass, deployment requires specific SA, automate."],caseStudyContext:null},{id:177,topic:"Monitoring & Logging",question:"Your team detected a spike of errors in an application running on Cloud Run in your production project. The application is configured to read messages from Pub/Sub topic A, process the messages, and write the messages to topic B. You want to conduct tests to identify the cause of the errors. You can use a set of mock messages for testing. What should you do?",options:{A:"Deploy the Pub/Sub and Cloud Run emulators on your local machine. Deploy the application locally, and change the logging level in the application to DEBUG or INFO. Write mock messages to topic A, and then analyze the logs.",B:"Use the gcloud CLI to write mock messages to topic A. Change the logging level in the application to DEBUG or INFO, and then analyze the logs.",C:"Deploy the Pub/Sub emulator on your local machine. Point the production application to your local Pub/Sub topics. Write mock messages to topic A, and then analyze the logs.",D:"Use the Google Cloud console to write mock messages to topic A. Change the logging level in the application to DEBUG or INFO, and then analyze the logs."},correctAnswer:["A"],explanation:"To safely diagnose production errors without impacting the live system, local testing with emulators is the best practice. Deploying the Pub/Sub emulator allows you to simulate topic A locally. Running the Cloud Run application container locally (e.g., via Docker) allows you to connect it to the local emulator. Increasing the logging level provides detailed diagnostics. Publishing mock messages to the local emulator topic A lets you observe the application's processing and identify the error's cause by analyzing the detailed local logs (A). Injecting mock messages into production (B, D) or pointing production to local emulators (C) is risky and inappropriate.",conditions:["Cloud Run app processing Pub/Sub messages (A->B) has errors in prod, need to test/debug with mock messages."],caseStudyContext:null},{id:178,topic:"IAM & Security",question:"You are developing a Java Web Server that needs to interact with Google Cloud services via the Google Cloud API on the user's behalf. Users should be able to authenticate to the Google Cloud API using their Google Cloud identities. Which workflow should you implement in your web application?",options:{A:`1. When a user arrives at your application, prompt them for their Google username and password.
2. Store an SHA password hash in your application's database along with the user's username.
3. The application authenticates to the Google Cloud API using HTTPs requests with the user's username and password hash in the Authorization request header.`,B:`1. When a user arrives at your application, prompt them for their Google username and password.
2. Forward the user's username and password in an HTTPS request to the Google Cloud authorization server, and request an access token.
3. The Google server validates the user's credentials and returns an access token to the application.
4. The application uses the access token to call the Google Cloud API.`,C:`1. When a user arrives at your application, route them to a Google Cloud consent screen with a list of requested permissions that prompts the user to sign in with SSO to their Google Account.
2. After the user signs in and provides consent, your application receives an authorization code from a Google server.
3. The Google server returns the authorization code to the user, which is stored in the browser's cookies.
4. The user authenticates to the Google Cloud API using the authorization code in the cookie.`,D:`1. When a user arrives at your application, route them to a Google Cloud consent screen with a list of requested permissions that prompts the user to sign in with SSO to their Google Account.
2. After the user signs in and provides consent, your application receives an authorization code from a Google server.
3. The application requests a Google Server to exchange the authorization code with an access token.
4. The Google server responds with the access token that is used by the application to call the Google Cloud API.`},correctAnswer:["D"],explanation:"This describes the standard OAuth 2.0 Authorization Code Grant Flow, which is the recommended method for web server applications to obtain access tokens to call Google APIs on behalf of users. The application redirects the user to Google for authentication and consent (Step 1). Google redirects back to the application with an authorization code (Step 2). The application backend then securely exchanges this code with Google for an access token (and optionally a refresh token) (Step 3). The application uses the access token to make API calls on the user's behalf (Step 4). Options A and B involve handling user passwords directly, which is insecure and violates OAuth principles. Option C incorrectly describes how the authorization code is handled and used.",conditions:["Java web server needs to call Google Cloud APIs on behalf of users using their Google identities."],caseStudyContext:null},{id:179,topic:"Compute",question:"You recently developed a new application. You want to deploy the application on Cloud Run without a Dockerfile. Your organization requires that all container images are pushed to a centrally managed container repository. How should you build your container using Google Cloud services? (Choose two.)",options:{A:"Push your source code to Artifact Registry.",B:"Submit a Cloud Build job to push the image.",C:"Use the pack build command with pack CLI.",D:"Include the --source flag with the gcloud run deploy CLI command.",E:"Include the --platform=kubernetes flag with the gcloud run deploy CLI command."},correctAnswer:["C","D"],explanation:"To build a container image without a Dockerfile for Cloud Run, Google Cloud Buildpacks can be used. This can be done implicitly with `gcloud run deploy --source .` (D), which uses Cloud Build and Buildpacks behind the scenes, builds the image, pushes it to Artifact Registry (creating a repo if needed), and deploys. Alternatively, you can use the `pack` CLI (C) (part of the Buildpacks project) locally or within a CI/CD system like Cloud Build to explicitly build the image using buildpacks, then manually push it to the central Artifact Registry, and finally deploy to Cloud Run. Both methods achieve building without a Dockerfile. Pushing source to AR (A) is incorrect. Submitting a standard Cloud Build job (B) typically assumes a Dockerfile or explicit build steps. --platform=kubernetes (E) is for Cloud Run for Anthos.",conditions:["Deploy app to Cloud Run without Dockerfile, use Google Cloud build services, push image to central repo."],caseStudyContext:null},{id:180,topic:"Databases",question:"You work for an organization that manages an online ecommerce website. Your company plans to expand across the world; however, the estore currently serves one specific region. You need to select a SQL database and configure a schema that will scale as your organization grows. You want to create a table that stores all customer transactions and ensure that the customer (CustomerId) and the transaction (TransactionId) are unique. What should you do?",options:{A:"Create a Cloud SQL table that has TransactionId and CustomerId configured as primary keys. Use an incremental number for the TransactionId.",B:"Create a Cloud SQL table that has TransactionId and CustomerId configured as primary keys. Use a random string (UUID) for the Transactionid.",C:"Create a Cloud Spanner table that has TransactionId and CustomerId configured as primary keys. Use a random string (UUID) for the TransactionId.",D:"Create a Cloud Spanner table that has TransactionId and CustomerId configured as primary keys. Use an incremental number for the TransactionId."},correctAnswer:["C"],explanation:"The requirement for global expansion and scaling points towards Cloud Spanner, a globally distributed, horizontally scalable SQL database. Cloud SQL is regional. To ensure uniqueness of (CustomerId, TransactionId) and avoid potential hotspotting issues with globally distributed writes, using a randomly generated UUID for TransactionId as part of the composite primary key is the best practice for Spanner (C). Incremental numbers (A, D) can lead to write hotspots in distributed systems.",conditions:["Ecommerce app expanding globally, need scalable SQL DB for transactions, ensure unique (CustomerId, TransactionId)."],caseStudyContext:null},{id:181,topic:"IAM & Security",question:"You are developing an application that will store and access sensitive unstructured data objects in a Cloud Storage bucket. To comply with regulatory requirements, you need to ensure that all data objects are available for at least 7 years after their initial creation. Objects created more than 3 years ago are accessed very infrequently (less than once a year). You need to configure object storage while ensuring that storage cost is optimized. What should you do? (Choose two.)",options:{A:"Set a retention policy on the bucket with a period of 7 years.",B:"Use IAM Conditions to provide access to objects 7 years after the object creation date.",C:"Enable Object Versioning to prevent objects from being accidentally deleted for 7 years after object creation.",D:"Create an object lifecycle policy on the bucket that moves objects from Standard Storage to Archive Storage after 3 years.",E:"Implement a Cloud Function that checks the age of each object in the bucket and moves the objects older than 3 years to a second bucket with the Archive Storage class. Use Cloud Scheduler to trigger the Cloud Function on a daily schedule."},correctAnswer:["A","D"],explanation:"Requirement 1 (Retain >= 7 years): Bucket Retention Policy (A) is the direct way to enforce this, preventing deletion/modification for the specified duration. Requirement 2 (Cost optimization): Data accessed less than once a year after 3 years fits the Archive Storage class. An Object Lifecycle Management policy (D) can automatically transition objects from Standard to Archive storage based on age (e.g., after 3 years), optimizing costs. Object Versioning (C) adds cost and doesn't enforce retention time. IAM (B) controls access, not retention. Cloud Function (E) is overly complex for standard lifecycle management.",conditions:["Store sensitive GCS objects, retain >= 7 years, optimize cost for objects > 3 years accessed < 1/year."],caseStudyContext:null},{id:182,topic:"Compute",question:"You are developing an application using different microservices that must remain internal to the cluster. You want the ability to configure each microservice with a specific number of replicas. You also want the ability to address a specific microservice from any other microservice in a uniform way, regardless of the number of replicas the microservice scales to. You plan to implement this solution on Google Kubernetes Engine. What should you do?",options:{A:"Deploy each microservice as a Deployment. Expose the Deployment in the cluster using a Service, and use the Service DNS name to address it from other microservices within the cluster.",B:"Deploy each microservice as a Deployment. Expose the Deployment in the cluster using an Ingress, and use the Ingress IP address to address the Deployment from other microservices within the cluster.",C:"Deploy each microservice as a Pod. Expose the Pod in the cluster using a Service, and use the Service DNS name to address the microservice from other microservices within the cluster.",D:"Deploy each microservice as a Pod. Expose the Pod in the cluster using an Ingress, and use the Ingress IP address to address the Pod from other microservices within the cluster."},correctAnswer:["A"],explanation:"Kubernetes Deployments manage ReplicaSets to ensure a desired number of Pod replicas for a microservice. Kubernetes Services provide a stable abstraction (IP address and DNS name) to access the pods managed by a Deployment. For internal-only communication, a Service of type ClusterIP is used. Other microservices connect using the Service's DNS name, and the Service load balances requests across the available Pod replicas (A). Ingress (B, D) is for external access. Using raw Pods (C, D) doesn't provide replica management or scaling.",conditions:["Internal GKE microservices, configure replicas, uniform addressing independent of scaling."],caseStudyContext:null},{id:183,topic:"Monitoring & Logging",question:"You are developing an application that will handle requests from end users. You need to secure a Cloud Function called by the application to allow authorized end users to authenticate to the function via the application while restricting access to unauthorized users. You will integrate Google Sign-In as part of the solution and want to follow Google-recommended best practices. What should you do?",options:{A:"Deploy from a source code repository and grant users the roles/cloudfunctions.viewer role.",B:"Deploy from a source code repository and grant users the roles/cloudfunctions.invoker role",C:"Deploy from your local machine using gcloud and grant users the roles/cloudfunctions.admin role",D:"Deploy from your local machine using gcloud and grant users the roles/cloudfunctions.developer role"},correctAnswer:["B"],explanation:"To secure an HTTP Cloud Function so only authenticated users can call it, you should configure it to require authentication. When using Google Sign-In, the client application obtains an ID token after the user signs in. This ID token should be included in the `Authorization: Bearer <ID_TOKEN>` header when calling the Cloud Function. The function (or an API Gateway in front of it) validates the token. To authorize the *user* to call the function, their identity needs the `roles/cloudfunctions.invoker` permission on the function. Granting this role directly to users or, preferably, to a group they belong to, enforces authorization (B). Viewer, Admin, Developer roles are incorrect for invocation permission.",conditions:["Secure Cloud Function called by app, authenticate end users via Google Sign-In, restrict access."],caseStudyContext:null},{id:184,topic:"IAM & Security",question:"You are running a web application on Google Kubernetes Engine that you inherited. You want to determine whether the application is using libraries with known vulnerabilities or is vulnerable to XSS attacks. Which service should you use?",options:{A:"Google Cloud Armor",B:"Debugger",C:"Web Security Scanner",D:"Error Reporting"},correctAnswer:["C"],explanation:"Web Security Scanner (part of Security Command Center) is designed to scan running web applications (hosted on GKE, App Engine, GCE) for common vulnerabilities like Cross-Site Scripting (XSS) and usage of outdated/vulnerable libraries (C). Cloud Armor (A) is for WAF/DDoS. Debugger (B) is for inspecting code execution. Error Reporting (D) aggregates application errors.",conditions:["Inherited GKE web app, check for vulnerable libraries and XSS."],caseStudyContext:null},{id:185,topic:"Storage",question:"You are building a highly available and globally accessible application that will serve static content to users. You need to configure the storage and serving components. You want to minimize management overhead and latency while maximizing reliability for users. What should you do?",options:{A:`1. Create a managed instance group. Replicate the static content across the virtual machines (VMs)
2. Create an external HTTP(S) load balancer.
3. Enable Cloud CDN, and send traffic to the managed instance group.`,B:`1. Create an unmanaged instance group. Replicate the static content across the VMs.
2. Create an external HTTP(S) load balancer
3. Enable Cloud CDN, and send traffic to the unmanaged instance group.`,C:`1. Create a Standard storage class, regional Cloud Storage bucket. Put the static content in the bucket
2. Reserve an external IP address, and create an external HTTP(S) load balancer
3. Enable Cloud CDN, and send traffic to your backend bucket`,D:`1. Create a Standard storage class, multi-regional Cloud Storage bucket. Put the static content in the bucket.
2. Reserve an external IP address, and create an external HTTP(S) load balancer.
3. Enable Cloud CDN, and send traffic to your backend bucket.`},correctAnswer:["D"],explanation:"For globally serving static content with high availability, low latency, and minimal management: 1. Store content in a multi-regional Cloud Storage bucket (D) for high durability and availability across regions. 2. Use an external HTTP(S) Load Balancer with Cloud CDN enabled. Configure the load balancer to use the GCS bucket as a backend. Cloud CDN caches content at edge locations globally, providing low latency. This setup is fully managed and highly reliable. Using GCE instances (A, B) requires managing VMs and replicating content, increasing overhead. A regional bucket (C) doesn't offer the same global availability as multi-regional.",conditions:["Serve static content globally, highly available, low latency, minimize management overhead."],caseStudyContext:null},{id:186,topic:"Case Study Analysis",question:"Case study - HipLocal: HipLocal wants to reduce the latency of their services for users in global locations. They have created read replicas of their database in locations where their users reside and configured their service to read traffic using those replicas. How should they further reduce latency for all database interactions with the least amount of effort?",options:{A:"Migrate the database to Bigtable and use it to serve all global user traffic.",B:"Migrate the database to Cloud Spanner and use it to serve all global user traffic.",C:"Migrate the database to Firestore in Datastore mode and use it to serve all global user traffic.",D:"Migrate the services to Google Kubernetes Engine and use a load balancer service to better scale the application."},correctAnswer:["B"],explanation:"HipLocal uses MySQL and needs global scale with low latency and consistency (implied by 'consistent experience' requirement). Read replicas help read latency but not write latency or global consistency. Migrating to Cloud Spanner (B) provides a globally distributed, strongly consistent database with low latency reads (via follower replicas) and manageable write latency globally. Bigtable (A) and Firestore (C) are NoSQL and represent a larger migration effort from MySQL. Migrating services to GKE (D) doesn't solve the database latency/distribution problem.",conditions:["HipLocal case study context, reduce global DB latency (already using read replicas), least effort."],caseStudyContext:`<strong>Company overview</strong>
HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.

<strong>Executive statement</strong>
We are the number one local community app; its time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.

<strong><strong>Solution Concept </strong><br></strong>
HipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.

<strong><strong>Existing Technical Environment </strong><br></strong>
HipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:
<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>

<strong><strong>Business Requirements </strong><br></strong>
HipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:
<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>

<strong>Technical requirements</strong>
<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>`},{id:187,topic:"Case Study Analysis",question:"Case study - HipLocal: Which Google Cloud product addresses HipLocals <strong>Business Requirements </strong><br> for service level indicators and objectives?",options:{A:"Cloud Profiler",B:"Cloud Monitoring",C:"Cloud Trace",D:"Cloud Logging"},correctAnswer:["B"],explanation:"The business requirement is to 'Define service level indicators (SLIs) and service level objectives (SLOs).' Cloud Monitoring (B) provides the functionality to define SLIs based on metrics (e.g., availability, latency), set SLOs (target levels for SLIs), track error budgets, and create alerts based on SLO compliance. Profiler (A), Trace (C), and Logging (D) are for performance analysis, latency tracing, and log management, respectively, not directly for defining and tracking SLOs.",conditions:["HipLocal case study context, need product to define/track SLIs/SLOs."],caseStudyContext:`<strong>Company overview</strong>
HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.

<strong>Executive statement</strong>
We are the number one local community app; its time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.

<strong><strong>Solution Concept </strong><br></strong>
HipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.

<strong><strong>Existing Technical Environment </strong><br></strong>
HipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:
<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>

<strong><strong>Business Requirements </strong><br></strong>
HipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:
<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>

<strong>Technical requirements</strong>
<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>`},{id:188,topic:"Case Study Analysis",question:"Case study - HipLocal: A recent security audit discovers that HipLocals database credentials for their Compute Engine-hosted MySQL databases are stored in plain text on persistent disks. HipLocal needs to reduce the risk of these credentials being stolen. What should they do?",options:{A:"Create a service account and download its key. Use the key to authenticate to Cloud Key Management Service (KMS) to obtain the database credentials.",B:"Create a service account and download its key. Use the key to authenticate to Cloud Key Management Service (KMS) to obtain a key used to decrypt the database credentials.",C:"Create a service account and grant it the roles/iam.serviceAccountUser role. Impersonate as this account and authenticate using the Cloud SQL Proxy.",D:"Grant the roles/secretmanager.secretAccessor role to the Compute Engine service account. Store and access the database credentials with the Secret Manager API."},correctAnswer:["D"],explanation:"Storing credentials in plain text is insecure. Secret Manager is Google Cloud's managed service for securely storing and accessing secrets like database credentials. The application running on GCE should use its attached service account identity to authenticate to the Secret Manager API. Grant the GCE service account the `roles/secretmanager.secretAccessor` role to allow it to retrieve the stored database credentials at runtime (D). Using KMS (A, B) is for key management, not directly storing arbitrary secrets like passwords. Option C relates to impersonation, not secret retrieval, and assumes Cloud SQL Proxy is used (the scenario mentions MySQL on GCE).",conditions:["HipLocal case study context, MySQL DB creds stored insecurely on GCE PD, need secure solution."],caseStudyContext:`<strong>Company overview</strong>
HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.

<strong>Executive statement</strong>
We are the number one local community app; its time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.

<strong><strong>Solution Concept </strong><br></strong>
HipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.

<strong><strong>Existing Technical Environment </strong><br></strong>
HipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:
<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>

<strong><strong>Business Requirements </strong><br></strong>
HipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:
<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>

<strong>Technical requirements</strong>
<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>`},{id:189,topic:"Case Study Analysis",question:"Case study - HipLocal: HipLocal is expanding into new locations. They must capture additional data each time the application is launched in a new European country. This is causing delays in the development process due to constant schema changes and a lack of environments for conducting testing on the application changes. How should they resolve the issue while meeting the <strong>Business Requirements </strong><br>?",options:{A:"Create new Cloud SQL instances in Europe and North America for testing and deployment. Provide developers with local MySQL instances to conduct testing on the application changes.",B:"Migrate data to Bigtable. Instruct the development teams to use the Cloud SDK to emulate a local Bigtable development environment.",C:"Move from Cloud SQL to MySQL hosted on Compute Engine. Replicate hosts across regions in the Americas and Europe. Provide developers with local MySQL instances to conduct testing on the application changes.",D:"Migrate data to Firestore in Native mode and set up instances in Europe and North America. Instruct the development teams to use the Cloud SDK to emulate a local Firestore in Native mode development environment."},correctAnswer:["D"],explanation:"The core issues are development delays due to 'constant schema changes' and lack of testing environments. This points towards needing a database with a flexible schema and good local development/testing support. Firestore (D) is a NoSQL document database with a flexible schema, accommodating evolving data requirements easily. It supports regional instances (Europe/NA) for data locality (GDPR requirement). Crucially, Firestore has excellent local emulator support via the SDK, allowing developers to test locally without needing dedicated cloud instances, addressing the testing environment bottleneck. Cloud SQL (A, C) has a rigid schema. Bigtable (B) is less suited for general application data and local emulation might be more complex.",conditions:["HipLocal case study context, need to capture varying data per country (schema changes), dev delays due to lack of test environments."],caseStudyContext:`<strong>Company overview</strong>
HipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.

<strong>Executive statement</strong>
We are the number one local community app; its time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.

<strong><strong>Solution Concept </strong><br></strong>
HipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.

<strong><strong>Existing Technical Environment </strong><br></strong>
HipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:
<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>

<strong><strong>Business Requirements </strong><br></strong>
HipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:
<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>

<strong>Technical requirements</strong>
<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>`},{id:190,topic:"Databases",question:"You are writing from a Go application to a Cloud Spanner database. You want to optimize your applications performance using Google-recommended best practices. What should you do?",options:{A:"Write to Cloud Spanner using Cloud Client Libraries.",B:"Write to Cloud Spanner using Google API Client Libraries",C:"Write to Cloud Spanner using a custom gRPC client library.",D:"Write to Cloud Spanner using a third-party HTTP client library."},correctAnswer:["A"],explanation:"Google Cloud Client Libraries (A) are the idiomatic and recommended way to interact with Google Cloud services, including Spanner, from applications. They provide high-level abstractions, handle authentication (using ADC), retries, and other complexities, simplifying development and often incorporating performance best practices (like session pooling for Spanner). Google API Client Libraries (B) are lower-level, auto-generated libraries. Building custom gRPC (C) or HTTP (D) clients is unnecessary and complex when official client libraries exist.",conditions:["Write to Spanner from Go app, optimize performance using best practices."],caseStudyContext:null},{id:191,topic:"IAM & Security",question:"You have an application deployed in Google Kubernetes Engine (GKE). You need to update the application to make authorized requests to Google Cloud managed services. You want this to be a one-time setup, and you need to follow security best practices of auto-rotating your security keys and storing them in an encrypted store. You already created a service account with appropriate access to the Google Cloud service. What should you do next?",options:{A:"Assign the Google Cloud service account to your GKE Pod using Workload Identity.",B:"Export the Google Cloud service account, and share it with the Pod as a Kubernetes Secret.",C:"Export the Google Cloud service account, and embed it in the source code of the application.",D:"Export the Google Cloud service account, and upload it to HashiCorp Vault to generate a dynamic service account for your application."},correctAnswer:["A"],explanation:"The requirements emphasize security best practices: auto-rotating keys and encrypted storage. Manually exporting and managing service account keys (B, C, D) violates these principles, as keys need manual rotation and secure storage becomes the application's responsibility. Workload Identity (A) is the recommended solution. It binds a GKE service account (used by the Pod) to a Google service account (GSA). The Pod uses its automatically mounted, auto-rotated KSA token to get short-lived GSA credentials from the metadata server, eliminating the need to handle GSA keys directly.",conditions:["GKE app needs to access GCP services, one-time setup, follow best practices (auto-rotated keys, encrypted store), GSA already created."],caseStudyContext:null},{id:192,topic:"IAM & Security",question:"You are planning to deploy hundreds of microservices in your Google Kubernetes Engine (GKE) cluster. How should you secure communication between the microservices on GKE using a managed service?",options:{A:"Use global HTTP(S) Load Balancing with managed SSL certificates to protect your services",B:"Deploy open source Istio in your GKE cluster, and enable mTLS in your Service Mesh",C:"Install cert-manager on GKE to automatically renew the SSL certificates.",D:"Install Anthos Service Mesh, and enable mTLS in your Service Mesh."},correctAnswer:["D"],explanation:"To secure inter-service communication within a GKE cluster using a *managed* service, Anthos Service Mesh (ASM) is the recommended Google Cloud offering. ASM is a managed Istio distribution that provides features like automatic mutual TLS (mTLS) encryption and authentication between services in the mesh (D). Deploying open source Istio (B) requires self-management. Global Load Balancing (A) secures traffic *into* the cluster, not between microservices. Cert-manager (C) manages TLS certificates, often for Ingress, but doesn't inherently provide mTLS for inter-service traffic like a service mesh does.",conditions:["Secure communication between microservices within GKE cluster, use managed service."],caseStudyContext:null},{id:193,topic:"Storage",question:"You are developing an application that will store and access sensitive unstructured data objects in a Cloud Storage bucket. To comply with regulatory requirements, you need to ensure that all data objects are available for at least 7 years after their initial creation. Objects created more than 3 years ago are accessed very infrequently (less than once a year). You need to configure object storage while ensuring that storage cost is optimized. What should you do? (Choose two.)",options:{A:"Set a retention policy on the bucket with a period of 7 years.",B:"Use IAM Conditions to provide access to objects 7 years after the object creation date.",C:"Enable Object Versioning to prevent objects from being accidentally deleted for 7 years after object creation.",D:"Create an object lifecycle policy on the bucket that moves objects from Standard Storage to Archive Storage after 3 years.",E:"Implement a Cloud Function that checks the age of each object in the bucket and moves the objects older than 3 years to a second bucket with the Archive Storage class. Use Cloud Scheduler to trigger the Cloud Function on a daily schedule."},correctAnswer:["A","D"],explanation:"Requirement 1 (Retain >= 7 years): Bucket Retention Policy (A) is the direct way to enforce this, preventing deletion/modification for the specified duration. Requirement 2 (Cost optimization): Data accessed less than once a year after 3 years fits the Archive Storage class. An Object Lifecycle Management policy (D) can automatically transition objects from Standard to Archive storage based on age (e.g., after 3 years), optimizing costs. Object Versioning (C) adds cost and doesn't enforce retention time. IAM (B) controls access, not retention. Cloud Function (E) is overly complex for standard lifecycle management.",conditions:["Store sensitive GCS objects, retain >= 7 years, optimize cost for objects > 3 years accessed < 1/year."],caseStudyContext:null},{id:194,topic:"Compute",question:"You are developing an application using different microservices that must remain internal to the cluster. You want the ability to configure each microservice with a specific number of replicas. You also want the ability to address a specific microservice from any other microservice in a uniform way, regardless of the number of replicas the microservice scales to. You plan to implement this solution on Google Kubernetes Engine. What should you do?",options:{A:"Deploy each microservice as a Deployment. Expose the Deployment in the cluster using a Service, and use the Service DNS name to address it from other microservices within the cluster.",B:"Deploy each microservice as a Deployment. Expose the Deployment in the cluster using an Ingress, and use the Ingress IP address to address the Deployment from other microservices within the cluster.",C:"Deploy each microservice as a Pod. Expose the Pod in the cluster using a Service, and use the Service DNS name to address the microservice from other microservices within the cluster.",D:"Deploy each microservice as a Pod. Expose the Pod in the cluster using an Ingress, and use the Ingress IP address to address the Pod from other microservices within the cluster."},correctAnswer:["A"],explanation:"Kubernetes Deployments manage ReplicaSets to ensure a desired number of Pod replicas for a microservice. Kubernetes Services provide a stable abstraction (IP address and DNS name) to access the pods managed by a Deployment. For internal-only communication, a Service of type ClusterIP is used. Other microservices connect using the Service's DNS name, and the Service load balances requests across the available Pod replicas (A). Ingress (B, D) is for external access. Using raw Pods (C, D) doesn't provide replica management or scaling.",conditions:["Internal GKE microservices, configure replicas, uniform addressing independent of scaling."],caseStudyContext:null},{id:195,topic:"Monitoring & Logging",question:"You are building an application that uses a distributed microservices architecture. You want to measure the performance and system resource utilization in one of the microservices written in Java. What should you do?",options:{A:"Instrument the service with Cloud Profiler to measure CPU utilization and method-level execution times in the service.",B:"Instrument the service with Debugger to investigate service errors.",C:"Instrument the service with Cloud Trace to measure request latency.",D:"Instrument the service with OpenCensus to measure service latency, and write custom metrics to Cloud Monitoring."},correctAnswer:["A"],explanation:"Cloud Profiler is specifically designed to continuously analyze CPU usage and memory allocation of production applications with low overhead, attributing it back to the source code (e.g., Java methods). This directly addresses the need to measure performance (method execution times) and resource utilization (CPU, memory) (A). Debugger (B) is for inspecting state. Cloud Trace (C) measures end-to-end latency across services but not detailed internal resource usage per function. OpenCensus (D) is a library for collecting metrics/traces, but Profiler provides the specific CPU/memory analysis needed.",conditions:["Measure performance (method times) and resource utilization (CPU/memory) of a Java microservice."],caseStudyContext:null},{id:196,topic:"Networking",question:"Your team is responsible for maintaining an application that aggregates news articles from many different sources. Your monitoring dashboard contains publicly accessible real-time reports and runs on a Compute Engine instance as a web application. External stakeholders and analysts need to access these reports via a secure channel without authentication. How should you configure this secure channel?",options:{A:"Add a public IP address to the instance. Use the service account key of the instance to encrypt the traffic.",B:"Use Cloud Scheduler to trigger Cloud Build every hour to create an export from the reports. Store the reports in a public Cloud Storage bucket.",C:"Add an HTTP(S) load balancer in front of the monitoring dashboard. Configure Identity-Aware Proxy to secure the communication channel.",D:"Add an HTTP(S) load balancer in front of the monitoring dashboard. Set up a Google-managed SSL certificate on the load balancer for traffic encryption."},correctAnswer:["D"],explanation:"The requirements are: public access, secure channel (HTTPS), no authentication needed. Placing an external HTTP(S) Load Balancer in front of the GCE instance allows terminating SSL/TLS encryption at the load balancer. Using a Google-managed SSL certificate simplifies certificate provisioning and renewal (D). This provides a secure HTTPS channel without requiring application-level changes or user authentication. Service account keys (A) aren't used for traffic encryption. Batch exports (B) aren't real-time. IAP (C) enforces authentication, which is explicitly not required.",conditions:["Publicly accessible GCE web dashboard, secure channel (HTTPS) needed, no user authentication required."],caseStudyContext:null},{id:197,topic:"Compute",question:"You are planning to add unit tests to your application. You need to be able to assert that published Pub/Sub messages are processed by your subscriber in order. You want the unit tests to be cost-effective and reliable. What should you do?",options:{A:"Implement a mocking framework.",B:"Create a topic and subscription for each tester.",C:"Add a filter by tester to the subscription.",D:"Use the Pub/Sub emulator."},correctAnswer:["D"],explanation:"To reliably test Pub/Sub interactions locally, including message ordering, without incurring costs or depending on the live service, the Pub/Sub emulator (D) is the best practice. It runs locally, mimicking the Pub/Sub API. Mocking frameworks (A) can test the application's interaction logic but not the actual Pub/Sub behavior like ordering guarantees. Creating real topics/subscriptions per tester (B) is costly and less reliable for unit tests. Filters (C) don't apply to this testing scenario.",conditions:["Unit test Pub/Sub subscriber, assert message ordering, cost-effective, reliable."],caseStudyContext:null},{id:198,topic:"Compute",question:`You have an application deployed in Google Kubernetes Engine (GKE) that reads and processes Pub/Sub messages. Each Pod handles a fixed number of messages per minute. The rate at which messages are published to the Pub/Sub topic varies considerably throughout the day and week, including occasional large batches of messages published at a single moment.

You want to scale your GKE Deployment to be able to process messages in a timely manner. What GKE feature should you use to automatically adapt your workload?`,options:{A:"Vertical Pod Autoscaler in Auto mode",B:"Vertical Pod Autoscaler in Recommendation mode",C:"Horizontal Pod Autoscaler based on an external metric",D:"Horizontal Pod Autoscaler based on resources utilization"},correctAnswer:["C"],explanation:"The workload needs to scale based on the number of messages waiting in Pub/Sub, not directly on the Pods' CPU/memory usage (which might be low if there are no messages). This backlog size is an external metric relative to the GKE cluster. The Horizontal Pod Autoscaler (HPA) can be configured to scale based on external metrics, such as the number of unacknowledged messages in a Pub/Sub subscription (C). Scaling based on resource utilization (D) wouldn't react correctly to the backlog size. VPA (A, B) adjusts pod resources, not replica count.",conditions:["GKE app processes Pub/Sub messages, variable message rate, scale Deployment based on backlog."],caseStudyContext:null},{id:199,topic:"Compute",question:"You are using Cloud Run to host a web application. You need to securely obtain the application project ID and region where the application is running and display this information to users. You want to use the most performant approach. What should you do?",options:{A:"Use HTTP requests to query the available metadata server at the http://metadata.google.internal/ endpoint with the Metadata-Flavor: Google header.",B:"In the Google Cloud console, navigate to the Project Dashboard and gather configuration details. Navigate to the Cloud Run Variables & Secrets tab, and add the desired environment variables in Key:Value format.",C:"In the Google Cloud console, navigate to the Project Dashboard and gather configuration details. Write the application configuration information to Cloud Run's in-memory container filesystem.",D:"Make an API call to the Cloud Asset Inventory API from the application and format the request to include instance metadata."},correctAnswer:["A"],explanation:"Cloud Run provides a metadata server accessible from within the container instance at `http://metadata.google.internal/`. Querying this server via HTTP requests (with the `Metadata-Flavor: Google` header) is the standard, secure, and most performant way to retrieve instance/project metadata like project ID and region directly from within the running application (A). Manually configuring environment variables (B) or writing to the filesystem (C) requires external steps and isn't dynamic. Calling external APIs like Cloud Asset Inventory (D) adds unnecessary latency and complexity.",conditions:["Cloud Run app needs to get its own Project ID and Region, secure and performant approach."],caseStudyContext:null},{id:200,topic:"IAM & Security",question:"You need to deploy resources from your laptop to Google Cloud using Terraform. Resources in your Google Cloud environment must be created using a service account. Your Cloud Identity has the roles/iam.serviceAccountTokenCreator Identity and Access Management (IAM) role and the necessary permissions to deploy the resources using Terraform. You want to set up your development environment to deploy the desired resources following Google-recommended best practices. What should you do?",options:{A:`1. Download the service accounts key file in JSON format, and store it locally on your laptop.
2. Set the GOOGLE_APPLICATION_CREDENTIALS environment variable to the path of your downloaded key file.`,B:`1. Run the following command from a command line: gcloud config set auth/impersonate_service_account service-account-name@project.iam.gserviceacccount.com.
2. Set the GOOGLE_OAUTH_ACCESS_TOKEN environment variable to the value that is returned by the gcloud auth print-access-token command.`,C:`1. Run the following command from a command line: gcloud auth application-default login.
2. In the browser window that opens, authenticate using your personal credentials.`,D:`1. Store the service account's key file in JSON format in Hashicorp Vault.
2. Integrate Terraform with Vault to retrieve the key file dynamically, and authenticate to Vault using a short-lived access token.`},correctAnswer:["B"],explanation:"The best practice is to avoid downloading and storing service account keys locally (eliminates A, D). Since your user identity has permission to impersonate the target service account (`roles/iam.serviceAccountTokenCreator`), you can use impersonation. The `gcloud config set auth/impersonate_service_account` command configures gcloud itself for impersonation, but Terraform needs credentials. A common pattern is to use `gcloud auth print-access-token --impersonate-service-account=<SA_EMAIL>` to get a short-lived access token for the service account and set this token in the `GOOGLE_OAUTH_ACCESS_TOKEN` environment variable for Terraform to use (closest match is B, although step 1 isn't directly needed for Terraform). Option C uses your user credentials, not the required service account.",conditions:["Deploy GCP resources via Terraform from laptop, must use specific SA, user has impersonation rights, follow best practices."],caseStudyContext:null},{id:201,topic:"Monitoring & Logging",question:"Your company uses Cloud Logging to manage large volumes of log data. You need to build a real-time log analysis architecture that pushes logs to a third-party application for processing. What should you do?",options:{A:"Create a Cloud Logging log export to Pub/Sub.",B:"Create a Cloud Logging log export to BigQuery.",C:"Create a Cloud Logging log export to Cloud Storage.",D:"Create a Cloud Function to read Cloud Logging log entries and send them to the third-party application."},correctAnswer:["A"],explanation:"To push logs in real-time from Cloud Logging to an external system (like a third-party application), the standard and recommended method is to create a log sink (export) configured with Pub/Sub as the destination (A). Cloud Logging forwards matching log entries to the specified Pub/Sub topic almost instantly. The third-party application can then subscribe to this topic to receive the logs. Exporting to BigQuery (B) or Cloud Storage (C) is for batch analysis or archival, not real-time pushing. A Cloud Function (D) polling logs adds latency and complexity compared to the direct Pub/Sub sink.",conditions:["Real-time push of Cloud Logging logs to a third-party application."],caseStudyContext:null},{id:202,topic:"Storage",question:"You are developing a new public-facing application that needs to retrieve specific properties in the metadata of users objects in their respective Cloud Storage buckets. Due to privacy and data residency requirements, you must retrieve only the metadata and not the object data. You want to maximize the performance of the retrieval process. How should you retrieve the metadata?",options:{A:"Use the patch method.",B:"Use the compose method.",C:"Use the copy method.",D:"Use the fields request parameter."},correctAnswer:["D"],explanation:"Most Google Cloud REST APIs, including the Cloud Storage JSON API, support partial responses using the `fields` query parameter. To retrieve only specific metadata properties (and not the object content) for performance and privacy, you should use the `objects.get` method and specify the desired metadata fields in the `fields` parameter (e.g., `fields=name,contentType,timeCreated`). This minimizes the data transferred (D). Patch (A) is for partial updates. Compose (B) combines objects. Copy (C) copies objects.",conditions:["Retrieve specific metadata properties (not data) of GCS objects for users, maximize performance, respect privacy."],caseStudyContext:null},{id:203,topic:"Compute",question:`You are deploying a microservices application to Google Kubernetes Engine (GKE) that will broadcast livestreams. You expect unpredictable traffic patterns and large variations in the number of concurrent users. Your application must meet the following requirements:

 Scales automatically during popular events and maintains high availability
 Is resilient in the event of hardware failures

How should you configure the deployment parameters? (Choose two.)`,options:{A:"Distribute your workload evenly using a multi-zonal node pool.",B:"Distribute your workload evenly using multiple zonal node pools.",C:"Use cluster autoscaler to resize the number of nodes in the node pool, and use a Horizontal Pod Autoscaler to scale the workload.",D:"Create a managed instance group for Compute Engine with the cluster nodes. Configure autoscaling rules for the managed instance group.",E:"Create alerting policies in Cloud Monitoring based on GKE CPU and memory utilization. Ask an on-duty engineer to scale the workload by executing a script when CPU and memory usage exceed predefined thresholds."},correctAnswer:["A","C"],explanation:"Requirement 1 (Resilience): Use a multi-zonal node pool (A) to spread nodes across multiple zones within a region, ensuring workload survival during a zone failure. Multiple single-zone pools (B) don't offer the same automatic balancing. Requirement 2 (Scaling): Use the GKE cluster autoscaler to automatically add/remove nodes based on Pod scheduling needs, and use the Horizontal Pod Autoscaler (HPA) to automatically adjust the number of application Pod replicas based on metrics like CPU or custom metrics (C). GKE nodes *are* MIGs, but you configure scaling via cluster autoscaler/HPA, not directly on the MIG (D). Manual scaling (E) is explicitly against the requirements.",conditions:["Deploy GKE microservices (livestreaming), unpredictable traffic, need auto-scaling, high availability, resilience to hardware failure."],caseStudyContext:null},{id:204,topic:"Storage",question:`You work for a rapidly growing financial technology startup. You manage the payment processing application written in Go and hosted on Cloud Run in the Singapore region (asia-southeast1). The payment processing application processes data stored in a Cloud Storage bucket that is also located in the Singapore region.

The startup plans to expand further into the Asia Pacific region. You plan to deploy the Payment Gateway in Jakarta, Hong Kong, and Taiwan over the next six months. Each location has data residency requirements that require customer data to reside in the country where the transaction was made. You want to minimize the cost of these deployments. What should you do?`,options:{A:"Create a Cloud Storage bucket in each region, and create a Cloud Run service of the payment processing application in each region.",B:"Create a Cloud Storage bucket in each region, and create three Cloud Run services of the payment processing application in the Singapore region.",C:"Create three Cloud Storage buckets in the Asia multi-region, and create three Cloud Run services of the payment processing application in the Singapore region.",D:"Create three Cloud Storage buckets in the Asia multi-region, and create three Cloud Run revisions of the payment processing application in the Singapore region."},correctAnswer:["A"],explanation:"Data residency requirements mandate storing data within specific countries/regions (Jakarta, HK, Taiwan). This requires separate Cloud Storage buckets located in the respective regions (or appropriate multi-regions covering *only* those locations, but regional is more precise). To minimize latency and potential data egress costs, the Cloud Run service processing the data should also be deployed in the same region as the data it processes. Therefore, deploying both a bucket and a Cloud Run service in *each* required region (A) is the correct approach to meet residency, performance, and cost requirements. Processing data in Singapore from other regions (B, C, D) violates residency and incurs latency/egress costs.",conditions:["Cloud Run app processes data from GCS bucket (both currently Singapore). Expand to Jakarta, HK, Taiwan. Strict data residency per location. Minimize cost."],caseStudyContext:null},{id:205,topic:"Databases",question:"You recently joined a new team that has a Cloud Spanner database instance running in production. Your manager has asked you to optimize the Spanner instance to reduce cost while maintaining high reliability and availability of the database. What should you do?",options:{A:"Use Cloud Logging to check for error logs, and reduce Spanner processing units by small increments until you find the minimum capacity required.",B:"Use Cloud Trace to monitor the requests per sec of incoming requests to Spanner, and reduce Spanner processing units by small increments until you find the minimum capacity required.",C:"Use Cloud Monitoring to monitor the CPU utilization, and reduce Spanner processing units by small increments until you find the minimum capacity required.",D:"Use Snapshot Debugger to check for application errors, and reduce Spanner processing units by small increments until you find the minimum capacity required."},correctAnswer:["C"],explanation:"Cloud Spanner cost is primarily driven by the amount of compute capacity (nodes or processing units) provisioned and storage used. To optimize cost by reducing compute capacity safely, you need to monitor resource utilization. Cloud Monitoring provides key Spanner metrics, including CPU utilization. The recommended practice is to monitor CPU utilization (specifically high-priority CPU) and ensure it stays below recommended thresholds (e.g., 65% for regional, 45% for multi-region) to maintain performance and availability. By monitoring this metric (C), you can incrementally reduce processing units while ensuring utilization stays within safe limits. Error logs (A), request rates (B), or Debugger (D) don't directly indicate if compute capacity can be safely reduced.",conditions:["Optimize existing Cloud Spanner instance cost, maintain reliability/availability."],caseStudyContext:null},{id:206,topic:"Monitoring & Logging",question:"You recently deployed a Go application on Google Kubernetes Engine (GKE). The operations team has noticed that the application's CPU usage is high even when there is low production traffic. The operations team has asked you to optimize your application's CPU resource consumption. You want to determine which Go functions consume the largest amount of CPU. What should you do?",options:{A:"Deploy a Fluent Bit daemonset on the GKE cluster to log data in Cloud Logging. Analyze the logs to get insights into your application codes performance.",B:"Create a custom dashboard in Cloud Monitoring to evaluate the CPU performance metrics of your application.",C:"Connect to your GKE nodes using SSH. Run the top command on the shell to extract the CPU utilization of your application.",D:"Modify your Go application to capture profiling data. Analyze the CPU metrics of your application in flame graphs in Profiler."},correctAnswer:["D"],explanation:"To identify which specific *functions* within the Go application are consuming the most CPU, you need code-level profiling. Cloud Profiler collects CPU and memory profiles from running applications (including Go apps on GKE) with low overhead. It requires instrumenting the application (D) to include the Profiler agent. The collected data can then be visualized as flame graphs in the Cloud Console, clearly showing CPU time spent in different functions (D). Logging (A), high-level monitoring metrics (B), or node-level `top` (C) do not provide the necessary function-level granularity.",conditions:["Go app on GKE has high CPU usage at low traffic, need to find CPU-intensive functions."],caseStudyContext:null},{id:207,topic:"IAM & Security",question:"Your team manages a Google Kubernetes Engine (GKE) cluster where an application is running. A different team is planning to integrate with this application. Before they start the integration, you need to ensure that the other team cannot make changes to your application, but they can deploy the integration on GKE. What should you do?",options:{A:"Using Identity and Access Management (IAM), grant the Viewer IAM role on the cluster project to the other team.",B:"Create a new GKE cluster. Using Identity and Access Management (IAM), grant the Editor role on the cluster project to the other team.",C:"Create a new namespace in the existing cluster. Using Identity and Access Management (IAM), grant the Editor role on the cluster project to the other team.",D:"Create a new namespace in the existing cluster. Using Kubernetes role-based access control (RBAC), grant the Admin role on the new namespace to the other team."},correctAnswer:["D"],explanation:"To allow another team to deploy their *own* applications (the integration) into your GKE cluster without letting them modify *your* existing application, use Kubernetes namespaces and RBAC. Create a new namespace specifically for the other team. Then, create a Kubernetes Role (or ClusterRole bound to the namespace via RoleBinding) that grants permissions needed for deployment (like creating Deployments, Services, etc.) *only within that new namespace*. Grant this Role to the other team's users or service accounts (D). Project-level IAM roles (A, B, C) are too broad and grant permissions across the entire cluster or project, not namespace-specific deployment rights.",conditions:["Allow another team to deploy their integration app to your GKE cluster, prevent them from modifying your existing app."],caseStudyContext:null},{id:208,topic:"Monitoring & Logging",question:"You have recently instrumented a new application with OpenTelemetry, and you want to check the latency of your application requests in Trace. You want to ensure that a specific request is always traced. What should you do?",options:{A:"Wait 10 minutes, then verify that Trace captures those types of requests automatically.",B:"Write a custom script that sends this type of request repeatedly from your dev project.",C:"Use the Trace API to apply custom attributes to the trace.",D:"Add the X-Cloud-Trace-Context header to the request with the appropriate parameters."},correctAnswer:["D"],explanation:"Cloud Trace uses sampling to decide which requests to trace to manage volume and cost. To *force* a specific request to be traced, regardless of the sampling decision, you need to add the `X-Cloud-Trace-Context` HTTP header to the incoming request. Setting the trace options flag within this header to `1` (trace=true) signals to the tracing libraries (like OpenTelemetry configured with a Cloud Trace exporter) and downstream services that this request must be traced (D). Waiting (A), sending repeated requests (B), or adding attributes (C) doesn't override the sampling mechanism.",conditions:["Application instrumented with OpenTelemetry sending data to Cloud Trace, need to force tracing for a specific request."],caseStudyContext:null},{id:209,topic:"Compute",question:`You are trying to connect to your Google Kubernetes Engine (GKE) cluster using kubectl from Cloud Shell. You have deployed your GKE cluster with a public endpoint. From Cloud Shell, you run the following command:

\`gcloud container clusters get-credentials CLUSTER_NAME --region=REGION\`

You notice that the kubectl commands time out without returning an error message. What is the most likely cause of this issue?`,options:{A:"Your user account does not have privileges to interact with the cluster using kubectl.",B:"Your Cloud Shell external IP address is not part of the authorized networks of the cluster.",C:"The Cloud Shell is not part of the same VPC as the GKE cluster.",D:"A VPC firewall is blocking access to the clusters endpoint."},correctAnswer:["B"],explanation:"GKE clusters with public endpoints can optionally restrict access to the control plane API server using 'authorized networks'. If this feature is enabled, only connections from specified IP CIDR blocks are allowed. Cloud Shell instances have ephemeral external IP addresses. If authorized networks are enabled on the GKE cluster and the Cloud Shell IP is not included in the allowed list, attempts to connect via `kubectl` (which talks to the API server endpoint) will time out (B). Lack of privileges (A) usually results in a 401/403 error, not a timeout. Cloud Shell runs outside your VPC (C). VPC firewalls (D) don't typically block outbound access to GKE public endpoints.",conditions:["Connect to GKE public endpoint via kubectl from Cloud Shell, commands time out."],caseStudyContext:null},{id:210,topic:"Storage",question:"You are developing a web application that contains private images and videos stored in a Cloud Storage bucket. Your users are anonymous and do not have Google Accounts. You want to use your application-specific logic to control access to the images and videos. How should you configure access?",options:{A:"Cache each web application user's IP address to create a named IP table using Google Cloud Armor. Create a Google Cloud Armor security policy that allows users to access the backend bucket.",B:"Grant the Storage Object Viewer IAM role to allUsers. Allow users to access the bucket after authenticating through your web application.",C:"Configure Identity-Aware Proxy (IAP) to authenticate users into the web application. Allow users to access the bucket after authenticating through IAP.",D:"Generate a signed URL that grants read access to the bucket. Allow users to access the URL after authenticating through your web application."},correctAnswer:["D"],explanation:"The bucket contains private objects, and access needs to be controlled by application logic for anonymous users. Signed URLs (D) are the standard solution. The web application, after performing its own authentication/authorization checks, generates a short-lived Signed URL granting temporary read access to a specific object. This URL is given to the client (browser), allowing direct download from GCS without the user needing Google credentials or the object being public. Granting `allUsers` (B) makes objects public. IAP (C) requires Google accounts. Cloud Armor (A) controls access at the network edge, not fine-grained object access based on application logic.",conditions:["Serve private GCS objects (images/videos) to anonymous users based on application logic."],caseStudyContext:null},{id:211,topic:"Compute",question:"You have deployed an application in Google Kubernetes Engine (GKE). You need to update the application to make authorized requests to Google Cloud managed services. You want this to be a one-time setup, and you need to follow security best practices of auto-rotating your security keys and storing them in an encrypted store. You already created a service account with appropriate access to the Google Cloud service. What should you do next?",options:{A:"Assign the Google Cloud service account to your GKE Pod using Workload Identity.",B:"Export the Google Cloud service account, and share it with the Pod as a Kubernetes Secret.",C:"Export the Google Cloud service account, and embed it in the source code of the application.",D:"Export the Google Cloud service account, and upload it to HashiCorp Vault to generate a dynamic service account for your application."},correctAnswer:["A"],explanation:"This is identical to Q191. Workload Identity (A) is the best practice. It avoids exporting/managing GSA keys (B, C, D) by binding a KSA (used by the Pod) to the pre-created GSA. The Pod then uses its automatically managed KSA token to obtain short-lived GSA credentials, adhering to auto-rotation and secure storage principles.",conditions:["GKE app needs to access GCP services, one-time setup, follow best practices (auto-rotated keys, encrypted store), GSA already created."],caseStudyContext:null},{id:212,topic:"Compute",question:"You are a developer at a large organization. You are deploying a web application to Google Kubernetes Engine (GKE). The DevOps team has built a CI/CD pipeline that uses Cloud Deploy to deploy the application to Dev, Test, and Prod clusters in GKE. After Cloud Deploy successfully deploys the application to the Dev cluster, you want to automatically promote it to the Test cluster. How should you configure this process following Google-recommended best practices?",options:{A:`1. Create a Cloud Build trigger that listens for SUCCEEDED Pub/Sub messages from the clouddeploy-operations topic.
2. Configure Cloud Build to include a step that promotes the application to the Test cluster.`,B:`1. Create a Cloud Function that calls the Google Cloud Deploy API to promote the application to the Test cluster.
2. Configure this function to be triggered by SUCCEEDED Pub/Sub messages from the cloud-builds topic.`,C:`1. Create a Cloud Function that calls the Google Cloud Deploy API to promote the application to the Test cluster.
2. Configure this function to be triggered by SUCCEEDED Pub/Sub messages from the clouddeploy-operations topic.`,D:`1. Create a Cloud Build pipeline that uses the gke-deploy builder.
2. Create a Cloud Build trigger that listens for SUCCEEDED Pub/Sub messages from the cloud-builds topic.
3. Configure this pipeline to run a deployment step to the Test cluster.`},correctAnswer:["C"],explanation:"Cloud Deploy publishes notifications about its operations (like successful rollouts) to a specific Pub/Sub topic (`clouddeploy-operations`). To automate promotion upon successful Dev deployment, listen for the `SUCCEEDED` event for the Dev target on this topic. A Cloud Function is a suitable lightweight, event-driven mechanism to react to this Pub/Sub message (C). The function's logic would call the Cloud Deploy API (or `gcloud deploy releases promote`) to promote the successful release to the Test target. Using Cloud Build (A) works but is potentially heavier than a simple function for this task. Listening to the wrong topic (`cloud-builds`) (B, D) is incorrect.",conditions:["Using Cloud Deploy for GKE deployments (Dev, Test, Prod). Automate promotion Dev->Test after successful Dev deploy."],caseStudyContext:null},{id:213,topic:"IAM & Security",question:"Your application is running as a container in a Google Kubernetes Engine cluster. You need to add a secret to your application using a secure approach. What should you do?",options:{A:"Create a Kubernetes Secret, and pass the Secret as an environment variable to the container.",B:"Enable Application-layer Secret Encryption on the cluster using a Cloud Key Management Service (KMS) key.",C:"Store the credential in Cloud KMS. Create a Google service account (GSA) to read the credential from Cloud KMS. Export the GSA as a .json file, and pass the .json file to the container as a volume which can read the credential from Cloud KMS.",D:"Store the credential in Secret Manager. Create a Google service account (GSA) to read the credential from Secret Manager. Create a Kubernetes service account (KSA) to run the container. Use Workload Identity to configure your KSA to act as a GSA."},correctAnswer:["D"],explanation:"Storing secrets directly in Kubernetes Secrets (A) is not the most secure method as they are only base64 encoded by default. Application-layer encryption (B) secures K8s secrets at rest in etcd but doesn't change how they are accessed. Using KMS directly (C) is for key management, not arbitrary secrets, and involves insecure key handling. The most secure Google-recommended approach is to store the secret in Secret Manager, grant a GSA access, and use Workload Identity to allow the Pod (via its KSA) to authenticate as the GSA and retrieve the secret directly from the Secret Manager API at runtime (D).",conditions:["Add secret to GKE application securely."],caseStudyContext:null},{id:214,topic:"IAM & Security",question:"You are a developer at a financial institution. You use Cloud Shell to interact with Google Cloud services. User data is currently stored on an ephemeral disk; however, a recently passed regulation mandates that you can no longer store sensitive information on an ephemeral disk. You need to implement a new storage solution for your user data. You want to minimize code changes. Where should you store your user data?",options:{A:"Store user data on a Cloud Shell home disk, and log in at least every 120 days to prevent its deletion.",B:"Store user data on a persistent disk in a Compute Engine instance.",C:"Store user data in a Cloud Storage bucket.",D:"Store user data in BigQuery tables."},correctAnswer:["B"],explanation:"The constraint is minimizing code changes, implying the current interaction might be file-based on an ephemeral disk (perhaps associated with a GCE instance accessed via Cloud Shell, or Cloud Shell's own temporary disk beyond $HOME). A GCE Persistent Disk (B) offers durable block storage mountable as a standard filesystem, providing the most direct replacement for file-based operations with minimal code changes. Cloud Shell home disk (A) is persistent but small and tied to Cloud Shell activity. Cloud Storage (C) is object storage (API changes needed). BigQuery (D) is a data warehouse (schema/query changes needed).",conditions:["Interact via Cloud Shell, currently store user data on ephemeral disk, must move to persistent storage, minimize code changes."],caseStudyContext:null},{id:215,topic:"Compute",question:"You are a lead developer working on a new retail system that runs on Cloud Run and Firestore in Datastore mode. A web UI requirement is for the system to display a list of available products when users access the system and for the user to be able to browse through all products. You have implemented this requirement in the minimum viable product (MVP) phase by returning a list of all available products stored in Firestore.",options:{A:"Create a user-managed service account with a custom Identity and Access Management (IAM) role.",B:"Create a user-managed service account with the Storage Admin Identity and Access Management (IAM) role.",C:"Create a user-managed service account with the Project Editor Identity and Access Management (IAM) role.",D:"Use the default service account linked to the Cloud Run revision in production."},correctAnswer:["A"],explanation:"To follow the principle of least privilege when granting Cloud Run access to other GCP services (like Cloud Storage in this implied context, although Firestore is mentioned earlier), you should create a dedicated, user-managed service account. Assign a custom IAM role to this service account that contains *only* the permissions required by the application (e.g., `storage.objects.get` if reading from GCS) (A). Using broad roles like Storage Admin (B) or Project Editor (C), or the default service account (D - which is often Editor by default), grants excessive permissions.",conditions:["Cloud Run application needs access to other GCP resources (e.g., Cloud Storage), follow least privilege."],caseStudyContext:null},{id:216,topic:"Compute",question:"Your team is developing unit tests for Cloud Function code. The code is stored in a Cloud Source Repositories repository. You are responsible for implementing the tests. Only a specific service account has the necessary permissions to deploy the code to Cloud Functions. You want to ensure that the code cannot be deployed without first passing the tests. How should you configure the unit testing process?",options:{A:"Configure Cloud Build to deploy the Cloud Function. If the code passes the tests, a deployment approval is sent to you.",B:"Configure Cloud Build to deploy the Cloud Function, using the specific service account as the build agent. Run the unit tests after successful deployment.",C:"Configure Cloud Build to run the unit tests. If the code passes the tests, the developer deploys the Cloud Function.",D:"Configure Cloud Build to run the unit tests, using the specific service account as the build agent. If the code passes the tests, Cloud Build deploys the Cloud Function."},correctAnswer:["D"],explanation:"The goal is an automated pipeline where deployment happens only after tests pass, using a specific service account for deployment. Cloud Build can orchestrate this. The pipeline should first run the unit tests. If the tests succeed, the next step should deploy the Cloud Function. Since a specific service account is required for deployment, Cloud Build needs to execute the deployment step using (or acting as) that service account (D). Running tests *after* deployment (B) defeats the purpose of gating. Manual deployment (C) or manual approval (A) doesn't meet the automation goal.",conditions:["Unit test Cloud Function code, deploy only if tests pass, deployment requires specific SA, automate."],caseStudyContext:null},{id:217,topic:"Monitoring & Logging",question:"Your team detected a spike of errors in an application running on Cloud Run in your production project. The application is configured to read messages from Pub/Sub topic A, process the messages, and write the messages to topic B. You want to conduct tests to identify the cause of the errors. You can use a set of mock messages for testing. What should you do?",options:{A:"Deploy the Pub/Sub and Cloud Run emulators on your local machine. Deploy the application locally, and change the logging level in the application to DEBUG or INFO. Write mock messages to topic A, and then analyze the logs.",B:"Use the gcloud CLI to write mock messages to topic A. Change the logging level in the application to DEBUG or INFO, and then analyze the logs.",C:"Deploy the Pub/Sub emulator on your local machine. Point the production application to your local Pub/Sub topics. Write mock messages to topic A, and then analyze the logs.",D:"Use the Google Cloud console to write mock messages to topic A. Change the logging level in the application to DEBUG or INFO, and then analyze the logs."},correctAnswer:["A"],explanation:"To safely diagnose production errors without impacting the live system, local testing with emulators is the best practice. Deploying the Pub/Sub emulator allows you to simulate topic A locally. Running the Cloud Run application container locally (e.g., via Docker) allows you to connect it to the local emulator. Increasing the logging level provides detailed diagnostics. Publishing mock messages to the local emulator topic A lets you observe the application's processing and identify the error's cause by analyzing the detailed local logs (A). Injecting mock messages into production (B, D) or pointing production to local emulators (C) is risky and inappropriate.",conditions:["Cloud Run app processing Pub/Sub messages (A->B) has errors in prod, need to test/debug with mock messages."],caseStudyContext:null},{id:218,topic:"IAM & Security",question:"You are developing a Java Web Server that needs to interact with Google Cloud services via the Google Cloud API on the user's behalf. Users should be able to authenticate to the Google Cloud API using their Google Cloud identities. Which workflow should you implement in your web application?",options:{A:`1. When a user arrives at your application, prompt them for their Google username and password.
2. Store an SHA password hash in your application's database along with the user's username.
3. The application authenticates to the Google Cloud API using HTTPs requests with the user's username and password hash in the Authorization request header.`,B:`1. When a user arrives at your application, prompt them for their Google username and password.
2. Forward the user's username and password in an HTTPS request to the Google Cloud authorization server, and request an access token.
3. The Google server validates the user's credentials and returns an access token to the application.
4. The application uses the access token to call the Google Cloud API.`,C:`1. When a user arrives at your application, route them to a Google Cloud consent screen with a list of requested permissions that prompts the user to sign in with SSO to their Google Account.
2. After the user signs in and provides consent, your application receives an authorization code from a Google server.
3. The Google server returns the authorization code to the user, which is stored in the browser's cookies.
4. The user authenticates to the Google Cloud API using the authorization code in the cookie.`,D:`1. When a user arrives at your application, route them to a Google Cloud consent screen with a list of requested permissions that prompts the user to sign in with SSO to their Google Account.
2. After the user signs in and provides consent, your application receives an authorization code from a Google server.
3. The application requests a Google Server to exchange the authorization code with an access token.
4. The Google server responds with the access token that is used by the application to call the Google Cloud API.`},correctAnswer:["D"],explanation:"This describes the standard OAuth 2.0 Authorization Code Grant Flow, which is the recommended method for web server applications to obtain access tokens to call Google APIs on behalf of users. The application redirects the user to Google for authentication and consent (Step 1). Google redirects back to the application with an authorization code (Step 2). The application backend then securely exchanges this code with Google for an access token (and optionally a refresh token) (Step 3). The application uses the access token to make API calls on the user's behalf (Step 4). Options A and B involve handling user passwords directly, which is insecure and violates OAuth principles. Option C incorrectly describes how the authorization code is handled and used.",conditions:["Java web server needs to call Google Cloud APIs on behalf of users using their Google identities."],caseStudyContext:null},{id:219,topic:"Compute",question:"You recently developed a new application. You want to deploy the application on Cloud Run without a Dockerfile. Your organization requires that all container images are pushed to a centrally managed container repository. How should you build your container using Google Cloud services? (Choose two.)",options:{A:"Push your source code to Artifact Registry.",B:"Submit a Cloud Build job to push the image.",C:"Use the pack build command with pack CLI.",D:"Include the --source flag with the gcloud run deploy CLI command.",E:"Include the --platform=kubernetes flag with the gcloud run deploy CLI command."},correctAnswer:["C","D"],explanation:"To build a container image without a Dockerfile for Cloud Run, Google Cloud Buildpacks can be used. This can be done implicitly with `gcloud run deploy --source .` (D), which uses Cloud Build and Buildpacks behind the scenes, builds the image, pushes it to Artifact Registry (creating a repo if needed), and deploys. Alternatively, you can use the `pack` CLI (C) (part of the Buildpacks project) locally or within a CI/CD system like Cloud Build to explicitly build the image using buildpacks, then manually push it to the central Artifact Registry, and finally deploy to Cloud Run. Both methods achieve building without a Dockerfile. Pushing source to AR (A) is incorrect. Submitting a standard Cloud Build job (B) typically assumes a Dockerfile or explicit build steps. --platform=kubernetes (E) is for Cloud Run for Anthos.",conditions:["Deploy app to Cloud Run without Dockerfile, use Google Cloud build services, push image to central repo."],caseStudyContext:null},{id:220,topic:"Databases",question:"You work for an organization that manages an online ecommerce website. Your company plans to expand across the world; however, the estore currently serves one specific region. You need to select a SQL database and configure a schema that will scale as your organization grows. You want to create a table that stores all customer transactions and ensure that the customer (CustomerId) and the transaction (TransactionId) are unique. What should you do?",options:{A:"Create a Cloud SQL table that has TransactionId and CustomerId configured as primary keys. Use an incremental number for the TransactionId.",B:"Create a Cloud SQL table that has TransactionId and CustomerId configured as primary keys. Use a random string (UUID) for the Transactionid.",C:"Create a Cloud Spanner table that has TransactionId and CustomerId configured as primary keys. Use a random string (UUID) for the TransactionId.",D:"Create a Cloud Spanner table that has TransactionId and CustomerId configured as primary keys. Use an incremental number for the TransactionId."},correctAnswer:["C"],explanation:"The requirement for global expansion and scaling points towards Cloud Spanner, a globally distributed, horizontally scalable SQL database. Cloud SQL is regional. To ensure uniqueness of (CustomerId, TransactionId) and avoid potential hotspotting issues with globally distributed writes, using a randomly generated UUID for TransactionId as part of the composite primary key is the best practice for Spanner (C). Incremental numbers (A, D) can lead to write hotspots in distributed systems.",conditions:["Ecommerce app expanding globally, need scalable SQL DB for transactions, ensure unique (CustomerId, TransactionId)."],caseStudyContext:null},{id:221,topic:"Monitoring & Logging",question:"You are monitoring a web application that is written in Go and deployed in Google Kubernetes Engine. You notice an increase in CPU and memory utilization. You need to determine which source code is consuming the most CPU and memory resources. What should you do?",options:{A:"Download, install, and start the Snapshot Debugger agent in your VM. Take debug snapshots of the functions that take the longest time. Review the call stack frame, and identify the local variables at that level in the stack.",B:"Import the Cloud Profiler package into your application, and initialize the Profiler agent. Review the generated flame graph in the Google Cloud console to identify time-intensive functions.",C:"Import OpenTelemetry and Trace export packages into your application, and create the trace provider. Review the latency data for your application on the Trace overview page, and identify where bottlenecks are occurring.",D:"Create a Cloud Logging query that gathers the web application's logs. Write a Python script that calculates the difference between the timestamps from the beginning and the end of the application's longest functions to identify time-intensive functions."},correctAnswer:["B"],explanation:"Cloud Profiler is designed to analyze CPU and memory usage attributed to specific functions or lines of code in production applications with low overhead. Instrumenting the Go application with the Cloud Profiler agent allows continuous collection of this data. The results can be viewed as flame graphs in the Cloud Console, directly showing which parts of the code consume the most CPU or allocate the most memory (B). Debugger snapshots (A) capture state at a point in time. Trace (C) focuses on request latency across services. Analyzing logs (D) provides limited insight into internal resource consumption.",conditions:["Go app on GKE, increased CPU/memory usage, need to find resource-intensive source code."],caseStudyContext:null},{id:222,topic:"Compute",question:"You have a container deployed on Google Kubernetes Engine. The container can sometimes be slow to launch, so you have implemented a liveness probe. You notice that the liveness probe occasionally fails on launch. What should you do?",options:{A:"Add a startup probe.",B:"Increase the initial delay for the liveness probe.",C:"Increase the CPU limit for the container.",D:"Add a readiness probe."},correctAnswer:["A"],explanation:"Liveness probes check if a container is running; if it fails, the container is restarted. Readiness probes check if a container is ready to serve traffic. If a container is slow to start, a liveness probe might fail before the application is fully initialized, causing unnecessary restarts. A startup probe (A) is specifically designed for this scenario. It runs first, and Kubernetes disables liveness/readiness checks until the startup probe succeeds (or times out). This prevents the liveness probe from killing a slow-starting container prematurely. Increasing initial delay (B) can work but might be hard to tune perfectly. CPU limits (C) might be relevant but aren't the direct fix. Readiness probes (D) don't prevent liveness failures during startup.",conditions:["GKE container slow to launch, liveness probe fails occasionally on launch."],caseStudyContext:null},{id:223,topic:"Networking",question:"You manage your company's ecommerce platform's payment system, which runs on Google Cloud. Your company must retain user logs for 1 year for internal auditing purposes and for 3 years to meet compliance requirements. You need to store new user logs on Google Cloud to minimize on-premises storage usage and ensure that they are easily searchable. You want to minimize effort while ensuring that the logs are stored correctly. What should you do?",options:{A:"Use an external HTTP(S) load balancer to route a predetermined percentage of traffic to two different color schemes of your application. Analyze the results to determine whether there is a statistically significant difference in sales.",B:"Use an external HTTP(S) load balancer to route traffic to the original color scheme while the new deployment is created and tested. After testing is complete, reroute all traffic to the new color scheme. Analyze the results to determine whether there is a statistically significant difference in sales.",C:"Use an external HTTP(S) load balancer to mirror traffic to the new version of your application. Analyze the results to determine whether there is a statistically significant difference in sales.",D:"Enable a feature flag that displays the new color scheme to half of all users. Monitor sales to see whether they increase for this group of users."},correctAnswer:["A"],explanation:"A/B testing involves comparing two (or more) versions of something by exposing them to different segments of users simultaneously and measuring the results. To test the effect of a new color scheme on sales, you need to deploy both the old and new schemes and direct a portion of live traffic to each. Using a load balancer to split traffic based on weights (e.g., 50% to version A, 50% to version B) is a standard way to implement this randomization (A). Option B describes blue/green deployment. Option C describes traffic mirroring (shadow testing). Option D uses feature flags, which is another valid A/B testing technique, but load balancer traffic splitting is also a direct and common method.",conditions:["Test new website color scheme effect on sales using A/B testing on live production traffic."],caseStudyContext:null},{id:224,topic:"Compute",question:"You are a developer at a large corporation. You manage three Google Kubernetes Engine clusters on Google Cloud. Your teams developers need to switch from one cluster to another regularly without losing access to their preferred development tools. You want to configure access to these multiple clusters while following Google-recommended best practices. What should you do?",options:{A:"Ask the developers to use Cloud Shell and run gcloud container clusters get-credential to switch to another cluster.",B:"In a configuration file, define the clusters, users, and contexts. Share the file with the developers and ask them to use kubect1 contig to add cluster, user, and context details.",C:"Ask the developers to install the gcloud CLI on their workstation and run gcloud container clusters get-credentials to switch to another cluster.",D:"Ask the developers to open three terminals on their workstation and use kubect1 config to configure access to each cluster."},correctAnswer:["C"],explanation:"To allow developers to use their local tools (IDEs, etc.) while easily switching between GKE cluster contexts, installing the `gcloud` CLI locally is the standard approach. The `gcloud container clusters get-credentials CLUSTER_NAME --zone/region REGION_NAME` command automatically fetches cluster details and authentication info (using the developer's gcloud login) and automatically configures the local `kubectl` (kubeconfig file) with the necessary contexts. Developers can then switch context using `kubectl config use-context` (C). Cloud Shell (A) requires working within the browser. Manually managing kubeconfig files (B, D) is cumbersome and error-prone compared to the `gcloud` integration.",conditions:["Developers need to switch between multiple GKE clusters regularly from their workstations, use preferred tools, follow best practices."],caseStudyContext:null},{id:225,topic:"Compute",question:"You are developing an online gaming platform as a microservices application on Google Kubernetes Engine (GKE). Users on social media are complaining about long loading times for certain URL requests to the application. You need to investigate performance bottlenecks in the application and identify which HTTP requests have a significantly high latency span in user requests. What should you do?",options:{A:"Use an external HTTP(S) load balancer to route traffic to the original color scheme while the new deployment is created and tested. After testing is complete, reroute all traffic to the new color scheme. Analyze the results to determine whether there is a statistically significant difference in sales.",B:"Deploy your application on Google Kubernetes Engine with Anthos Service Mesh. Use traffic splitting to direct a subset of user traffic to the new version based on the user-agent header.",C:"Deploy your application on App Engine. Use traffic splitting to direct a subset of user traffic to the new version based on the IP address.",D:"Deploy your application on Compute Engine. Use Traffic Director to direct a subset of user traffic to the new version based on predefined weights."},correctAnswer:["B"],explanation:"The requirement is to test with production users based on their *operating system* and allow quick rollback. Anthos Service Mesh (ASM) provides fine-grained traffic control, including routing based on HTTP request headers like `User-Agent`, which contains OS information. You can configure ASM to route users with specific OSes (e.g., iOS, Android) to the new version while others see the old version (B). ASM also facilitates quick rollback by adjusting routing rules. Cloud Run (A) traffic splitting is typically based on percentage or tags. App Engine (C) splits by IP, cookie, or random weight. Traffic Director (D) can route based on headers but ASM provides a more integrated service mesh solution for GKE.",conditions:["A/B test new application version, route subset of production users based on OS, quick rollback needed."],caseStudyContext:null},{id:226,topic:"Compute",question:`Your team is writing a backend application to implement the business logic for an interactive voice response (IVR) system that will support a payroll application. The IVR system has the following technical characteristics:

 Each customer phone call is associated with a unique IVR session.
 The IVR system creates a separate persistent gRPC connection to the backend for each session.
 If the connection is interrupted, the IVR system establishes a new connection, causing a slight latency for that call.

You need to determine which compute environment should be used to deploy the backend application. Using current call data, you determine that:

 Call duration ranges from 1 to 30 minutes.
 Calls are typically made during business hours.
 There are significant spikes of calls around certain known dates (e.g., pay days), or when large payroll changes occur.

You want to minimize cost, effort, and operational overhead. Where should you deploy the backend application?`,options:{A:"Compute Engine",B:"Google Kubernetes Engine cluster in Standard mode",C:"Cloud Functions",D:"Cloud Run"},correctAnswer:["D"],explanation:"The application needs to handle persistent gRPC connections (up to 30 mins), scale efficiently for unpredictable spikes, and minimize cost/overhead. Cloud Run (D) supports gRPC (including streaming) and scales automatically based on requests/connections, down to zero when idle, minimizing cost. While Cloud Functions (C) are serverless, they have shorter execution time limits (max 9 mins for HTTP, 60 mins for Gen2 event-driven, but less ideal for persistent connections). GKE (B) and Compute Engine (A) provide more control but require more management overhead for infrastructure and scaling compared to fully managed Cloud Run.",conditions:["Backend for IVR system, persistent gRPC connections (1-30 min), unpredictable traffic spikes, minimize cost/effort/overhead."],caseStudyContext:null},{id:227,topic:"Databases",question:"You are developing an application hosted on Google Cloud that uses a MySQL relational database schema. The application will have a large volume of reads and writes to the database and will require backups and ongoing capacity planning. Your team does not have time to fully manage the database but can take on small administrative tasks. How should you host the database?",options:{A:"Configure Cloud SQL to host the database, and import the schema into Cloud SQL.",B:"Deploy MySQL from the Google Cloud Marketplace to the database using a client, and import the schema.",C:"Configure Bigtable to host the database, and import the data into Bigtable.",D:"Configure Cloud Spanner to host the database, and import the schema into Cloud Spanner.",E:"Configure Firestore to host the database, and import the data into Firestore."},correctAnswer:["A"],explanation:"The requirements are: MySQL relational schema, large read/write volume, backups needed, capacity planning needed, minimal full management but small admin tasks OK. Cloud SQL for MySQL (A) is the fully managed relational database service for MySQL on Google Cloud. It handles backups, patching, replication, and offers scaling options (addressing capacity planning), significantly reducing management overhead compared to self-hosting MySQL on GCE (implied by B). Bigtable (C) and Firestore (E) are NoSQL. Cloud Spanner (D) is relational but typically for larger, globally distributed needs and might be overkill/more complex than managed MySQL if regional scope is sufficient.",conditions:["Host MySQL DB on GCP, large R/W volume, need backups/capacity planning, minimize full management."],caseStudyContext:null},{id:228,topic:"Compute",question:"You are developing a new web application using Cloud Run and committing code to Cloud Source Repositories. You want to deploy new code in the most efficient way possible. You have already created a Cloud Build YAML file that builds a container and runs the following command: gcloud run deploy. What should you do next?",options:{A:"Create a Pub/Sub topic to be notified when code is pushed to the repository. Create a Pub/Sub trigger that runs the build file when an event is published to the topic.",B:"Create a build trigger that runs the build file in response to a repository code being pushed to the development branch.",C:"Create a webhook build trigger that runs the build file in response to HTTP POST calls to the webhook URL.",D:"Create a Cron job that runs the following command every 24 hours: gcloud builds submit."},correctAnswer:["B"],explanation:"To automate deployment on code commits, Cloud Build triggers are used. Since the code is in Cloud Source Repositories, you can create a build trigger directly linked to the repository. Configuring the trigger to activate when code is pushed to a specific branch (like the development or main branch) (B) is the standard and most efficient way to integrate CSR and Cloud Build for CI/CD. Pub/Sub triggers (A) or webhooks (C) are for other event sources. Cron jobs (D) are for scheduled tasks, not commit-driven builds.",conditions:["Automate Cloud Run deployment from CSR using existing Cloud Build config (build + deploy steps), trigger on code push."],caseStudyContext:null},{id:229,topic:"Compute",question:"You are a developer at a large organization. You are deploying a web application to Google Kubernetes Engine (GKE). The DevOps team has built a CI/CD pipeline that uses Cloud Deploy to deploy the application to Dev, Test, and Prod clusters in GKE. After Cloud Deploy successfully deploys the application to the Dev cluster, you want to automatically promote it to the Test cluster. How should you configure this process following Google-recommended best practices?",options:{A:`1. Create a Cloud Build trigger that listens for SUCCEEDED Pub/Sub messages from the clouddeploy-operations topic.
2. Configure Cloud Build to include a step that promotes the application to the Test cluster.`,B:`1. Create a Cloud Function that calls the Google Cloud Deploy API to promote the application to the Test cluster.
2. Configure this function to be triggered by SUCCEEDED Pub/Sub messages from the cloud-builds topic.`,C:`1. Create a Cloud Function that calls the Google Cloud Deploy API to promote the application to the Test cluster.
2. Configure this function to be triggered by SUCCEEDED Pub/Sub messages from the clouddeploy-operations topic.`,D:`1. Create a Cloud Build pipeline that uses the gke-deploy builder.
2. Create a Cloud Build trigger that listens for SUCCEEDED Pub/Sub messages from the cloud-builds topic.
3. Configure this pipeline to run a deployment step to the Test cluster.`},correctAnswer:["C"],explanation:"Cloud Deploy emits events to Pub/Sub on the `clouddeploy-operations` topic. To automate promotion after a successful Dev deployment, listen for the `SUCCEEDED` event for the Dev target on this topic. A Cloud Function is a suitable lightweight, event-driven mechanism to react to this Pub/Sub message (C). The function's logic would call the Cloud Deploy API (or `gcloud deploy releases promote`) to promote the successful release to the Test target. Using Cloud Build (A) works but is potentially heavier than a simple function for this task. Listening to the wrong topic (`cloud-builds`) (B, D) is incorrect.",conditions:["Using Cloud Deploy for GKE (Dev, Test, Prod), automate promotion Dev->Test after successful Dev deploy."],caseStudyContext:null},{id:230,topic:"IAM & Security",question:"Your application is running as a container in a Google Kubernetes Engine cluster. You need to add a secret to your application using a secure approach. What should you do?",options:{A:"Create a Kubernetes Secret, and pass the Secret as an environment variable to the container.",B:"Enable Application-layer Secret Encryption on the cluster using a Cloud Key Management Service (KMS) key.",C:"Store the credential in Cloud KMS. Create a Google service account (GSA) to read the credential from Cloud KMS. Export the GSA as a .json file, and pass the .json file to the container as a volume which can read the credential from Cloud KMS.",D:"Store the credential in Secret Manager. Create a Google service account (GSA) to read the credential from Secret Manager. Create a Kubernetes service account (KSA) to run the container. Use Workload Identity to configure your KSA to act as a GSA."},correctAnswer:["D"],explanation:"Storing secrets directly in Kubernetes Secrets (A) is not the most secure method as they are only base64 encoded by default. Application-layer encryption (B) secures K8s secrets at rest in etcd but doesn't change how they are accessed. Using KMS directly (C) is for key management, not arbitrary secrets, and involves insecure key export. The most secure Google-recommended approach is to store the secret in Secret Manager, grant a GSA access, and use Workload Identity to allow the Pod (via its KSA) to authenticate as the GSA and retrieve the secret directly from the Secret Manager API at runtime (D).",conditions:["Add secret to GKE application securely."],caseStudyContext:null},{id:231,topic:"Storage",question:"You are a developer at a financial institution. You use Cloud Shell to interact with Google Cloud services. User data is currently stored on an ephemeral disk; however, a recently passed regulation mandates that you can no longer store sensitive information on an ephemeral disk. You need to implement a new storage solution for your user data. You want to minimize code changes. Where should you store your user data?",options:{A:"Store user data on a Cloud Shell home disk, and log in at least every 120 days to prevent its deletion.",B:"Store user data on a persistent disk in a Compute Engine instance.",C:"Store user data in a Cloud Storage bucket.",D:"Store user data in BigQuery tables."},correctAnswer:["B"],explanation:"The constraint is minimizing code changes, implying the current interaction might be file-based on an ephemeral disk (perhaps associated with a GCE instance accessed via Cloud Shell, or Cloud Shell's own temporary disk beyond $HOME). A GCE Persistent Disk (B) offers durable block storage mountable as a standard filesystem, providing the most direct replacement for file-based operations with minimal code changes. Cloud Shell home disk (A) is persistent but small and tied to Cloud Shell activity. Cloud Storage (C) is object storage (API changes needed). BigQuery (D) is a data warehouse (schema/query changes needed).",conditions:["Interact via Cloud Shell, currently store user data on ephemeral disk, must move to persistent storage, minimize code changes."],caseStudyContext:null},{id:232,topic:"Storage",question:"You recently developed a web application to transfer log data to a Cloud Storage bucket daily. Authenticated users will regularly review logs from the prior two weeks for critical events. After that, logs will be reviewed once annually by an external auditor. Data must be stored for a period of no less than 7 years. You want to propose a storage solution that meets these requirements and minimizes costs. What should you do? (Choose two.)",options:{A:"Use the Bucket Lock feature to set the retention policy on the data.",B:"Run a scheduled job to set the storage class to Coldline for objects older than 14 days.",C:"Create a JSON Web Token (JWT) for users needing access to the Coldline storage buckets.",D:"Create a lifecycle management policy to set the storage class to Coldline for objects older than 14 days.",E:"Create a lifecycle management policy to set the storage class to Nearline for objects older than 14 days."},correctAnswer:["A","D"],explanation:"Requirement 1 (Retain >= 7 years): Use Bucket Lock (or a standard retention policy) to enforce the 7-year retention period (A). Requirement 2 (Cost/Access Pattern): Logs are reviewed regularly for 14 days (implying Standard storage initially), then only annually. Coldline storage is suitable for data accessed about once per quarter. Archive is for less than once a year. A lifecycle policy should transition the data after the initial 14-day review period to a colder, cheaper class. Coldline (D) fits the 'once annually' access pattern better than Nearline (E - for monthly access). JWTs (C) are for authentication. Scheduled jobs (B) are less efficient than lifecycle policies.",conditions:["Store logs in GCS, review regularly for 14 days, then annually for audits, retain >= 7 years, minimize cost."],caseStudyContext:null},{id:233,topic:"Compute",question:"Your team is developing a Cloud Function triggered by Cloud Storage events. You want to accelerate testing and development of your Cloud Function while following Google-recommended best practices. What should you do?",options:{A:"Create a new Cloud Function that is triggered when Cloud Audit Logs detects the cloudfunctions.functions.sourceCodeSet operation in the original Cloud Function. Send mock requests to the new function to evaluate the functionality.",B:"Make a copy of the Cloud Function, and rewrite the code to be HTTP-triggered. Edit and test the new version by triggering the HTTP endpoint. Send mock requests to the new function to evaluate the functionality.",C:"Install the Functions Frameworks library, and configure the Cloud Function on localhost. Make a copy of the function, and make edits to the new version. Test the new version using curl.",D:"Make a copy of the Cloud Function in the Google Cloud console. Use the Cloud console's in-line editor to make source code changes to the new function. Modify your web application to call the new function, and test the new version in production"},correctAnswer:["C"],explanation:"Functions Frameworks provide a standard way to write Cloud Functions code that can be run locally for testing and development, mimicking the Cloud Functions runtime environment. By installing the framework for the function's language (C), developers can run the function locally, invoke it (e.g., using `curl` with a payload simulating the GCS event), set breakpoints, and iterate much faster than deploying to the cloud for each test. This accelerates development significantly. Rewriting as HTTP (B) changes the trigger type. Using Audit Logs (A) or testing in production (D) are not suitable for rapid local development.",conditions:["Develop/test GCS-triggered Cloud Function, accelerate dev/test cycle, follow best practices."],caseStudyContext:null},{id:234,topic:"Compute",question:"Your team is setting up a build pipeline for an application that will run in Google Kubernetes Engine (GKE). For security reasons, you only want images produced by the pipeline to be deployed to your GKE cluster. Which combination of Google Cloud services should you use?",options:{A:"Cloud Build, Cloud Storage, and Binary Authorization",B:"Google Cloud Deploy, Cloud Storage, and Google Cloud Armor",C:"Google Cloud Deploy, Artifact Registry, and Google Cloud Armor",D:"Cloud Build, Artifact Registry, and Binary Authorization"},correctAnswer:["D"],explanation:"The pipeline needs: 1. Build service: Cloud Build is the managed build service. 2. Image storage: Artifact Registry is the recommended managed registry for container images (and other artifacts). 3. Deployment enforcement: Binary Authorization enforces that only container images meeting specific criteria (e.g., signed by trusted authorities, passed vulnerability scans) can be deployed to GKE. This combination (D) directly addresses the requirements. Cloud Storage (A, B) isn't a container registry. Cloud Deploy (B, C) is for deployment orchestration, not build or enforcement. Cloud Armor (B, C) is for network security.",conditions:["Build pipeline for GKE app, deploy only images produced by the pipeline."],caseStudyContext:null},{id:235,topic:"Monitoring & Logging",question:"You are supporting a business-critical application in production deployed on Cloud Run. The application is reporting HTTP 500 errors that are affecting the usability of the application. You want to be alerted when the number of errors exceeds 15% of the requests within a specific time window. What should you do?",options:{A:"Create a Cloud Function that consumes the Cloud Monitoring API. Use Cloud Scheduler to trigger the Cloud Function daily and alert you if the number of errors is above the defined threshold.",B:"Navigate to the Cloud Run page in the Google Cloud console, and select the service from the services list. Use the Metrics tab to visualize the number of errors for that revision, and refresh the page daily.",C:"Create an alerting policy in Cloud Monitoring that alerts you if the number of errors is above the defined threshold.",D:"Create a Cloud Function that consumes the Cloud Monitoring API. Use Cloud Composer to trigger the Cloud Function daily and alert you if the number of errors is above the defined threshold."},correctAnswer:["C"],explanation:"Cloud Monitoring provides native alerting capabilities based on metrics. Cloud Run automatically sends metrics like request count and response codes (including 5xx errors) to Cloud Monitoring. You can create an alerting policy directly in Cloud Monitoring (C) that calculates the ratio of 5xx responses to total requests over a specified time window and triggers an alert if this ratio exceeds 15%. This is the standard, most efficient way. Using Cloud Functions (A, D) or manual checking (B) is unnecessarily complex or manual.",conditions:["Cloud Run app reporting HTTP 500 errors, need alert when error rate > 15% in a time window."],caseStudyContext:null},{id:236,topic:"Compute",question:"You need to build a public API that authenticates, enforces quotas, and reports metrics for API callers. Which tool should you use to complete this architecture?",options:{A:"App Engine",B:"Cloud Endpoints",C:"Identity-Aware Proxy",D:"GKE Ingress for HTTP(S) Load Balancing"},correctAnswer:["B"],explanation:"Cloud Endpoints (and its more comprehensive counterpart, Apigee) is Google Cloud's managed API gateway solution. It's specifically designed to handle common API management tasks like authentication (API keys, JWT, OAuth), quota enforcement, monitoring, logging, and providing a developer portal (B). App Engine (A) can host APIs but lacks built-in management features. IAP (C) secures access based on user identity, not typically for public API management. GKE Ingress (D) exposes services but doesn't provide the full suite of API management features.",conditions:["Build public API, requires authentication, quotas, metrics."],caseStudyContext:null},{id:237,topic:"Compute",question:"You noticed that your application was forcefully shut down during a Deployment update in Google Kubernetes Engine. Your application didnt close the database connection before it was terminated. You want to update your application to make sure that it completes a graceful shutdown. What should you do?",options:{A:"Update your code to process a received SIGTERM signal to gracefully disconnect from the database.",B:"Configure a PodDisruptionBudget to prevent the Pod from being forcefully shut down.",C:"Increase the terminationGracePeriodSeconds for your application.",D:"Configure a PreStop hook to shut down your application."},correctAnswer:["A"],explanation:"When Kubernetes terminates a Pod (e.g., during deployment updates), it sends a SIGTERM signal to the containers. Applications should trap this signal and initiate a graceful shutdown, including closing database connections, finishing requests, etc. (A). If the application doesn't exit within the `terminationGracePeriodSeconds`, Kubernetes sends SIGKILL. PodDisruptionBudgets (B) limit *voluntary* disruptions, not the shutdown process itself. Increasing the grace period (C) only helps if the application *already* handles SIGTERM but needs more time. A PreStop hook (D) can run commands before SIGTERM but handling the signal directly in the application is the most common and robust approach.",conditions:["GKE application shut down forcefully during update, didn't close DB connection, need graceful shutdown."],caseStudyContext:null},{id:238,topic:"Databases",question:`You are a lead developer working on a new retail system that runs on Cloud Run and Firestore in Datastore mode. A web UI requirement is for the system to display a list of available products when users access the system and for the user to be able to browse through all products. You have implemented this requirement in the minimum viable product (MVP) phase by returning a list of all available products stored in Firestore.

A few months after go-live, you notice that Cloud Run instances are terminated with HTTP 500: Container instances are exceeding memory limits errors during busy times. This error coincides with spikes in the number of Datastore entity reads. You need to prevent Cloud Run from crashing and decrease the number of Datastore entity reads. You want to use a solution that optimizes system performance. What should you do?`,options:{A:"Modify the query that returns the product list using integer offsets.",B:"Modify the query that returns the product list using limits.",C:"Modify the Cloud Run configuration to increase the memory limits.",D:"Modify the query that returns the product list using cursors."},correctAnswer:["D"],explanation:"The issue is that fetching *all* products overwhelms Cloud Run's memory, likely because the list is very large. Increasing memory (C) is a temporary fix that doesn't scale. Efficiently handling large result sets requires pagination. Firestore (in Datastore mode) recommends using query cursors (D) for pagination. Cursors mark a position in the result set, allowing you to fetch the next 'page' of results efficiently. Using limits (B) alone retrieves only the first N results. Using integer offsets (A) is inefficient in Datastore mode as skipped entities are still read internally.",conditions:["Cloud Run app lists all products from Firestore (Datastore mode), crashes with memory errors during high load/reads, need to fix."],caseStudyContext:null},{id:239,topic:"Compute",question:`You need to deploy an internet-facing microservices application to Google Kubernetes Engine (GKE). You want to validate new features using the A/B testing method. You have the following requirements for deploying new container image releases:
 There is no downtime when new container images are deployed.
 New production releases are tested and verified using a subset of production users.

What should you do?`,options:{A:`1. Configure your CI/CD pipeline to update the Deployment manifest file by replacing the container version with the latest version.
2. Recreate the Pods in your cluster by applying the Deployment manifest file.
3. Validate the application's performance by comparing its functionality with the previous release version, and roll back if an issue arises.`,B:`1. Create a second namespace on GKE for the new release version.
2. Create a Deployment configuration for the second namespace with the desired number of Pods.
3. Deploy new container versions in the second namespace.
4. Update the Ingress configuration to route traffic to the namespace with the new container versions.`,C:`1. Install the Anthos Service Mesh on your GKE cluster.
2. Create two Deployments on the GKE cluster, and label them with different version names.
3. Implement an Istio routing rule to send a small percentage of traffic to the Deployment that references the new version of the application.`,D:`1. Implement a rolling update pattern by replacing the Pods gradually with the new release version.
2. Validate the application's performance for the new subset of users during the rollout, and roll back if an issue arises.`},correctAnswer:["C"],explanation:"A/B testing requires routing a subset of users to a new version while the rest see the old version. Option C describes a canary deployment suitable for A/B testing using a service mesh (Anthos Service Mesh/Istio). You deploy both versions concurrently (Step 2) and use service mesh routing rules to direct a specific percentage (or characteristic subset) of traffic to the new version (Step 3), allowing testing with production users. This achieves zero downtime. Recreate (A) causes downtime. Option B describes blue/green, not A/B testing with a subset. Rolling updates (D) affect users gradually but don't easily target a specific subset for testing.",conditions:["Deploy GKE microservices, validate features via A/B testing, zero downtime, test with subset of prod users."],caseStudyContext:null},{id:240,topic:"Compute",question:"Your team manages a large Google Kubernetes Engine (GKE) cluster. Several application teams currently use the same namespace to develop microservices for the cluster. Your organization plans to onboard additional teams to create microservices. You need to configure multiple environments while ensuring the security and optimal performance of each teams work. You want to minimize cost and follow Google-recommended best practices. What should you do?",options:{A:"Create new role-based access controls (RBAC) for each team in the existing cluster, and define resource quotas.",B:"Create a new namespace for each environment in the existing cluster, and define resource quotas.",C:"Create a new GKE cluster for each team.",D:"Create a new namespace for each team in the existing cluster, and define resource quotas."},correctAnswer:["D"],explanation:"Best practice for multi-tenancy within a single GKE cluster (to minimize cost vs. multiple clusters C) involves using namespaces for isolation. Create a separate namespace for *each team* (D) to isolate their resources and apply specific RBAC rules and ResourceQuotas. ResourceQuotas limit the total resources a namespace can consume, ensuring fair sharing and preventing one team from impacting others. RBAC alone (A) doesn't isolate resources. Namespaces per environment (B) doesn't isolate teams if multiple teams share an environment.",conditions:["Multiple teams developing microservices in large GKE cluster, onboard more teams, need multi-environment config, ensure security/performance per team, minimize cost, follow best practices."],caseStudyContext:null},{id:241,topic:"Compute",question:"You have a Java application deployed on Cloud Run. Your application requires access to a database hosted on Cloud SQL. Due to regulatory requirements, your connection to the Cloud SQL instance must use its internal IP address. How should you configure the connectivity while following Google-recommended best practices?",options:{A:"Configure your Cloud Run service with a Cloud SQL connection.",B:"Configure your Cloud Run service to use a Serverless VPC Access connector.",C:"Configure your application to use the Cloud SQL Java connector.",D:"Configure your application to connect to an instance of the Cloud SQL Auth proxy."},correctAnswer:["B"],explanation:"Cloud Run services run in a Google-managed environment separate from your VPC. To connect to a Cloud SQL instance using its private/internal IP address, Cloud Run needs a network path into your VPC. The Serverless VPC Access connector (B) provides this bridge, allowing Cloud Run services to reach resources within your VPC, including Cloud SQL instances with private IPs. Cloud SQL connections (A) usually refer to public IP or proxy methods. The Java connector (C) and Auth Proxy (D) primarily handle authentication and connection management, often over public IPs or requiring a network path which the VPC Access connector provides for private IPs.",conditions:["Cloud Run Java app needs to connect to Cloud SQL using internal IP address."],caseStudyContext:null},{id:242,topic:"Storage",question:"Your application stores customers content in a Cloud Storage bucket, with each object being encrypted with the customer's encryption key. The key for each object in Cloud Storage is entered into your application by the customer. You discover that your application is receiving an HTTP 4xx error when reading the object from Cloud Storage. What is a possible cause of this error?",options:{A:"You attempted the read operation on the object with the customer's base64-encoded key.",B:"You attempted the read operation without the base64-encoded SHA256 hash of the encryption key.",C:"You entered the same encryption algorithm specified by the customer when attempting the read operation.",D:"You attempted the read operation on the object with the base64-encoded SHA256 hash of the customer's key."},correctAnswer:["B"],explanation:"When using Customer-Supplied Encryption Keys (CSEK) with the Cloud Storage JSON API, requests (like GET for reading) must include *both* the base64-encoded AES-256 key itself (in headers like `x-goog-encryption-key`) *and* the base64-encoded SHA256 hash of that key (in headers like `x-goog-encryption-key-sha256`). A 4xx error (like 400 Bad Request) will occur if these headers are missing, malformed, or if the provided key/hash doesn't match the one used during upload. Option B correctly identifies that omitting the required hash header would cause an error. Providing only the key (A) or only the hash (D) is insufficient. Algorithm (C) is typically fixed (AES256).",conditions:["Reading GCS object encrypted with CSEK, receiving HTTP 4xx error."],caseStudyContext:null},{id:243,topic:"IAM & Security",question:"You have two Google Cloud projects, named Project A and Project B. You need to create a Cloud Function in Project A that saves the output in a Cloud Storage bucket in Project B. You want to follow the principle of least privilege. What should you do?",options:{A:`1. Create a Google service account in Project B.
2. Deploy the Cloud Function with the service account in Project A.
3. Assign this service account the roles/storage.objectCreator role on the storage bucket residing in Project B.`,B:`1. Create a Google service account in Project A
2. Deploy the Cloud Function with the service account in Project A.
3. Assign this service account the roles/storage.objectCreator role on the storage bucket residing in Project B.`,C:`1. Determine the default App Engine service account (PROJECT_ID@appspot.gserviceaccount.com) in Project A.
2. Deploy the Cloud Function with the default App Engine service account in Project A.
3. Assign the default App Engine service account the roles/storage.objectCreator role on the storage bucket residing in Project B.`,D:`1. Determine the default App Engine service account (PROJECT_ID@appspot.gserviceaccount.com) in Project B.
2. Deploy the Cloud Function with the default App Engine service account in Project A.
3. Assign the default App Engine service account the roles/storage.objectCreator role on the storage bucket residing in Project B.`},correctAnswer:["B"],explanation:"The Cloud Function runs in Project A, so its runtime identity (service account) must also belong to Project A. Following least privilege, create a dedicated service account in Project A specifically for this function. Deploy the function specifying this service account as its identity. Then, grant this service account (from Project A) the necessary permission (`roles/storage.objectCreator`) on the target resource (the bucket in Project B) (B). Using an SA from Project B (A, D) is incorrect for a function running in A. Using default service accounts (C, D) often grants broader permissions than necessary.",conditions:["Cloud Function in Project A writes to GCS bucket in Project B, follow least privilege."],caseStudyContext:null},{id:244,topic:"Monitoring & Logging",question:"A governmental regulation was recently passed that affects your application. For compliance purposes, you are now required to send a duplicate of specific application logs from your applications project to a project that is restricted to the security team. What should you do?",options:{A:"Create user-defined log buckets in the security teams project. Configure a Cloud Logging sink to route your applications logs to log buckets in the security teams project.",B:"Create a job that copies the logs from the _Required log bucket into the security teams log bucket in their project.",C:"Modify the _Default log bucket sink rules to reroute the logs into the security teams log bucket.",D:"Create a job that copies the System Event logs from the _Required log bucket into the security teams log bucket in their project."},correctAnswer:["A"],explanation:"Cloud Logging sinks allow routing log entries matching a filter to supported destinations, including log buckets in *other* projects. To send specific application logs to a secure project, create a sink in the application's project with a filter matching those logs. Set the destination as a log bucket created in the security team's project (A). The sink needs appropriate permissions granted via its writer identity in the destination project. Copying logs manually or via jobs (B, D) is inefficient. Modifying the `_Default` sink (C) affects all logs, not just specific ones. `_Required` bucket is for specific audit logs.",conditions:["Send duplicate of specific application logs from Project A to a secure Log Bucket in Project B for compliance."],caseStudyContext:null},{id:245,topic:"Compute",question:"You plan to deploy a new Go application to Cloud Run. The source code is stored in Cloud Source Repositories. You need to configure a fully managed, automated, continuous deployment pipeline that runs when a source code commit is made. You want to use the simplest deployment solution. What should you do?",options:{A:"Configure a cron job on your workstations to periodically run gcloud run deploy --source in the working directory.",B:"Configure a Jenkins trigger to run the container build and deploy process for each source code commit to Cloud Source Repositories.",C:"Configure continuous deployment of new revisions from a source repository for Cloud Run using buildpacks.",D:"Use Cloud Build with a trigger configured to run the container build and deploy process for each source code commit to Cloud Source Repositories."},correctAnswer:["D"],explanation:"For a fully managed, automated CD pipeline triggered by commits to CSR, Cloud Build is the standard GCP solution. Create a Cloud Build trigger linked to the CSR repository, configured to activate on commits to a specific branch. Define a `cloudbuild.yaml` file (or use inferred build steps) that builds the Go application container and deploys it to Cloud Run using `gcloud run deploy` (D). This is simpler and more integrated than managing Jenkins (B) or relying on cron jobs (A). Option C describes the buildpack mechanism but not the full automated pipeline setup with triggers.",conditions:["Automated CD pipeline for Go app (CSR -> Cloud Run), trigger on commit, fully managed, simplest solution."],caseStudyContext:null},{id:246,topic:"Networking",question:"Your team has created an application that is hosted on a Google Kubernetes Engine (GKE) cluster. You need to connect the application to a legacy REST service that is deployed in two GKE clusters in two different regions. You want to connect your application to the target service in a way that is resilient. You also want to be able to run health checks on the legacy service on a separate port. How should you set up the connection? (Choose two.)",options:{A:"Use Traffic Director with a sidecar proxy to connect the application to the service.",B:"Use a proxyless Traffic Director configuration to connect the application to the service.",C:"Configure the legacy service's firewall to allow health checks originating from the sidecar proxy.",D:"Configure the legacy service's firewall to allow health checks originating from the application.",E:"Configure the legacy service's firewall to allow health checks originating from the Traffic Director control plane."},correctAnswer:["A","C"],explanation:"Traffic Director provides global load balancing and service discovery across multiple GKE clusters/regions, enhancing resilience. Using sidecar proxies (Envoy deployed via Traffic Director) (A) intercepts traffic from the calling application and routes it intelligently to healthy backends in either region. Traffic Director configures health checks for the backends. These health checks originate from the Envoy proxies co-located with the *calling* application (or dedicated Google health check systems integrated with TD, often represented by the proxy). Therefore, the legacy service's firewall must allow ingress for these health checks from the proxies (C). Proxyless gRPC doesn't apply to REST. Health checks don't originate directly from the app (D) or the TD control plane (E).",conditions:["Connect GKE app to legacy REST service in 2 GKE clusters (diff regions), resilient connection, health checks on separate port."],caseStudyContext:null},{id:247,topic:"Compute",question:`You have an application running in a production Google Kubernetes Engine (GKE) cluster. You use Cloud Deploy to automatically deploy your application to your production GKE cluster. As part of your development process, you are planning to make frequent changes to the applications source code and need to select the tools to test the changes before pushing them to your remote source code repository. Your toolset must meet the following requirements:
 Test frequent local changes automatically.
 Local deployment emulates production deployment.

Which tools should you use to test building and running a container on your laptop using minimal resources?`,options:{A:"Docker Compose and dockerd",B:"Terraform and kubeadm",C:"Minikube and Skaffold",D:"kaniko and Tekton"},correctAnswer:["C"],explanation:"To emulate a GKE production deployment locally for testing, you need a local Kubernetes environment and a tool to streamline the build/deploy cycle. Minikube provides a lightweight, single-node Kubernetes cluster running locally. Skaffold automates the inner development loop: it watches source code, automatically builds container images (using Docker, Jib, Buildpacks, etc.), pushes them (if needed), and deploys manifests to a Kubernetes cluster (like Minikube). This combination (C) closely emulates the K8s deployment environment locally with automation for frequent changes. Docker Compose (A) isn't Kubernetes. Terraform/kubeadm (B) are for infrastructure setup. Kaniko/Tekton (D) are build/pipeline tools often used in CI/CD, not primarily for local dev loops.",conditions:["Test local code changes for GKE app automatically before pushing, local deployment must emulate production, minimal local resources."],caseStudyContext:null},{id:248,topic:"Compute",question:`You are deploying a Python application to Cloud Run using Cloud Source Repositories and Cloud Build. The Cloud Build pipeline is shown below:

\`\`\`yaml
steps:
- name: python:3.9
  entrypoint: pip
  args: ['install', '-r', 'requirements.txt', '--user']
- name: python:3.9
  entrypoint: python
  args: ['main.py']
- name: gcr.io/cloud-builders/docker
  args: ['build', '-t', 'gcr.io/myproject/myimage', '.']
- name: gcr.io/cloud-builders/docker
  args: ['push', 'gcr.io/myproject/myimage']
- name: gcr.io/google.com/cloudsdktool/cloud-sdk
  entrypoint: gcloud
  args: ['run', 'deploy', 'myservice', '--image=gcr.io/myproject/myimage', '--region=us-central1']
\`\`\`

You want to optimize deployment times and avoid unnecessary steps. What should you do?`,options:{A:"Remove the step that pushes the container to Artifact Registry.",B:"Deploy a new Docker registry in a VPC, and use Cloud Build worker pools inside the VPC to run the build pipeline.",C:"Store image artifacts in a Cloud Storage bucket in the same region as the Cloud Run instance.",D:"Add the --cache-from argument to the Docker build step in your build config file."},correctAnswer:["D"],explanation:"The provided pipeline installs dependencies and runs the app outside Docker, then builds and pushes a Docker image, and finally deploys it. To optimize build times, especially the `docker build` step, leveraging Docker layer caching is crucial. Adding the `--cache-from` argument to the `docker build` step, pointing to a previously built image (e.g., the latest successful build of `gcr.io/myproject/myimage`), allows Docker to reuse unchanged layers, significantly speeding up the build, especially dependency installation (D). Removing the push (A) prevents deployment. A custom registry (B) adds complexity. Storing artifacts in GCS (C) isn't standard for Docker builds.",conditions:["Optimize Cloud Build pipeline (Python app build/push/deploy to Cloud Run)."],caseStudyContext:null},{id:249,topic:"Compute",question:"You are developing an event-driven application. You have created a topic to receive messages sent to Pub/Sub. You want those messages to be processed in real time. You need the application to be independent from any other system and only incur costs when new messages arrive. How should you configure the architecture?",options:{A:"Deploy the application on Compute Engine. Use a Pub/Sub push subscription to process new messages in the topic.",B:"Deploy your code on Cloud Functions. Use a Pub/Sub trigger to invoke the Cloud Function. Use the Pub/Sub API to create a pull subscription to the Pub/Sub topic and read messages from it.",C:"Deploy the application on Google Kubernetes Engine. Use the Pub/Sub API to create a pull subscription to the Pub/Sub topic and read messages from it.",D:"Deploy your code on Cloud Functions. Use a Pub/Sub trigger to handle new messages in the topic."},correctAnswer:["D"],explanation:"The requirements are: event-driven (Pub/Sub trigger), real-time processing, independence, pay-per-use (only when messages arrive). Cloud Functions (D) perfectly match this. They are serverless, event-driven, and you only pay when they execute. Configuring a Pub/Sub trigger automatically invokes the function in near real-time whenever a message arrives on the topic. Compute Engine (A) and GKE (C) incur costs even when idle and require managing infrastructure. Option B unnecessarily adds a manual pull subscription within the triggered function.",conditions:["Event-driven app, process Pub/Sub messages in real time, independent system, pay only on message arrival."],caseStudyContext:null},{id:250,topic:"Monitoring & Logging",question:"You have an application running on Google Kubernetes Engine (GKE). The application is currently using a logging library and is outputting to standard output. You need to export the logs to Cloud Logging, and you need the logs to include metadata about each request. You want to use the simplest method to accomplish this. What should you do?",options:{A:"Change your applications logging library to the Cloud Logging library, and configure your application to export logs to Cloud Logging.",B:"Update your application to output logs in JSON format, and add the necessary metadata to the JSON.",C:"Update your application to output logs in CSV format, and add the necessary metadata to the CSV.",D:"Install the Fluent Bit agent on each of your GKE nodes, and have the agent export all logs from /var/log."},correctAnswer:["B"],explanation:"GKE clusters with Cloud Operations for GKE enabled (default) automatically collect logs written to standard output (stdout) and standard error (stderr) by containers and send them to Cloud Logging. To include custom metadata and ensure logs are parsed correctly by Cloud Logging, the simplest method is to structure the log output as single-line JSON objects written to stdout (B). Cloud Logging can automatically detect and parse JSON, making the fields (including your custom metadata) searchable. Changing libraries (A) requires more code changes. CSV (C) isn't automatically parsed like JSON. The agent (Fluent Bit/fluentd) is already running by default (D).",conditions:["GKE app logs to stdout, need logs in Cloud Logging with request metadata, simplest method."],caseStudyContext:null},{id:251,topic:"Networking",question:"You are working on a new application that is deployed on Cloud Run and uses Cloud Functions. Each time new features are added, new Cloud Functions and Cloud Run services are deployed. You use ENV variables to keep track of the services and enable interservice communication, but the maintenance of the ENV variables has become difficult. You want to implement dynamic discovery in a scalable way. What should you do?",options:{A:"Configure your microservices to use the Cloud Run Admin and Cloud Functions APIs to query for deployed Cloud Run services and Cloud Functions in the Google Cloud project.",B:"Create a Service Directory namespace. Use API calls to register the services during deployment, and query during runtime.",C:"Rename the Cloud Functions and Cloud Run services endpoint is using a well-documented naming convention.",D:"Deploy Hashicorp Consul on a single Compute Engine instance. Register the services with Consul during deployment, and query during runtime."},correctAnswer:["B"],explanation:"Manually managing endpoints via environment variables becomes difficult as services change. Service Directory is Google Cloud's managed service discovery platform. You can register your Cloud Run and Cloud Function services (and other services) with Service Directory during deployment (e.g., via API calls in CI/CD). At runtime, other services can query Service Directory (via API or DNS) to dynamically discover the current endpoints of the services they need to call (B). Querying Admin APIs directly (A) is less efficient. Naming conventions (C) don't provide dynamic resolution. Self-managing Consul (D) adds operational overhead compared to managed Service Directory.",conditions:["Cloud Run/Cloud Function microservices, ENV vars for service endpoints hard to maintain, need scalable dynamic discovery."],caseStudyContext:null},{id:252,topic:"Compute",question:"You work for a financial services company that has a container-first approach. Your team develops microservices applications. A Cloud Build pipeline creates the container image, runs regression tests, and publishes the image to Artifact Registry. You need to ensure that only containers that have passed the regression tests are deployed to Google Kubernetes Engine (GKE) clusters. You have already enabled Binary Authorization on the GKE clusters. What should you do next?",options:{A:"Create an attestor and a policy. After a container image has successfully passed the regression tests, use Cloud Build to run Kritis Signer to create an attestation for the container image.",B:"Deploy Voucher Server and Voucher Client components. After a container image has successfully passed the regression tests, run Voucher Client as a step in the Cloud Build pipeline.",C:"Set the Pod Security Standard level to Restricted for the relevant namespaces. Use Cloud Build to digitally sign the container images that have passed the regression tests.",D:"Create an attestor and a policy. Create an attestation for the container images that have passed the regression tests as a step in the Cloud Build pipeline."},correctAnswer:["D"],explanation:"Binary Authorization requires attestations to verify image compliance. The process is: 1. Define an attestor (representing the testing process) and a policy requiring attestation from this attestor. 2. In the Cloud Build pipeline, *after* regression tests pass, add a step to create an attestation for the specific image digest, referencing the attestor. This can be done using `gcloud beta container binauthz attestations create` (D). Kritis Signer (A) or Voucher (B) are specific tools/frameworks that can *implement* attestation creation, but the core step is creating the attestation itself. Pod Security Standards (C) relate to pod runtime security, not deployment authorization based on prior checks.",conditions:["GKE deployment, images built/tested/pushed via Cloud Build, Binary Authorization enabled, need to ensure only tested images are deployed."],caseStudyContext:null},{id:253,topic:"IAM & Security",question:`You are reviewing and updating your Cloud Build steps to adhere to best practices. Currently, your build steps include:

1. Pull the source code from a source repository.
2. Build a container image
3. Upload the built image to Artifact Registry.

You need to add a step to perform a vulnerability scan of the built container image, and you want the results of the scan to be available to your deployment pipeline running in Google Cloud. You want to minimize changes that could disrupt other teams processes. What should you do?`,options:{A:"Enable Binary Authorization, and configure it to attest that no vulnerabilities exist in a container image.",B:"Upload the built container images to your Docker Hub instance, and scan them for vulnerabilities.",C:"Enable the Container Scanning API in Artifact Registry, and scan the built container images for vulnerabilities.",D:"Add Artifact Registry to your Aqua Security instance, and scan the built container images for vulnerabilities."},correctAnswer:["C"],explanation:"Artifact Registry integrates with Container Analysis to provide automated vulnerability scanning. By enabling the Container Scanning API (C), images pushed to Artifact Registry will be automatically scanned for known OS and language pack vulnerabilities. The results are stored in Container Analysis and accessible via API or the Cloud Console. This is a managed solution, requires minimal change (just enabling the API), and makes results available within GCP for downstream pipeline steps (like Binary Authorization attestors or custom checks). Binary Authorization (A) *enforces* policies based on scan results/attestations, but doesn't perform the scan. External registries (B) or tools (D) add complexity.",conditions:["Add vulnerability scanning step to Cloud Build pipeline (after image push to AR), results available to deployment pipeline, minimize disruption."],caseStudyContext:null},{id:254,topic:"Monitoring & Logging",question:"You are developing an online gaming platform as a microservices application on Google Kubernetes Engine (GKE). Users on social media are complaining about long loading times for certain URL requests to the application. You need to investigate performance bottlenecks in the application and identify which HTTP requests have a significantly high latency span in user requests. What should you do?",options:{A:"Configure GKE workload metrics using kubectl. Select all Pods to send their metrics to Cloud Monitoring. Create a custom dashboard of application metrics in Cloud Monitoring to determine performance bottlenecks of your GKE cluster.",B:"Update your microservices to log HTTP request methods and URL paths to STDOUT. Use the logs router to send container logs to Cloud Logging. Create filters in Cloud Logging to evaluate the latency of user requests across different methods and URL paths.",C:"Instrument your microservices by installing the OpenTelemetry tracing package. Update your application code to send traces to Trace for inspection and analysis. Create an analysis report on Trace to analyze user requests.",D:"Install tcpdump on your GKE nodes. Run tcpdump to capture network traffic over an extended period of time to collect data. Analyze the data files using Wireshark to determine the cause of high latency."},correctAnswer:["C"],explanation:"To understand latency within and across microservices for specific user requests, distributed tracing is the appropriate tool. Instrumenting the microservices with OpenTelemetry (or OpenCensus) allows capturing trace data (spans showing time spent in different parts of the request path). Sending this data to Cloud Trace enables visualization and analysis of request latency, identifying which services or operations contribute most to the overall delay for specific URLs (C). Metrics (A) provide aggregated data, not per-request traces. Logs (B) can show timing but tracing gives a clearer picture of distributed calls. Network dumps (D) are very low-level and hard to analyze for application-level latency.",conditions:["GKE microservices app, users report slow loading times for specific URLs, need to identify high-latency requests/bottlenecks."],caseStudyContext:null},{id:255,topic:"Compute",question:`You need to load-test a set of REST API endpoints that are deployed to Cloud Run. The API responds to HTTP POST requests. Your load tests must meet the following requirements:
 Load is initiated from multiple parallel threads.
 User traffic to the API originates from multiple source IP addresses.
 Load can be scaled up using additional test instances.

You want to follow Google-recommended best practices. How should you configure the load testing?`,options:{A:"Create an image that has cURL installed, and configure cURL to run a test plan. Deploy the image in a managed instance group, and run one instance of the image for each VM.",B:"Create an image that has cURL installed, and configure cURL to run a test plan. Deploy the image in an unmanaged instance group, and run one instance of the image for each VM.",C:"Deploy a distributed load testing framework on a private Google Kubernetes Engine cluster. Deploy additional Pods as needed to initiate more traffic and support the number of concurrent users.",D:"Download the container image of a distributed load testing framework on Cloud Shell. Sequentially start several instances of the container on Cloud Shell to increase the load on the API."},correctAnswer:["C"],explanation:"The requirements point to needing a distributed, scalable load generation system. Deploying a distributed load testing framework (like Locust, k6, JMeter) on GKE (C) allows you to easily scale the number of load-generating Pods (workers) to achieve high parallelism and simulate many users/threads. GKE can distribute these pods across multiple nodes, potentially providing different source IPs (especially if using multiple node pools or external IPs). Using simple cURL on GCE (A, B) is less sophisticated and harder to scale/coordinate. Cloud Shell (D) is not suitable for scalable load generation.",conditions:["Load test Cloud Run REST API (POST requests), requirements: parallel threads, multiple source IPs, scalable load."],caseStudyContext:null},{id:256,topic:"IAM & Security",question:"Your team is creating a serverless web application on Cloud Run. The application needs to access images stored in a private Cloud Storage bucket. You want to give the application Identity and Access Management (IAM) permission to access the images in the bucket, while also securing the services using Google-recommended best practices. What should you do?",options:{A:"Enforce signed URLs for the desired bucket. Grant the Storage Object Viewer IAM role on the bucket to the Compute Engine default service account.",B:"Enforce public access prevention for the desired bucket. Grant the Storage Object Viewer IAM role on the bucket to the Compute Engine default service account.",C:"Enforce signed URLs for the desired bucket. Create and update the Cloud Run service to use a user-managed service account. Grant the Storage Object Viewer IAM role on the bucket to the service account.",D:"Enforce public access prevention for the desired bucket. Create and update the Cloud Run service to use a user-managed service account. Grant the Storage Object Viewer IAM role on the bucket to the service account."},correctAnswer:["D"],explanation:"Best practices: 1. Keep buckets private: Enforce public access prevention. 2. Least privilege: Create a dedicated user-managed service account for the Cloud Run service. Grant this service account only the necessary permissions (e.g., `roles/storage.objectViewer`) on the specific bucket. 3. Configure Cloud Run to run *as* this service account. This allows the application to authenticate using its service account identity (via ADC) without needing keys (D). Signed URLs (A, C) are for granting access *to end users*, not for service-to-service access. Using the default Compute Engine SA (A, B) grants overly broad permissions.",conditions:["Cloud Run app needs access to private GCS bucket, follow security best practices (least privilege, private bucket)."],caseStudyContext:null},{id:257,topic:"Compute",question:"You are using Cloud Run to host a global ecommerce web application. Your companys design team is creating a new color scheme for the web app. You have been tasked with determining whether the new color scheme will increase sales. You want to conduct testing on live production traffic. How should you design the study?",options:{A:"Use an external HTTP(S) load balancer to route a predetermined percentage of traffic to two different color schemes of your application. Analyze the results to determine whether there is a statistically significant difference in sales.",B:"Use an external HTTP(S) load balancer to route traffic to the original color scheme while the new deployment is created and tested. After testing is complete, reroute all traffic to the new color scheme. Analyze the results to determine whether there is a statistically significant difference in sales.",C:"Use an external HTTP(S) load balancer to mirror traffic to the new version of your application. Analyze the results to determine whether there is a statistically significant difference in sales.",D:"Enable a feature flag that displays the new color scheme to half of all users. Monitor sales to see whether they increase for this group of users."},correctAnswer:["A"],explanation:"This is a classic A/B testing scenario aimed at comparing user behavior (sales) between two versions (color schemes). Deploying both versions (e.g., as separate Cloud Run revisions or services) and using a mechanism to split live traffic randomly between them is required. An external HTTP(S) Load Balancer configured for weighted traffic splitting (A) can route, for example, 50% of users to the old scheme and 50% to the new scheme, allowing for direct comparison of sales metrics. Option B is blue/green. Option C is traffic mirroring. Option D (feature flags) is another valid A/B technique, but LB splitting is also common and directly supported.",conditions:["A/B test new website color scheme on Cloud Run using live production traffic, measure impact on sales."],caseStudyContext:null},{id:258,topic:"Compute",question:"You are a developer at a large corporation. You manage three Google Kubernetes Engine clusters on Google Cloud. Your teams developers need to switch from one cluster to another regularly without losing access to their preferred development tools. You want to configure access to these multiple clusters while following Google-recommended best practices. What should you do?",options:{A:"Ask the developers to use Cloud Shell and run gcloud container clusters get-credential to switch to another cluster.",B:"In a configuration file, define the clusters, users, and contexts. Share the file with the developers and ask them to use kubect1 contig to add cluster, user, and context details.",C:"Ask the developers to install the gcloud CLI on their workstation and run gcloud container clusters get-credentials to switch to another cluster.",D:"Ask the developers to open three terminals on their workstation and use kubect1 config to configure access to each cluster."},correctAnswer:["C"],explanation:"To allow developers to use their local tools (IDEs, etc.) while easily switching between GKE cluster contexts, installing the `gcloud` CLI locally is the standard approach. The `gcloud container clusters get-credentials CLUSTER_NAME --zone/region REGION_NAME` command automatically fetches cluster details and authentication info (using the developer's gcloud login) and automatically configures the local `kubectl` (kubeconfig file) with the necessary contexts. Developers can then switch context using `kubectl config use-context` (C). Cloud Shell (A) requires working within the browser. Manually managing kubeconfig files (B, D) is cumbersome and error-prone compared to the `gcloud` integration.",conditions:["Developers need to switch between multiple GKE clusters regularly from their workstations, use preferred tools, follow best practices."],caseStudyContext:null},{id:259,topic:"Databases",question:`You are a lead developer working on a new retail system that runs on Cloud Run and Firestore. A web UI requirement is for the user to be able to browse through all products. A few months after go-live, you notice that Cloud Run instances are terminated with HTTP 500: Container instances are exceeding memory limits errors during busy times. This error coincides with spikes in the number of Firestore queries.

You need to prevent Cloud Run from crashing and decrease the number of Firestore queries. You want to use a solution that optimizes system performance. What should you do?`,options:{A:"Modify the query that returns the product list using cursors with limits.",B:"Create a custom index over the products.",C:"Modify the query that returns the product list using integer offsets.",D:"Modify the Cloud Run configuration to increase the memory limits."},correctAnswer:["A"],explanation:"Similar to Q238, the root cause is fetching too many product documents at once, overwhelming Cloud Run's memory. The solution is pagination. Firestore's recommended pagination method uses query cursors combined with limits. A limit restricts the number of documents returned per page, and a cursor marks the position to start the next page fetch (A). This drastically reduces the number of entities read per request and the memory needed by Cloud Run. Offsets (C) are inefficient. Custom indexes (B) improve query speed but don't solve fetching too much data. Increasing memory (D) is a temporary fix.",conditions:["Cloud Run app lists all products from Firestore, crashes with memory errors during high load/queries, need to fix."],caseStudyContext:null},{id:260,topic:"Compute",question:"You are a developer at a large organization. Your team uses Git for source code management (SCM). You want to ensure that your team follows Google-recommended best practices to manage code to drive higher rates of software delivery. Which SCM process should your team use?",options:{A:"Each developer commits their code to the main branch before each product release, conducts testing, and rolls back if integration issues are detected.",B:"Each group of developers copies the repository, commits their changes to their repository, and merges their code into the main repository before each product release.",C:"Each developer creates a branch for their own work, commits their changes to their branch, and merges their code into the main branch daily.",D:"Each group of developers creates a feature branch from the main branch for their work, commits their changes to their branch, and merges their code into the main branch after the change advisory board approves it."},correctAnswer:["C"],explanation:"Google (and DevOps best practices like DORA) recommend trunk-based development or short-lived feature branches with frequent integration into the main branch (daily or more often) to enable high software delivery rates. Option C describes this: developers work on short-lived branches and merge back daily, minimizing merge conflicts and enabling continuous integration. Committing directly to main before release (A) is risky. Copying repos (B) is not standard Git workflow. Long-lived feature branches with delayed merges pending CAB approval (D) slow down delivery and increase merge complexity.",conditions:["Select SCM process, follow Google best practices, drive high software delivery rates."],caseStudyContext:null},{id:261,topic:"Compute",question:"You have a web application that publishes messages to Pub/Sub. You plan to build new versions of the application locally and want to quickly test Pub/Sub integration for each new build. How should you configure local testing?",options:{A:"Install Cloud Code on the integrated development environment (IDE). Navigate to Cloud APIs, and enable Pub/Sub against a valid Google Project ID. When developing locally, configure your application to call pubsub.googleapis.com.",B:"Install the Pub/Sub emulator using gcloud, and start the emulator with a valid Google Project ID. When developing locally, configure your application to use the local emulator with ${gcloud beta emulators pubsub env-init}.",C:"In the Google Cloud console, navigate to the API Library, and enable the Pub/Sub API. When developing locally, configure your application to call pubsub.googleapis.com.",D:"Install the Pub/Sub emulator using gcloud, and start the emulator with a valid Google Project IWhen developing locally, configure your application to use the local emulator by exporting the PUBSUB_EMULATOR_HOST variable."},correctAnswer:["B"],explanation:"For local testing of Pub/Sub integration, the emulator is the recommended approach. You install it via `gcloud components install pubsub-emulator`. Start the emulator. To configure your application (running on the same machine) to use the emulator, the `gcloud beta emulators pubsub env-init` command prints the necessary environment variable (`PUBSUB_EMULATOR_HOST`) settings. You can either export these manually (like in D) or, more conveniently, use shell evaluation like `eval $(gcloud beta emulators pubsub env-init)` to set them automatically in the current shell (implied by B's syntax). Calling the real API (A, C) isn't local testing.",conditions:["Test Pub/Sub publishing integration locally for new app builds."],caseStudyContext:null},{id:262,topic:"Monitoring & Logging",question:`Your ecommerce application receives external requests and forwards them to third-party API services for credit card processing, shipping, and inventory management as shown in the diagram.

[Diagram: User -> Ecommerce App -> [Credit Card API, Shipping API, Inventory API]]

Your customers are reporting that your application is running slowly at unpredictable times. The application doesnt report any metrics. You need to determine the cause of the inconsistent performance. What should you do?`,options:{A:"Install the OpenTelemetry library for your respective language, and instrument your application.",B:"Install the Ops Agent inside your container and configure it to gather application metrics.",C:"Modify your application to read and forward the X-Cloud-Trace-Context header when it calls the downstream services.",D:"Enable Managed Service for Prometheus on the Google Kubernetes Engine cluster to gather application metrics."},correctAnswer:["A"],explanation:"The problem involves unpredictable slowness likely related to dependencies (third-party APIs). Distributed tracing is needed to pinpoint where latency occurs. OpenTelemetry (A) is the industry standard for instrumenting applications to generate traces (and metrics/logs). By adding OpenTelemetry libraries and instrumenting the calls within the ecommerce app (including calls to the third-party APIs), you can capture end-to-end traces and send them to a backend like Cloud Trace for analysis, identifying which service calls are slow. Ops Agent (B) collects system/runtime metrics. Forwarding trace context (C) is part of tracing but requires instrumentation first. Prometheus (D) is for metrics, not tracing.",conditions:["Ecommerce app calls 3rd-party APIs, slow/inconsistent performance, no metrics currently, need to find cause."],caseStudyContext:null},{id:263,topic:"Compute",question:"You are developing a new application. You want the application to be triggered only when a given file is updated in your Cloud Storage bucket. Your trigger might change, so your process must support different types of triggers. You want the configuration to be simple so that multiple team members can update the triggers in the future. What should you do?",options:{A:"Configure Cloud Storage events to be sent to Pub/Sub, and use Pub/Sub events to trigger a Cloud Build job that executes your application.",B:"Create an Eventarc trigger that monitors your Cloud Storage bucket for a specific filename, and set the target as Cloud Run.",C:"Configure a Cloud Function that executes your application and is triggered when an object is updated in Cloud Storage.",D:"Configure a Firebase function that executes your application and is triggered when an object is updated in Cloud Storage."},correctAnswer:["B"],explanation:"Eventarc is Google Cloud's unified eventing platform. It allows routing events from various sources (like Cloud Storage, Pub/Sub, Audit Logs) to different targets (like Cloud Run, Cloud Functions, Workflows). It supports flexible filtering (e.g., specific filename patterns) and decouples the event source from the target, making it easy to change triggers or targets later (B). Direct Cloud Function triggers (C) are simpler for basic GCS events but less flexible if trigger types change. Pub/Sub+Cloud Build (A) adds extra steps. Firebase functions (D) are typically for Firebase projects.",conditions:["Trigger application when specific GCS file updated, support changing trigger types, simple config for team updates."],caseStudyContext:null},{id:264,topic:"Compute",question:"You are defining your system tests for an application running in Cloud Run in a Google Cloud project. You need to create a testing environment that is isolated from the production environment. You want to fully automate the creation of the testing environment with the least amount of effort and execute automated tests. What should you do?",options:{A:"Using Cloud Build, execute Terraform scripts to create a new Google Cloud project and a Cloud Run instance of your application in the Google Cloud project.",B:"Using Cloud Build, execute a Terraform script to deploy a new Cloud Run revision in the existing Google Cloud project. Use traffic splitting to send traffic to your test environment.",C:"Using Cloud Build, execute gcloud commands to create a new Google Cloud project and a Cloud Run instance of your application in the Google Cloud project.",D:"Using Cloud Build, execute gcloud commands to deploy a new Cloud Run revision in the existing Google Cloud project. Use traffic splitting to send traffic to your test environment."},correctAnswer:["A"],explanation:"The requirement is a *fully isolated* testing environment, automated creation with least effort. Creating a completely new, ephemeral Google Cloud project for each test run (A or C) provides the strongest isolation. Using Infrastructure as Code (IaC) like Terraform (A) is the standard way to fully automate the creation and teardown of cloud resources, including projects and Cloud Run services, making it repeatable and manageable. Using gcloud commands (C) works but Terraform is generally preferred for complex infrastructure automation. Deploying revisions within the *same* project (B, D) does not provide full isolation from the production environment.",conditions:["Create isolated test environment for Cloud Run app, fully automated creation, least effort, run automated tests."],caseStudyContext:null},{id:265,topic:"Compute",question:"You are a cluster administrator for Google Kubernetes Engine (GKE). Your organizations clusters are enrolled in a release channel. You need to be informed of relevant events that affect your GKE clusters, such as available upgrades and security bulletins. What should you do?",options:{A:"Configure cluster notifications to be sent to a Pub/Sub topic.",B:"Execute a scheduled query against the google_cloud_release_notes BigQuery dataset.",C:"Query the GKE API for available versions.",D:"Create an RSS subscription to receive a daily summary of the GKE release notes."},correctAnswer:["A"],explanation:"GKE provides a Cluster Notifications feature that allows administrators to subscribe to specific types of cluster-related events, including available upgrades and security bulletins. These notifications can be published to a Pub/Sub topic (A). You can then integrate this topic with other systems (e.g., alerting, ticketing) to ensure relevant teams are informed. Querying BigQuery datasets (B) or the GKE API (C) requires active polling. RSS feeds (D) provide general release notes but not specific notifications tailored to your cluster's channel and version.",conditions:["Need notifications for GKE cluster events (upgrades, security bulletins) for clusters in release channels."],caseStudyContext:null},{id:266,topic:"Compute",question:"You are tasked with using C++ to build and deploy a microservice for an application hosted on Google Cloud. The code needs to be containerized and use several custom software libraries that your team has built. You do not want to maintain the underlying infrastructure of the application. How should you deploy the microservice?",options:{A:"Use Cloud Functions to deploy the microservice.",B:"Use Cloud Build to create the container, and deploy it on Cloud Run.",C:"Use Cloud Shell to containerize your microservice, and deploy it on a Container-Optimized OS Compute Engine instance.",D:"Use Cloud Shell to containerize your microservice, and deploy it on standard Google Kubernetes Engine."},correctAnswer:["B"],explanation:"The requirements are: C++ microservice, containerized, uses custom libraries, minimize infrastructure maintenance (prefer serverless/managed). Cloud Run (B) is a fully managed platform for running stateless containers. It supports custom containers built using any language (including C++) and libraries. You can use Cloud Build to automate the container build process (including compiling C++ code and linking custom libraries) and then deploy the resulting container to Cloud Run. Cloud Functions (A) doesn't natively support C++. GCE (C) and GKE (D) require managing underlying VMs or clusters, violating the minimal infrastructure maintenance requirement.",conditions:["Deploy C++ containerized microservice using custom libraries, minimize infra maintenance."],caseStudyContext:null},{id:267,topic:"IAM & Security",question:"You need to containerize a web application that will be hosted on Google Cloud behind a global load balancer with SSL certificates. You dont have the time to develop authentication at the application level, and you want to offload SSL encryption and management from your application. You want to configure the architecture using managed services where possible. What should you do?",options:{A:"Host the application on Google Kubernetes Engine, and deploy an NGINX Ingress Controller to handle authentication.",B:"Host the application on Google Kubernetes Engine, and deploy cert-manager to manage SSL certificates.",C:"Host the application on Compute Engine, and configure Cloud Endpoints for your application.",D:"Host the application on Google Kubernetes Engine, and use Identity-Aware Proxy (IAP) with Cloud Load Balancing and Google-managed certificates."},correctAnswer:["D"],explanation:"Requirements: Containerized app, global LB, managed SSL, offload auth, managed services. Hosting on GKE meets the container requirement. Google Cloud Load Balancing (GCLB) provides global load balancing and can use Google-managed SSL certificates, offloading SSL management. Identity-Aware Proxy (IAP) integrates with GCLB to provide managed authentication based on Google identities, eliminating the need for application-level auth development (D). Nginx Ingress (A) requires self-management. Cert-manager (B) manages certs but doesn't handle auth. Cloud Endpoints (C) is an API gateway, not typically used for securing a standard web app this way, and GCE isn't container-native.",conditions:["Containerized web app, global LB, managed SSL, offload authentication, managed services."],caseStudyContext:null},{id:268,topic:"Compute",question:"You manage a system that runs on stateless Compute Engine VMs and Cloud Run instances. Cloud Run is connected to a VPC, and the ingress setting is set to Internal. You want to schedule tasks on Cloud Run. You create a service account and grant it the roles/run.invoker Identity and Access Management (IAM) role. When you create a schedule and test it, a 403 Permission Denied error is returned in Cloud Logging. What should you do?",options:{A:"Grant the service account the roles/run.developer IAM role.",B:"Configure a cron job on the Compute Engine VMs to trigger Cloud Run on schedule.",C:"Change the Cloud Run ingress setting to 'Internal and Cloud Load Balancing.'",D:"Use Cloud Scheduler with Pub/Sub to invoke Cloud Run."},correctAnswer:["D"],explanation:"Cloud Run services with 'Internal' ingress can only be invoked by resources within the same VPC network or by specific authorized Google Cloud services like Cloud Tasks or Pub/Sub via push subscriptions configured with appropriate authentication. Cloud Scheduler cannot directly invoke an internal Cloud Run service. The standard pattern for scheduling internal Cloud Run services is to have Cloud Scheduler publish a message to a Pub/Sub topic. Create a Pub/Sub push subscription targeting the Cloud Run service's HTTPS endpoint, configured to use the service account with the `run.invoker` role for authentication (D). Granting broader roles (A) or changing ingress (C) doesn't solve the invocation path issue. Triggering from GCE (B) works but uses GCE instead of a managed scheduler.",conditions:["Schedule tasks on Cloud Run service with Internal ingress, getting 403 error from scheduler."],caseStudyContext:null},{id:269,topic:"Databases",question:"You work on an application that relies on Cloud Spanner as its main datastore. New application features have occasionally caused performance regressions. You want to prevent performance issues by running an automated performance test with Cloud Build for each commit made. If multiple commits are made at the same time, the tests might run concurrently. What should you do?",options:{A:"Create a new project with a random name for every build. Load the required data. Delete the project after the test is run.",B:"Create a new Cloud Spanner instance for every build. Load the required data. Delete the Cloud Spanner instance after the test is run.",C:"Create a project with a Cloud Spanner instance and the required data. Adjust the Cloud Build build file to automatically restore the data to its previous state after the test is run.",D:"Start the Cloud Spanner emulator locally. Load the required data. Shut down the emulator after the test is run."},correctAnswer:["B"],explanation:"Performance testing requires an environment that closely resembles production, ruling out the emulator (D) which has limitations. Tests need isolation, especially when run concurrently. Creating a completely new project (A) is excessive overhead. Reusing a single instance and restoring data (C) creates contention and makes concurrent runs unreliable. Creating a dedicated, ephemeral Cloud Spanner *instance* for each build (B) provides the necessary isolation for concurrent performance tests against a real Spanner environment. While potentially costly, it's the most reliable way to get accurate, isolated performance results.",conditions:["Automated performance testing for Spanner app in Cloud Build per commit, handle concurrent tests."],caseStudyContext:null},{id:270,topic:"Compute",question:"Your company's security team uses Identity and Access Management (IAM) to track which users have access to which resources. You need to create a version control system that can integrate with your security team's processes. You want your solution to support fast release cycles and frequent merges to your main branch to minimize merge conflicts. What should you do?",options:{A:"Create a Cloud Source Repositories repository, and use trunk-based development.",B:"Create a Cloud Source Repositories repository, and use feature-based development.",C:"Create a GitHub repository, mirror it to a Cloud Source Repositories repository, and use trunk-based development.",D:"Create a GitHub repository, mirror it to a Cloud Source Repositories repository, and use feature-based development."},correctAnswer:["A"],explanation:"The requirements are IAM integration and support for fast cycles/frequent merges (trunk-based development). Cloud Source Repositories (CSR) integrates directly with Google Cloud IAM for access control (A, B). Trunk-based development involves developers merging small changes frequently into the main ('trunk') branch, minimizing merge conflicts and enabling fast cycles (A, C). Combining CSR for IAM integration and trunk-based development (A) directly meets the requirements. Using GitHub and mirroring (C, D) adds complexity and potential delays, although IAM can still control CSR access. Feature-based development (B, D) typically involves longer-lived branches, counter to the frequent merge requirement.",conditions:["Create VCS integrated with GCP IAM, support fast release cycles/frequent merges."],caseStudyContext:null},{id:271,topic:"Compute",question:`You recently developed an application that monitors a large number of stock prices. You need to configure Pub/Sub to receive messages and update the current stock price in an in-memory database. A downstream service needs the most up-to-date prices in the in-memory database to perform stock trading transactions. Each message contains three pieces or information:

 Stock symbol
 Stock price
 Timestamp for the update

How should you set up your Pub/Sub subscription?`,options:{A:"Create a push subscription with exactly-once delivery enabled.",B:"Create a pull subscription with both ordering and exactly-once delivery turned off.",C:"Create a pull subscription with ordering enabled, using the stock symbol as the ordering key.",D:"Create a push subscription with both ordering and exactly-once delivery turned off."},correctAnswer:["D"],explanation:"The downstream service needs the *most up-to-date* price. This implies that processing older messages for the same stock symbol is unnecessary or even harmful. Therefore, message ordering (C) is not required and might even introduce delays if an older message blocks newer ones for the same key. Exactly-once delivery (A) adds overhead and is only available for pull subscriptions; since only the latest price matters, at-least-once delivery is acceptable. A push subscription (D) delivers messages with low latency as they arrive, fitting the need for up-to-date prices, without the overhead of ordering or exactly-once guarantees.",conditions:["Pub/Sub subscription for stock price updates, downstream needs most up-to-date price for trading."],caseStudyContext:null},{id:272,topic:"Databases",question:"You are a developer at a social media company. The company runs their social media website on-premises and uses MySQL as a backend to store user profiles and user posts. Your company plans to migrate to Google Cloud, and your learn will migrate user profile information to Firestore. You are tasked with designing the Firestore collections. What should you do?",options:{A:"Create one root collection for user profiles, and create one root collection for user posts.",B:"Create one root collection for user profiles, and create one subcollection for each user's posts.",C:"Create one root collection for user profiles, and store each user's post as a nested list in the user profile document.",D:"Create one root collection for user posts, and create one subcollection for each user's profile."},correctAnswer:["B"],explanation:"Firestore uses a collection/document model. User profiles fit naturally into a root collection (`users`) where each document represents a user (`users/{userId}`). User posts have a one-to-many relationship with users. Storing posts in a subcollection under the corresponding user document (`users/{userId}/posts/{postId}`) (B) is the standard Firestore pattern for this. It allows efficient retrieval of all posts for a specific user. Separate root collections (A) make relating posts to users harder. Nested lists (C) are limited by document size. Storing profiles under posts (D) is illogical.",conditions:["Migrate MySQL user profiles/posts to Firestore, design collections."],caseStudyContext:null},{id:273,topic:"Monitoring & Logging",question:"Your team has created an application that is hosted on a Google Kubemetes Engine (GKE) cluster. You need to connect the application to a legacy REST service that is deployed in two GKE clusters in two different regions. You want to connect your application to the legacy service in a way that is resilient and requires the fewest number of steps. You also want to be able to run probe-based health checks on the legacy service on a separate port. How should you set up the connection? (Choose two.)",options:{A:"Create a Cloud Function that consumes the Monitoring API. Create a schedule to trigger the Cloud Function hourly and alert you if the average memory consumption is outside the defined range.",B:"In Cloud Monitoring, create an alerting policy to notify you if the average memory consumption is outside the defined range.",C:"Create a Cloud Function that runs on a schedule, executes kubectl top on all the workloads on the cluster, and sends an email alert if the average memory consumption is outside the defined range.",D:"Write a script that pulls the memory consumption of the instance at the OS level and sends an email alert if the average memory consumption is outside the defined range."},correctAnswer:["B"],explanation:"GKE clusters integrated with Cloud Operations automatically send container and pod metrics, including memory consumption, to Cloud Monitoring. The simplest way to get alerts based on these metrics is to create an alerting policy directly within Cloud Monitoring (B). You can define conditions based on the average memory consumption metric (e.g., `kubernetes.io/container/memory/used_bytes`), set the thresholds (20% and 80%), and configure notification channels. Using Cloud Functions (A, C) or custom scripts (D) adds unnecessary complexity for standard metric-based alerting.",conditions:["Alert when average GKE container memory consumption is <20% or >80%."],caseStudyContext:null},{id:274,topic:"Compute",question:"You manage a microservice-based ecommerce platform on Google Cloud that sends confirmation emails to a third-party email service provider using a Cloud Function. Your company just launched a marketing campaign, and some customers are reporting that they have not received order confirmation emails. You discover that the services triggering the Cloud Function are receiving HTTP 500 errors. You need to change the way emails are handled to minimize email loss. What should you do?",options:{A:"Increase the Cloud Function's timeout to nine minutes.",B:"Configure the sender application to publish the outgoing emails in a message to a Pub/Sub topic. Update the Cloud Function configuration to consume the Pub/Sub queue.",C:"Configure the sender application to write emails to Memorystore and then trigger the Cloud Function. When the function is triggered, it reads the email details from Memorystore and sends them to the email service.",D:"Configure the sender application to retry the execution of the Cloud Function every one second if a request fails."},correctAnswer:["B"],explanation:"Receiving HTTP 500 errors from the Cloud Function suggests it might be failing intermittently, possibly due to issues calling the third-party email service or internal problems, especially under load from the marketing campaign. Directly calling the function synchronously means failures can lead to lost emails. Decoupling the sending service from the email function using Pub/Sub (B) provides resilience. The sender publishes the email request to a topic. The Cloud Function becomes a Pub/Sub-triggered function, consuming messages from the topic's subscription. Pub/Sub automatically handles retries if the function fails temporarily, minimizing email loss. Increasing timeout (A), using Memorystore (C), or simple retries (D) don't provide the same level of decoupling and resilience.",conditions:["Cloud Function sends emails via 3rd-party API, triggering services get 500 errors from function, need to minimize email loss."],caseStudyContext:null},{id:275,topic:"Compute",question:"You have a web application that publishes messages to Pub/Sub. You plan to build new versions of the application locally and need to quickly test Pub/Sub integration for each new build. How should you configure local testing?",options:{A:"In the Google Cloud console, navigate to the API Library, and enable the Pub/Sub API. When developing locally configure your application to call pubsub.googleapis.com.",B:"Install the Pub/Sub emulator using gcloud, and start the emulator with a valid Google Project ID. When developing locally, configure your applicat.cn to use the local emulator by exporting the PUBSUB_EMULATOR_HOST variable.",C:"Run the gcloud config set api_endpoint_overrides/pubsub https://pubsubemulator.googleapis.com.com/ command to change the Pub/Sub endpoint prior to starting the application.",D:"Install Cloud Code on the integrated development environment (IDE). Navigate to Cloud APIs, and enable Pub/Sub against a valid Google Project IWhen developing locally, configure your application to call pubsub.googleapis.com."},correctAnswer:["B"],explanation:"This is similar to Q261. To test Pub/Sub integration locally quickly and reliably, use the Pub/Sub emulator. Install it via gcloud. Start the emulator. Configure your local application to connect to the emulator endpoint, typically by setting the `PUBSUB_EMULATOR_HOST` environment variable (B). This directs the Pub/Sub client library calls to the local emulator instead of the live service. Calling the real API (A, D) is not local testing. Setting gcloud config overrides (C) affects gcloud itself, not necessarily the application's client library.",conditions:["Test Pub/Sub publishing integration locally for new app builds."],caseStudyContext:null},{id:276,topic:"Databases",question:`You recently developed an application that monitors a large number of stock prices. You need to configure Pub/Sub to receive a high volume messages and update the current stock price in a single large in-memory database. A downstream service needs the most up-to-date prices in the in-memory database to perform stock trading transactions. Each message contains three pieces or information:
 Stock symbol
 Stock price
 Timestamp for the update

How should you set up your Pub/Sub subscription?`,options:{A:"Create a pull subscription with exactly-once delivery enabled.",B:"Create a push subscription with both ordering and exactly-once delivery turned off.",C:"Create a push subscription with exactly-once delivery enabled.",D:"Create a pull subscription with both ordering and exactly-once delivery turned off."},correctAnswer:["B"],explanation:"The critical requirement is updating an in-memory DB with the *most up-to-date* price for low-latency downstream access. Ordering messages per stock (C) can delay updates if an old message blocks newer ones. Exactly-once delivery (A, C) adds overhead and is only available for pull; it's unnecessary if overwriting with the latest price is acceptable. A push subscription (B, C) minimizes latency by sending messages directly as they arrive. Since ordering and exactly-once aren't strictly needed and low latency is key, a simple push subscription with default at-least-once delivery and no ordering (B) is the most appropriate fit.",conditions:["Pub/Sub for high-volume stock prices, update in-memory DB, downstream needs latest price, low latency important."],caseStudyContext:null},{id:277,topic:"Networking",question:"Your team has created an application that is hosted on a Google Kubemetes Engine (GKE) cluster. You need to connect the application to a legacy REST service that is deployed in two GKE clusters in two different regions. You want to connect your application to the legacy service in a way that is resilient and requires the fewest number of steps. You also want to be able to run probe-based health checks on the legacy service on a separate port. How should you set up the connection? (Choose two.)",options:{A:"Use Traffic Director with a sidecar proxy to connect the application to the service.",B:"Set up a proxyless Traffic Director configuration for the application.",C:"Configure the legacy service's firewall to allow health checks originating from the sidecar proxy.",D:"Configure the legacy service's firewall to allow health checks originating from the application.",E:"Configure the legacy service's firewall to allow health checks originating from the Traffic Director control plane."},correctAnswer:["A","C"],explanation:"This is identical to Q246. Traffic Director provides resilient, multi-region routing. Using sidecar proxies (A) allows Traffic Director to manage traffic to the legacy REST services. Traffic Director configures health checks, which are executed by the sidecar proxies (or associated Google health checkers). Therefore, the firewalls for the legacy service clusters must allow ingress traffic for these health checks on the specified port, originating from the proxies/health checkers (C).",conditions:["Connect GKE app to legacy REST service in 2 GKE clusters (diff regions), resilient connection, health checks on separate port, fewest steps."],caseStudyContext:null},{id:278,topic:"Monitoring & Logging",question:"You are monitoring a web application that is written in Go and deployed in Google Kubernetes Engine. You notice an increase in CPU and memory utilization. You need to determine which function is consuming the most CPU and memory resources. What should you do?",options:{A:"Add print commands to the application source code to log when each function is called, and redeploy the application.",B:"Create a Cloud Logging query that gathers the web application s logs. Write a Python script that calculates the difference between the timestamps from the beginning and the end of the application's longest functions to identify time-intensive functions.",C:"Import OpenTelemetry and Trace export packages into your application, and create the trace provider. Review the latency data for your application on the Trace overview page, and identify which functions cause the most latency.",D:"Import the Cloud Profiler package into your application, and initialize the Profiler agent. Review the generated flame graph in the Google Cloud console to identify time-intensive functions."},correctAnswer:["D"],explanation:"This is identical to Q206. Cloud Profiler is the tool for identifying which specific functions within an application (including Go) are consuming CPU and memory. Instrumenting the app with the Profiler agent and analyzing the resulting flame graphs (D) provides this function-level insight.",conditions:["Go app on GKE, increased CPU/memory usage, need to find resource-intensive functions."],caseStudyContext:null},{id:279,topic:"Compute",question:`You are developing a flower ordering application. Currently you have three microservices:
 Order Service (receives the orders)
 Order Fulfillment Service (processes the orders)
 Notification Service (notifies the customer when the order is filled)

You need to determine how the services will communicate with each other. You want incoming orders to be processed quickly and you need to collect order information for fulfillment. You also want to make sure orders are not lost between your services and are able to communicate asynchronously. How should the requests be processed?`,options:{A:"[Diagram: Order request -> Order Service -> Order Fulfillment Service -> Notification Service]",B:"[Diagram: Order request -> Order Service -> Pub/Sub -> Order Fulfillment Service -> Notification Service]",C:"[Diagram: Order request -> Order Service -> Order Fulfillment Service -> Pub/Sub -> Notification Service]",D:"[Diagram: Order request -> Order Service -> Pub/Sub queue -> Order Fulfillment Service -> Firestore Database -> Pub/Sub queue -> Notification Service]"},correctAnswer:["D"],explanation:"The requirements are: process orders quickly (implies decoupling Order Service), collect info for fulfillment, ensure orders aren't lost, asynchronous communication. Option D provides the best decoupling and resilience: Order Service receives request, quickly publishes an 'order received' event to Pub/Sub. Fulfillment Service subscribes, processes the order (persisting details likely needed, e.g., to Firestore), and then publishes an 'order fulfilled' event to another Pub/Sub topic. Notification Service subscribes to the 'fulfilled' topic to send the notification. This makes the flow asynchronous, resilient (Pub/Sub retains messages), and decoupled. Options A, B, C show less robust or incomplete flows.",conditions:["Microservices communication (Order, Fulfillment, Notification), process quickly, no lost orders, asynchronous."],caseStudyContext:null}],Vg={quizTitle:nd,quizItems:id},Kg=Object.freeze(Object.defineProperty({__proto__:null,default:Vg,quizItems:id,quizTitle:nd},Symbol.toStringTag,{value:"Module"})),Dl=Object.assign({"./gcp-ace-data.json":Hg,"./gcp-dev-data.json":Kg}),Qg=(e,t)=>{if(!t||typeof t!="object")return console.error(`Invalid data structure for course ID derived as '${e}'. Expected an object. Skipping.`),null;Array.isArray(t.quizItems)||(console.warn(`'quizItems' is not an array or is missing for course ID '${e}'. Assuming no questions.`),t.quizItems=[]);const o={};if(t.caseStudies&&typeof t.caseStudies=="object"&&t.caseStudies!==null){for(const r in t.caseStudies)if(Object.prototype.hasOwnProperty.call(t.caseStudies,r)){const s=t.caseStudies[r];s&&typeof s.caseStudyId=="string"&&typeof s.caseStudyText=="string"&&s.caseStudyId===r?o[r]={caseStudyId:s.caseStudyId,caseStudyText:s.caseStudyText}:console.warn(`Invalid or mismatched case study item structure for key '${r}' in course ID '${e}'. Skipping this case study. Expected caseStudyId to match key.`)}}else console.warn(`'caseStudies' is not a valid object or is missing/null for course ID '${e}'. Defaulting to empty object.`);const n=t.quizItems.map((r,s)=>({...r,id:r.id??`${e}-q${s}`})),i=[...new Set(n.map(r=>r.topic).filter(r=>r!=null))];i.sort();const a=e.replace(/-/g," ").replace(/\b\w/g,r=>r.toUpperCase());return{id:e,title:t.quizTitle||a,quizItems:n,topics:i,caseStudies:o}};let jn=null;const ad=()=>{var t;if(jn)return jn;const e=[];for(const o in Dl){const n=o.match(/\.\/(.*?)-data\.json$/),i=n?n[1]:o,a=(t=Dl[o])==null?void 0:t.default;if(a){const r=Qg(i,a);r&&e.push(r)}else console.warn(`Could not load data from module: ${o}`)}return e.sort((o,n)=>o.title.localeCompare(n.title)),jn=e,jn},rd=()=>ad(),wn=e=>e?ad().find(o=>o.id===e):void 0,Fg=(e,t)=>{const o=wn(e);return!o||!Array.isArray(o.quizItems)?[]:t?o.quizItems.filter(n=>n.topic===t):o.quizItems},Si=(e,t)=>{if(!e)return;const o=wn(e);if(!(!o||!Array.isArray(o.quizItems)))return o.quizItems.find(n=>String(n.id)===String(t))},sd=(e,t)=>{if(!e||!t)return;const o=wn(e);if(!(!o||typeof o.caseStudies!="object"||o.caseStudies===null))return o.caseStudies[t]},_g=({inProgressQuizzes:e,completedQuizHistory:t,getQuizStateKey:o,refreshQuizStates:n})=>{const i=Yi(),a=rd(),r=(s,l)=>{const c=l??"full";console.log(`Navigating to quiz: /quiz/${s}/${c}`),i(`/quiz/${s}/${c}`)};return g.jsx(Og,{courses:a,onSelectQuiz:r,inProgressQuizzes:e,completedQuizHistory:t,getQuizStateKey:o,onRefreshStates:n,passThreshold:80})},ld="quizState_",Ft=e=>{if(!e||!e.courseId)return null;const o=(e.topicId===null?"full":e.topicId).replace(/[^a-zA-Z0-9_-]/g,"_");return`${ld}${e.courseId}_${o}`},cd=()=>{const[e,t]=A.useState({}),o=A.useCallback(()=>{if(typeof window>"u")return{};console.log("Reloading all in-progress states from localStorage...");const s={};return rd().forEach(c=>{const h=Ft({courseId:c.id,topicId:null});if(h)try{const m=localStorage.getItem(h);if(m){const f=JSON.parse(m);f&&typeof f.currentIndex=="number"&&Array.isArray(f.userAnswers)?s[h]=f:(console.warn(`Invalid state structure found for key ${h}, removing.`),localStorage.removeItem(h))}}catch(m){console.error(`Error parsing ${h}`,m),localStorage.removeItem(h)}(c.topics||[]).forEach(m=>{if(!m)return;const f=Ft({courseId:c.id,topicId:m});if(f)try{const v=localStorage.getItem(f);if(v){const w=JSON.parse(v);w&&typeof w.currentIndex=="number"&&Array.isArray(w.userAnswers)?s[f]=w:(console.warn(`Invalid state structure found for key ${f}, removing.`),localStorage.removeItem(f))}}catch(v){console.error(`Error parsing ${f}`,v),localStorage.removeItem(f)}})}),console.log("Finished reloading states:",s),s},[]);A.useEffect(()=>{t(o())},[o]);const n=A.useCallback((s,l)=>{const c=Ft(s);if(!(!c||typeof window>"u"))if(l===null)localStorage.removeItem(c),t(h=>{const m={...h};return delete m[c],m}),console.log(`Cleared state for ${c}`);else{const h=l.userAnswers.map(f=>f instanceof Set?Array.from(f):f),m={...l,userAnswers:h,courseId:s.courseId,topicId:s.topicId};localStorage.setItem(c,JSON.stringify(m)),t(f=>({...f,[c]:l})),console.log(`Saved state for ${c}:`,m)}},[]),i=A.useCallback(s=>{const l=Ft(s);if(!l||typeof window>"u")return null;const c=localStorage.getItem(l);if(c===null)return console.log(`No saved state found for key ${l}`),null;try{const h=JSON.parse(c);if(h&&typeof h.currentIndex=="number"&&Array.isArray(h.userAnswers)&&Array.isArray(h.questionIds)&&h.questionIds.length===h.userAnswers.length){const m=h.userAnswers.map((w,S)=>{if(!h.questionIds||S>=h.questionIds.length)return console.warn(`Index ${S} out of bounds for questionIds in saved state for key ${l}`),null;const T=Si(s.courseId,h.questionIds[S]),d=(T==null?void 0:T.correctAnswer)??[];return Array.isArray(d)&&d.length>1&&Array.isArray(w)?new Set(w):typeof w=="string"?w:null}),f=h.checkedAnswers&&h.checkedAnswers.length===h.questionIds.length?h.checkedAnswers:Array(h.questionIds.length).fill(!1),v={...h,userAnswers:m,checkedAnswers:f};return console.log(`Loaded state for key ${l}:`,v),v}throw new Error(`Invalid state structure for key ${l}`)}catch(h){return console.error(`Error loading or parsing quiz state for ${l}:`,h),localStorage.removeItem(l),null}},[Ft]),a=A.useCallback(()=>{typeof window>"u"||(Object.keys(localStorage).forEach(s=>{s.startsWith(ld)&&localStorage.removeItem(s)}),localStorage.removeItem("quizResults"),t({}),console.log("Cleared all in-progress quiz states."))},[]),r=A.useCallback(()=>{t(o())},[o]);return{inProgressQuizzes:e,saveQuizState:n,loadQuizState:i,clearAllQuizStates:a,getQuizStateKey:Ft,refreshInProgressStates:r}},$g=(e=[])=>Array.isArray(e)&&e.length>1,Jg=(e,t=[])=>Array.isArray(t)&&t.includes(e),Xg=({questionData:e,onAnswer:t,onCheckAnswer:o,userAnswer:n,showFeedback:i,isChecked:a,answerStatus:r,questionIndex:s,totalQuestionsInQuiz:l,courseId:c})=>{const{question:h,options:m={},correctAnswer:f=[],explanation:v,topic:w,id:S,conditions:T,caseStudyId:d}=e??{},[u,p]=A.useState(void 0),y=$g(f),[b,k]=A.useState(!1),[P,I]=A.useState(!1);A.useEffect(()=>{if(d&&c){const Y=sd(c,d);p(Y)}else p(void 0)},[d,c]);const q=i&&!y||a;A.useEffect(()=>{q||k(!1)},[q]);const E=Y=>{if(q)return;const{value:de,checked:Ue,type:L}=Y.target;if(L==="checkbox"){const we=n instanceof Set?n:new Set,C=new Set(we);Ue?C.add(de):C.delete(de),t(s,C)}else t(s,de)},te=()=>{o&&o(s)},Xe=Y=>{if(!Y)return null;const de=/(```[\s\S]*?```)/g;return Y.split(de).map((L,we)=>{if(L.startsWith("```")&&L.endsWith("```")){const C=L.substring(3,L.length-3).trim();return g.jsx("pre",{className:"my-3 p-3 bg-gray-100 border rounded overflow-x-auto",children:g.jsx("code",{className:"text-sm font-mono",children:C})},`code-${we}`)}else return g.jsx("span",{style:{whiteSpace:"pre-line"},children:L},`text-${we}`)})};return g.jsxs("div",{className:`question-container bg-white shadow-md rounded-lg p-5 mb-6 border-t-4 ${q&&r==="correct"?"border-green-500":""} ${q&&r==="incorrect"?"border-red-500":""} ${q?"":"border-slate-200"}`,"data-question-id":S,children:[g.jsxs("div",{className:"flex justify-end mb-3",children:[" ",d&&g.jsx("button",{className:"text-xs bg-blue-100 text-blue-700 px-3 py-1 rounded-full hover:bg-blue-200 focus:outline-none focus:ring-2 focus:ring-blue-400 focus:ring-opacity-50",onClick:()=>I(Y=>!Y),children:P?"Hide Details":"View Case Study"})]}),P&&u&&u.caseStudyText&&g.jsxs("div",{className:"context-box question-context mb-4 p-3 bg-slate-50 border rounded text-sm text-slate-700 animate-fadeIn",children:[" ",g.jsx("strong",{children:"Case Study:"}),g.jsx("div",{dangerouslySetInnerHTML:{__html:u.caseStudyText}})]}),P&&T&&T.length>0&&g.jsxs("div",{className:"question-conditions mb-4 p-3 bg-slate-50 border rounded text-sm animate-fadeIn",children:[" ",g.jsx("strong",{className:"text-slate-800",children:"Conditions:"}),g.jsx("ul",{className:"list-disc list-inside mt-1 text-slate-700",children:T.map((Y,de)=>g.jsx("li",{children:Y},de))})]}),g.jsx("h4",{className:"question-title text-xs font-semibold text-slate-500 uppercase tracking-wide mb-3 pb-2 border-b",children:`Question ${s+1} / ${l} | Topic: ${w||"General"}`}),g.jsx("div",{className:"question-text text-slate-800 leading-relaxed",children:Xe(h)}),y&&!q&&g.jsx("p",{className:"select-all-notice text-sm italic text-slate-500 my-3",children:"(Select all that apply)"}),g.jsxs("div",{className:"options-container mt-4 space-y-3",children:[Object.entries(m).map(([Y,de])=>{const Ue=y?n instanceof Set&&n.has(Y):n===Y;let L="option-label block border rounded-md p-3 cursor-pointer transition duration-150 ease-in-out ",we=null;const C=Jg(Y,f);return L+="border-slate-300 bg-white ",q||(L+="hover:bg-slate-50 hover:border-slate-400 "),q&&(L+="cursor-default ",Ue&&C?(L+=" border-green-500 bg-green-50 ring-1 ring-green-500",we=g.jsx("span",{className:"feedback-icon correct text-green-600 font-bold ml-auto pl-2",children:"  Correct"})):Ue&&!C?(L+=" border-red-500 bg-red-50 ring-1 ring-red-500",we=g.jsx("span",{className:"feedback-icon incorrect text-red-600 font-bold ml-auto pl-2",children:"  Incorrect"})):!Ue&&C?L+="border-green-500 border-dashed bg-green-50":L+="border-slate-300 opacity-60"),g.jsx("div",{className:L,children:g.jsxs("label",{className:"flex items-center cursor-pointer w-full",children:[" ",g.jsx("input",{type:y?"checkbox":"radio",name:`question_${S||s}`,value:Y,checked:Ue||!1,onChange:E,disabled:q,className:`form-${y?"checkbox":"radio"} ${q?"cursor-default opacity-70":"cursor-pointer"} text-emerald-600 border-gray-300 rounded focus:ring-emerald-500`}),g.jsxs("span",{className:`option-key font-semibold mx-2 ${q?"opacity-70":""}`,children:[Y,")"]}),g.jsx("span",{className:`option-value flex-grow ${q?"opacity-70":""}`,style:{whiteSpace:"pre-line"},children:de}),we]})},Y)}),!Object.keys(m).length&&g.jsx("p",{className:"text-red-600",children:"Error: No options found for this question."})]}),y&&!a&&o&&g.jsx("div",{className:"mt-4 text-right",children:g.jsx($,{variant:"primary",onClick:te,disabled:!(n instanceof Set&&n.size>0),children:"Check Answer"})}),q&&(typeof v=="string"&&v.trim().length>0||typeof v=="object"&&v!==null&&(v.correct||v.incorrect))&&g.jsxs("div",{className:"mt-4   pt-3",children:[g.jsxs($,{variant:"link",size:"sm",onClick:()=>k(!b),className:"text-blue-600 hover:text-blue-800",children:[b?"Hide":"Show"," Explanation "]}),b&&g.jsxs("div",{className:"explanation-text mt-2 p-4 bg-slate-50 border border-slate-200 rounded text-sm text-slate-700 leading-normal animate-fadeIn",children:[f.length>0&&g.jsx("p",{className:"mb-2",children:g.jsxs("strong",{children:["Correct Answer(s): ",f.join(", ")]})}),typeof v=="object"&&v!==null?g.jsxs("div",{children:[v.correct&&g.jsxs("div",{style:{whiteSpace:"pre-line",marginBottom:"0.5em"},children:[g.jsx("strong",{children:"Explanation (Correct):"})," ",g.jsx("i",{children:v.correct})]}),v.incorrect&&typeof v.incorrect=="object"&&g.jsxs("div",{children:[g.jsx("strong",{children:"Incorrect Options:"}),g.jsx("ul",{style:{marginTop:"0.25em"},children:Object.entries(v.incorrect).map(([Y,de])=>g.jsxs("li",{children:[g.jsxs("b",{children:[Y,":"]})," ",g.jsx("i",{children:String(de)})]},Y))})]})]}):g.jsx("div",{style:{whiteSpace:"pre-line"},children:g.jsx("i",{children:v})})]})]})]})},Zg=({progress:e,label:t,barColor:o="bg-emerald-500",backgroundColor:n="bg-slate-200",height:i="h-3"})=>{const a=Math.max(0,Math.min(100,e));return g.jsxs("div",{className:"progress-bar-container w-full my-2",children:[t&&g.jsxs("div",{className:"flex justify-between mb-1",children:[g.jsx("span",{className:"text-xs font-medium text-slate-700",children:t}),g.jsxs("span",{className:"text-xs font-medium text-emerald-700",children:[a.toFixed(0),"%"]})]}),g.jsx("div",{className:`w-full ${n} rounded-full ${i} overflow-hidden`,children:g.jsx("div",{className:`${o} ${i} rounded-full transition-all duration-300 ease-out`,style:{width:`${a}%`},role:"progressbar","aria-valuenow":a,"aria-valuemin":0,"aria-valuemax":100})}),!t&&g.jsx("div",{className:"text-right mt-1",children:g.jsxs("span",{className:"text-xs font-medium text-emerald-700",children:[a.toFixed(0),"%"]})})]})},em=({quizConfig:e,onComplete:t,onBack:o})=>{const{courseId:n,topicId:i}=e,{saveQuizState:a,loadQuizState:r,getQuizStateKey:s}=cd(),[l,c]=A.useState([]),[h,m]=A.useState(0),[f,v]=A.useState([]),[w,S]=A.useState(null),[T,d]=A.useState(!1),[u,p]=A.useState([]),[y,b]=A.useState([]),[k,P]=A.useState(!1),I=A.useMemo(()=>wn(n),[n]),q=A.useMemo(()=>s(e),[e,s]);A.useEffect(()=>{var G;if(console.log(`Quiz effect running. Key: ${q}, CourseID: ${n}, TopicID: ${i}`),!q||!n){console.warn("Quiz effect: quizKey or courseId is missing, cannot load/initialize."),P(!0),c([]);return}const C=r(e);if(C&&Array.isArray(C.questionIds)&&C.questionIds.length>0){console.log("Loading saved state:",C);const D=C.questionIds.map(Z=>Si(n,Z)).filter(Z=>Z!==void 0);D.length!==C.questionIds.length&&console.warn("Mismatch between saved question IDs and available questions. Resetting quiz.");const z=C.userAnswers.map((Z,Pe)=>{const se=D[Pe];return se?E(se.correctAnswer||[])&&Array.isArray(Z)?new Set(Z):Z:null}),M=C.checkedAnswers&&C.checkedAnswers.length===D.length?C.checkedAnswers:Array(D.length).fill(!1),O=z.map((Z,Pe)=>{const se=D[Pe];return!se||E(se.correctAnswer||[])||Z===null?null:(se.correctAnswer||[]).includes(Z)?"correct":"incorrect"});c(D),m(C.currentIndex<D.length?C.currentIndex:0),v(z),S(C.startTime??Date.now()),b(M),p(O);const re=C.currentIndex<D.length?C.currentIndex:0;d(re<O.length&&O[re]!==null&&re<D.length&&!E(((G=D[re])==null?void 0:G.correctAnswer)||[]))}else{console.log("Initializing new quiz state...");const M=[...Fg(n,i)].sort(()=>.5-Math.random());c(M);const O=M.length;v(Array(O).fill(null)),p(Array(O).fill(null)),b(Array(O).fill(!1)),m(0),S(Date.now()),d(!1)}P(!0)},[e,n,i,r,s]),A.useEffect(()=>{if(!k||!q||w===null||l.length===0)return;const C=f.map(D=>D instanceof Set?Array.from(D):D),G={currentIndex:h,userAnswers:C,startTime:w,questionIds:l.map(D=>D.id),courseId:n,topicId:i,checkedAnswers:y};console.log(`Save effect: Saving state for key ${q}`),a(e,G)},[f,h,l,w,q,a,e,k,y,n,i]);const E=(C=[])=>Array.isArray(C)&&C.length>1,te=A.useCallback((C,G)=>{if(!k)return;v(z=>{const M=[...z];return M[C]=G,M});const D=l[C];if(D&&!E(D.correctAnswer||[])){const z=(D.correctAnswer||[]).includes(G);p(M=>{const O=[...M];return O[C]=z?"correct":"incorrect",O}),d(!0)}else p(z=>{const M=[...z];return M[C]=null,M}),b(z=>{const M=[...z];return M[C]=!1,M}),d(!1)},[l,k]),Xe=A.useCallback(C=>{if(!k)return;const G=l[C],D=f[C];let z=!1;if(G&&D instanceof Set){const M=G.correctAnswer||[];z=M.length===D.size&&M.every(O=>D.has(O))}p(M=>{const O=[...M];return O[C]=z?"correct":"incorrect",O}),b(M=>{const O=[...M];return O[C]=!0,O}),d(!1)},[l,f,k]),Y=()=>{d(!1);const C=h+1;if(C<l.length){m(C);const G=l[C],D=u[C];G&&!E(G.correctAnswer||[])&&D!==null&&d(!0)}},de=()=>{d(!1);const C=h-1;if(C>=0){m(C);const G=l[C],D=u[C];G&&!E(G.correctAnswer||[])&&D!==null&&d(!0)}},Ue=()=>{if(w===null||l.length===0||!k)return;const C=Date.now(),G=Math.round((C-w)/1e3);let D=0;const z=l.map((M,O)=>{let re=!1;const Z=f[O],Pe=M.options||{},se=M.correctAnswer||[];return E(se)?Z instanceof Set&&(re=se.length===Z.size&&se.every(Ni=>Z.has(Ni))):re=se.includes(Z??""),re&&D++,{questionId:M.id,userAnswer:Z,correctAnswer:se,isCorrect:re,options:Pe}});q&&a(e,null),t({score:D,totalQuestions:l.length,answers:z,duration:G,questionIds:l.map(M=>M.id),courseId:n,topicId:i,timestamp:Date.now(),percentage:l.length>0?D/l.length*100:0})},L=l[h];if(!k)return g.jsx("div",{className:"text-center p-8",children:"Loading quiz state..."});if(l.length===0)return g.jsxs("div",{className:"text-center p-8",children:[g.jsx("p",{className:"mb-4",children:"No questions available for this selection."}),g.jsx($,{onClick:o,variant:"secondary",children:"Back to Selection"})]});if(!L)return console.error("Error: currentQuestionData is undefined at index",h),g.jsxs("div",{className:"text-center p-8",children:[g.jsx("p",{className:"mb-4 text-red-600",children:"Error loading current question."}),g.jsx($,{onClick:o,variant:"secondary",children:"Back to Selection"})]});const we=(h+1)/l.length*100;return g.jsxs("div",{className:"quiz-container max-w-3xl mx-auto p-4 md:p-6 bg-white rounded-lg shadow-lg border border-slate-200",children:[g.jsxs("div",{className:"quiz-header mb-6",children:[g.jsxs("h2",{className:"text-xl md:text-2xl font-semibold text-center text-slate-800 mb-3",children:[(I==null?void 0:I.title)||"Quiz"," ",i?`(${i})`:""]}),g.jsx(Zg,{progress:we})]}),L&&g.jsx(Xg,{questionData:L,onAnswer:te,onCheckAnswer:E(L.correctAnswer||[])?Xe:void 0,userAnswer:f[h],showFeedback:T,isChecked:y[h]||!1,answerStatus:u[h]||null,questionIndex:h,totalQuestionsInQuiz:l.length,courseId:n},L.id||h),g.jsxs("div",{className:"navigation-buttons mt-8 flex justify-between items-center",children:[g.jsx($,{onClick:de,disabled:h===0,variant:"secondary",children:"Previous"}),g.jsx("span",{className:"text-sm text-slate-600"}),h<l.length-1?g.jsx($,{onClick:Y,disabled:!E((L==null?void 0:L.correctAnswer)||[])&&f[h]===null||!E((L==null?void 0:L.correctAnswer)||[])&&!T||E((L==null?void 0:L.correctAnswer)||[])&&!y[h],variant:"primary",children:"Next"}):g.jsx($,{onClick:Ue,disabled:!E((L==null?void 0:L.correctAnswer)||[])&&f[h]===null||!E((L==null?void 0:L.correctAnswer)||[])&&!T||E((L==null?void 0:L.correctAnswer)||[])&&!y[h],variant:"success",children:"Finish Quiz"})]}),g.jsx("div",{className:"mt-6 text-center",children:g.jsxs($,{onClick:o,variant:"outline",size:"sm",children:[" ","Quit Quiz"]})})]})},tm=({onQuizCompleteForNav:e,onQuizExitForNav:t})=>{const{courseId:o,topicParam:n}=mg(),i=n==="full"?null:n??null,a=A.useMemo(()=>o?{courseId:o,topicId:i,type:i?"topic":"full"}:(console.error("QuizPage: courseId is missing from URL parameters."),null),[o,i]);return a?g.jsx(em,{quizConfig:a,onComplete:e,onBack:t}):g.jsx(Zu,{to:"/",replace:!0})};function Il(e,t){if(typeof window<"u")try{const o=localStorage.getItem(e);if(o!==null)return JSON.parse(o)}catch(o){console.warn(`Error reading localStorage key "${e}":`,o)}return t}function ud(e,t){const[o,n]=A.useState(()=>Il(e,t));A.useEffect(()=>{if(typeof window<"u")try{localStorage.setItem(e,JSON.stringify(o)),console.log(`Saved key '${e}' to localStorage:`,o)}catch(a){console.warn(`Error writing to localStorage key "${e}":`,a)}},[e,o]);const i=A.useCallback(()=>{console.log(`Refreshing value for key '${e}' from localStorage...`),n(Il(e,t))},[e,t]);return[o,n,i]}const om=({score:e,totalQuestions:t})=>{const o=e,n=t-e,i=t>0?e/t*100:0;return g.jsxs("div",{className:"result-chart-placeholder my-6 p-4 border rounded bg-slate-50 text-center",children:[g.jsx("h4",{className:"text-lg font-semibold mb-2",children:"Score Breakdown"}),g.jsxs("div",{className:"flex justify-around items-center text-sm mb-2",children:[g.jsxs("span",{children:["Correct: ",g.jsx("span",{className:"font-bold text-green-600",children:o})," "]}),g.jsxs("span",{children:["Incorrect: ",g.jsx("span",{className:"font-bold text-red-600",children:n})]})]}),g.jsx("div",{className:"w-full bg-gray-200 rounded-full h-2.5 dark:bg-gray-700 my-2",children:g.jsx("div",{className:"bg-green-500 h-2.5 rounded-full",style:{width:`${i}%`}})}),g.jsxs("span",{className:"text-sm font-medium",children:[i.toFixed(1),"%"]})]})},nm=({results:e,onRetry:t})=>{const[o]=ud("quizResults",[]),[n,i]=A.useState({}),{score:a,totalQuestions:r,answers:s,duration:l,questionIds:c=[],courseId:h,topicId:m}=e,f=r>0?a/r*100:0,v=A.useMemo(()=>wn(h),[h]),w=m,S=A.useMemo(()=>o.filter(u=>u.courseId===h&&u.topicId===m).sort((u,p)=>(p.timestamp||0)-(u.timestamp||0)),[o,h,m]);A.useEffect(()=>{(()=>{const p={};if(c&&h)for(const y of c){const b=Si(h,y);if(b!=null&&b.caseStudyId&&!p[b.caseStudyId]){const k=sd(h,b.caseStudyId);k&&(p[b.caseStudyId]=k)}}i(p)})()},[c,h]);const T=(u="")=>{if(!u)return"";const p=/```([\s\S]*?)```/g;return u.split(p).map((b,k)=>k%2===1?g.jsx("pre",{className:"my-2 p-2 bg-gray-100 border rounded overflow-x-auto text-xs",children:g.jsx("code",{children:b.trim()})},k):g.jsx("span",{style:{whiteSpace:"pre-line"},children:b},k))},d=(u,p)=>{if(u==null)return"Not Answered";if(!p||Object.keys(p).length===0)return"Options Missing/Invalid";let y;return u instanceof Set?y=Array.from(u):y=Array.isArray(u)?u:[u],y.length===0?"Not Answered":(y.sort(),y.map(b=>{const k=p[b]?`: ${p[b]}`:" (Option text missing)";return`${b}${k}`}).join("; "))};return g.jsxs("div",{className:"results-container max-w-4xl mx-auto p-4 md:p-6",children:[g.jsx("div",{className:"text-right mb-4",children:g.jsx($,{onClick:t,variant:"secondary",children:"Select New Quiz"})}),g.jsxs("h2",{className:"text-2xl font-bold text-center mb-4",children:["Quiz Results ",v!=null&&v.title?`for ${v.title}`:h?`for Course ${h}`:""," ",w?`(${w})`:"(Full Quiz)"]}),g.jsxs("div",{className:"text-center",children:[g.jsxs("p",{className:"results-summary text-xl my-3",children:["You scored ",g.jsx("strong",{children:a})," out of ",g.jsx("strong",{children:r})," (",f.toFixed(1),"%)"]}),g.jsxs("p",{className:"results-duration text-sm text-slate-500 mb-5",children:["Duration: ",l," seconds"]})]}),g.jsx(om,{score:a,totalQuestions:r}),g.jsx("h3",{className:"text-xl font-semibold mt-8 mb-4 border-b pb-2",children:"Detailed Results:"}),g.jsx("ul",{className:"detailed-results-list space-y-6",children:c&&c.length>0?c.map((u,p)=>{const y=Si(h,u),b=s[p],k=y!=null&&y.caseStudyId?n[y.caseStudyId]:void 0;if(!y||!b)return console.error(`Could not find question or result for ID/Index: ${u} at resultIndex ${p}`),g.jsxs("li",{className:"result-item error p-4 border border-red-300 bg-red-50 rounded",children:["Error displaying details for question ",u]},`error-${u}-${p}`);const{userAnswer:P,correctAnswer:I,isCorrect:q}=b;return g.jsxs("li",{className:`result-item border rounded-lg p-4 ${q?"bg-green-50 border-green-200":"bg-red-50 border-red-200"}`,children:[y.caseStudyId&&k&&k.caseStudyText&&g.jsxs("div",{className:"context-box question-context mb-3 p-2 bg-slate-100 border border-slate-200 rounded text-xs",children:[g.jsx("strong",{children:"Case Study:"}),g.jsx("div",{style:{whiteSpace:"pre-wrap"},dangerouslySetInnerHTML:{__html:k.caseStudyText.replace(/(\r\n\r\n|\n\n)/g,"<br/><br/>").replace(/(\r\n|\n)/g,"<br/>")}})]}),y.conditions&&y.conditions.length>0&&g.jsxs("div",{className:"question-conditions mb-3 p-2 bg-slate-100 border border-slate-200 rounded text-xs",children:[g.jsx("strong",{children:"Conditions:"}),g.jsx("ul",{className:"list-disc list-inside ml-4 mt-1",children:y.conditions.map((E,te)=>g.jsx("li",{children:E},te))})]}),g.jsx("div",{className:"result-question-title mb-2",children:g.jsxs("strong",{className:"text-base font-semibold",children:["Q",p+1," (",(y==null?void 0:y.topic)||"N/A","):"]})}),g.jsx("div",{className:"result-question-text text-sm mb-3 text-slate-700",children:T(y==null?void 0:y.question)}),b.options&&Object.keys(b.options).length>0&&g.jsxs("div",{className:"result-answers mt-2 pl-3 border-l-2 border-slate-300",children:[g.jsxs("p",{className:`user-answer text-sm mb-1 ${q?"text-green-700":"text-red-700"}`,children:["Your answer: ",d(P,b.options),q?g.jsx("span",{className:"feedback-icon correct ml-2",children:""}):g.jsx("span",{className:"feedback-icon incorrect ml-2",children:""})]}),!q&&g.jsxs("p",{className:"correct-answer text-sm font-medium text-green-800",children:["Correct answer(s): ",d(I,b.options)]})]}),y.explanation&&g.jsxs("div",{className:"explanation-text mt-3 p-3 bg-blue-50 border border-blue-200 rounded text-xs text-blue-800",children:[g.jsx("strong",{children:"Explanation:"}),q&&typeof y.explanation=="object"&&"correct"in y.explanation?g.jsxs("span",{children:[" ",g.jsx("i",{children:y.explanation.correct})]}):!q&&typeof y.explanation=="object"&&"incorrect"in y.explanation?g.jsxs(g.Fragment,{children:[typeof P=="string"&&y.explanation.incorrect[P]&&g.jsxs("span",{children:[" ",g.jsx("i",{children:y.explanation.incorrect[P]})]}),Array.isArray(P)&&P.map((E,te)=>y.explanation.incorrect[E]?g.jsx("div",{children:g.jsx("i",{children:y.explanation.incorrect[E]})},te):null),!(typeof P=="string"&&y.explanation.incorrect[P]||Array.isArray(P)&&P.some(E=>y.explanation.incorrect[E]))&&y.explanation.correct&&g.jsxs("span",{children:[" ",g.jsx("i",{children:y.explanation.correct})]})]}):g.jsxs("span",{children:[" ",g.jsx("i",{children:typeof y.explanation=="string"?y.explanation:""})]})]})]},`${u}-${p}`)}):g.jsx("li",{children:"No questions were answered in this session."})}),g.jsx("hr",{className:"my-8"}),g.jsxs("h3",{className:"text-lg font-semibold mb-3",children:["Past Results for ",(v==null?void 0:v.title)??(h||"Unknown Course")," ",w?`(${w})`:"(Full Quiz)"," (Last ",S.length,"):"]}),g.jsx("ul",{className:"past-results-list list-none p-4 border rounded bg-slate-50 max-h-60 overflow-y-auto",children:S.length>0?S.map((u,p)=>g.jsxs("li",{className:"text-sm py-1  border-slate-200 last:border-b-0",children:[new Date(u.timestamp||0).toLocaleString()," - Score: ",u.score,"/",u.totalQuestions,typeof u.percentage=="number"?` (${u.percentage.toFixed(0)}%)`:""," - Duration: ",u.duration,"s"]},p)):g.jsx("li",{className:"text-slate-500 italic",children:"No past results found for this specific quiz configuration."})})]})},im=({addResultToHistory:e,onNavigateHome:t})=>{var i;const n=(i=To().state)==null?void 0:i.results;return A.useEffect(()=>{n&&typeof n.timestamp=="number"?(console.log("ResultsPage mounted, adding result to history:",n),e(n)):n?console.error("Results object missing valid timestamp, cannot add to history:",n):console.log("ResultsPage mounted without results state.")},[n]),n?g.jsx(nm,{results:n,onRetry:t}):g.jsxs("div",{className:"text-center p-10",children:[g.jsx("h2",{className:"text-xl text-red-600 mb-4",children:"No quiz results found to display."}),g.jsx("p",{className:"text-sm text-slate-500 mb-6",children:"Please complete a quiz first."}),g.jsx($,{onClick:t,children:"Go to Quiz Selection"})]})},am=({clearAllQuizStates:e})=>{const t=()=>{window.confirm("Are you sure you want to clear all saved quiz progress and results? This cannot be undone.")&&(e(),alert("All quiz data cleared."))};return g.jsxs("div",{className:"max-w-md mx-auto mt-10  bg-white rounded-md  p-10",children:[g.jsx("h2",{className:"text-2xl font-semibold  text-center",children:"Settings"}),g.jsxs("div",{className:"text-center",children:[g.jsx($,{variant:"danger",onClick:t,children:"Clear All Saved Progress & Results"}),g.jsx("p",{className:"text-xs text-gray-500 mt-2",children:"This will remove all in-progress quizzes and past results history from this browser."})]})]})},rm=({title:e})=>g.jsxs("header",{style:{backgroundColor:"#2d3748",color:"white",padding:"1rem"},children:[" ",g.jsxs("nav",{style:{display:"flex",justifyContent:"space-between",alignItems:"center",maxWidth:"1200px",margin:"0 auto"},children:[g.jsx("div",{children:g.jsx(kl,{to:"/",style:{color:"white",textDecoration:"none",fontSize:"1.25rem",fontWeight:"bold"},children:e})}),g.jsx("div",{children:g.jsx(kl,{to:"/settings",style:{color:"#cbd5e0",textDecoration:"none",padding:"0.5rem 1rem",borderRadius:"4px"},onMouseOver:t=>t.currentTarget.style.backgroundColor="#4a5568",onMouseOut:t=>t.currentTarget.style.backgroundColor="transparent",children:"Settings"})})]})]}),sm=({children:e,appTitle:t})=>g.jsxs("div",{style:{minHeight:"100vh",display:"flex",flexDirection:"column"},children:[g.jsx(rm,{title:t}),g.jsx("main",{className:"container",style:{flexGrow:1},children:e})]});function lm(){const e="GCP Certification Quiz Hub",t=Yi(),{inProgressQuizzes:o,getQuizStateKey:n,clearAllQuizStates:i,refreshInProgressStates:a}=cd(),[r,s]=ud("quizResults",[]),l=A.useMemo(()=>{console.log("--- Recalculating completedQuizHistory map ---");const f={};return!r||!Array.isArray(r)||r.length===0?(console.log("No results history found or invalid format."),{}):(r.forEach((v,w)=>{let S=null;if(typeof v.timestamp=="string"){const u=new Date(v.timestamp);isNaN(u.getTime())||(S=u.getTime())}else typeof v.timestamp=="number"&&!isNaN(v.timestamp)&&(S=v.timestamp);if(!v||typeof v.courseId!="string"||S===null||typeof v.score!="number"||typeof v.totalQuestions!="number"){console.warn(`Skipping invalid result at index ${w}:`,v);return}const T={courseId:v.courseId,topicId:v.topicId,type:v.topicId?"topic":"full"},d=n(T);if(d){const u=v.percentage??(v.totalQuestions>0?v.score/v.totalQuestions*100:0),p=f[d],y=p==null?void 0:p.timestamp;(y===void 0||S>y)&&(f[d]={score:v.score,totalQuestions:v.totalQuestions,percentage:u,timestamp:S})}else console.warn(`Could not generate key for result at index ${w}:`,v)}),console.log("--- Finished processing. Final completedQuizHistory map:",JSON.stringify(f)),f)},[r,n]),c=A.useCallback(f=>{console.log("Adding result to history (received from ResultsPage):",f),s(v=>{const w={...f,timestamp:typeof f.timestamp=="number"?f.timestamp:Date.now()};return[...v.filter(d=>d.timestamp!==w.timestamp),w].sort((d,u)=>(d.timestamp??0)-(u.timestamp??0)).slice(-50)})},[s]),h=A.useCallback(f=>{console.log("Quiz complete, preparing to navigate to results page.");const v=f.topicId??"full";t(`/results/${f.courseId}/${v}`,{state:{results:f}})},[t]),m=A.useCallback(()=>{console.log("Quiz exited manually, navigating home."),t("/")},[t]);return g.jsx(sm,{appTitle:e,children:g.jsxs(Bg,{children:[g.jsx(_t,{path:"/",element:g.jsx(_g,{inProgressQuizzes:o,completedQuizHistory:l,getQuizStateKey:n,refreshQuizStates:a})}),g.jsx(_t,{path:"/quiz/:courseId/:topicParam",element:g.jsx(tm,{onQuizCompleteForNav:h,onQuizExitForNav:m})}),g.jsx(_t,{path:"/results/:courseId/:topicParam",element:g.jsx(im,{addResultToHistory:c,onNavigateHome:()=>t("/")})}),g.jsx(_t,{path:"/settings",element:g.jsx(am,{clearAllQuizStates:i})}),g.jsx(_t,{path:"*",element:g.jsx(Zu,{to:"/",replace:!0})})]})})}function cm(){return g.jsx(jg,{basename:"/quiz-app",children:g.jsx(lm,{})})}ma.createRoot(document.getElementById("root")).render(g.jsx(Yl.StrictMode,{children:g.jsx(cm,{})}));
