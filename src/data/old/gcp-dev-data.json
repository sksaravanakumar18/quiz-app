{
    "quizTitle": "Google Cloud Professional Developer Practice Questions",
    "quizItems": [
        {
            "id": 1,
            "topic": "Storage",
            "question": "You want to upload files from an on-premises virtual machine to Google Cloud Storage as part of a data migration. These files will be consumed by Cloud DataProc Hadoop cluster in a GCP environment.\nWhich command should you use?",
            "options": {
                "A": "gsutil cp [LOCAL_OBJECT] gs://[DESTINATION_BUCKET_NAME]/",
                "B": "gcloud cp [LOCAL_OBJECT] gs://[DESTINATION_BUCKET_NAME]/",
                "C": "hadoop fs cp [LOCAL_OBJECT] gs://[DESTINATION_BUCKET_NAME]/",
                "D": "gcloud dataproc cp [LOCAL_OBJECT] gs://[DESTINATION_BUCKET_NAME]/"
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "The `gsutil cp` command is the standard and recommended tool for copying files between a local machine and Google Cloud Storage. `gcloud compute scp` is used for copying files to/from Compute Engine instances, not Cloud Storage directly. `hadoop fs cp` is used within a Hadoop environment, typically with HDFS or compatible filesystems, although the Cloud Storage connector allows its use, `gsutil` is more direct for uploads from on-premises. `gcloud dataproc` commands manage Dataproc clusters and jobs, not direct file transfers to GCS.",
            "conditions": [
                "Upload files from on-premises VM to GCS for Dataproc consumption."
            ],
            "caseStudyContext": null
        },
        {
            "id": 2,
            "topic": "Monitoring & Logging",
            "question": "You migrated your applications to Google Cloud Platform and kept your existing monitoring platform. You now find that your notification system is too slow for time critical problems.\nWhat should you do?",
            "options": {
                "A": "Replace your entire monitoring platform with Stackdriver.",
                "B": "Install the Stackdriver agents on your Compute Engine instances.",
                "C": "Use Stackdriver to capture and alert on logs, then ship them to your existing platform.",
                "D": "Migrate some traffic back to your old platform and perform AB testing on the two platforms concurrently."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "The core issue is slow notifications for critical problems from the existing on-premises system, even though apps are now on GCP. Replacing the entire system (A) might be excessive if only notifications are slow. Installing agents (B) is necessary to get metrics/logs into Cloud Monitoring (formerly Stackdriver) but doesn't directly solve the notification latency issue. Migrating traffic back (D) is irrelevant to fixing monitoring notifications. Using Cloud Monitoring/Logging for fast alerting on GCP-based logs (C) addresses the critical problem directly while allowing the continued use of the existing platform for other purposes by shipping logs to it. This is the least disruptive and most targeted solution.",
            "conditions": [
                "Apps migrated to GCP, existing on-prem monitoring platform retained, notification system is too slow for critical issues."
            ],
            "caseStudyContext": null
        },
        {
            "id": 3,
            "topic": "Databases",
            "question": "You are planning to migrate a MySQL database to the managed Cloud SQL database for Google Cloud. You have Compute Engine virtual machine instances that will connect with this Cloud SQL instance. You do not want to whitelist IPs for the Compute Engine instances to be able to access Cloud SQL.\nWhat should you do?",
            "options": {
                "A": "Enable private IP for the Cloud SQL instance.",
                "B": "Whitelist a project to access Cloud SQL, and add Compute Engine instances in the whitelisted project.",
                "C": "Create a role in Cloud SQL that allows access to the database from external instances, and assign the Compute Engine instances to that role.",
                "D": "Create a CloudSQL instance on one project. Create Compute engine instances in a different project. Create a VPN between these two projects to allow internal access to CloudSQL."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "Enabling private IP for the Cloud SQL instance allows Compute Engine instances within the same VPC network to connect directly using internal IP addresses, bypassing the need for public IP whitelisting. Project whitelisting (B) isn't a standard Cloud SQL access method. Cloud SQL roles (C) control database-level permissions, not network connectivity. Setting up a VPN between projects (D) adds unnecessary complexity when private IP within the same VPC can achieve the goal.",
            "conditions": [
                "Migrating MySQL to Cloud SQL, GCE instances need access, avoid IP whitelisting."
            ],
            "caseStudyContext": null
        },
        {
            "id": 4,
            "topic": "Networking",
            "question": "You have deployed an HTTP(s) Load Balancer with the gcloud commands shown below. \n\nHealth checks to port 80 on the Compute Engine virtual machine instance are failing and no traffic is sent to your instances. You want to resolve the problem.\nWhich commands should you run?",
            "options": {
                "A": "gcloud compute instances add-access-config ${NAME}-backend-instance-1",
                "B": "gcloud compute instances add-tags ${NAME}-backend-instance-1 --tags http-server",
                "C": "gcloud compute firewall-rules create allow-lb --network load-balancer --allow tcp --source-ranges 130.211.0.0/22,35.191.0.0/16 --direction INGRESS",
                "D": "gcloud compute firewall-rules create allow-lb --network load-balancer --allow tcp --destination-ranges 130.211.0.0/22,35.191.0.0/16 -direction EGRESS"
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "Google Cloud HTTP(S) Load Balancer health checks originate from specific IP ranges (130.211.0.0/22 and 35.191.0.0/16). Health checks are incoming (INGRESS) connections to the backend instances. Therefore, an INGRESS firewall rule is required to allow TCP traffic from these source ranges to the instances on the appropriate port (implicitly port 80 here, based on the scenario). Option A adds an external IP, not relevant. Option B adds a tag, but doesn't create the rule. Option D creates an EGRESS rule, which is incorrect.",
            "conditions": [
                "HTTP(s) Load Balancer deployed, health checks to port 80 failing, no traffic reaching instances."
            ],
            "caseStudyContext": null
        },
        {
            "id": 5,
            "topic": "Compute",
            "question": "Your website is deployed on Compute Engine. Your marketing team wants to test conversion rates between 3 different website designs.\nWhich approach should you use?",
            "options": {
                "A": "Deploy the website on App Engine and use traffic splitting.",
                "B": "Deploy the website on App Engine as three separate services.",
                "C": "Deploy the website on Cloud Functions and use traffic splitting.",
                "D": "Deploy the website on Cloud Functions as three separate functions."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "App Engine's traffic splitting feature is specifically designed for scenarios like A/B testing or gradual rollouts. It allows routing percentages of traffic to different versions within the same service, making it easy to compare performance (like conversion rates) between designs under the same URL. Deploying as separate services (B) would typically involve different URLs. Cloud Functions (C, D) are generally for event-driven code or APIs, not typically for hosting full websites, although possible, App Engine is a better fit.",
            "conditions": [
                "Website currently on GCE, need A/B testing for 3 designs."
            ],
            "caseStudyContext": null
        },
        {
            "id": 6,
            "topic": "Compute",
            "question": "You need to copy directory local-scripts and all of its contents from your local workstation to a Compute Engine virtual machine instance.\nWhich command should you use?",
            "options": {
                "A": "gsutil cp --project ‫ג‬€my-gcp-project‫ג‬€ -r ~/local-scripts/ gcp-instance-name:~/server-scripts/ --zone ‫ג‬€us-east1-b‫ג‬€",
                "B": "gsutil cp --project ‫ג‬€my-gcp-project‫ג‬€ -R ~/local-scripts/ gcp-instance-name:~/server-scripts/ --zone ‫ג‬€us-east1-b‫ג‬€",
                "C": "gcloud compute scp --project ‫ג‬€my-gcp-project‫ג‬€ --recurse ~/local-scripts/ gcp-instance-name:~/server-scripts/ --zone ‫ג‬€us-east1-b‫ג‬€",
                "D": "gcloud compute mv --project ‫ג‬€my-gcp-project‫ג‬€ --recurse ~/local-scripts/ gcp-instance-name:~/server-scripts/ --zone ‫ג‬€us-east1-b‫ג‬€"
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "`gcloud compute scp` is the command used to securely copy files and directories between a local machine and a Compute Engine instance. The `--recurse` flag is used to copy directories and their contents. `gsutil cp` is used for Cloud Storage operations. `gcloud compute mv` does not exist; file moving on the instance would use standard Linux commands via SSH.",
            "conditions": [
                "Copy directory from local workstation to GCE instance."
            ],
            "caseStudyContext": null
        },
        {
            "id": 7,
            "topic": "Monitoring & Logging",
            "question": "You are deploying your application to a Compute Engine virtual machine instance with the Stackdriver Monitoring Agent installed. Your application is a unix process on the instance. You want to be alerted if the unix process has not run for at least 5 minutes. You are not able to change the application to generate metrics or logs.\nWhich alert condition should you configure?",
            "options": {
                "A": "Uptime check",
                "B": "Process health",
                "C": "Metric absence",
                "D": "Metric threshold"
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "Cloud Monitoring (formerly Stackdriver) provides process health monitoring via its agent. This allows you to define alerting policies based on whether a process matching a certain pattern is running or not, or if the count of matching processes crosses a threshold. Since the application cannot be modified, relying on its presence monitored by the agent (Process health) is the direct approach. Uptime checks monitor external endpoints. Metric absence/threshold require specific metrics, which the application isn't generating.",
            "conditions": [
                "GCE instance with Monitoring Agent, monitor specific Unix process, alert if not running for 5 mins, cannot modify app."
            ],
            "caseStudyContext": null
        },
        {
            "id": 8,
            "topic": "Databases",
            "question": "You have two tables in an ANSI-SQL compliant database with identical columns that you need to quickly combine into a single table, removing duplicate rows from the result set.\nWhat should you do?",
            "options": {
                "A": "Use the JOIN operator in SQL to combine the tables.",
                "B": "Use nested WITH statements to combine the tables.",
                "C": "Use the UNION operator in SQL to combine the tables.",
                "D": "Use the UNION ALL operator in SQL to combine the tables."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "The SQL `UNION` operator combines the result sets of two or more SELECT statements and removes duplicate rows. `UNION ALL` also combines result sets but includes all rows, including duplicates. `JOIN` combines rows from tables based on a related column. `WITH` statements (Common Table Expressions) help organize complex queries but don't inherently combine tables like UNION.",
            "conditions": [
                "Combine two SQL tables with identical columns, remove duplicates."
            ],
            "caseStudyContext": null
        },
        {
            "id": 9,
            "topic": "Compute",
            "question": "You have an application deployed in production. When a new version is deployed, some issues don't arise until the application receives traffic from users in production. You want to reduce both the impact and the number of users affected.\nWhich deployment strategy should you use?",
            "options": {
                "A": "Blue/green deployment",
                "B": "Canary deployment",
                "C": "Rolling deployment",
                "D": "Recreate deployment"
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "Canary deployment involves rolling out the new version to a small subset of users first. This allows monitoring for issues with minimal impact before exposing the version to the entire user base. Blue/green swaps all traffic at once after testing the new environment, affecting all users simultaneously if issues occur post-switch. Rolling updates gradually replace old instances, potentially affecting more users than an initial canary release. Recreate involves downtime.",
            "conditions": [
                "Deploy new version, issues appear only in production, need to reduce impact and number of users affected."
            ],
            "caseStudyContext": null
        },
        {
            "id": 10,
            "topic": "Databases",
            "question": "Your company wants to expand their users outside the United States for their popular application. The company wants to ensure 99.999% availability of the database for their application and also wants to minimize the read latency for their users across the globe.\nWhich two actions should they take? (Choose two.)",
            "options": {
                "A": "Create a multi-regional Cloud Spanner instance with \"nam-asia-eur1\" configuration.",
                "B": "Create a multi-regional Cloud Spanner instance with \"nam3\" configuration.",
                "C": "Create a cluster with at least 3 Spanner nodes.",
                "D": "Create a cluster with at least 1 Spanner node.",
                "E": "Create a minimum of two Cloud Spanner instances in separate regions with at least one node.",
                "F": "Create a Cloud Dataflow pipeline to replicate data across different databases."
            },
            "correctAnswer": [
                "A",
                "C"
            ],
            "explanation": "Cloud Spanner multi-region configurations provide 99.999% availability. The `nam-eur-asia1` (likely intended by \"nam-asia-eur1\") configuration places replicas across North America, Europe, and Asia, minimizing read latency for global users (A). Spanner instances require compute capacity (nodes) for performance and availability; a minimum of 3 nodes is generally recommended for production multi-region instances to handle replication and load (C). 'nam3' (B) is a regional configuration. 1 node (D) is insufficient for HA/performance. Separate instances (E) don't provide the strong consistency guarantees of a single multi-region instance. Dataflow (F) is not needed for Spanner's internal replication.",
            "conditions": [
                "Global user base, 99.999% database availability needed, minimize global read latency."
            ],
            "caseStudyContext": null
        },
        {
            "id": 11,
            "topic": "Storage",
            "question": "You need to migrate an internal file upload API with an enforced 500-MB file size limit to App Engine.\nWhat should you do?",
            "options": {
                "A": "Use FTP to upload files.",
                "B": "Use CPanel to upload files.",
                "C": "Use signed URLs to upload files.",
                "D": "Change the API to be a multipart file upload API."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "App Engine has request size limitations (typically 32MB, though Flex can handle larger requests, large uploads are still problematic). Using signed URLs allows the client application to upload large files directly to Google Cloud Storage, bypassing App Engine's request limits. The App Engine application only needs to generate the signed URL. FTP/CPanel (A, B) are not relevant App Engine mechanisms. Multipart upload via the API (D) would still likely hit App Engine limits.",
            "conditions": [
                "Migrate file upload API (500MB limit) to App Engine."
            ],
            "caseStudyContext": null
        },
        {
            "id": 12,
            "topic": "Compute",
            "question": "You are planning to deploy your application in a Google Kubernetes Engine (GKE) cluster. The application exposes an HTTP-based health check at /healthz. You want to use this health check endpoint to determine whether traffic should be routed to the pod by the load balancer.\nWhich code snippet should you include in your Pod configuration?\nA.\nlivenessProbe:\n  httpGet:\n    path: /healthz\n    port: 80\nB.\nreadinessProbe:\n  httpGet:\n    path: /healthz\n    port: 80\nC.\nloadbalancerHealthCheck:\n  httpGet:\n    path: /healthz\n    port: 80\nD.\nhealthCheck:\n  httpGet:\n    path: /healthz\n    port: 80",
            "options": {
                "A": "livenessProbe:\n  httpGet:\n    path: /healthz\n    port: 80",
                "B": "readinessProbe:\n  httpGet:\n    path: /healthz\n    port: 80",
                "C": "loadbalancerHealthCheck:\n  httpGet:\n    path: /healthz\n    port: 80",
                "D": "healthCheck:\n  httpGet:\n    path: /healthz\n    port: 80"
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "Kubernetes uses readiness probes to determine if a Pod is ready to receive traffic. If the readiness probe fails, the Pod's IP address is removed from the corresponding Service's endpoints, effectively taking it out of the load balancer's rotation. Liveness probes determine if a container needs to be restarted. `loadbalancerHealthCheck` and `healthCheck` are not standard Kubernetes Pod spec fields for this purpose.",
            "conditions": [
                "GKE deployment, use /healthz endpoint to control LB traffic routing."
            ],
            "caseStudyContext": null
        },
        {
            "id": 13,
            "topic": "Databases",
            "question": "You are writing a single-page web application with a user-interface that communicates with a third-party API for content using XMLHttpRequest. The data displayed on the UI by the API results is less critical than other data displayed on the same web page, so it is acceptable for some requests to not have the API data displayed in the UI. However, calls made to the API should not delay rendering of other parts of the user interface. You want your application to perform well when the API response is an error or a timeout.\nWhat should you do?",
            "options": {
                "A": "Set the asynchronous option for your requests to the API to false and omit the widget displaying the API results when a timeout or error is encountered.",
                "B": "Set the asynchronous option for your request to the API to true and omit the widget displaying the API results when a timeout or error is encountered.",
                "C": "Catch timeout or error exceptions from the API call and keep trying with exponential backoff until the API response is successful.",
                "D": "Catch timeout or error exceptions from the API call and display the error response in the UI widget."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "To prevent the API call from blocking the rendering of the main UI, the XMLHttpRequest should be asynchronous (true). Since the API data is less critical and it's acceptable for it not to display, omitting the widget gracefully handles timeout or error scenarios without disrupting the user experience for the rest of the page. Retrying indefinitely (C) isn't necessary for non-critical data. Displaying the error (D) might not be the desired user experience.",
            "conditions": [
                "Single-page web app, third-party API call via XMLHttpRequest, API data less critical, call must not block UI, handle API errors/timeouts gracefully."
            ],
            "caseStudyContext": null
        },
        {
            "id": 14,
            "topic": "Compute",
            "question": "You are developing a JPEG image-resizing API hosted on Google Kubernetes Engine (GKE). Callers of the service will exist within the same GKE cluster. You want clients to be able to get the IP address of the service.\nWhat should you do?",
            "options": {
                "A": "Define a GKE Service. Clients should use the name of the A record in Cloud DNS to find the service's cluster IP address.",
                "B": "Define a GKE Service. Clients should use the service name in the URL to connect to the service.",
                "C": "Define a GKE Endpoint. Clients should get the endpoint name from the appropriate environment variable in the client container.",
                "D": "Define a GKE Endpoint. Clients should get the endpoint name from Cloud DNS."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "Within a GKE cluster, Kubernetes provides internal DNS for service discovery. When you define a Service (typically of type ClusterIP for internal services), Kubernetes automatically creates a DNS record `<service-name>.<namespace-name>.svc.cluster.local`. Pods within the cluster can resolve this service name to the Service's ClusterIP. Using just the service name (e.g., `http://my-service`) often works if the client and service are in the same namespace due to DNS search path configuration. This is the standard Kubernetes way for intra-cluster communication. Endpoints are usually managed by Services.",
            "conditions": [
                "GKE hosted API, clients are within the same cluster, need service discovery."
            ],
            "caseStudyContext": null
        },
        {
            "id": 15,
            "topic": "Compute",
            "question": "You are using Cloud Build to build and test application source code stored in Cloud Source Repositories. The build process requires a build tool not available in the Cloud Build environment.\nWhat should you do?",
            "options": {
                "A": "Download the binary from the internet during the build process.",
                "B": "Build a custom cloud builder image and reference the image in your build steps.",
                "C": "Include the binary in your Cloud Source Repositories repository and reference it in your build scripts.",
                "D": "Ask to have the binary added to the Cloud Build environment by filing a feature request against the Cloud Build public Issue Tracker."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "If a required tool isn't available in the standard Cloud Build builders, the recommended approach is to create a custom builder image (a Docker image containing the necessary tool) and push it to a registry (like Artifact Registry). Then, reference this custom builder image in your `cloudbuild.yaml` configuration file for the relevant build steps. Downloading during build (A) can be slow and unreliable. Including binaries in source control (C) is generally bad practice. Filing a feature request (D) won't solve the immediate need.",
            "conditions": [
                "Cloud Build pipeline needs a tool not available in default builders."
            ],
            "caseStudyContext": null
        },
        {
            "id": 16,
            "topic": "Monitoring & Logging",
            "question": "You are deploying your application to a Compute Engine virtual machine instance. Your application is configured to write its log files to disk. You want to view the logs in Stackdriver Logging without changing the application code.\nWhat should you do?",
            "options": {
                "A": "Install the Stackdriver Logging Agent and configure it to send the application logs.",
                "B": "Use a Stackdriver Logging Library to log directly from the application to Stackdriver Logging.",
                "C": "Provide the log file folder path in the metadata of the instance to configure it to send the application logs.",
                "D": "Change the application to log to /var/log so that its logs are automatically sent to Stackdriver Logging."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "The Cloud Logging Agent (or the newer Ops Agent) is designed specifically to collect logs from files on Compute Engine instances (and other sources) and forward them to Cloud Logging. Option A requires installing and configuring the agent to monitor the specific log file paths, achieving the goal without application code changes. Using a library (B) requires code changes. Instance metadata (C) doesn't directly configure log shipping. Logging to /var/log (D) might work for some standard system logs, but custom application logs usually require explicit agent configuration.",
            "conditions": [
                "GCE application logs to disk files, need logs in Cloud Logging, cannot change app code."
            ],
            "caseStudyContext": null
        },
        {
            "id": 17,
            "topic": "Storage",
            "question": "Your service adds text to images that it reads from Cloud Storage. During busy times of the year, requests to Cloud Storage fail with an HTTP 429 \"Too Many Requests\" status code.\nHow should you handle this error?",
            "options": {
                "A": "Add a cache-control header to the objects.",
                "B": "Request a quota increase from the GCP Console.",
                "C": "Retry the request with a truncated exponential backoff strategy.",
                "D": "Change the storage class of the Cloud Storage bucket to Multi-regional."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "HTTP 429 errors indicate rate limiting. The standard and recommended way to handle rate limiting and transient server-side errors (like 5xx) is to implement retries with exponential backoff. This involves waiting progressively longer periods between retries, up to a maximum limit, to avoid overwhelming the service. Cache-control (A) affects client-side caching. Quota increases (B) might be possible but aren't the immediate error handling mechanism. Storage class (D) affects cost/availability/latency, not rate limits directly.",
            "conditions": [
                "Reading from GCS, encountering HTTP 429 errors during busy times."
            ],
            "caseStudyContext": null
        },
        {
            "id": 18,
            "topic": "Compute",
            "question": "You are building an API that will be used by Android and iOS apps. The API must:\n* Support HTTPs\n* Minimize bandwidth cost\n* Integrate easily with mobile apps\nWhich API architecture should you use?",
            "options": {
                "A": "RESTful APIs",
                "B": "MQTT for APIs",
                "C": "gRPC-based APIs",
                "D": "SOAP-based APIs"
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "gRPC uses HTTP/2 and Protocol Buffers for serialization, which is significantly more bandwidth-efficient than text-based formats like JSON used in typical REST APIs. It supports HTTPS and has good library support for mobile platforms (Android/iOS). While REST (A) offers easy integration, gRPC (C) better meets the 'minimize bandwidth cost' requirement. MQTT (B) is primarily for messaging, not request/response APIs. SOAP (D) is generally verbose and less suitable for mobile.",
            "conditions": [
                "API for mobile apps, requires HTTPS, minimize bandwidth, easy integration."
            ],
            "caseStudyContext": null
        },
        {
            "id": 19,
            "topic": "Databases",
            "question": "Your application takes an input from a user and publishes it to the user's contacts. This input is stored in a table in Cloud Spanner. Your application is more sensitive to latency and less sensitive to consistency.\nHow should you perform reads from Cloud Spanner for this application?",
            "options": {
                "A": "Perform Read-Only transactions.",
                "B": "Perform stale reads using single-read methods.",
                "C": "Perform strong reads using single-read methods.",
                "D": "Perform stale reads using read-write transactions."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "When latency is more critical than having the absolute latest data (strong consistency), Cloud Spanner's stale reads are the optimal choice. They can often be served faster by any available replica that has data within the specified staleness bound, reducing potential contention or coordination overhead. Single-read methods are generally preferred over read-only transactions for simple reads due to lower overhead. Read-write transactions (D) are for writes and don't support stale reads.",
            "conditions": [
                "Reading from Cloud Spanner, latency sensitive, consistency tolerant."
            ],
            "caseStudyContext": null
        },
        {
            "id": 20,
            "topic": "Compute",
            "question": "Your application is deployed in a Google Kubernetes Engine (GKE) cluster. When a new version of your application is released, your CI/CD tool updates the spec.template.spec.containers[0].image value to reference the Docker image of your new application version. When the Deployment object applies the change, you want to deploy at least 1 replica of the new version and maintain the previous replicas until the new replica is healthy.\nWhich change should you make to the GKE Deployment object shown below?",
            "options": {
                "A": "Set the Deployment strategy to RollingUpdate with maxSurge set to 0, maxUnavailable set to 1.",
                "B": "Set the Deployment strategy to RollingUpdate with maxSurge set to 1, maxUnavailable set to 0.",
                "C": "Set the Deployment strategy to Recreate with maxSurge set to 0, maxUnavailable set to 1.",
                "D": "Set the Deployment strategy to Recreate with maxSurge set to 1, maxUnavailable set to 0."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "The RollingUpdate strategy allows zero-downtime updates. `maxSurge` defines how many pods can be created *above* the desired count during an update. `maxUnavailable` defines how many pods can be *unavailable* during the update. To deploy a new replica (`maxSurge=1`) while keeping all previous replicas running until the new one is healthy (`maxUnavailable=0`), option B is the correct configuration. Recreate strategy (C, D) involves downtime.",
            "conditions": [
                "GKE Deployment update, deploy 1 new replica while keeping all old replicas until new is healthy."
            ],
            "caseStudyContext": null
        },
        {
            "id": 21,
            "topic": "Storage",
            "question": "You plan to make a simple HTML application available on the internet. This site keeps information about FAQs for your application. The application is static and contains images, HTML, CSS, and Javascript. You want to make this application available on the internet with as few steps as possible.\nWhat should you do?",
            "options": {
                "A": "Upload your application to Cloud Storage.",
                "B": "Upload your application to an App Engine environment.",
                "C": "Create a Compute Engine instance with Apache web server installed. Configure Apache web server to host the application.",
                "D": "Containerize your application first. Deploy this container to Google Kubernetes Engine (GKE) and assign an external IP address to the GKE pod hosting the application."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "Google Cloud Storage provides the simplest and most cost-effective way to host static website content (HTML, CSS, JavaScript, images). You simply upload the files to a bucket, configure public access (or use a Load Balancer/CDN), and optionally set up a custom domain. App Engine (B), GCE (C), and GKE (D) are overkill and involve significantly more setup and management for purely static content.",
            "conditions": [
                "Host static website (HTML, CSS, JS, images), minimal steps required."
            ],
            "caseStudyContext": null
        },
        {
            "id": 22,
            "topic": "Monitoring & Logging",
            "question": "Your company has deployed a new API to App Engine Standard environment. During testing, the API is not behaving as expected. You want to monitor the application over time to diagnose the problem within the application code without redeploying the application.\nWhich tool should you use?",
            "options": {
                "A": "Stackdriver Trace",
                "B": "Stackdriver Monitoring",
                "C": "Stackdriver Debug Snapshots",
                "D": "Stackdriver Debug Logpoints"
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "Cloud Debugger Logpoints (part of the former Stackdriver suite, now Operations Suite) allow developers to inject logging statements into running applications (including App Engine) without stopping them or redeploying code. This lets you gather diagnostic information over time from specific code locations to understand unexpected behavior. Snapshots (C) capture state at one point. Trace (A) focuses on latency across requests. Monitoring (B) tracks metrics and overall health.",
            "conditions": [
                "App Engine app misbehaving, need to diagnose code issue over time without redeploying."
            ],
            "caseStudyContext": null
        },
        {
            "id": 23,
            "topic": "Monitoring & Logging",
            "question": "You want to use the Stackdriver Logging Agent to send an application's log file to Stackdriver from a Compute Engine virtual machine instance. After installing the Stackdriver Logging Agent, what should you do first?",
            "options": {
                "A": "Enable the Error Reporting API on the project.",
                "B": "Grant the instance full access to all Cloud APIs.",
                "C": "Configure the application log file as a custom source.",
                "D": "Create a Stackdriver Logs Export Sink with a filter that matches the application's log entries."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "After installing the Logging Agent (or Ops Agent), you must configure it to tell it *which* files to monitor and how to parse them. This involves defining the application's log file path as a custom log source in the agent's configuration file (e.g., fluentd or otel config). Enabling Error Reporting (A) is separate. Granting full access (B) violates least privilege (appropriate scopes/roles are needed, usually assigned during install or via instance service account). Creating sinks (D) is for routing logs *after* they are in Cloud Logging.",
            "conditions": [
                "Use Logging Agent on GCE to send app logs from file."
            ],
            "caseStudyContext": null
        },
        {
            "id": 24,
            "topic": "Databases",
            "question": "Your company has a BigQuery data mart that provides analytics information to hundreds of employees. One user of wants to run jobs without interrupting important workloads. This user isn't concerned about the time it takes to run these jobs. You want to fulfill this request while minimizing cost to the company and the effort required on your part.\nWhat should you do?",
            "options": {
                "A": "Ask the user to run the jobs as batch jobs.",
                "B": "Create a separate project for the user to run jobs.",
                "C": "Add the user as a job.user role in the existing project.",
                "D": "Allow the user to run jobs when important workloads are not running."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "BigQuery batch jobs are queued and run when idle resources are available, ensuring they don't interfere with interactive or high-priority queries. They are ideal for non-time-sensitive tasks and offer a lower cost (often free tier applicable). This directly addresses the user's requirements with minimal effort. A separate project (B) adds overhead. The `job.user` role (C) grants permission to run jobs but doesn't control priority. Running off-hours (D) requires manual coordination and isn't guaranteed not to conflict.",
            "conditions": [
                "BigQuery data mart, user needs to run low-priority, non-time-sensitive jobs without interrupting others, minimize cost/effort."
            ],
            "caseStudyContext": null
        },
        {
            "id": 25,
            "topic": "Monitoring & Logging",
            "question": "You want to notify on-call engineers about a service degradation in production while minimizing development time.\nWhat should you do?",
            "options": {
                "A": "Use Cloud Function to monitor resources and raise alerts.",
                "B": "Use Cloud Pub/Sub to monitor resources and raise alerts.",
                "C": "Use Stackdriver Error Reporting to capture errors and raise alerts.",
                "D": "Use Stackdriver Monitoring to monitor resources and raise alerts."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "Cloud Monitoring (formerly Stackdriver) is Google Cloud's native monitoring solution. It allows you to create alerting policies based on metrics (e.g., latency, error rates, resource utilization) that indicate service degradation. Setting up these alerts requires configuration through the console or API, minimizing development time compared to building custom monitoring with Cloud Functions (A) or Pub/Sub (B). Error Reporting (C) focuses specifically on application errors, while Monitoring covers broader service health.",
            "conditions": [
                "Need notifications for service degradation, minimize development time."
            ],
            "caseStudyContext": null
        },
        {
            "id": 26,
            "topic": "Compute",
            "question": "You are writing a single-page web application with a user-interface that communicates with a third-party API for content using XMLHttpRequest. The data displayed on the UI by the API results is less critical than other data displayed on the same web page, so it is acceptable for some requests to not have the API data displayed in the UI. However, calls made to the API should not delay rendering of other parts of the user interface. You want your application to perform well when the API response is an error or a timeout.\nWhat should you do?",
            "options": {
                "A": "Set the asynchronous option for your requests to the API to false and omit the widget displaying the API results when a timeout or error is encountered.",
                "B": "Set the asynchronous option for your request to the API to true and omit the widget displaying the API results when a timeout or error is encountered.",
                "C": "Catch timeout or error exceptions from the API call and keep trying with exponential backoff until the API response is successful.",
                "D": "Catch timeout or error exceptions from the API call and display the error response in the UI widget."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "Setting the asynchronous option to `true` ensures the API request doesn't block the main browser thread, allowing the rest of the UI to render without delay. Since the data is less critical and it's acceptable for it not to appear, the best way to handle timeouts or errors is to simply omit the widget or relevant UI section, providing a smooth experience for the user regarding the rest of the page. Setting async to `false` (A) would block rendering. Retrying indefinitely (C) isn't suitable for non-critical data. Displaying the raw error (D) might not be ideal UI.",
            "conditions": [
                "Single-page app, non-critical API call via XMLHttpRequest, must not block UI, handle errors/timeouts gracefully."
            ],
            "caseStudyContext": null
        },
        {
            "id": 27,
            "topic": "IAM & Security",
            "question": "You are creating a web application that runs in a Compute Engine instance and writes a file to any user's Google Drive. You need to configure the application to authenticate to the Google Drive API. What should you do?",
            "options": {
                "A": "Use an OAuth Client ID that uses the https://www.googleapis.com/auth/drive.file scope to obtain an access token for each user.",
                "B": "Use an OAuth Client ID with delegated domain-wide authority.",
                "C": "Use the App Engine service account and https://www.googleapis.com/auth/drive.file scope to generate a signed JSON Web Token (JWT).",
                "D": "Use the App Engine service account with delegated domain-wide authority."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "To act on behalf of individual users and access their private data (like Google Drive), the application must obtain user consent via the OAuth 2.0 protocol. This involves registering the application to get an OAuth Client ID, requesting the appropriate scope (`drive.file` allows per-file access created or opened by the app), and guiding the user through the consent flow to obtain an access token specific to that user. Domain-wide delegation (B, D) allows a service account to impersonate users within a G Suite domain, but requires admin setup and is less suitable for accessing *any* user's Drive. Service accounts (C, D) represent the application itself, not individual users.",
            "conditions": [
                "GCE web application needs to write files to any user's Google Drive."
            ],
            "caseStudyContext": null
        },
        {
            "id": 28,
            "topic": "Compute",
            "question": "You are creating a Google Kubernetes Engine (GKE) cluster and run this command:\n\nThe command fails with the error:\n\nERROR: (gcloud.container.clusters.create) ResponseError: code=403, message=Insufficient regional quota to satisfy request: resource \"CPUS\": request requires 'XX' and is short 'YY'.. Need more quota? http://...\n\nYou want to resolve the issue. What should you do?",
            "options": {
                "A": "Request additional GKE quota in the GCP Console.",
                "B": "Request additional Compute Engine quota in the GCP Console.",
                "C": "Open a support case to request additional GKE quota.",
                "D": "Decouple services in the cluster, and rewrite new clusters to function with fewer cores."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "The error message explicitly mentions \"Insufficient regional quota... resource \\\"CPUS\\\"\". GKE nodes are Compute Engine instances. Therefore, CPU quota limits are part of Compute Engine quotas, not a separate GKE quota. You need to request an increase in the Compute Engine CPU quota for the specific region in the GCP Console. Decoupling services (D) doesn't address the quota issue directly.",
            "conditions": [
                "GKE cluster creation command fails with CPU quota error."
            ],
            "caseStudyContext": null
        },
        {
            "id": 29,
            "topic": "Compute",
            "question": "You are parsing a log file that contains three columns: a timestamp, an account number (a string), and a transaction amount (a number). You want to calculate the sum of all transaction amounts for each unique account number efficiently.\nWhich data structure should you use?",
            "options": {
                "A": "A linked list",
                "B": "A hash table",
                "C": "A two-dimensional array",
                "D": "A comma-delimited string"
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "A hash table (also known as a dictionary or map) provides efficient (average O(1)) lookup, insertion, and update operations based on a key. In this case, the account number can be used as the key, and the transaction amount sum can be the value. As you parse the log, you can quickly look up the account number in the hash table and update its corresponding sum. Linked lists, arrays, and strings would require less efficient searching (O(n)) to find and update the sum for each account.",
            "conditions": [
                "Parse log file (timestamp, account_number, amount), efficiently sum amounts per unique account number."
            ],
            "caseStudyContext": null
        },
        {
            "id": 30,
            "topic": "Databases",
            "question": "Your company has a BigQuery dataset named \"Master\" that keeps information about employee travel and expenses. This information is organized by employee department. That means employees should only be able to view information for their department. You want to apply a security framework to enforce this requirement with the minimum number of steps.\nWhat should you do?",
            "options": {
                "A": "Create a separate dataset for each department. Create a view with an appropriate WHERE clause to select records from a particular dataset for the specific department. Authorize this view to access records from your Master dataset. Give employees the permission to this department-specific dataset.",
                "B": "Create a separate dataset for each department. Create a data pipeline for each department to copy appropriate information from the Master dataset to the specific dataset for the department. Give employees the permission to this department-specific dataset.",
                "C": "Create a dataset named Master dataset. Create a separate view for each department in the Master dataset. Give employees access to the specific view for their department.",
                "D": "Create a dataset named Master dataset. Create a separate table for each department in the Master dataset. Give employees access to the specific table for their department."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "Authorized views are the standard and simplest method in BigQuery for implementing row-level or column-level security. You create a view within the same dataset (`Master`) for each department, filtering the data using a `WHERE` clause (e.g., `WHERE department = 'Sales'`). You then grant employees access only to their department's specific view, not the underlying table. This avoids data duplication (unlike B or D) and the complexity of managing multiple datasets (unlike A).",
            "conditions": [
                "BigQuery dataset 'Master', row-level security based on employee department, minimum steps."
            ],
            "caseStudyContext": null
        },
        {
            "id": 31,
            "topic": "Monitoring & Logging",
            "question": "You have an application in production. It is deployed on Compute Engine virtual machine instances controlled by a managed instance group. Traffic is routed to the instances via a HTTP(s) load balancer. Your users are unable to access your application. You want to implement a monitoring technique to alert you when the application is unavailable.\nWhich technique should you choose?",
            "options": {
                "A": "Smoke tests",
                "B": "Stackdriver uptime checks",
                "C": "Cloud Load Balancing - heath checks",
                "D": "Managed instance group - heath checks"
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "Cloud Monitoring Uptime Checks are specifically designed to monitor the external availability of applications (web servers, VMs, etc.) from various locations around the world. They can be configured to check HTTP(S) endpoints and trigger alerts if the application becomes unavailable. Load balancer and MIG health checks (C, D) are used internally to manage traffic routing and instance health within GCP, but uptime checks provide the external perspective on availability and integrated alerting needed here. Smoke tests (A) are a type of test, not a specific monitoring technique in this context.",
            "conditions": [
                "GCE app in MIG behind HTTP(S) LB, users cannot access, need alerting for unavailability."
            ],
            "caseStudyContext": null
        },
        {
            "id": 32,
            "topic": "Storage",
            "question": "You are load testing your server application. During the first 30 seconds, you observe that a previously inactive Cloud Storage bucket is now servicing 2000 write requests per second and 7500 read requests per second. Your application is now receiving intermittent 5xx and 429 HTTP responses from the Cloud Storage JSON API as the demand escalates. You want to decrease the failed responses from the Cloud Storage API.\nWhat should you do?",
            "options": {
                "A": "Distribute the uploads across a large number of individual storage buckets.",
                "B": "Use the XML API instead of the JSON API for interfacing with Cloud Storage.",
                "C": "Pass the HTTP response codes back to clients that are invoking the uploads from your application.",
                "D": "Limit the upload rate from your application clients so that the dormant bucket's peak request rate is reached more gradually."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "Cloud Storage buckets automatically scale their I/O capacity based on sustained traffic. However, a sudden, massive increase in requests to a previously inactive or low-traffic bucket can exceed the initial capacity, leading to 5xx or 429 errors. The recommended practice is to gradually ramp up the request rate (D) to allow the bucket time to scale. Distributing across many buckets (A) can help but adds complexity. Changing the API (B) is unlikely to solve scaling issues. Passing errors back (C) doesn't prevent them.",
            "conditions": [
                "Load testing GCS bucket, high initial request rate on inactive bucket, receiving 5xx/429 errors."
            ],
            "caseStudyContext": null
        },
        {
            "id": 33,
            "topic": "Storage",
            "question": "Your application is controlled by a managed instance group. You want to share a large read-only data set between all the instances in the managed instance group. You want to ensure that each instance can start quickly and can access the data set via its filesystem with very low latency. You also want to minimize the total cost of the solution.\nWhat should you do?",
            "options": {
                "A": "Move the data to a Cloud Storage bucket, and mount the bucket on the filesystem using Cloud Storage FUSE.",
                "B": "Move the data to a Cloud Storage bucket, and copy the data to the boot disk of the instance via a startup script.",
                "C": "Move the data to a Compute Engine persistent disk, and attach the disk in read-only mode to multiple Compute Engine virtual machine instances.",
                "D": "Move the data to a Compute Engine persistent disk, take a snapshot, create multiple disks from the snapshot, and attach each disk to its own instance."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "Compute Engine allows attaching a standard or SSD persistent disk in read-only mode to multiple VM instances simultaneously within the same zone. This provides low-latency, block-level filesystem access (C). GCS FUSE (A) provides file system access to GCS but generally has higher latency than local PDs. Copying data on startup (B) slows down instance start time. Creating disks from snapshots (D) duplicates data, increasing costs compared to sharing one disk.",
            "conditions": [
                "Share large read-only dataset across MIG instances, fast start, low latency filesystem access, minimize cost."
            ],
            "caseStudyContext": null
        },
        {
            "id": 34,
            "topic": "Networking",
            "question": "You are developing an HTTP API hosted on a Compute Engine virtual machine instance that needs to be invoked by multiple clients within the same Virtual Private Cloud (VPC). You want clients to be able to get the IP address of the service.\nWhat should you do?",
            "options": {
                "A": "Reserve a static external IP address and assign it to an HTTP(S) load balancing service's forwarding rule. Clients should use this IP address to connect to the service.",
                "B": "Reserve a static external IP address and assign it to an HTTP(S) load balancing service's forwarding rule. Then, define an A record in Cloud DNS. Clients should use the name of the A record to connect to the service.",
                "C": "Ensure that clients use Compute Engine internal DNS by connecting to the instance name with the url https://[INSTANCE_NAME].[ZONE].c.[PROJECT_ID].internal/.",
                "D": "Ensure that clients use Compute Engine internal DNS by connecting to the instance name with the url https://[API_NAME]/[API_VERSION]/."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "For communication *within* the same VPC, using external IPs and load balancers (A, B) is unnecessary and less efficient. Compute Engine provides an internal DNS service that allows instances to resolve each other's internal IP addresses using their fully qualified internal DNS names (`[INSTANCE_NAME].[ZONE].c.[PROJECT_ID].internal`). This is the standard and recommended way for internal service discovery (C). Option D uses an invalid URL format.",
            "conditions": [
                "GCE hosted HTTP API, clients within same VPC need to discover/connect."
            ],
            "caseStudyContext": null
        },
        {
            "id": 35,
            "topic": "Monitoring & Logging",
            "question": "Your application is logging to Stackdriver. You want to get the count of all requests on all /api/alpha/* endpoints.\nWhat should you do?",
            "options": {
                "A": "Add a Stackdriver counter metric for path:/api/alpha/.",
                "B": "Add a Stackdriver counter metric for endpoint:/api/alpha/*.",
                "C": "Export the logs to Cloud Storage and count lines matching /api/alpha.",
                "D": "Export the logs to Cloud Pub/Sub and count lines matching /api/alpha."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "Cloud Logging allows creating log-based metrics to count log entries matching specific filters. You can define a counter metric based on log entries where the request path or endpoint matches the pattern `/api/alpha/*`. Option B uses a plausible filter syntax (`endpoint:/api/alpha/*`) which is suitable for matching URL paths with wildcards. Option A (`path:/api/alpha/`) likely wouldn't match sub-paths correctly. Exporting logs (C, D) requires external processing and is less efficient for simple counting.",
            "conditions": [
                "Count log entries for requests matching path /api/alpha/* in Cloud Logging."
            ],
            "caseStudyContext": null
        },
        {
            "id": 36,
            "topic": "Compute",
            "question": "You want to re-architect a monolithic application so that it follows a microservices model. You want to accomplish this efficiently while minimizing the impact of this change to the business.\nWhich approach should you take?",
            "options": {
                "A": "Deploy the application to Compute Engine and turn on autoscaling.",
                "B": "Replace the application's features with appropriate microservices in phases.",
                "C": "Refactor the monolithic application with appropriate microservices in a single effort and deploy it.",
                "D": "Build a new application with the appropriate microservices separate from the monolith and replace it when it is complete."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "Migrating a monolith to microservices carries risk. The Strangler Fig pattern, which involves replacing features or functionalities incrementally with microservices (B), is the recommended approach to minimize risk and business impact. This allows gradual rollout, testing, and learning. A big-bang refactor (C) or replacement (D) is much riskier. Simply deploying to GCE with autoscaling (A) doesn't change the monolithic architecture.",
            "conditions": [
                "Re-architect monolith to microservices, minimize business impact."
            ],
            "caseStudyContext": null
        },
        {
            "id": 37,
            "topic": "Databases",
            "question": "Your existing application keeps user state information in a single MySQL database. This state information is very user-specific and depends heavily on how long a user has been using an application. The MySQL database is causing challenges to maintain and enhance the schema for various users.\nWhich storage option should you choose?",
            "options": {
                "A": "Cloud SQL",
                "B": "Cloud Storage",
                "C": "Cloud Spanner",
                "D": "Cloud Datastore/Firestore"
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "The key challenges are maintaining and enhancing the schema due to user-specific variations. This points towards needing a flexible schema, which NoSQL databases excel at. Firestore (or Datastore mode) is a managed, scalable NoSQL document database suitable for user profiles and semi-structured data, allowing schema flexibility. Cloud SQL (A) and Spanner (C) are relational and have stricter schemas. Cloud Storage (B) is object storage, not ideal for structured state data.",
            "conditions": [
                "Storing user state, currently MySQL, schema maintenance/enhancement challenges due to user variations."
            ],
            "caseStudyContext": null
        },
        {
            "id": 38,
            "topic": "Networking",
            "question": "You are building a new API. You want to minimize the cost of storing and reduce the latency of serving images.\nWhich architecture should you use?",
            "options": {
                "A": "App Engine backed by Cloud Storage",
                "B": "Compute Engine backed by Persistent Disk",
                "C": "Transfer Appliance backed by Cloud Filestore",
                "D": "Cloud Content Delivery Network (CDN) backed by Cloud Storage"
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "For serving static assets like images globally with low latency and cost, Cloud Storage combined with Cloud CDN is the standard best practice. Cloud Storage offers low-cost, scalable object storage. Cloud CDN caches the images at edge locations closer to users, significantly reducing latency and egress costs. App Engine (A) or Compute Engine (B) serving images directly is less efficient and more costly than using a CDN. Transfer Appliance/Filestore (C) are not relevant for serving web content.",
            "conditions": [
                "API needs to store and serve images, minimize storage cost, reduce serving latency."
            ],
            "caseStudyContext": null
        },
        {
            "id": 39,
            "topic": "IAM & Security",
            "question": "Your company's development teams want to use Cloud Build in their projects to build and push Docker images to Container Registry. The operations team requires all Docker images to be published to a centralized, securely managed Docker registry that the operations team manages.\nWhat should you do?",
            "options": {
                "A": "Use Container Registry to create a registry in each development team's project. Configure the Cloud Build build to push the Docker image to the project's registry. Grant the operations team access to each development team's registry.",
                "B": "Create a separate project for the operations team that has Container Registry configured. Assign appropriate permissions to the Cloud Build service account in each developer team's project to allow access to the operation team's registry.",
                "C": "Create a separate project for the operations team that has Container Registry configured. Create a Service Account for each development team and assign the appropriate permissions to allow it access to the operations team's registry. Store the service account key file in the source code repository and use it to authenticate against the operations team's registry.",
                "D": "Create a separate project for the operations team that has the open source Docker Registry deployed on a Compute Engine virtual machine instance. Create a username and password for each development team. Store the username and password in the source code repository and use it to authenticate against the operations team's Docker registry."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "To centralize registry management under the Ops team while allowing Dev teams' Cloud Build pipelines to push images, create a dedicated Ops project with Container Registry (or Artifact Registry). Then, grant the Cloud Build service account from each Dev project the necessary IAM permissions (e.g., Storage Object Creator/Admin on the underlying GCS bucket, or Artifact Registry Writer role) on the Ops team's registry. This avoids managing registries per project (A) and insecure practices like storing keys in source code (C, D) or running unmanaged registries (D).",
            "conditions": [
                "Dev teams use Cloud Build, Ops manages a centralized/secure container registry, Cloud Build needs push access."
            ],
            "caseStudyContext": null
        },
        {
            "id": 40,
            "topic": "Compute",
            "question": "You are planning to deploy your application in a Google Kubernetes Engine (GKE) cluster. Your application can scale horizontally, and each instance of your application needs to have a stable network identity and its own persistent disk.\nWhich GKE object should you use?",
            "options": {
                "A": "Deployment",
                "B": "StatefulSet",
                "C": "ReplicaSet",
                "D": "ReplicaController"
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "StatefulSets are designed for applications that require stable, unique network identifiers (e.g., predictable hostnames like `my-app-0`, `my-app-1`), stable persistent storage per replica, and ordered, graceful deployment and scaling. Deployments are for stateless applications where replicas are interchangeable. ReplicaSets/ReplicaControllers manage pod replicas but don't provide stable identities or storage.",
            "conditions": [
                "GKE application, needs horizontal scaling, stable network identity per instance, persistent disk per instance."
            ],
            "caseStudyContext": null
        },
        {
            "id": 41,
            "topic": "Compute",
            "question": "You are using Cloud Build to build a Docker image. You need to modify the build to execute unit and run integration tests. When there is a failure, you want the build history to clearly display the stage at which the build failed.\nWhat should you do?",
            "options": {
                "A": "Add RUN commands in the Dockerfile to execute unit and integration tests.",
                "B": "Create a Cloud Build build config file with a single build step to compile unit and integration tests.",
                "C": "Create a Cloud Build build config file that will spawn a separate cloud build pipeline for unit and integration tests.",
                "D": "Create a Cloud Build build config file with separate cloud builder steps to compile and execute unit and integration tests."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "Cloud Build configuration files (`cloudbuild.yaml`) allow defining multiple build steps. Defining separate steps for compilation, unit tests, and integration tests (D) provides clear separation in the build history. If a step fails, Cloud Build marks that specific step as failed, making it easy to identify the stage of failure. Running tests inside the Dockerfile (A) makes the image larger and couples testing with image building. A single step (B) obscures which part failed. Spawning separate pipelines (C) adds unnecessary complexity.",
            "conditions": [
                "Cloud Build Docker build, need to add unit/integration tests, clearly show failed stage."
            ],
            "caseStudyContext": null
        },
        {
            "id": 42,
            "topic": "IAM & Security",
            "question": "Your code is running on Cloud Functions in project A. It is supposed to write an object in a Cloud Storage bucket owned by project B. However, the write call is failing with the error \"403 Forbidden\".\nWhat should you do to correct the problem?",
            "options": {
                "A": "Grant your user account the roles/storage.objectCreator role for the Cloud Storage bucket.",
                "B": "Grant your user account the roles/iam.serviceAccountUser role for the service-PROJECTA@gcf-admin-robot.iam.gserviceaccount.com service account.",
                "C": "Grant the service-PROJECTA@gcf-admin-robot.iam.gserviceaccount.com service account the roles/storage.objectCreator role for the Cloud Storage bucket.",
                "D": "Enable the Cloud Storage API in project B."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "Cloud Functions execute using an identity, which by default is the project-specific Cloud Functions service agent (`service-<PROJECT_NUMBER>@gcf-admin-robot.iam.gserviceaccount.com`) or a specified runtime service account. A 403 Forbidden error indicates the identity making the call lacks permission. To allow the function in Project A to write to a bucket in Project B, the function's service account identity (from Project A) must be granted the appropriate role (like `roles/storage.objectCreator`) on the target bucket in Project B.",
            "conditions": [
                "Cloud Function in Project A writing to GCS bucket in Project B fails with 403."
            ],
            "caseStudyContext": null
        },
        {
            "id": 43,
            "topic": "Case Study Analysis",
            "question": "Case study - HipLocal: HipLocal's .net-based auth service fails under intermittent load. What should they do?",
            "options": {
                "A": "Use App Engine for autoscaling.",
                "B": "Use Cloud Functions for autoscaling.",
                "C": "Use a Compute Engine cluster for the service.",
                "D": "Use a dedicated Compute Engine virtual machine instance for the service."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "The case study's technical requirements state a need to 'Move to serverless architecture to facilitate elastic scaling.' The auth service is .NET based and fails under load. App Engine (specifically Flexible environment) supports .NET, is serverless (or server-managed), and provides automatic scaling, addressing both the technical requirement and the load issue. Cloud Functions (B) are less suitable for full services. Compute Engine options (C, D) are not serverless and require more management.",
            "conditions": [
                "HipLocal case study context, .NET auth service fails under load, need autoscaling, prefer serverless."
            ],
            "caseStudyContext": "<strong>Company overview</strong>\nHipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.\n\n<strong>Executive statement</strong>\nWe are the number one local community app; it’s time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.\n\n<strong><strong>Solution Concept </strong><br></strong>\nHipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.\n\n<strong><strong>Existing Technical Environment </strong><br></strong>\nHipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:\n<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>\n\n<strong><strong>Business Requirements </strong><br></strong>\nHipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:\n<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>\n\n<strong>Technical requirements</strong>\n<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>"
        },
        {
            "id": 44,
            "topic": "Case Study Analysis",
            "question": "Case study - HipLocal: HipLocal's APIs are having occasional application failures. They want to collect application information specifically to troubleshoot the issue. What should they do?",
            "options": {
                "A": "Take frequent snapshots of the virtual machines.",
                "B": "Install the Cloud Logging agent on the virtual machines.",
                "C": "Install the Cloud Monitoring agent on the virtual machines.",
                "D": "Use Cloud Trace to look for performance bottlenecks."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "The case study explicitly states 'The application has no logging.' To troubleshoot application failures, collecting logs is essential. Installing the Cloud Logging agent on the Compute Engine VMs where the APIs run will enable log collection without modifying the application code itself. Snapshots (A) are for backups. Monitoring agent (C) collects metrics, not detailed application logs for failure analysis. Trace (D) analyzes latency but requires application instrumentation and might not directly reveal the cause of failures without logs.",
            "conditions": [
                "HipLocal case study context, APIs on GCE failing occasionally, need troubleshooting info, application has no logging currently."
            ],
            "caseStudyContext": "<strong>Company overview</strong>\nHipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.\n\n<strong>Executive statement</strong>\nWe are the number one local community app; it’s time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.\n\n<strong><strong>Solution Concept </strong><br></strong>\nHipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.\n\n<strong><strong>Existing Technical Environment </strong><br></strong>\nHipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:\n<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>\n\n<strong><strong>Business Requirements </strong><br></strong>\nHipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:\n<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>\n\n<strong>Technical requirements</strong>\n<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>"
        },
        {
            "id": 45,
            "topic": "Case Study Analysis",
            "question": "Case study - HipLocal: HipLocal has connected their Hadoop infrastructure to GCP using Cloud Interconnect in order to query data stored on persistent disks. Which IP strategy should they use?",
            "options": {
                "A": "Create manual subnets.",
                "B": "Create an auto mode subnet.",
                "C": "Create multiple peered VPCs.",
                "D": "Provision a single instance for NAT."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "Connecting an on-premises network (like the one hosting Hadoop) to GCP via Cloud Interconnect requires careful IP address planning to avoid conflicts between on-prem ranges and VPC subnet ranges. Using custom mode VPC with manual subnets (A) provides full control over VPC subnet IP ranges, allowing HipLocal to ensure they don't overlap with their existing on-premises IP space. Auto mode VPC (B) assigns predefined CIDR ranges per region, which might conflict with on-prem ranges.",
            "conditions": [
                "HipLocal case study context, connecting on-prem Hadoop to GCP via Cloud Interconnect, need VPC IP strategy."
            ],
            "caseStudyContext": "<strong>Company overview</strong>\nHipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.\n\n<strong>Executive statement</strong>\nWe are the number one local community app; it’s time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.\n\n<strong><strong>Solution Concept </strong><br></strong>\nHipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.\n\n<strong><strong>Existing Technical Environment </strong><br></strong>\nHipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:\n<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>\n\n<strong><strong>Business Requirements </strong><br></strong>\nHipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:\n<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>\n\n<strong>Technical requirements</strong>\n<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>"
        },
        {
            "id": 46,
            "topic": "Case Study Analysis",
            "question": "Case study - HipLocal: Which service should HipLocal use to enable access to internal apps?",
            "options": {
                "A": "Cloud VPN",
                "B": "Cloud Armor",
                "C": "Virtual Private Cloud",
                "D": "Cloud Identity-Aware Proxy"
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "The technical requirements specifically state: 'Provide authorized access to internal apps in a secure manner.' Cloud Identity-Aware Proxy (IAP) is designed for this exact purpose. It provides secure, identity-based access control to applications hosted on GCP (or even on-prem with connectors) without requiring a traditional VPN. Cloud VPN (A) provides network connectivity. Cloud Armor (B) provides DDoS/WAF protection. VPC (C) is the network itself.",
            "conditions": [
                "HipLocal case study context, need secure access to internal apps."
            ],
            "caseStudyContext": "<strong>Company overview</strong>\nHipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.\n\n<strong>Executive statement</strong>\nWe are the number one local community app; it’s time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.\n\n<strong><strong>Solution Concept </strong><br></strong>\nHipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.\n\n<strong><strong>Existing Technical Environment </strong><br></strong>\nHipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:\n<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>\n\n<strong><strong>Business Requirements </strong><br></strong>\nHipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:\n<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>\n\n<strong>Technical requirements</strong>\n<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>"
        },
        {
            "id": 47,
            "topic": "Case Study Analysis",
            "question": "Case study - HipLocal: HipLocal wants to reduce the number of on-call engineers and eliminate manual scaling. Which two services should they choose? (Choose two.)",
            "options": {
                "A": "Use Google App Engine services.",
                "B": "Use serverless Google Cloud Functions.",
                "C": "Use Knative to build and deploy serverless applications.",
                "D": "Use Google Kubernetes Engine for automated deployments.",
                "E": "Use a large Google Compute Engine cluster for deployments."
            },
            "correctAnswer": [
                "A",
                "B"
            ],
            "explanation": "The technical requirements include 'Move to serverless architecture to facilitate elastic scaling' and a business requirement is 'Reduce infrastructure management time and cost'. Google App Engine (A) and Cloud Functions (B) are fully managed, serverless platforms that automatically handle scaling based on load, eliminating manual scaling and reducing operational overhead for on-call engineers. Knative (C) provides serverless capabilities but often runs on GKE (D), which still requires some cluster management. GCE (E) requires manual scaling configuration.",
            "conditions": [
                "HipLocal case study context, reduce on-call engineers, eliminate manual scaling, prefer serverless."
            ],
            "caseStudyContext": "<strong>Company overview</strong>\nHipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.\n\n<strong>Executive statement</strong>\nWe are the number one local community app; it’s time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.\n\n<strong><strong>Solution Concept </strong><br></strong>\nHipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.\n\n<strong><strong>Existing Technical Environment </strong><br></strong>\nHipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:\n<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>\n\n<strong><strong>Business Requirements </strong><br></strong>\nHipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:\n<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>\n\n<strong>Technical requirements</strong>\n<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>"
        },
        {
            "id": 48,
            "topic": "Case Study Analysis",
            "question": "Case study - HipLocal: In order to meet their <strong>Business Requirements </strong><br>, how should HipLocal store their application state?",
            "options": {
                "A": "Use local SSDs to store state.",
                "B": "Put a memcache layer in front of MySQL.",
                "C": "Move the state storage to Cloud Spanner.",
                "D": "Replace the MySQL instance with Cloud SQL."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "<strong>Business Requirements </strong><br> include expanding globally, supporting 10x users, and ensuring a consistent experience across regions. The current state is stored in a single MySQL instance. This won't scale globally or provide a consistent low-latency experience. Cloud Spanner (C) is a globally distributed, strongly consistent, horizontally scalable relational database designed for these requirements. Cloud SQL (D) is regional. Memcache (B) helps with read latency but doesn't solve the global scaling/consistency issue for writes. Local SSDs (A) are ephemeral or local to a single VM.",
            "conditions": [
                "HipLocal case study context, store application state, meet requirements of global expansion, high concurrency, consistency."
            ],
            "caseStudyContext": "<strong>Company overview</strong>\nHipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.\n\n<strong>Executive statement</strong>\nWe are the number one local community app; it’s time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.\n\n<strong><strong>Solution Concept </strong><br></strong>\nHipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.\n\n<strong><strong>Existing Technical Environment </strong><br></strong>\nHipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:\n<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>\n\n<strong><strong>Business Requirements </strong><br></strong>\nHipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:\n<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>\n\n<strong>Technical requirements</strong>\n<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>"
        },
        {
            "id": 49,
            "topic": "Case Study Analysis",
            "question": "Case study - HipLocal: Which service should HipLocal use for their public APIs?",
            "options": {
                "A": "Cloud Armor",
                "B": "Cloud Functions",
                "C": "Cloud Endpoints",
                "D": "Shielded Virtual Machines"
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "The technical requirements state 'APIs require strong authentication and authorization.' Cloud Endpoints (or Apigee) provides a managed API gateway solution that handles authentication, authorization, monitoring, quotas, and other API management tasks. Cloud Armor (A) is for WAF/DDoS. Cloud Functions (B) can host APIs but lack built-in management features. Shielded VMs (D) relate to infrastructure security, not API management.",
            "conditions": [
                "HipLocal case study context, need service for public APIs, requires auth/authz."
            ],
            "caseStudyContext": "<strong>Company overview</strong>\nHipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.\n\n<strong>Executive statement</strong>\nWe are the number one local community app; it’s time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.\n\n<strong><strong>Solution Concept </strong><br></strong>\nHipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.\n\n<strong><strong>Existing Technical Environment </strong><br></strong>\nHipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:\n<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>\n\n<strong><strong>Business Requirements </strong><br></strong>\nHipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:\n<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>\n\n<strong>Technical requirements</strong>\n<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>"
        },
        {
            "id": 50,
            "topic": "Case Study Analysis",
            "question": "Case study - HipLocal: HipLocal wants to improve the resilience of their MySQL deployment, while also meeting their business and technical requirements. Which configuration should they choose?",
            "options": {
                "A": "Use the current single instance MySQL on Compute Engine and several read-only MySQL servers on Compute Engine.",
                "B": "Use the current single instance MySQL on Compute Engine, and replicate the data to Cloud SQL in an external master configuration.",
                "C": "Replace the current single instance MySQL instance with Cloud SQL, and configure high availability.",
                "D": "Replace the current single instance MySQL instance with Cloud SQL, and Google provides redundancy without further configuration."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "HipLocal currently uses a single MySQL instance on GCE. To improve resilience and reduce management ('Reduce infrastructure management time and cost'), migrating to the managed Cloud SQL service is recommended. Configuring Cloud SQL for High Availability (HA) (C) provides automatic failover to a standby instance in a different zone within the same region, significantly improving resilience over a single instance. Adding read replicas (A) improves read scalability but not write resilience. External master setup (B) is complex. Cloud SQL requires explicit HA configuration (D is incorrect).",
            "conditions": [
                "HipLocal case study context, improve resilience of MySQL deployment (currently single GCE instance), meet business/tech reqs."
            ],
            "caseStudyContext": "<strong>Company overview</strong>\nHipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.\n\n<strong>Executive statement</strong>\nWe are the number one local community app; it’s time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.\n\n<strong><strong>Solution Concept </strong><br></strong>\nHipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.\n\n<strong><strong>Existing Technical Environment </strong><br></strong>\nHipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:\n<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>\n\n<strong><strong>Business Requirements </strong><br></strong>\nHipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:\n<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>\n\n<strong>Technical requirements</strong>\n<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>"
        },
        {
            "id": 51,
            "topic": "Monitoring & Logging",
            "question": "Your application is running in multiple Google Kubernetes Engine clusters. It is managed by a Deployment in each cluster. The Deployment has created multiple replicas of your Pod in each cluster. You want to view the logs sent to stdout for all of the replicas in your Deployment in all clusters.\nWhich command should you use?",
            "options": {
                "A": "kubectl logs [PARAM]",
                "B": "gcloud logging read [PARAM]",
                "C": "kubectl exec ‫ג‬€\"it [PARAM] journalctl",
                "D": "gcloud compute ssh [PARAM] ‫ג‬€\"-command= ‫ג‬€sudo journalctl‫ג‬€"
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "GKE integrates with Cloud Logging, which aggregates logs from all nodes and pods across clusters within a project (or configured scope). `gcloud logging read` allows querying these aggregated logs using filters based on resource type (k8s_container, k8s_pod, k8s_cluster), labels (like app name), etc., enabling viewing logs from all replicas across multiple clusters. `kubectl logs` targets pods within a specific cluster context. `kubectl exec` runs commands inside a pod. `gcloud compute ssh` connects to GCE nodes.",
            "conditions": [
                "View stdout logs for all replicas of a Deployment across multiple GKE clusters."
            ],
            "caseStudyContext": null
        },
        {
            "id": 52,
            "topic": "Compute",
            "question": "You are using Cloud Build to create a new Docker image on each source code commit to a Cloud Source Repositories repository. Your application is built on every commit to the master branch. You want to release specific commits made to the master branch in an automated method.\nWhat should you do?",
            "options": {
                "A": "Manually trigger the build for new releases.",
                "B": "Create a build trigger on a Git tag pattern. Use a Git tag convention for new releases.",
                "C": "Create a build trigger on a Git branch name pattern. Use a Git branch naming convention for new releases.",
                "D": "Commit your source code to a second Cloud Source Repositories repository with a second Cloud Build trigger. Use this repository for new releases only."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "While builds happen on every commit to master, releases usually correspond to specific, tagged commits. Cloud Build triggers can be configured to activate based on Git tags matching a specific pattern (e.g., `v*.*.*` or `release-*`). This allows developers to tag a specific commit on the master branch when it's ready for release, automatically triggering the release build/deployment process. Triggering on branch name (C) doesn't target specific commits. Manual triggers (A) aren't automated. A second repo (D) adds unnecessary complexity.",
            "conditions": [
                "Cloud Build builds on every master commit, need automated release for specific commits."
            ],
            "caseStudyContext": null
        },
        {
            "id": 53,
            "topic": "Databases",
            "question": "You are designing a schema for a table that will be moved from MySQL to Cloud Bigtable. The MySQL table is as follows:\n\nCREATE TABLE events (\n  Account_id VARCHAR(32),\n  Event_timestamp TIMESTAMP,\n  Event_type VARCHAR(16),\n  Event_details VARCHAR(1024),\n  PRIMARY KEY (Account_id, Event_timestamp)\n);\n\nHow should you design a row key for Cloud Bigtable for this table?",
            "options": {
                "A": "Set Account_id as a key.",
                "B": "Set Account_id_Event_timestamp as a key.",
                "C": "Set Event_timestamp_Account_id as a key.",
                "D": "Set Event_timestamp as a key."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "The goal is to uniquely identify each event row and design for efficient querying and avoiding hotspots. The MySQL primary key is composite (Account_id, Event_timestamp). A common Bigtable pattern is to concatenate primary key components. Starting the row key with a high-cardinality field like `Account_id` helps distribute writes across nodes, avoiding hotspots that can occur if keys start with a monotonically increasing value like a timestamp. Therefore, `Account_id#Event_timestamp` (or similar concatenation represented by B) is generally the best practice.",
            "conditions": [
                "Migrating MySQL table with composite PK (Account_id, Event_timestamp) to Bigtable, design row key."
            ],
            "caseStudyContext": null
        },
        {
            "id": 54,
            "topic": "Monitoring & Logging",
            "question": "You want to view the memory usage of your application deployed on Compute Engine.\nWhat should you do?",
            "options": {
                "A": "Install the Stackdriver Client Library.",
                "B": "Install the Stackdriver Monitoring Agent.",
                "C": "Use the Stackdriver Metrics Explorer.",
                "D": "Use the Google Cloud Platform Console."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "By default, Compute Engine instances send basic metrics (like CPU usage, disk I/O, network traffic) to Cloud Monitoring. However, memory utilization and disk space utilization are not included by default. To collect these metrics, you must install the Cloud Monitoring Agent (or the newer Ops Agent) on the instance. Once the agent is installed and sending data, you can view memory usage in Metrics Explorer (C) or the Cloud Console (D), but installation (B) is the prerequisite.",
            "conditions": [
                "Need to view memory usage of GCE application."
            ],
            "caseStudyContext": null
        },
        {
            "id": 55,
            "topic": "Monitoring & Logging",
            "question": "You have an analytics application that runs hundreds of queries on BigQuery every few minutes using BigQuery API. You want to find out how much time these queries take to execute.\nWhat should you do?",
            "options": {
                "A": "Use Stackdriver Monitoring to plot slot usage.",
                "B": "Use Stackdriver Trace to plot API execution time.",
                "C": "Use Stackdriver Trace to plot query execution time.",
                "D": "Use Stackdriver Monitoring to plot query execution times."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "BigQuery automatically sends metrics about job execution, including query execution time, to Cloud Monitoring. You can view these metrics (like `job/query_execution_times`) in Metrics Explorer or create dashboards and alerts based on them. Plotting slot usage (A) shows resource consumption, not execution time. Cloud Trace (B, C) is typically used for tracing requests across distributed services, not for monitoring the duration of individual BigQuery jobs submitted via the API.",
            "conditions": [
                "Need to find execution time for BigQuery queries run via API."
            ],
            "caseStudyContext": null
        },
        {
            "id": 56,
            "topic": "Databases",
            "question": "You are designing a schema for a Cloud Spanner customer database. You want to store a phone number array field in a customer table. You also want to allow users to search customers by phone number.\nHow should you design this schema?",
            "options": {
                "A": "Create a table named Customers. Add an Array field in a table that will hold phone numbers for the customer.",
                "B": "Create a table named Customers. Create a table named Phones. Add a CustomerId field in the Phones table to find the CustomerId from a phone number.",
                "C": "Create a table named Customers. Add an Array field in a table that will hold phone numbers for the customer. Create a secondary index on the Array field.",
                "D": "Create a table named Customers as a parent table. Create a table named Phones, and interleave this table into the Customer table. Create an index on the phone number field in the Phones table."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "Cloud Spanner does not support secondary indexes directly on ARRAY type columns (eliminating C). To efficiently search by phone number when a customer can have multiple numbers, a separate, related table is needed. Interleaving the `Phones` table within the `Customers` table (D) physically co-locates a customer's phones with their main record, which can improve performance for queries fetching a customer and their phones. Creating a secondary index on the phone number column in the `Phones` table enables efficient lookups by phone number. A non-interleaved table (B) is also possible but interleaving is often preferred for 1-to-many relationships frequently accessed together.",
            "conditions": [
                "Spanner DB, store multiple phone numbers per customer, search by phone number."
            ],
            "caseStudyContext": null
        },
        {
            "id": 57,
            "topic": "Compute",
            "question": "You are deploying a single website on App Engine that needs to be accessible via the URL http://www.altostrat.com/.\nWhat should you do?",
            "options": {
                "A": "Verify domain ownership with Webmaster Central. Create a DNS CNAME record to point to the App Engine canonical name ghs.googlehosted.com.",
                "B": "Verify domain ownership with Webmaster Central. Define an A record pointing to the single global App Engine IP address.",
                "C": "Define a mapping in dispatch.yaml to point the domain www.altostrat.com to your App Engine service. Create a DNS CNAME record to point to the App Engine canonical name ghs.googlehosted.com.",
                "D": "Define a mapping in dispatch.yaml to point the domain www.altostrat.com to your App Engine service. Define an A record pointing to the single global App Engine IP address."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "To map a custom domain to an App Engine application, you first need to verify ownership of the domain (e.g., via Google Search Console/Webmaster Central). Then, you configure the domain mapping within the GCP project settings for App Engine. Finally, you update your domain's DNS records. For a `www` subdomain (or other subdomains), the standard practice is to create a CNAME record pointing to `ghs.googlehosted.com`. A records (B, D) point to specific IP addresses, which isn't the recommended method for App Engine's managed infrastructure. `dispatch.yaml` (C, D) is used for routing requests to different services within an App Engine app, not for mapping external domains.",
            "conditions": [
                "Deploy App Engine website, make accessible via custom domain www.altostrat.com."
            ],
            "caseStudyContext": null
        },
        {
            "id": 58,
            "topic": "IAM & Security",
            "question": "You are running an application on App Engine that you inherited. You want to find out whether the application is using insecure binaries or is vulnerable to XSS attacks.\nWhich service should you use?",
            "options": {
                "A": "Cloud Amor",
                "B": "Stackdriver Debugger",
                "C": "Cloud Security Scanner",
                "D": "Stackdriver Error Reporting"
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "Cloud Security Scanner is specifically designed to scan App Engine (Standard and Flex), GKE, and Compute Engine web applications for common vulnerabilities, including cross-site scripting (XSS), outdated libraries (which might include insecure binaries), mixed content, and others. Cloud Armor (A) is for WAF/DDoS protection. Debugger (B) is for inspecting running code. Error Reporting (D) aggregates application errors.",
            "conditions": [
                "App Engine application, check for insecure binaries and XSS vulnerabilities."
            ],
            "caseStudyContext": null
        },
        {
            "id": 59,
            "topic": "Storage",
            "question": "You are working on a social media application. You plan to add a feature that allows users to upload images. These images will be 2 MB ‫ג‬€\" 1 GB in size. You want to minimize their infrastructure operations overhead for this feature.\nWhat should you do?",
            "options": {
                "A": "Change the application to accept images directly and store them in the database that stores other user information.",
                "B": "Change the application to create signed URLs for Cloud Storage. Transfer these signed URLs to the client application to upload images to Cloud Storage.",
                "C": "Set up a web server on GCP to accept user images and create a file store to keep uploaded files. Change the application to retrieve images from the file store.",
                "D": "Create a separate bucket for each user in Cloud Storage. Assign a separate service account to allow write access on each bucket. Transfer service account credentials to the client application based on user information. The application uses this service account to upload images to Cloud Storage."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "Storing large binary objects like images/videos directly in databases (A) is inefficient. Setting up dedicated web servers and file stores (C) increases operational overhead. Managing buckets and service accounts per user (D) is extremely complex and insecure (transferring SA creds to client). The best practice for minimizing operational overhead and handling large uploads securely is to use Signed URLs (B). The application backend generates a short-lived URL granting the client permission to upload directly to a specific Cloud Storage location, offloading the transfer from the application servers.",
            "conditions": [
                "Social media app feature: user image uploads (2MB-1GB). Minimize infrastructure operations overhead."
            ],
            "caseStudyContext": null
        },
        {
            "id": 60,
            "topic": "Compute",
            "question": "Your application is built as a custom machine image. You have multiple unique deployments of the machine image. Each deployment is a separate managed instance group with its own template. Each deployment requires a unique set of configuration values. You want to provide these unique values to each deployment but use the same custom machine image in all deployments. You want to use out-of-the-box features of Compute Engine.\nWhat should you do?",
            "options": {
                "A": "Place the unique configuration values in the persistent disk.",
                "B": "Place the unique configuration values in a Cloud Bigtable table.",
                "C": "Place the unique configuration values in the instance template startup script.",
                "D": "Place the unique configuration values in the instance template instance metadata."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "Compute Engine instance metadata allows you to define key-value pairs specific to an instance or, more relevantly here, within an instance template used by a managed instance group. Applications running on the instances can query the metadata server to retrieve these values at runtime. This allows you to use the same base image but provide different configurations (e.g., database endpoints, API keys) to different deployments by setting unique metadata in each instance template (D). Storing config on PD (A) is possible but less standard. Bigtable (B) is overkill. Embedding in startup scripts (C) makes updates harder.",
            "conditions": [
                "Deploy multiple unique MIGs from same custom image, need unique config per deployment, use GCE features."
            ],
            "caseStudyContext": null
        },
        {
            "id": 61,
            "topic": "Monitoring & Logging",
            "question": "Your application performs well when tested locally, but it runs significantly slower after you deploy it to a Compute Engine instance. You need to diagnose the problem. What should you do?",
            "options": {
                "A": "File a ticket with Cloud Support indicating that the application performs faster locally.",
                "B": "Use Cloud Debugger snapshots to look at a point-in-time execution of the application.",
                "C": "Use Cloud Profiler to determine which functions within the application take the longest amount of time.",
                "D": "Add logging commands to the application and use Cloud Logging to check where the latency problem occurs."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "When an application runs slower in a different environment, performance profiling is needed to identify bottlenecks. Cloud Profiler continuously collects CPU usage and memory allocation data from production applications with low overhead. It attributes this information to the source code, helping pinpoint which functions or code paths are consuming the most resources or time (C). Filing a ticket (A) isn't a diagnostic step. Debugger snapshots (B) show state at one point, not performance over time. Adding logging (D) can help but Profiler is specifically designed for performance analysis.",
            "conditions": [
                "Application slower on GCE than locally, need to diagnose performance issue."
            ],
            "caseStudyContext": null
        },
        {
            "id": 62,
            "topic": "Compute",
            "question": "You have an application running in App Engine. Your application is instrumented with Stackdriver Trace. The /product-details request reports details about four known unique products at /sku-details as shown below. You want to reduce the time it takes for the request to complete.\nWhat should you do?",
            "options": {
                "A": "Increase the size of the instance class.",
                "B": "Change the Persistent Disk type to SSD.",
                "C": "Change /product-details to perform the requests in parallel.",
                "D": "Store the /sku-details information in a database, and replace the webservice call with a database query."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "The trace likely shows four sequential calls to `/sku-details`, each taking some time, contributing to the total latency of `/product-details`. Since fetching details for one product likely doesn't depend on another, these calls can be made concurrently (in parallel). This significantly reduces the total time, as the calls execute largely simultaneously instead of one after another (C). Increasing instance size (A) or changing disk type (B - App Engine doesn't use user-managed PDs) might not help if the bottleneck is sequential network calls. Storing in a DB (D) might be an option but parallelizing existing calls is a more direct fix for the observed sequential bottleneck.",
            "conditions": [
                "App Engine app, Trace shows sequential calls to /sku-details within /product-details request, need to reduce latency."
            ],
            "caseStudyContext": null
        },
        {
            "id": 63,
            "topic": "Databases",
            "question": "Your company has a data warehouse that keeps your application information in BigQuery. The BigQuery data warehouse keeps 2 PBs of user data. Recently, your company expanded your user base to include EU users and needs to comply with these requirements:\n\n✑ Your company must be able to delete all user account information upon user request.\n✑ All EU user data must be stored in a single region specifically for EU users.\nWhich two actions should you take? (Choose two.)",
            "options": {
                "A": "Use BigQuery federated queries to query data from Cloud Storage.",
                "B": "Create a dataset in the EU region that will keep information about EU users only.",
                "C": "Create a Cloud Storage bucket in the EU region to store information for EU users only.",
                "D": "Re-upload your data using to a Cloud Dataflow pipeline by filtering your user records out.",
                "E": "Use DML statements in BigQuery to update/delete user records based on their requests."
            },
            "correctAnswer": [
                "B",
                "E"
            ],
            "explanation": "To comply with the EU data residency requirement, create a separate BigQuery dataset located in an EU region specifically for EU user data (B). This ensures their data stays within the EU. To comply with the right to erasure (delete user data on request), use BigQuery's Data Manipulation Language (DML) statements, specifically `DELETE`, to remove specific user records from the tables (E). Federated queries (A), GCS buckets (C), and Dataflow filtering during re-upload (D) don't directly address both requirements within the BigQuery warehouse context.",
            "conditions": [
                "BigQuery data warehouse (2PB), EU users added, GDPR compliance needed (data deletion, EU data residency)."
            ],
            "caseStudyContext": null
        },
        {
            "id": 64,
            "topic": "Compute",
            "question": "Your App Engine standard configuration is as follows:\nservice: production\ninstance_class: B1\nYou want to limit the application to 5 instances.\nWhich code snippet should you include in your configuration?",
            "options": {
                "A": "manual_scaling: instances: 5 min_pending_latency: 30ms",
                "B": "manual_scaling: max_instances: 5 idle_timeout: 10m",
                "C": "basic_scaling: instances: 5 min_pending_latency: 30ms",
                "D": "basic_scaling: max_instances: 5 idle_timeout: 10m"
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "App Engine Standard offers several scaling types. `manual_scaling` requires setting a fixed number of `instances`. `basic_scaling` scales based on request load up to a maximum. `automatic_scaling` (default) is more dynamic. To limit the *maximum* number of instances while still allowing scaling based on load (implied since no fixed instance count is desired), `basic_scaling` with the `max_instances` parameter set to 5 is the correct approach (D). Option A sets a fixed number. Option B uses `max_instances` with `manual_scaling`, which is contradictory. Option C uses `instances` with `basic_scaling`, which is incorrect syntax.",
            "conditions": [
                "App Engine Standard app, instance class B1, limit maximum instances to 5."
            ],
            "caseStudyContext": null
        },
        {
            "id": 65,
            "topic": "Databases",
            "question": "Your analytics system executes queries against a BigQuery dataset. The SQL query is executed in batch and passes the contents of a SQL file to the BigQuery CLI. Then it redirects the BigQuery CLI output to another process. However, you are getting a permission error from the BigQuery CLI when the queries are executed.\nYou want to resolve the issue. What should you do?",
            "options": {
                "A": "Grant the service account BigQuery Data Viewer and BigQuery Job User roles.",
                "B": "Grant the service account BigQuery Data Editor and BigQuery Data Viewer roles.",
                "C": "Create a view in BigQuery from the SQL query and SELECT* from the view in the CLI.",
                "D": "Create a new dataset in BigQuery, and copy the source table to the new dataset Query the new dataset and table from the CLI."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "To execute a query using the BigQuery CLI (or API), the identity running the command (likely a service account in an automated system) needs two key permissions: permission to run jobs (specifically query jobs), provided by the `roles/bigquery.jobUser` role, and permission to read the data from the tables being queried, provided by the `roles/bigquery.dataViewer` role (or higher). Granting both roles (A) provides the necessary permissions following the principle of least privilege. Data Editor (B) is excessive if only reading is needed. Creating views (C) or copying data (D) doesn't fix the permission issue for running the query itself.",
            "conditions": [
                "BigQuery query executed via CLI (batch mode) fails with permission error."
            ],
            "caseStudyContext": null
        },
        {
            "id": 66,
            "topic": "Compute",
            "question": "Your application is running on Compute Engine and is showing sustained failures for a small number of requests. You have narrowed the cause down to a single Compute Engine instance, but the instance is unresponsive to SSH.\nWhat should you do next?",
            "options": {
                "A": "Reboot the machine.",
                "B": "Enable and check the serial port output.",
                "C": "Delete the machine and create a new one.",
                "D": "Take a snapshot of the disk and attach it to a new machine."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "When an instance is unresponsive via SSH, the next step is often to check the serial console output. The serial console logs kernel messages and system events, including boot processes and potential crash information, which can be accessed even if the network stack or SSH daemon is down. This can help diagnose the root cause of the unresponsiveness (B). Rebooting (A) might fix it temporarily but doesn't diagnose. Deleting (C) loses diagnostic data. Attaching the disk elsewhere (D) is a more involved diagnostic step, usually done after checking serial logs.",
            "conditions": [
                "Single GCE instance failing, unresponsive to SSH."
            ],
            "caseStudyContext": null
        },
        {
            "id": 67,
            "topic": "Compute",
            "question": "You configured your Compute Engine instance group to scale automatically according to overall CPU usage. However, your application's response latency increases sharply before the cluster has finished adding up instances. You want to provide a more consistent latency experience for your end users by changing the configuration of the instance group autoscaler.\nWhich two configuration changes should you make? (Choose two.)",
            "options": {
                "A": "Add the label ‫ג‬€AUTOSCALE‫ג‬€ to the instance group template.",
                "B": "Decrease the cool-down period for instances added to the group.",
                "C": "Increase the target CPU usage for the instance group autoscaler.",
                "D": "Decrease the target CPU usage for the instance group autoscaler.",
                "E": "Remove the health-check for individual VMs in the instance group."
            },
            "correctAnswer": [
                "B",
                "D"
            ],
            "explanation": "The latency increases before scaling completes, suggesting the autoscaler reacts too slowly or too late. To make it react sooner, decrease the target CPU utilization threshold (D) – this triggers scaling at a lower average CPU usage. To allow the autoscaler to add more instances more quickly after a scale-out event (reducing the delay before it can scale out again if needed), decrease the cool-down period (B). Increasing target CPU (C) would worsen the problem. Labels (A) and health checks (E) are not directly related to autoscaling thresholds or speed.",
            "conditions": [
                "MIG autoscaling on CPU usage, latency spikes before scaling completes."
            ],
            "caseStudyContext": null
        },
        {
            "id": 68,
            "topic": "Compute",
            "question": "You have an application controlled by a managed instance group. When you deploy a new version of the application, costs should be minimized and the number of instances should not increase. You want to ensure that, when each new instance is created, the deployment only continues if the new instance is healthy.\nWhat should you do?",
            "options": {
                "A": "Perform a rolling-action with maxSurge set to 1, maxUnavailable set to 0.",
                "B": "Perform a rolling-action with maxSurge set to 0, maxUnavailable set to 1",
                "C": "Perform a rolling-action with maxHealthy set to 1, maxUnhealthy set to 0.",
                "D": "Perform a rolling-action with maxHealthy set to 0, maxUnhealthy set to 1."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "The requirement is to minimize cost and *not increase* the instance count during the update. This means `maxSurge` (extra instances) must be 0. To allow the update to proceed by replacing instances one by one, `maxUnavailable` should be set to 1 (or more, but 1 minimizes disruption). This configuration (B) ensures one old instance is terminated, then one new instance is created and must become healthy (pass health checks) before the next old instance is terminated. Option A increases instance count. C and D use invalid parameters.",
            "conditions": [
                "MIG update, minimize cost, instance count must not increase, proceed only if new instance is healthy."
            ],
            "caseStudyContext": null
        },
        {
            "id": 69,
            "topic": "IAM & Security",
            "question": "Your application requires service accounts to be authenticated to GCP products via credentials stored on its host Compute Engine virtual machine instances. You want to distribute these credentials to the host instances as securely as possible.\nWhat should you do?",
            "options": {
                "A": "Use HTTP signed URLs to securely provide access to the required resources.",
                "B": "Use the instance's service account Application Default Credentials to authenticate to the required resources.",
                "C": "Generate a P12 file from the GCP Console after the instance is deployed, and copy the credentials to the host instance before starting the application.",
                "D": "Commit the credential JSON file into your application's source repository, and have your CI/CD process package it with the software that is deployed to the instance."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "The most secure and recommended way for applications running on Compute Engine to authenticate to Google Cloud APIs is by using the service account attached to the instance. Google Cloud client libraries automatically find these credentials via Application Default Credentials (ADC). This avoids the need to manually generate, distribute, and manage key files (like P12 or JSON keys) stored on the instance (C) or in source code (D), which is insecure. Signed URLs (A) are for granting temporary access to specific resources, not for general application authentication.",
            "conditions": [
                "GCE application needs SA credentials stored on host, distribute securely."
            ],
            "caseStudyContext": null
        },
        {
            "id": 70,
            "topic": "Compute",
            "question": "Your application is deployed in a Google Kubernetes Engine (GKE) cluster. You want to expose this application publicly behind a Cloud Load Balancing HTTP(S) load balancer.\nWhat should you do?",
            "options": {
                "A": "Configure a GKE Ingress resource.",
                "B": "Configure a GKE Service resource.",
                "C": "Configure a GKE Ingress resource with type: LoadBalancer.",
                "D": "Configure a GKE Service resource with type: LoadBalancer."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "In GKE, the standard way to expose HTTP(S) services publicly via a Google Cloud HTTP(S) Load Balancer is by creating an Ingress resource (A). The GKE Ingress controller automatically provisions and configures the necessary Cloud Load Balancer components based on the Ingress definition. A Service resource (B) primarily handles internal load balancing or node port exposure. A Service of `type: LoadBalancer` (D) provisions a Network Load Balancer (L4), not an HTTP(S) Load Balancer (L7). Ingress does not have a `type: LoadBalancer` field (C).",
            "conditions": [
                "Expose GKE application publicly via HTTP(S) Load Balancer."
            ],
            "caseStudyContext": null
        },
        {
            "id": 71,
            "topic": "Migration",
            "question": "Your company is planning to migrate their on-premises Hadoop environment to the cloud. Increasing storage cost and maintenance of data stored in HDFS is a major concern for your company. You also want to make minimal changes to existing data analytics jobs and existing architecture.\nHow should you proceed with the migration?",
            "options": {
                "A": "Migrate your data stored in Hadoop to BigQuery. Change your jobs to source their information from BigQuery instead of the on-premises Hadoop environment.",
                "B": "Create Compute Engine instances with HDD instead of SSD to save costs. Then perform a full migration of your existing environment into the new one in Compute Engine instances.",
                "C": "Create a Cloud Dataproc cluster on Google Cloud Platform, and then migrate your Hadoop environment to the new Cloud Dataproc cluster. Move your HDFS data into larger HDD disks to save on storage costs.",
                "D": "Create a Cloud Dataproc cluster on Google Cloud Platform, and then migrate your Hadoop code objects to the new cluster. Move your data to Cloud Storage and leverage the Cloud Dataproc connector to run jobs on that data."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "To address HDFS cost/maintenance while minimizing job changes, the best approach is to decouple compute and storage. Use Cloud Dataproc (managed Hadoop/Spark) for running the existing jobs (minimal changes) and migrate the HDFS data to Google Cloud Storage (GCS), which is cost-effective and low-maintenance. Dataproc clusters can directly process data in GCS using the Cloud Storage connector (D). Migrating data to BigQuery (A) requires rewriting jobs. Migrating HDFS to GCE disks (B, C) doesn't fully address the cost/maintenance concerns compared to GCS.",
            "conditions": [
                "Migrate on-prem Hadoop to cloud, reduce HDFS cost/maintenance, minimal changes to jobs/architecture."
            ],
            "caseStudyContext": null
        },
        {
            "id": 72,
            "topic": "Storage",
            "question": "Your data is stored in Cloud Storage buckets. Fellow developers have reported that data downloaded from Cloud Storage is resulting in slow API performance.\nYou want to research the issue to provide details to the GCP support team.\nWhich command should you run?",
            "options": {
                "A": "gsutil test ‫ג‬€\"o output.json gs://my-bucket",
                "B": "gsutil perfdiag ‫ג‬€\"o output.json gs://my-bucket",
                "C": "gcloud compute scp example-instance:~/test-data ‫ג‬€\"o output.json gs://my-bucket",
                "D": "gcloud services test ‫ג‬€\"o output.json gs://my-bucket"
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "The `gsutil perfdiag` command is specifically designed to run diagnostic tests against a Cloud Storage bucket to measure performance for various operations (uploads, downloads, metadata requests) and collect detailed information useful for troubleshooting performance issues with GCP support. The output can be saved to a file (using `-o`) for sharing. `gsutil test` runs integration tests for gsutil itself. `gcloud compute scp` copies files to/from GCE. `gcloud services test` doesn't exist in this context.",
            "conditions": [
                "Slow GCS download performance reported, need to gather diagnostic data for support."
            ],
            "caseStudyContext": null
        },
        {
            "id": 73,
            "topic": "Compute",
            "question": "You are using Cloud Build build to promote a Docker image to Development, Test, and Production environments. You need to ensure that the same Docker image is deployed to each of these environments.\nHow should you identify the Docker image in your build?",
            "options": {
                "A": "Use the latest Docker image tag.",
                "B": "Use a unique Docker image name.",
                "C": "Use the digest of the Docker image.",
                "D": "Use a semantic version Docker image tag."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "Docker image digests (e.g., sha256:...) are immutable content hashes of the image layers and configuration. Using the digest guarantees that the exact same image content is being referenced across environments, regardless of whether tags (like 'latest' or 'v1.2.0') have been moved or overwritten. Tags (A, D) are mutable pointers and don't guarantee immutability. Unique names (B) don't solve the versioning/content identity problem.",
            "conditions": [
                "Promote same Docker image across Dev, Test, Prod using Cloud Build."
            ],
            "caseStudyContext": null
        },
        {
            "id": 74,
            "topic": "Compute",
            "question": "Your company has created an application that uploads a report to a Cloud Storage bucket. When the report is uploaded to the bucket, you want to publish a message to a Cloud Pub/Sub topic. You want to implement a solution that will take a small amount to effort to implement.\nWhat should you do?",
            "options": {
                "A": "Configure the Cloud Storage bucket to trigger Cloud Pub/Sub notifications when objects are modified.",
                "B": "Create an App Engine application to receive the file; when it is received, publish a message to the Cloud Pub/Sub topic.",
                "C": "Create a Cloud Function that is triggered by the Cloud Storage bucket. In the Cloud Function, publish a message to the Cloud Pub/Sub topic.",
                "D": "Create an application deployed in a Google Kubernetes Engine cluster to receive the file; when it is received, publish a message to the Cloud Pub/Sub topic."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "Cloud Storage has a built-in feature to directly send notifications to a Pub/Sub topic upon specific object events (like creation/finalization). This requires minimal configuration via the console or `gsutil notification` command (A) and is the simplest, least-effort solution. Using a Cloud Function trigger (C) works but adds an extra service layer. Options B and D involve building custom receiving applications, which is significantly more effort.",
            "conditions": [
                "Publish Pub/Sub message when report uploaded to GCS bucket, minimal effort."
            ],
            "caseStudyContext": null
        },
        {
            "id": 75,
            "topic": "Databases",
            "question": "Your teammate has asked you to review the code below, which is adding a credit to an account balance in Cloud Datastore.\n\n```java\n// Pseudo-code\nfunc AddCredit(acctId, amount) {\n  acct = datastore.Get(acctId)\n  new_balance = acct.balance + amount\n  acct.balance = new_balance\n  datastore.Put(acct)\n  return acct\n}\n```\n\nWhich improvement should you suggest your teammate make?",
            "options": {
                "A": "Get the entity with an ancestor query.",
                "B": "Get and put the entity in a transaction.",
                "C": "Use a strongly consistent transactional database.",
                "D": "Don't return the account entity from the function."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "The read-modify-write pattern shown (Get, calculate new balance, Put) is susceptible to race conditions if multiple requests try to update the same account concurrently. The final balance could be incorrect. Performing the Get and Put operations within a Datastore transaction (B) ensures atomicity. The transaction will retry if the entity was modified between the Get and Put, guaranteeing a consistent update. Ancestor queries (A) relate to consistency within entity groups. Suggesting a different database (C) isn't reviewing the code. Return value (D) is irrelevant to the core consistency problem.",
            "conditions": [
                "Code review for Datastore balance update, potential concurrency issue."
            ],
            "caseStudyContext": null
        },
        {
            "id": 76,
            "topic": "Compute",
            "question": "Your company stores their source code in a Cloud Source Repositories repository. Your company wants to build and test their code on each source code commit to the repository and requires a solution that is managed and has minimal operations overhead.\nWhich method should they use?",
            "options": {
                "A": "Use Cloud Build with a trigger configured for each source code commit.",
                "B": "Use Jenkins deployed via the Google Cloud Platform Marketplace, configured to watch for source code commits.",
                "C": "Use a Compute Engine virtual machine instance with an open source continuous integration tool, configured to watch for source code commits.",
                "D": "Use a source code commit trigger to push a message to a Cloud Pub/Sub topic that triggers an App Engine service to build the source code."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "Cloud Build is Google Cloud's fully managed CI/CD platform. It integrates directly with Cloud Source Repositories and allows creating triggers that automatically start builds on events like code commits (A). This meets the requirements of being managed and having minimal operational overhead. Jenkins (B) or other CI tools on GCE (C) require managing the CI server infrastructure. Using Pub/Sub and App Engine (D) is overly complex for a standard CI build process.",
            "conditions": [
                "Build/test code on each commit to CSR, require managed solution, minimal ops overhead."
            ],
            "caseStudyContext": null
        },
        {
            "id": 77,
            "topic": "IAM & Security",
            "question": "You are writing a Compute Engine hosted application in project A that needs to securely authenticate to a Cloud Pub/Sub topic in project B.\nWhat should you do?",
            "options": {
                "A": "Configure the instances with a service account owned by project B. Add the service account as a Cloud Pub/Sub publisher to project A.",
                "B": "Configure the instances with a service account owned by project A. Add the service account as a publisher on the topic.",
                "C": "Configure Application Default Credentials to use the private key of a service account owned by project B. Add the service account as a Cloud Pub/Sub publisher to project A.",
                "D": "Configure Application Default Credentials to use the private key of a service account owned by project A. Add the service account as a publisher on the topic"
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "The Compute Engine instance runs in Project A and should use a service account from Project A as its identity. To allow this service account (the actor) to publish to a topic (the resource) in Project B, you need to grant the service account from Project A the necessary IAM role (e.g., `roles/pubsub.publisher`) on the specific topic in Project B. This follows the principle of granting permissions on the resource being accessed to the identity accessing it (B). Option A incorrectly uses an SA from Project B on Project A's instance. Options C and D involve managing private keys, which is less secure than using the attached service account via ADC.",
            "conditions": [
                "GCE app in Project A needs to publish to Pub/Sub topic in Project B securely."
            ],
            "caseStudyContext": null
        },
        {
            "id": 78,
            "topic": "IAM & Security",
            "question": "You are developing a corporate tool on Compute Engine for the finance department, which needs to authenticate users and verify that they are in the finance department. All company employees use G Suite.\nWhat should you do?",
            "options": {
                "A": "Enable Cloud Identity-Aware Proxy on the HTTP(s) load balancer and restrict access to a Google Group containing users in the finance department. Verify the provided JSON Web Token within the application.",
                "B": "Enable Cloud Identity-Aware Proxy on the HTTP(s) load balancer and restrict access to a Google Group containing users in the finance department. Issue client-side certificates to everybody in the finance team and verify the certificates in the application.",
                "C": "Configure Cloud Armor Security Policies to restrict access to only corporate IP address ranges. Verify the provided JSON Web Token within the application.",
                "D": "Configure Cloud Armor Security Policies to restrict access to only corporate IP address ranges. Issue client side certificates to everybody in the finance team and verify the certificates in the application."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "Cloud Identity-Aware Proxy (IAP) integrates with Google Workspace (G Suite) identities. By enabling IAP on the Load Balancer fronting the GCE application, you can enforce authentication. You can then restrict access based on Google Group membership (e.g., a 'finance-dept' group). IAP verifies the user's identity and group membership. It also forwards a signed JWT header to the backend application, which the application can optionally verify to get user identity details (A). Client certificates (B, D) add complexity. Cloud Armor (C, D) restricts by IP, not user identity/group.",
            "conditions": [
                "Internal GCE tool for finance dept, authenticate G Suite users, verify finance dept membership."
            ],
            "caseStudyContext": null
        },
        {
            "id": 79,
            "topic": "Hybrid & Multi-cloud",
            "question": "Your API backend is running on multiple cloud providers. You want to generate reports for the network latency of your API.\nWhich two steps should you take? (Choose two.)",
            "options": {
                "A": "Use Zipkin collector to gather data.",
                "B": "Use Fluentd agent to gather data.",
                "C": "Use Stackdriver Trace to generate reports.",
                "D": "Use Stackdriver Debugger to generate report.",
                "E": "Use Stackdriver Profiler to generate report."
            },
            "correctAnswer": [
                "A",
                "C"
            ],
            "explanation": "To trace requests across multiple cloud providers, you need a distributed tracing system compatible with different environments. OpenTelemetry or Zipkin (A) are common open standards/tools for instrumenting applications to generate and collect trace data. This collected data can then be sent to a backend for analysis and reporting. Cloud Trace (formerly Stackdriver Trace) (C) can ingest trace data (including from Zipkin via adapters/collectors) and provides analysis, visualization, and reporting capabilities for latency. Fluentd (B) is for logs. Debugger (D) and Profiler (E) are for debugging code state and resource usage, respectively.",
            "conditions": [
                "API backend on multiple clouds, need network latency reports."
            ],
            "caseStudyContext": null
        },
        {
            "id": 80,
            "topic": "Case Study Analysis",
            "question": "Case study - HipLocal: Which database should HipLocal use for storing user activity?",
            "options": {
                "A": "BigQuery",
                "B": "Cloud SQL",
                "C": "Cloud Spanner",
                "D": "Cloud Datastore"
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "The business requirement is 'Obtain user activity metrics to better understand how to monetize their product.' This implies analyzing potentially large volumes of user activity data (clicks, views, interactions). BigQuery (A) is Google Cloud's data warehouse service, specifically designed for large-scale analytics on structured and semi-structured data. While other databases (B, C, D) store data, BigQuery is the optimal choice for the analytical purpose stated in the requirement.",
            "conditions": [
                "HipLocal case study context, store user activity, need metrics for monetization."
            ],
            "caseStudyContext": "<strong>Company overview</strong>\nHipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.\n\n<strong>Executive statement</strong>\nWe are the number one local community app; it’s time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.\n\n<strong><strong>Solution Concept </strong><br></strong>\nHipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.\n\n<strong><strong>Existing Technical Environment </strong><br></strong>\nHipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:\n<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>\n\n<strong><strong>Business Requirements </strong><br></strong>\nHipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:\n<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>\n\n<strong>Technical requirements</strong>\n<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>"
        },
        {
            "id": 81,
            "topic": "Case Study Analysis",
            "question": "Case study - HipLocal: HipLocal is configuring their access controls. Which firewall configuration should they implement?",
            "options": {
                "A": "Block all traffic on port 443.",
                "B": "Allow all traffic into the network.",
                "C": "Allow traffic on port 443 for a specific tag.",
                "D": "Allow all traffic on port 443 into the network."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "HipLocal needs to expose its application APIs (currently on GCE). Following security best practices, they should restrict ingress traffic as much as possible. Allowing traffic only on necessary ports (like 443 for HTTPS) and targeting specific instances using network tags (C) is the principle of least privilege applied to firewall rules. Blocking 443 (A) would deny access. Allowing all traffic (B) or all traffic on 443 to *all* instances (D) is too permissive.",
            "conditions": [
                "HipLocal case study context, configure firewall rules for application access."
            ],
            "caseStudyContext": "<strong>Company overview</strong>\nHipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.\n\n<strong>Executive statement</strong>\nWe are the number one local community app; it’s time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.\n\n<strong><strong>Solution Concept </strong><br></strong>\nHipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.\n\n<strong><strong>Existing Technical Environment </strong><br></strong>\nHipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:\n<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>\n\n<strong><strong>Business Requirements </strong><br></strong>\nHipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:\n<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>\n\n<strong>Technical requirements</strong>\n<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>"
        },
        {
            "id": 82,
            "topic": "Case Study Analysis",
            "question": "Case study - HipLocal: HipLocal's data science team wants to analyze user reviews. How should they prepare the data?",
            "options": {
                "A": "Use the Cloud Data Loss Prevention API for redaction of the review dataset.",
                "B": "Use the Cloud Data Loss Prevention API for de-identification of the review dataset.",
                "C": "Use the Cloud Natural Language Processing API for redaction of the review dataset.",
                "D": "Use the Cloud Natural Language Processing API for de-identification of the review dataset."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "User reviews may contain Personally Identifiable Information (PII). To analyze reviews while complying with regulations (like GDPR mentioned in requirements), PII must be handled appropriately. The Cloud Data Loss Prevention (DLP) API is designed to discover, classify, and de-identify (e.g., redact, mask, tokenize) sensitive data (B). Redaction (A) is one specific form of de-identification. The Natural Language API (C, D) analyzes text content (sentiment, entities) but isn't the primary tool for PII de-identification.",
            "conditions": [
                "HipLocal case study context, analyze user reviews, prepare data appropriately."
            ],
            "caseStudyContext": "<strong>Company overview</strong>\nHipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.\n\n<strong>Executive statement</strong>\nWe are the number one local community app; it’s time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.\n\n<strong><strong>Solution Concept </strong><br></strong>\nHipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.\n\n<strong><strong>Existing Technical Environment </strong><br></strong>\nHipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:\n<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>\n\n<strong><strong>Business Requirements </strong><br></strong>\nHipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:\n<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>\n\n<strong>Technical requirements</strong>\n<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>"
        },
        {
            "id": 83,
            "topic": "Case Study Analysis",
            "question": "Case study - HipLocal: In order for HipLocal to store application state and meet their stated <strong>Business Requirements </strong><br>, which database service should they migrate to?",
            "options": {
                "A": "Cloud Spanner",
                "B": "Cloud Datastore",
                "C": "Cloud Memorystore as a cache",
                "D": "Separate Cloud SQL clusters for each region"
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "This question is similar to #48. <strong>Business Requirements </strong><br> include global expansion, 10x user support, and consistent experience across regions. The current single MySQL instance won't suffice. Cloud Spanner (A) is the globally distributed, strongly consistent, horizontally scalable relational database service best suited to meet these requirements. Datastore (B) is NoSQL. Memorystore (C) is a cache, not primary state storage. Separate Cloud SQL clusters (D) add management complexity and don't offer the same seamless global consistency as Spanner.",
            "conditions": [
                "HipLocal case study context, store application state, meet global scale/consistency requirements."
            ],
            "caseStudyContext": "<strong>Company overview</strong>\nHipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.\n\n<strong>Executive statement</strong>\nWe are the number one local community app; it’s time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.\n\n<strong><strong>Solution Concept </strong><br></strong>\nHipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.\n\n<strong><strong>Existing Technical Environment </strong><br></strong>\nHipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:\n<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>\n\n<strong><strong>Business Requirements </strong><br></strong>\nHipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:\n<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>\n\n<strong>Technical requirements</strong>\n<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>"
        },
        {
            "id": 84,
            "topic": "Compute",
            "question": "You have an application deployed in production. When a new version is deployed, you want to ensure that all production traffic is routed to the new version of your application. You also want to keep the previous version deployed so that you can revert to it if there is an issue with the new version.\nWhich deployment strategy should you use?",
            "options": {
                "A": "Blue/green deployment",
                "B": "Canary deployment",
                "C": "Rolling deployment",
                "D": "Recreate deployment"
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "Blue/green deployment involves having two identical production environments (Blue and Green). The new version is deployed to the inactive environment (e.g., Green). Once tested, all traffic is switched (via load balancer/DNS) from the current environment (Blue) to the new one (Green). The old environment (Blue) is kept idle, allowing for instant rollback by simply switching traffic back if issues arise. This meets the requirements of routing all traffic to the new version while keeping the old one available for rollback (A). Canary (B) and Rolling (C) involve gradual traffic shifting. Recreate (D) involves downtime.",
            "conditions": [
                "Deploy new version, route all traffic to new version, keep old version for instant rollback."
            ],
            "caseStudyContext": null
        },
        {
            "id": 85,
            "topic": "Compute",
            "question": "You are porting an existing Apache/MySQL/PHP application stack from a single machine to Google Kubernetes Engine. You need to determine how to containerize the application. Your approach should follow Google-recommended best practices for availability.\nWhat should you do?",
            "options": {
                "A": "Package each component in a separate container. Implement readiness and liveness probes.",
                "B": "Package the application in a single container. Use a process management tool to manage each component.",
                "C": "Package each component in a separate container. Use a script to orchestrate the launch of the components.",
                "D": "Package the application in a single container. Use a bash script as an entrypoint to the container, and then spawn each component as a background job."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "The recommended best practice for containerizing multi-component applications like a LAMP stack is to package each major component (Apache, PHP-FPM, MySQL - though MySQL is often run outside the app cluster) into its own separate container (A). This allows independent scaling, updating, and resource management for each component. Using liveness and readiness probes ensures Kubernetes can manage the health and traffic routing for each container correctly. Running multiple processes in a single container (B, D) is an anti-pattern, making management and health checking difficult. Script orchestration (C) is less robust than Kubernetes' native mechanisms.",
            "conditions": [
                "Containerize Apache/MySQL/PHP stack for GKE, follow availability best practices."
            ],
            "caseStudyContext": null
        },
        {
            "id": 86,
            "topic": "Compute",
            "question": "You are developing an application that will be launched on Compute Engine instances into multiple distinct projects, each corresponding to the environments in your software development process (development, QA, staging, and production). The instances in each project have the same application code but a different configuration. During deployment, each instance should receive the application's configuration based on the environment it serves. You want to minimize the number of steps to configure this flow. What should you do?",
            "options": {
                "A": "When creating your instances, configure a startup script using the gcloud command to determine the project name that indicates the correct environment.",
                "B": "In each project, configure a metadata key ‫ג‬€environment‫ג‬€ whose value is the environment it serves. Use your deployment tool to query the instance metadata and configure the application based on the ‫ג‬€environment‫ג‬€ value.",
                "C": "Deploy your chosen deployment tool on an instance in each project. Use a deployment job to retrieve the appropriate configuration file from your version control system, and apply the configuration when deploying the application on each instance.",
                "D": "During each instance launch, configure an instance custom-metadata key named ‫ג‬€environment‫ג‬€ whose value is the environment the instance serves. Use your deployment tool to query the instance metadata, and configure the application based on the ‫ג‬€environment‫ג‬€ value."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "Project-level metadata allows defining key-value pairs that are accessible by all instances within that project. Setting an `environment` key in each project's metadata (B) provides a simple way for instances to identify their environment without needing instance-specific configuration during launch (unlike D). Startup scripts (A) could work but querying metadata is cleaner. Option C involves managing deployment tools per project.",
            "conditions": [
                "Deploy app to GCE in multiple projects (dev, QA, stage, prod), same code, different config per environment, minimize config steps."
            ],
            "caseStudyContext": null
        },
        {
            "id": 87,
            "topic": "Databases",
            "question": "You are developing an ecommerce application that stores customer, order, and inventory data as relational tables inside Cloud Spanner. During a recent load test, you discover that Spanner performance is not scaling linearly as expected. Which of the following is the cause?",
            "options": {
                "A": "The use of 64-bit numeric types for 32-bit numbers.",
                "B": "The use of the STRING data type for arbitrary-precision values.",
                "C": "The use of Version 1 UUIDs as primary keys that increase monotonically.",
                "D": "The use of LIKE instead of STARTS_WITH keyword for parameterized SQL queries."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "A common cause of non-linear scaling and performance issues in Cloud Spanner is hotspotting, where a disproportionate amount of traffic targets a single Spanner server/split. This frequently occurs when primary keys are chosen such that new inserts always go to the end of the key space (e.g., monotonically increasing timestamps or sequences). Version 1 UUIDs often contain a timestamp component, leading to monotonic increases and potential hotspotting (C). Data types (A, B) or query keywords (D) are less likely to cause fundamental scaling bottlenecks compared to poor key design.",
            "conditions": [
                "Spanner application, non-linear performance scaling under load."
            ],
            "caseStudyContext": null
        },
        {
            "id": 88,
            "topic": "Compute",
            "question": "You are developing an application that reads credit card data from a Pub/Sub subscription. You have written code and completed unit testing. You need to test the Pub/Sub integration before deploying to Google Cloud. What should you do?",
            "options": {
                "A": "Create a service to publish messages, and deploy the Pub/Sub emulator. Generate random content in the publishing service, and publish to the emulator.",
                "B": "Create a service to publish messages to your application. Collect the messages from Pub/Sub in production, and replay them through the publishing service.",
                "C": "Create a service to publish messages, and deploy the Pub/Sub emulator. Collect the messages from Pub/Sub in production, and publish them to the emulator.",
                "D": "Create a service to publish messages, and deploy the Pub/Sub emulator. Publish a standard set of testing messages from the publishing service to the emulator."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "The Pub/Sub emulator allows local development and testing of applications that interact with Pub/Sub without needing to connect to the actual Google Cloud service. To test the integration, you should deploy the emulator locally, configure your application to connect to it, and then publish a controlled set of *known* test messages (D) to the emulator topic. This allows verifying that your subscriber code correctly processes these specific test messages. Using random data (A) makes validation harder. Using production data (B, C) is risky, especially with credit card data.",
            "conditions": [
                "Test Pub/Sub integration locally before deployment, application reads credit card data."
            ],
            "caseStudyContext": null
        },
        {
            "id": 89,
            "topic": "Compute",
            "question": "You are designing an application that will subscribe to and receive messages from a single Pub/Sub topic and insert corresponding rows into a database. Your application runs on Linux and leverages preemptible virtual machines to reduce costs. You need to create a shutdown script that will initiate a graceful shutdown.\nWhat should you do?",
            "options": {
                "A": "Write a shutdown script that uses inter-process signals to notify the application process to disconnect from the database.",
                "B": "Write a shutdown script that broadcasts a message to all signed-in users that the Compute Engine instance is going down and instructs them to save current work and sign out.",
                "C": "Write a shutdown script that writes a file in a location that is being polled by the application once every five minutes. After the file is read, the application disconnects from the database.",
                "D": "Write a shutdown script that publishes a message to the Pub/Sub topic announcing that a shutdown is in progress. After the application reads the message, it disconnects from the database."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "Preemptible VMs receive a shutdown signal (ACPI G2 Soft Off) shortly before preemption. A shutdown script configured on the instance runs during this brief window. The most reliable way for the script to notify the running application process to shut down gracefully (finish processing, close DB connections) is via standard Linux inter-process signals like SIGTERM (A). Broadcasting to users (B) is irrelevant. Polling a file (C) is slow and unreliable. Publishing to the same Pub/Sub topic (D) adds delay and complexity.",
            "conditions": [
                "Application on preemptible VMs reads from Pub/Sub, writes to DB, needs graceful shutdown script."
            ],
            "caseStudyContext": null
        },
        {
            "id": 90,
            "topic": "Compute",
            "question": "You work for a web development team at a small startup. Your team is developing a Node.js application using Google Cloud services, including Cloud Storage and Cloud Build. The team uses a Git repository for version control. Your manager calls you over the weekend and instructs you to make an emergency update to one of the company's websites, and you're the only developer available. You need to access Google Cloud to make the update, but you don't have your work laptop. You are not allowed to store source code locally on a non-corporate computer. How should you set up your developer environment?",
            "options": {
                "A": "Use a text editor and the Git command line to send your source code updates as pull requests from a public computer.",
                "B": "Use a text editor and the Git command line to send your source code updates as pull requests from a virtual machine running on a public computer.",
                "C": "Use Cloud Shell and the built-in code editor for development. Send your source code updates as pull requests.",
                "D": "Use a Cloud Storage bucket to store the source code that you need to edit. Mount the bucket to a public computer as a drive, and use a code editor to update the code. Turn on versioning for the bucket, and point it to the team's Git repository."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "The requirement is to make changes without storing code locally on a non-corporate computer. Cloud Shell provides a browser-based Linux environment with pre-installed tools (gcloud, git, editors like Code-OSS) and persistent home directory storage. You can clone the repository within Cloud Shell, make changes using the built-in editor, and push commits/pull requests without any code residing on the local (public) computer (C). Options A and B involve storing code locally. Option D uses GCS inappropriately for code editing and version control.",
            "conditions": [
                "Emergency update needed, no work laptop, cannot store code locally on non-corporate machine."
            ],
            "caseStudyContext": null
        },
        {
            "id": 91,
            "topic": "Monitoring & Logging",
            "question": "Your team develops services that run on Google Kubernetes Engine. You need to standardize their log data using Google-recommended practices and make the data more useful in the fewest number of steps. What should you do? (Choose two.)",
            "options": {
                "A": "Create aggregated exports on application logs to BigQuery to facilitate log analytics.",
                "B": "Create aggregated exports on application logs to Cloud Storage to facilitate log analytics.",
                "C": "Write log output to standard output (stdout) as single-line JSON to be ingested into Cloud Logging as structured logs.",
                "D": "Mandate the use of the Logging API in the application code to write structured logs to Cloud Logging.",
                "E": "Mandate the use of the Pub/Sub API to write structured data to Pub/Sub and create a Dataflow streaming pipeline to normalize logs and write them to BigQuery for analytics."
            },
            "correctAnswer": [
                "A",
                "C"
            ],
            "explanation": "The recommended practice for logging in GKE is to write logs to standard output (stdout) or standard error (stderr). Writing logs as structured JSON (C) allows Cloud Logging to automatically parse them, making them easily searchable and filterable. To make the data more useful for complex analysis, exporting logs from Cloud Logging to BigQuery (A) is the standard approach, enabling powerful SQL-based querying. Using the Logging API directly (D) works but stdout is often simpler. GCS export (B) is less useful for analytics. Pub/Sub/Dataflow (E) adds complexity.",
            "conditions": [
                "Standardize GKE service logs, make useful for analysis, fewest steps, follow best practices."
            ],
            "caseStudyContext": null
        },
        {
            "id": 92,
            "topic": "Compute",
            "question": "You are designing a deployment technique for your new applications on Google Cloud. As part of your deployment planning, you want to use live traffic to gather performance metrics for both new and existing applications. You need to test against the full production load prior to launch. What should you do?",
            "options": {
                "A": "Use canary deployment",
                "B": "Use blue/green deployment",
                "C": "Use rolling updates deployment",
                "D": "Use A/B testing with traffic mirroring during deployment"
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "The requirement to test against the *full* production load *before* launch while gathering metrics points towards traffic mirroring (also known as shadowing). This involves deploying the new version alongside the old one and configuring the load balancer or service mesh to send a copy of the live production traffic to the new version without affecting user responses (which still come from the old version). This allows full load testing and metric comparison (D). Canary (A) and rolling updates (C) only expose a subset of users initially. Blue/green (B) tests the new environment but typically not with full live load *before* the switch.",
            "conditions": [
                "Deployment planning, gather metrics on new/old versions using live traffic, test against full production load before launch."
            ],
            "caseStudyContext": null
        },
        {
            "id": 93,
            "topic": "Storage",
            "question": "Your application uses the Cloud Storage API. You review the logs and discover multiple HTTP 503 Service Unavailable error responses from the API. Your application logs the error and does not take any further action. You want to implement Google-recommended retry logic to improve success rates.\nWhich approach should you take?",
            "options": {
                "A": "Retry the failures in batch after a set number of failures is logged.",
                "B": "Retry each failure at a set time interval up to a maximum number of times.",
                "C": "Retry each failure at increasing time intervals up to a maximum number of tries.",
                "D": "Retry each failure at decreasing time intervals up to a maximum number of tries."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "HTTP 503 errors indicate temporary server-side unavailability. The Google-recommended practice for handling such transient errors (and others like 429s) is to retry with truncated exponential backoff (C). This means retrying the request after a short delay, and if it fails again, increasing the delay exponentially for subsequent retries, up to a maximum delay and/or maximum number of retries. This avoids overwhelming the potentially recovering service. Fixed intervals (B) or decreasing intervals (D) are less effective. Batch retries (A) are not standard.",
            "conditions": [
                "Application gets HTTP 503 errors from GCS API, need to implement retry logic."
            ],
            "caseStudyContext": null
        },
        {
            "id": 94,
            "topic": "Compute",
            "question": "You need to redesign the ingestion of audit events from your authentication service to allow it to handle a large increase in traffic. Currently, the audit service and the authentication system run in the same Compute Engine virtual machine. You plan to use the following Google Cloud tools in the new architecture:\n\n✑ Multiple Compute Engine machines, each running an instance of the authentication service\n✑ Multiple Compute Engine machines, each running an instance of the audit service\n✑ Pub/Sub to send the events from the authentication services.\nHow should you set up the topics and subscriptions to ensure that the system can handle a large volume of messages and can scale efficiently?",
            "options": {
                "A": "Create one Pub/Sub topic. Create one pull subscription to allow the audit services to share the messages.",
                "B": "Create one Pub/Sub topic. Create one pull subscription per audit service instance to allow the services to share the messages.",
                "C": "Create one Pub/Sub topic. Create one push subscription with the endpoint pointing to a load balancer in front of the audit services.",
                "D": "Create one Pub/Sub topic per authentication service. Create one pull subscription per topic to be used by one audit service.",
                "E": "Create one Pub/Sub topic per authentication service. Create one push subscription per topic, with the endpoint pointing to one audit service."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "To decouple the authentication services (publishers) from the audit services (subscribers) and handle large, variable volume, a single Pub/Sub topic is appropriate for all auth services to publish events to. To allow multiple audit service instances to consume these messages in parallel and scale independently, a single shared *pull* subscription should be used (A). Each audit service instance pulls messages from this shared subscription, and Pub/Sub automatically distributes messages among the active pullers. Multiple subscriptions per instance (B) would duplicate messages. Push subscriptions (C, E) can be harder to scale reliably under high load compared to pull. Multiple topics (D, E) add unnecessary complexity.",
            "conditions": [
                "Redesign audit event ingestion, decouple auth (multiple GCE) and audit (multiple GCE) services using Pub/Sub, handle high volume, scale efficiently."
            ],
            "caseStudyContext": null
        },
        {
            "id": 95,
            "topic": "Compute",
            "question": "You are developing a marquee stateless web application that will run on Google Cloud. The rate of the incoming user traffic is expected to be unpredictable, with no traffic on some days and large spikes on other days. You need the application to automatically scale up and down, and you need to minimize the cost associated with running the application. What should you do?",
            "options": {
                "A": "Build the application in Python with Firestore as the database. Deploy the application to Cloud Run.",
                "B": "Build the application in C# with Firestore as the database. Deploy the application to App Engine flexible environment.",
                "C": "Build the application in Python with CloudSQL as the database. Deploy the application to App Engine standard environment.",
                "D": "Build the application in Python with Firestore as the database. Deploy the application to a Compute Engine managed instance group with autoscaling."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "The key requirements are unpredictable traffic, automatic scaling (including down to zero for no traffic), and cost minimization. Cloud Run (A) excels here: it's serverless, scales automatically based on requests (including to zero instances when idle, minimizing cost), and integrates well with serverless databases like Firestore. App Engine Standard (C) also scales to zero but Cloud SQL doesn't scale to zero cost. App Engine Flex (B) has a minimum instance count, increasing cost. GCE MIGs (D) also have minimum instances and more management overhead.",
            "conditions": [
                "Stateless web app, unpredictable traffic (spikes, zero traffic), auto scaling up/down, minimize cost."
            ],
            "caseStudyContext": null
        },
        {
            "id": 96,
            "topic": "IAM & Security",
            "question": "You have written a Cloud Function that accesses other Google Cloud resources. You want to secure the environment using the principle of least privilege. What should you do?",
            "options": {
                "A": "Create a new service account that has Editor authority to access the resources. The deployer is given permission to get the access token.",
                "B": "Create a new service account that has a custom IAM role to access the resources. The deployer is given permission to get the access token.",
                "C": "Create a new service account that has Editor authority to access the resources. The deployer is given permission to act as the new service account.",
                "D": "Create a new service account that has a custom IAM role to access the resources. The deployer is given permission to act as the new service account."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "The principle of least privilege dictates granting only the necessary permissions. Instead of broad roles like Editor (A, C), create a custom IAM role containing only the specific permissions the function needs to access other resources. Create a dedicated service account for the function and assign this custom role to it. When deploying the function, specify this user-managed service account as the runtime identity. The *deployer* (the user/SA performing the deployment) needs the `iam.serviceAccountUser` role (or the permission `iam.serviceAccounts.actAs`) on the function's service account to deploy the function *as* that service account (D). Options A/C use excessive permissions. Options A/B focus on access tokens, which isn't the primary mechanism here.",
            "conditions": [
                "Cloud Function needs access to other GCP resources, follow least privilege."
            ],
            "caseStudyContext": null
        },
        {
            "id": 97,
            "topic": "Compute",
            "question": "You are a SaaS provider deploying dedicated blogging software to customers in your Google Kubernetes Engine (GKE) cluster. You want to configure a secure multi-tenant platform to ensure that each customer has access to only their own blog and can't affect the workloads of other customers. What should you do?",
            "options": {
                "A": "Enable Application-layer Secrets on the GKE cluster to protect the cluster.",
                "B": "Deploy a namespace per tenant and use Network Policies in each blog deployment.",
                "C": "Use GKE Audit Logging to identify malicious containers and delete them on discovery.",
                "D": "Build a custom image of the blogging software and use Binary Authorization to prevent untrusted image deployments."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "Kubernetes namespaces provide logical isolation for resources within a cluster. Assigning each tenant (customer) their own namespace is a standard practice for multi-tenancy. Network Policies can then be applied within or between namespaces to restrict network traffic, ensuring tenants cannot access each other's workloads (B). Application-layer secrets (A) protect secrets at rest. Audit logging (C) is for monitoring, not prevention. Binary Authorization (D) controls image provenance, not runtime isolation.",
            "conditions": [
                "SaaS on GKE, multi-tenant, isolate tenants securely."
            ],
            "caseStudyContext": null
        },
        {
            "id": 98,
            "topic": "Compute",
            "question": "You have decided to migrate your Compute Engine application to Google Kubernetes Engine. You need to build a container image and push it to Artifact Registry using Cloud Build. What should you do? (Choose two.)",
            "options": {
                "A": "Run gcloud builds submit in the directory that contains the application source code.",
                "B": "Run gcloud run deploy app-name --image gcr.io/$PROJECT_ID/app-name in the directory that contains the application source code.",
                "C": "Run gcloud container images add-tag gcr.io/$PROJECT_ID/app-name gcr.io/$PROJECT_ID/app-name:latest in the directory that contains the application source code.",
                "D": "In the application source directory, create a file named cloudbuild.yaml that contains the following contents:\n\nsteps:\n- name: 'gcr.io/cloud-builders/docker'\n  args: ['build', '-t', 'gcr.io/$PROJECT_ID/app-name', '.']\n- name: 'gcr.io/cloud-builders/docker'\n  args: ['push', 'gcr.io/$PROJECT_ID/app-name']",
                "E": "In the application source directory, create a file named cloudbuild.yaml that contains the following contents:\n\nsteps:\n- name: 'gcr.io/cloud-builders/gcloud'\n  args: ['app', 'deploy']"
            },
            "correctAnswer": [
                "A",
                "D"
            ],
            "explanation": "To build and push a container image using Cloud Build, you need to provide build instructions and trigger the build. Creating a `cloudbuild.yaml` file (D) defines the steps: first `docker build` to create the image, then `docker push` to push it to a registry (Artifact Registry or Container Registry - gcr.io paths often work for AR via domain redirection). To initiate the build using this config file (or a Dockerfile if no config file is present), you run `gcloud builds submit` (A) from the directory containing the source code and configuration file. `gcloud run deploy` (B) deploys to Cloud Run. `gcloud container images add-tag` (C) tags existing images. `gcloud app deploy` (E) deploys to App Engine.",
            "conditions": [
                "Build container image from source using Cloud Build, push to Artifact Registry."
            ],
            "caseStudyContext": null
        },
        {
            "id": 99,
            "topic": "IAM & Security",
            "question": "You are developing an internal application that will allow employees to organize community events within your company. You deployed your application on a single Compute Engine instance. Your company uses Google Workspace (formerly G Suite), and you need to ensure that the company employees can authenticate to the application from anywhere. What should you do?",
            "options": {
                "A": "Add a public IP address to your instance, and restrict access to the instance using firewall rules. Allow your company's proxy as the only source IP address.",
                "B": "Add an HTTP(S) load balancer in front of the instance, and set up Identity-Aware Proxy (IAP). Configure the IAP settings to allow your company domain to access the website.",
                "C": "Set up a VPN tunnel between your company network and your instance's VPC location on Google Cloud. Configure the required firewall rules and routing information to both the on-premises and Google Cloud networks.",
                "D": "Add a public IP address to your instance, and allow traffic from the internet. Generate a random hash, and create a subdomain that includes this hash and points to your instance. Distribute this DNS address to your company's employees."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "The requirement is to allow authenticated employees access from anywhere, integrating with Google Workspace. Identity-Aware Proxy (IAP) provides identity-based access control for applications behind an HTTP(S) Load Balancer. It authenticates users via their Google Workspace account and checks IAM permissions (which can be granted to all users in the domain or specific groups) before allowing access (B). Restricting by corporate proxy IP (A) or VPN (C) limits access when employees are travelling. Obscurity via DNS (D) is not secure authentication.",
            "conditions": [
                "Internal GCE app, authenticate Google Workspace employees, access from anywhere."
            ],
            "caseStudyContext": null
        },
        {
            "id": 100,
            "topic": "Compute",
            "question": "Your development team is using Cloud Build to promote a Node.js application built on App Engine from your staging environment to production. The application relies on several directories of photos stored in a Cloud Storage bucket named webphotos-staging in the staging environment. After the promotion, these photos must be available in a Cloud Storage bucket named webphotos-prod in the production environment. You want to automate the process where possible. What should you do?",
            "options": {
                "A": "Manually copy the photos to webphotos-prod.",
                "B": "Add a startup script in the application's app.yami file to move the photos from webphotos-staging to webphotos-prod.",
                "C": "Add a build step in the cloudbuild.yaml file before the promotion step with the arguments:\n\n- name: gcr.io/cloud-builders/gsutil\n  args: ['cp','-r','gs://webphotos-staging','gs://webphotos-prod']\n  waitFor: ['-']",
                "D": "Add a build step in the cloudbuild.yaml file before the promotion step with the arguments:\n\n- name: gcr.io/cloud-builders/gsutil\n  args: ['mv','-r','gs://webphotos-staging','gs://webphotos-prod']\n  waitFor: ['-']"
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "The goal is to automate the copying of files between GCS buckets as part of a Cloud Build pipeline during promotion. Cloud Build steps can use standard builders like `gcr.io/cloud-builders/gsutil`. Adding a step that uses `gsutil cp -r` (recursive copy) (C) to copy the contents from the staging bucket to the production bucket before the App Engine deployment step is the correct automation approach. Manual copy (A) is not automated. Startup scripts (B) run on App Engine instances, not suitable for this type of data migration. Using `gsutil mv` (D) would delete the photos from staging, which might not be desirable.",
            "conditions": [
                "Cloud Build promotes App Engine app, need to copy photos from staging GCS bucket to prod GCS bucket automatically during promotion."
            ],
            "caseStudyContext": null
        },
        {
            "id": 101,
            "topic": "Networking",
            "question": "You are developing a web application that will be accessible over both HTTP and HTTPS and will run on Compute Engine instances. On occasion, you will need to SSH from your remote laptop into one of the Compute Engine instances to conduct maintenance on the app. How should you configure the instances while following Google-recommended best practices?",
            "options": {
                "A": "Set up a backend with Compute Engine web server instances with a private IP address behind a TCP proxy load balancer.",
                "B": "Configure the firewall rules to allow all ingress traffic to connect to the Compute Engine web servers, with each server having a unique external IP address.",
                "C": "Configure Cloud Identity-Aware Proxy API for SSH access. Then configure the Compute Engine servers with private IP addresses behind an HTTP(s) load balancer for the application web traffic.",
                "D": "Set up a backend with Compute Engine web server instances with a private IP address behind an HTTP(S) load balancer. Set up a bastion host with a public IP address and open firewall ports. Connect to the web instances using the bastion host."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "Best practice is to avoid exposing instances directly to the internet. Use an HTTP(S) Load Balancer for web traffic (handling HTTP/HTTPS) pointing to instances with private IPs only. For secure SSH access without public IPs or VPNs, use IAP TCP Forwarding. This allows authorized users to SSH to instances via an authenticated and authorized tunnel managed by Google (C). TCP Proxy (A) is for non-HTTP traffic. Allowing all ingress (B) is insecure. Bastion hosts (D) work but IAP is often preferred for its finer-grained access control and reduced infrastructure footprint.",
            "conditions": [
                "GCE web app accessible via HTTP/HTTPS, need occasional secure SSH access, follow best practices."
            ],
            "caseStudyContext": null
        },
        {
            "id": 102,
            "topic": "Monitoring & Logging",
            "question": "You have a mixture of packaged and internally developed applications hosted on a Compute Engine instance that is running Linux. These applications write log records as text in local files. You want the logs to be written to Cloud Logging. What should you do?",
            "options": {
                "A": "Pipe the content of the files to the Linux Syslog daemon.",
                "B": "Install a Google version of fluentd on the Compute Engine instance.",
                "C": "Install a Google version of collectd on the Compute Engine instance.",
                "D": "Using cron, schedule a job to copy the log files to Cloud Storage once a day."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "The standard Google Cloud Logging agent is based on fluentd (or Fluent Bit in the newer Ops Agent). Installing this agent (B) and configuring it to tail the application log files is the recommended way to stream logs from local files on a GCE instance to Cloud Logging. Piping to syslog (A) might work if the agent is configured to read syslog, but direct file monitoring is common. Collectd (C) is primarily for metrics collection. Copying to GCS (D) is batch, not real-time streaming, and doesn't integrate directly with Cloud Logging's features.",
            "conditions": [
                "Apps on Linux GCE instance write logs to local files, need logs in Cloud Logging."
            ],
            "caseStudyContext": null
        },
        {
            "id": 103,
            "topic": "Compute",
            "question": "You want to create `fully baked` or `golden` Compute Engine images for your application. You need to bootstrap your application to connect to the appropriate database according to the environment the application is running on (test, staging, production). What should you do?",
            "options": {
                "A": "Embed the appropriate database connection string in the image. Create a different image for each environment.",
                "B": "When creating the Compute Engine instance, add a tag with the name of the database to be connected. In your application, query the Compute Engine API to pull the tags for the current instance, and use the tag to construct the appropriate database connection string.",
                "C": "When creating the Compute Engine instance, create a metadata item with a key of ‫ג‬€DATABASE‫ג‬€ and a value for the appropriate database connection string. In your application, read the ‫ג‬€DATABASE‫ג‬€ environment variable, and use the value to connect to the appropriate database.",
                "D": "When creating the Compute Engine instance, create a metadata item with a key of ‫ג‬€DATABASE‫ג‬€ and a value for the appropriate database connection string. In your application, query the metadata server for the ‫ג‬€DATABASE‫ג‬€ value, and use the value to connect to the appropriate database."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "Golden images should be environment-agnostic. Configuration specific to an environment should be injected at deployment time. Instance metadata provides a mechanism for this. You can define a metadata key (e.g., `DATABASE`) in the instance template or during instance creation, holding the environment-specific connection string. The application inside the instance can then query the metadata server (a local endpoint) at startup to retrieve this value and connect to the correct database (D). Embedding config in the image (A) defeats the purpose of a golden image. Tags (B) are primarily for organization/filtering, not detailed config. Metadata isn't automatically exposed as environment variables (C).",
            "conditions": [
                "Use golden GCE images, bootstrap app with environment-specific DB connection string."
            ],
            "caseStudyContext": null
        },
        {
            "id": 104,
            "topic": "IAM & Security",
            "question": "You are developing a microservice-based application that will be deployed on a Google Kubernetes Engine cluster. The application needs to read and write to a Spanner database. You want to follow security best practices while minimizing code changes. How should you configure your application to retrieve Spanner credentials?",
            "options": {
                "A": "Configure the appropriate service accounts, and use Workload Identity to run the pods.",
                "B": "Store the application credentials as Kubernetes Secrets, and expose them as environment variables.",
                "C": "Configure the appropriate routing rules, and use a VPC-native cluster to directly connect to the database.",
                "D": "Store the application credentials using Cloud Key Management Service, and retrieve them whenever a database connection is made."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "Workload Identity is the recommended best practice for GKE applications to securely access Google Cloud services like Spanner. It allows you to bind a Kubernetes Service Account (KSA) used by your Pods to a Google Service Account (GSA) which has the necessary IAM permissions for Spanner. The application then uses the KSA token, which is automatically mounted, and Google Cloud client libraries transparently exchange it for short-lived GSA credentials via the metadata server. This avoids managing and distributing GSA keys (eliminating B, C, D variants that rely on stored keys).",
            "conditions": [
                "GKE microservice needs Spanner access, follow security best practices, minimize code changes."
            ],
            "caseStudyContext": null
        },
        {
            "id": 105,
            "topic": "IAM & Security",
            "question": "You are deploying your application on a Compute Engine instance that communicates with Cloud SQL. You will use Cloud SQL Proxy to allow your application to communicate to the database using the service account associated with the application's instance. You want to follow the Google-recommended best practice of providing minimum access for the role assigned to the service account. What should you do?",
            "options": {
                "A": "Assign the Project Editor role.",
                "B": "Assign the Project Owner role.",
                "C": "Assign the Cloud SQL Client role.",
                "D": "Assign the Cloud SQL Editor role."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "The principle of least privilege requires granting only the necessary permissions. For an application/service account to connect to a Cloud SQL instance (especially via the proxy), the only required role is `roles/cloudsql.client`. This role grants permissions to connect to instances but not to manage (create, delete, modify) them. Editor (A), Owner (B), and Cloud SQL Editor (D) roles grant excessive permissions beyond just connecting.",
            "conditions": [
                "GCE app uses Cloud SQL Proxy with instance SA, need minimum required IAM role for SA."
            ],
            "caseStudyContext": null
        },
        {
            "id": 106,
            "topic": "Compute",
            "question": "Your team develops stateless services that run on Google Kubernetes Engine (GKE). You need to deploy a new service that will only be accessed by other services running in the GKE cluster. The service will need to scale as quickly as possible to respond to changing load. What should you do?",
            "options": {
                "A": "Use a Vertical Pod Autoscaler to scale the containers, and expose them via a ClusterIP Service.",
                "B": "Use a Vertical Pod Autoscaler to scale the containers, and expose them via a NodePort Service.",
                "C": "Use a Horizontal Pod Autoscaler to scale the containers, and expose them via a ClusterIP Service.",
                "D": "Use a Horizontal Pod Autoscaler to scale the containers, and expose them via a NodePort Service."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "For stateless services needing rapid scaling based on load (like CPU or requests per second), the Horizontal Pod Autoscaler (HPA) is the standard Kubernetes mechanism. HPA adjusts the *number* of replicas (horizontal scaling). Vertical Pod Autoscaler (VPA) adjusts resources (CPU/memory) of existing pods. Since the service is only accessed internally within the cluster, a `ClusterIP` Service type is appropriate, as it provides a stable internal IP and DNS name without exposing the service externally. NodePort (B, D) exposes the service on each node's IP, which isn't necessary for purely internal communication.",
            "conditions": [
                "Stateless GKE service, internal access only, scale quickly with load."
            ],
            "caseStudyContext": null
        },
        {
            "id": 107,
            "topic": "Compute",
            "question": "You recently migrated a monolithic application to Google Cloud by breaking it down into microservices. One of the microservices is deployed using Cloud Functions. As you modernize the application, you make a change to the API of the service that is backward-incompatible. You need to support both existing callers who use the original API and new callers who use the new API. What should you do?",
            "options": {
                "A": "Leave the original Cloud Function as-is and deploy a second Cloud Function with the new API. Use a load balancer to distribute calls between the versions.",
                "B": "Leave the original Cloud Function as-is and deploy a second Cloud Function that includes only the changed API. Calls are automatically routed to the correct function.",
                "C": "Leave the original Cloud Function as-is and deploy a second Cloud Function with the new API. Use Cloud Endpoints to provide an API gateway that exposes a versioned API.",
                "D": "Re-deploy the Cloud Function after making code changes to support the new API. Requests for both versions of the API are fulfilled based on a version identifier included in the call."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "When introducing a backward-incompatible API change, you need to maintain the old version while deploying the new one. Deploying the new API as a separate Cloud Function is necessary. To manage both versions and route callers appropriately (e.g., via URL path like /v1/api and /v2/api), an API Gateway is needed. Cloud Endpoints (or Apigee) provides this capability, allowing you to define a versioned API specification that routes requests to the correct underlying Cloud Function based on the version requested (C). A simple load balancer (A) doesn't typically handle API version routing. Automatic routing (B) doesn't happen. Supporting both versions in one function (D) can become complex.",
            "conditions": [
                "Cloud Function microservice, backward-incompatible API change, need to support old and new API versions."
            ],
            "caseStudyContext": null
        },
        {
            "id": 108,
            "topic": "Databases",
            "question": "You are developing an application that will allow users to read and post comments on news articles. You want to configure your application to store and display user-submitted comments using Firestore. How should you design the schema to support an unknown number of comments and articles?",
            "options": {
                "A": "Store each comment in a subcollection of the article.",
                "B": "Add each comment to an array property on the article.",
                "C": "Store each comment in a document, and add the comment's key to an array property on the article.",
                "D": "Store each comment in a document, and add the comment's key to an array property on the user profile."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "Firestore's data model supports subcollections within documents. For a one-to-many relationship like articles-to-comments where the number of comments can be large and unknown, storing comments in a subcollection (`comments`) under the specific article document (`articles/{articleId}/comments/{commentId}`) is the recommended and scalable approach (A). This allows efficient querying of comments for a specific article. Storing comments in an array on the article document (B, C) is limited by the maximum document size (1 MiB) and becomes inefficient for large numbers of comments. Linking via user profile (D) doesn't model the article-comment relationship correctly.",
            "conditions": [
                "Firestore DB for comments on articles, unknown number of comments/articles."
            ],
            "caseStudyContext": null
        },
        {
            "id": 109,
            "topic": "Networking",
            "question": "You recently developed a new application that needs to call the Cloud Storage API from a Compute Engine instance that doesn't have a public IP address. What should you do?",
            "options": {
                "A": "Use Carrier Peering",
                "B": "Use VPC Network Peering",
                "C": "Use Shared VPC networks",
                "D": "Use Private Google Access"
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "Private Google Access allows Compute Engine instances with only private IP addresses within a VPC network to reach the public IP addresses of Google APIs and services (like Cloud Storage) without needing an external IP or NAT gateway. Traffic stays within Google's network. VPC Peering (B) and Shared VPC (C) connect VPC networks. Carrier Peering (A) connects to Google's edge network via partners.",
            "conditions": [
                "GCE instance with no public IP needs to access Cloud Storage API."
            ],
            "caseStudyContext": null
        },
        {
            "id": 110,
            "topic": "Compute",
            "question": "You are a developer working with the CI/CD team to troubleshoot a new feature that your team introduced. The CI/CD team used HashiCorp Packer to create a new Compute Engine image from your development branch. The image was successfully built, but is not booting up. You need to investigate the issue with the CI/CD team. What should you do?",
            "options": {
                "A": "Create a new feature branch, and ask the build team to rebuild the image.",
                "B": "Shut down the deployed virtual machine, export the disk, and then mount the disk locally to access the boot logs.",
                "C": "Install Packer locally, build the Compute Engine image locally, and then run it in your personal Google Cloud project.",
                "D": "Check Compute Engine OS logs using the serial port, and check the Cloud Logging logs to confirm access to the serial port."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "If a GCE image fails to boot, the primary diagnostic tool is the serial console output, which captures kernel and boot messages. You can view this output via the Cloud Console or `gcloud compute instances get-serial-port-output`. Enabling serial port logging sends this output to Cloud Logging for easier access and retention (D). Rebuilding (A, C) doesn't diagnose the current failure. Exporting the disk (B) is a much more complex troubleshooting step, usually considered after checking serial logs.",
            "conditions": [
                "Packer-built GCE image fails to boot, need to investigate."
            ],
            "caseStudyContext": null
        },
        {
            "id": 111,
            "topic": "Networking",
            "question": "You manage an application that runs in a Compute Engine instance. You also have multiple backend services executing in stand-alone Docker containers running in Compute Engine instances. The Compute Engine instances supporting the backend services are scaled by managed instance groups in multiple regions. You want your calling application to be loosely coupled. You need to be able to invoke distinct service implementations that are chosen based on the value of an HTTP header found in the request. Which Google Cloud feature should you use to invoke the backend services?",
            "options": {
                "A": "Traffic Director",
                "B": "Service Directory",
                "C": "Anthos Service Mesh",
                "D": "Internal HTTP(S) Load Balancing"
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "Traffic Director is a managed control plane for service mesh and proxy-based load balancing that provides advanced traffic management capabilities, including routing based on HTTP headers, paths, etc., across multiple regions and instance groups (including GCE and GKE). This allows selecting different backend services based on header values (A). Internal HTTP(S) LB (D) is regional and has less sophisticated routing rules. Service Directory (B) is for service registration/discovery. Anthos Service Mesh (C) provides similar capabilities but Traffic Director is the specific feature for advanced routing across environments.",
            "conditions": [
                "Invoke backend services (GCE MIGs, multi-region) from GCE app, route based on HTTP header value, loosely coupled."
            ],
            "caseStudyContext": null
        },
        {
            "id": 112,
            "topic": "Databases",
            "question": "You are developing an ecommerce application that uses App Engine standard environment and Memorystore for Redis. When a user logs into the app, the application caches the user's information (e.g., session, name, address, preferences), which is stored for quick retrieval during checkout.\nWhile testing your application in a browser, you get a 502 Bad Gateway error. You have determined that the application is not connecting to Memorystore. What is the reason for this error?",
            "options": {
                "A": "Your Memorystore for Redis instance was deployed without a public IP address.",
                "B": "You configured your Serverless VPC Access connector in a different region than your App Engine instance.",
                "C": "The firewall rule allowing a connection between App Engine and Memorystore was removed during an infrastructure update by the DevOps team.",
                "D": "You configured your application to use a Serverless VPC Access connector on a different subnet in a different availability zone than your App Engine instance."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "App Engine Standard needs a Serverless VPC Access connector to connect to resources (like Memorystore) on a VPC network via private IP. A critical requirement is that the App Engine service and the VPC Access connector must reside in the same region. If they are in different regions (B), the connection will fail, potentially leading to errors like 502 from App Engine if it cannot reach its required backend (Memorystore). Memorystore typically doesn't have public IPs (A). Firewall rules within the VPC usually allow traffic from the connector's IP range (C - less likely cause than region mismatch). Subnet/zone configuration for the connector (D) matters but region mismatch is a common failure point.",
            "conditions": [
                "App Engine Standard app uses Memorystore, gets 502 error, cannot connect to Memorystore."
            ],
            "caseStudyContext": null
        },
        {
            "id": 113,
            "topic": "Compute",
            "question": "You are designing a resource-sharing policy for applications used by different teams in a Google Kubernetes Engine cluster. You need to ensure that all applications can access the resources needed to run. What should you do? (Choose two.)",
            "options": {
                "A": "Specify the resource limits and requests in the object specifications.",
                "B": "Create a namespace for each team, and attach resource quotas to each namespace.",
                "C": "Create a LimitRange to specify the default compute resource requirements for each namespace.",
                "D": "Create a Kubernetes service account (KSA) for each application, and assign each KSA to the namespace.",
                "E": "Use the Anthos Policy Controller to enforce label annotations on all namespaces. Use taints and tolerations to allow resource sharing for namespaces."
            },
            "correctAnswer": [
                "B",
                "C"
            ],
            "explanation": "To manage resource sharing fairly among teams in a shared GKE cluster, use namespaces for isolation (B). Attach ResourceQuotas to each namespace (B) to limit the total amount of resources (CPU, memory, storage, object counts) a team's namespace can consume, preventing one team from starving others. Additionally, use LimitRanges within each namespace (C) to set default resource requests/limits for containers and constrain the minimum/maximum resources individual pods/containers can request, ensuring sensible resource usage within the quota.",
            "conditions": [
                "Manage resource sharing for multiple teams in one GKE cluster, ensure all apps get needed resources."
            ],
            "caseStudyContext": null
        },
        {
            "id": 114,
            "topic": "Compute",
            "question": "You are developing a new application that has the following design requirements:\n\n✑ Creation and changes to the application infrastructure are versioned and auditable.\n✑ The application and deployment infrastructure uses Google-managed services as much as possible.\n✑ The application runs on a serverless compute platform.\nHow should you design the application's architecture?",
            "options": {
                "A": "1. Store the application and infrastructure source code in a Git repository. 2. Use Cloud Build to deploy the application infrastructure with Terraform. 3. Deploy the application to a Cloud Function as a pipeline step.",
                "B": "1. Deploy Jenkins from the Google Cloud Marketplace, and define a continuous integration pipeline in Jenkins. 2. Configure a pipeline step to pull the application source code from a Git repository. 3. Deploy the application source code to App Engine as a pipeline step.",
                "C": "1. Create a continuous integration pipeline on Cloud Build, and configure the pipeline to deploy the application infrastructure using Deployment Manager templates. 2. Configure a pipeline step to create a container with the latest application source code. 3. Deploy the container to a Compute Engine instance as a pipeline step.",
                "D": "1. Deploy the application infrastructure using gcloud commands. 2. Use Cloud Build to define a continuous integration pipeline for changes to the application source code. 3. Configure a pipeline step to pull the application source code from a Git repository, and create a containerized application. 4. Deploy the new container on Cloud Run as a pipeline step."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "Requirement 1 (versioned/auditable infra): Store infrastructure code (Terraform or Deployment Manager) in Git. Requirement 2 (managed services): Cloud Build, Cloud Functions/Cloud Run/App Engine. Requirement 3 (serverless compute): Cloud Functions, Cloud Run, App Engine Standard. Option A uses Git, Cloud Build (managed), Terraform (IaC for auditability), and Cloud Functions (serverless). Option B uses Jenkins (not fully managed). Option C uses Compute Engine (not serverless). Option D uses gcloud commands initially (less auditable than IaC) and Cloud Run (serverless), but A's use of IaC from the start is better for versioning/auditing infrastructure.",
            "conditions": [
                "New app architecture: versioned/auditable infra, managed services preferred, serverless compute."
            ],
            "caseStudyContext": null
        },
        {
            "id": 115,
            "topic": "IAM & Security",
            "question": "You are creating and running containers across different projects in Google Cloud. The application you are developing needs to access Google Cloud services from within Google Kubernetes Engine (GKE). What should you do?",
            "options": {
                "A": "Assign a Google service account to the GKE nodes.",
                "B": "Use a Google service account to run the Pod with Workload Identity.",
                "C": "Store the Google service account credentials as a Kubernetes Secret.",
                "D": "Use a Google service account with GKE role-based access control (RBAC)."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "Workload Identity is the recommended and most secure way for GKE workloads (Pods) to access Google Cloud services. It allows binding a Kubernetes Service Account (KSA) to a Google Service Account (GSA). Pods running with the KSA can then authenticate as the GSA to Google APIs without needing GSA keys (B). Assigning SA to nodes (A) grants permissions to all pods on the node. Storing keys as K8s secrets (C) requires key management and rotation. RBAC (D) controls access *within* the cluster, not to external Google Cloud services.",
            "conditions": [
                "GKE application needs to access Google Cloud services securely across different projects."
            ],
            "caseStudyContext": null
        },
        {
            "id": 116,
            "topic": "Compute",
            "question": "You are containerizing a legacy application that stores its configuration on an NFS share. You need to deploy this application to Google Kubernetes Engine (GKE) and do not want the application serving traffic until after the configuration has been retrieved. What should you do?",
            "options": {
                "A": "Use the gsutil utility to copy files from within the Docker container at startup, and start the service using an ENTRYPOINT script.",
                "B": "Create a PersistentVolumeClaim on the GKE cluster. Access the configuration files from the volume, and start the service using an ENTRYPOINT script.",
                "C": "Use the COPY statement in the Dockerfile to load the configuration into the container image. Verify that the configuration is available, and start the service using an ENTRYPOINT script.",
                "D": "Add a startup script to the GKE instance group to mount the NFS share at node startup. Copy the configuration files into the container, and start the service using an ENTRYPOINT script."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "Kubernetes PersistentVolumes (PV) and PersistentVolumeClaims (PVC) are the standard way to manage persistent storage for containers. You can configure a PV backed by an existing NFS share (e.g., hosted by Filestore or elsewhere). The container definition in the Deployment/Pod spec can then reference the PVC to mount the NFS volume into the container's filesystem. The application can then read its configuration directly from the mounted volume (B). Using gsutil (A) is for GCS. Copying into the image (C) makes config static. Mounting NFS on the node (D) couples the pod to the node configuration.",
            "conditions": [
                "Containerize legacy app for GKE, config stored on NFS, must retrieve config before serving traffic."
            ],
            "caseStudyContext": null
        },
        {
            "id": 117,
            "topic": "Networking",
            "question": "Your team is developing a new application using a PostgreSQL database and Cloud Run. You are responsible for ensuring that all traffic is kept private on Google Cloud. You want to use managed services and follow Google-recommended best practices. What should you do?",
            "options": {
                "A": "1. Enable Cloud SQL and Cloud Run in the same project. 2. Configure a private IP address for Cloud SQL. Enable private services access. 3. Create a Serverless VPC Access connector. 4. Configure Cloud Run to use the connector to connect to Cloud SQL.",
                "B": "1. Install PostgreSQL on a Compute Engine virtual machine (VM), and enable Cloud Run in the same project. 2. Configure a private IP address for the VM. Enable private services access. 3. Create a Serverless VPC Access connector. 4. Configure Cloud Run to use the connector to connect to the VM hosting PostgreSQL.",
                "C": "1. Use Cloud SQL and Cloud Run in different projects. 2. Configure a private IP address for Cloud SQL. Enable private services access. 3. Create a Serverless VPC Access connector. 4. Set up a VPN connection between the two projects. Configure Cloud Run to use the connector to connect to Cloud SQL.",
                "D": "1. Install PostgreSQL on a Compute Engine VM, and enable Cloud Run in different projects. 2. Configure a private IP address for the VM. Enable private services access. 3. Create a Serverless VPC Access connector. 4. Set up a VPN connection between the two projects. Configure Cloud Run to use the connector to access the VM hosting PostgreSQL"
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "To keep traffic private between Cloud Run and a database, use managed services (Cloud SQL for PostgreSQL) and private networking. Configure Cloud SQL with a private IP (requires Private Services Access). Configure Cloud Run to use a Serverless VPC Access connector, which bridges Cloud Run's environment to your VPC network. This allows Cloud Run to connect to the Cloud SQL instance's private IP address via the connector, keeping traffic within Google's network (A). Installing PostgreSQL on GCE (B, D) adds management overhead. Using different projects and VPN (C, D) adds unnecessary complexity when resources can be in the same project/VPC.",
            "conditions": [
                "Cloud Run app needs PostgreSQL DB access, all traffic must be private, use managed services, follow best practices."
            ],
            "caseStudyContext": null
        },
        {
            "id": 118,
            "topic": "Storage",
            "question": "You are developing an application that will allow clients to download a file from your website for a specific period of time. How should you design the application to complete this task while following Google-recommended best practices?",
            "options": {
                "A": "Configure the application to send the file to the client as an email attachment.",
                "B": "Generate and assign a Cloud Storage-signed URL for the file. Make the URL available for the client to download.",
                "C": "Create a temporary Cloud Storage bucket with time expiration specified, and give download permissions to the bucket. Copy the file, and send it to the client.",
                "D": "Generate the HTTP cookies with time expiration specified. If the time is valid, copy the file from the Cloud Storage bucket, and make the file available for the client to download."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "Cloud Storage Signed URLs are designed for this exact use case: granting temporary, limited access (e.g., read-only for download) to a specific object without requiring the user to have Google credentials. The application generates a URL with an embedded signature and expiration time. Anyone with the URL can access the object until it expires (B). Email attachments (A) are impractical for large files and less secure. Temporary buckets (C) or cookies (D) are overly complex compared to signed URLs.",
            "conditions": [
                "Allow clients to download a file for a limited time."
            ],
            "caseStudyContext": null
        },
        {
            "id": 119,
            "topic": "Compute",
            "question": "Your development team has been asked to refactor an existing monolithic application into a set of composable microservices. Which design aspects should you implement for the new application? (Choose two.)",
            "options": {
                "A": "Develop the microservice code in the same programming language used by the microservice caller.",
                "B": "Create an API contract agreement between the microservice implementation and microservice caller.",
                "C": "Require asynchronous communications between all microservice implementations and microservice callers.",
                "D": "Ensure that sufficient instances of the microservice are running to accommodate the performance requirements.",
                "E": "Implement a versioning scheme to permit future changes that could be incompatible with the current interface."
            },
            "correctAnswer": [
                "B",
                "E"
            ],
            "explanation": "Key principles of microservices include loose coupling and independent evolution. Defining clear API contracts (B) (e.g., using OpenAPI spec) decouples callers from implementation details. Implementing a versioning scheme (E) allows APIs to evolve, including making backward-incompatible changes, without breaking existing consumers, who can continue using older versions. Language choice (A) is independent per service. Asynchronous communication (C) is a pattern, not a universal requirement. Ensuring sufficient instances (D) is an operational concern, not a fundamental design aspect of the interfaces.",
            "conditions": [
                "Refactor monolith to microservices, implement key design aspects."
            ],
            "caseStudyContext": null
        },
        {
            "id": 120,
            "topic": "Monitoring & Logging",
            "question": "You deployed a new application to Google Kubernetes Engine and are experiencing some performance degradation. Your logs are being written to Cloud Logging, and you are using a Prometheus sidecar model for capturing metrics. You need to correlate the metrics and data from the logs to troubleshoot the performance issue and send real-time alerts while minimizing costs. What should you do?",
            "options": {
                "A": "Create custom metrics from the Cloud Logging logs, and use Prometheus to import the results using the Cloud Monitoring REST API.",
                "B": "Export the Cloud Logging logs and the Prometheus metrics to Cloud Bigtable. Run a query to join the results, and analyze in Google Data Studio.",
                "C": "Export the Cloud Logging logs and stream the Prometheus metrics to BigQuery. Run a recurring query to join the results, and send notifications using Cloud Tasks.",
                "D": "Export the Prometheus metrics and use Cloud Monitoring to view them as external metrics. Configure Cloud Monitoring to create log-based metrics from the logs, and correlate them with the Prometheus data."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "Cloud Monitoring can ingest metrics from various sources, including Prometheus (often via the Ops Agent or dedicated exporters), treating them as external metrics. It can also create metrics based on log entries (log-based metrics). By bringing both Prometheus metrics and log-based metrics into Cloud Monitoring (D), you can correlate them within Monitoring's dashboards, use Metrics Explorer, and configure alerting policies based on combinations of these metrics, all within a single managed service, minimizing cost and complexity compared to exporting to BigQuery/Bigtable (B, C) or complex cross-system imports (A).",
            "conditions": [
                "GKE app performance issue, have logs in Cloud Logging, metrics in Prometheus sidecar, need to correlate logs/metrics, alerts needed, minimize cost."
            ],
            "caseStudyContext": null
        },
        {
            "id": 121,
            "topic": "Migration",
            "question": "You have been tasked with planning the migration of your company's application from on-premises to Google Cloud. Your company's monolithic application is an ecommerce website. The application will be migrated to microservices deployed on Google Cloud in stages. The majority of your company's revenue is generated through online sales, so it is important to minimize risk during the migration. You need to prioritize features and select the first functionality to migrate. What should you do?",
            "options": {
                "A": "Migrate the Product catalog, which has integrations to the frontend and product database.",
                "B": "Migrate Payment processing, which has integrations to the frontend, order database, and third-party payment vendor.",
                "C": "Migrate Order fulfillment, which has integrations to the order database, inventory system, and third-party shipping vendor.",
                "D": "Migrate the Shopping cart, which has integrations to the frontend, cart database, inventory system, and payment processing system."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "When migrating a monolith to microservices incrementally (Strangler Fig pattern), it's generally recommended to start with functionality that has fewer dependencies and lower risk, while still providing value. The Product Catalog (A) often fits this description – it primarily serves read-only data to the frontend and might have simpler database interactions compared to transactional systems like payments (B), fulfillment (C), or the shopping cart (D), which often involve more complex logic and dependencies.",
            "conditions": [
                "Migrate on-prem ecommerce monolith to microservices on GCP in stages, minimize risk, select first functionality."
            ],
            "caseStudyContext": null
        },
        {
            "id": 122,
            "topic": "Compute",
            "question": "Your team develops services that run on Google Kubernetes Engine. Your team's code is stored in Cloud Source Repositories. You need to quickly identify bugs in the code before it is deployed to production. You want to invest in automation to improve developer feedback and make the process as efficient as possible.\nWhat should you do?",
            "options": {
                "A": "Use Spinnaker to automate building container images from code based on Git tags.",
                "B": "Use Cloud Build to automate building container images from code based on Git tags.",
                "C": "Use Spinnaker to automate deploying container images to the production environment.",
                "D": "Use Cloud Build to automate building container images from code based on forked versions."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "The goal is automated building and (implicitly) testing triggered by code changes (specifically tags, often used for releases or candidates) to provide quick feedback before production deployment. Cloud Build integrates natively with Cloud Source Repositories and can be triggered by Git tags (B). It's a managed service for building/testing. Spinnaker (A, C) is primarily a continuous delivery (deployment) tool, though it can integrate with build systems. Building based on forked versions (D) isn't the standard trigger for pre-production testing.",
            "conditions": [
                "GKE services, code in CSR, automate build/test on Git tags for early bug detection, efficient feedback."
            ],
            "caseStudyContext": null
        },
        {
            "id": 123,
            "topic": "IAM & Security",
            "question": "Your team is developing an application in Google Cloud that executes with user identities maintained by Cloud Identity. Each of your application's users will have an associated Pub/Sub topic to which messages are published, and a Pub/Sub subscription where the same user will retrieve published messages. You need to ensure that only authorized users can publish and subscribe to their own specific Pub/Sub topic and subscription. What should you do?",
            "options": {
                "A": "Bind the user identity to the pubsub.publisher and pubsub.subscriber roles at the resource level.",
                "B": "Grant the user identity the pubsub.publisher and pubsub.subscriber roles at the project level.",
                "C": "Grant the user identity a custom role that contains the pubsub.topics.create and pubsub.subscriptions.create permissions.",
                "D": "Configure the application to run as a service account that has the pubsub.publisher and pubsub.subscriber roles."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "To enforce that a user can only access *their own* specific topic and subscription, permissions must be granted at the resource level (the specific topic and subscription). Granting the `roles/pubsub.publisher` role on the user's topic and the `roles/pubsub.subscriber` role on the user's subscription directly to the user's Cloud Identity achieves this granular control (A). Project-level roles (B) grant access to *all* topics/subscriptions in the project. Create permissions (C) are irrelevant. Using a single service account (D) doesn't enforce per-user permissions.",
            "conditions": [
                "App uses Cloud Identity users, each user has own Pub/Sub topic/subscription, users must only access their own resources."
            ],
            "caseStudyContext": null
        },
        {
            "id": 124,
            "topic": "Compute",
            "question": "You are evaluating developer tools to help drive Google Kubernetes Engine adoption and integration with your development environment, which includes VS Code and IntelliJ. What should you do?",
            "options": {
                "A": "Use Cloud Code to develop applications.",
                "B": "Use the Cloud Shell integrated Code Editor to edit code and configuration files.",
                "C": "Use a Cloud Notebook instance to ingest and process data and deploy models.",
                "D": "Use Cloud Shell to manage your infrastructure and applications from the command line."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "Cloud Code is specifically designed as a set of IDE extensions (for VS Code, IntelliJ) to streamline Kubernetes (including GKE) and Cloud Run development. It provides features like cluster interaction, Skaffold integration for build/run/debug cycles, YAML linting/completion, and debugging directly within the IDE (A). Cloud Shell editor (B) is browser-based. Cloud Notebooks (C) are for data science. Cloud Shell CLI (D) is useful but doesn't integrate directly into the IDE workflow like Cloud Code.",
            "conditions": [
                "Need developer tools for GKE, integrate with VS Code/IntelliJ."
            ],
            "caseStudyContext": null
        },
        {
            "id": 125,
            "topic": "Networking",
            "question": "You are developing an ecommerce web application that uses App Engine standard environment and Memorystore for Redis. When a user logs into the app, the application caches the user's information (e.g., session, name, address, preferences), which is stored for quick retrieval during checkout.\nWhile testing your application in a browser, you get a 502 Bad Gateway error. You have determined that the application is not connecting to Memorystore. What is the reason for this error?",
            "options": {
                "A": "Your Memorystore for Redis instance was deployed without a public IP address.",
                "B": "You configured your Serverless VPC Access connector in a different region than your App Engine instance.",
                "C": "The firewall rule allowing a connection between App Engine and Memorystore was removed during an infrastructure update by the DevOps team.",
                "D": "You configured your application to use a Serverless VPC Access connector on a different subnet in a different availability zone than your App Engine instance."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "App Engine Standard requires a Serverless VPC Access connector to communicate with resources like Memorystore on a private VPC network. A fundamental requirement for the connector is that it must be in the *same region* as the App Engine service using it. If the connector and App Engine are in different regions (B), the connection will fail, leading to application errors (potentially manifesting as a 502 if App Engine can't reach its backend cache). Memorystore doesn't use public IPs (A). Firewall rules (C) could be an issue, but region mismatch for the connector is a specific and common configuration error. Subnet/zone issues (D) are less likely to cause complete connection failure than a region mismatch.",
            "conditions": [
                "App Engine Standard app, uses Memorystore, gets 502 error, cannot connect to Memorystore."
            ],
            "caseStudyContext": null
        },
        {
            "id": 126,
            "topic": "IAM & Security",
            "question": "Your team develops services that run on Google Cloud. You need to build a data processing service and will use Cloud Functions. The data to be processed by the function is sensitive. You need to ensure that invocations can only happen from authorized services and follow Google-recommended best practices for securing functions. What should you do?",
            "options": {
                "A": "Enable Identity-Aware Proxy in your project. Secure function access using its permissions.",
                "B": "Create a service account with the Cloud Functions Viewer role. Use that service account to invoke the function.",
                "C": "Create a service account with the Cloud Functions Invoker role. Use that service account to invoke the function.",
                "D": "Create an OAuth 2.0 client ID for your calling service in the same project as the function you want to secure. Use those credentials to invoke the function."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "To securely invoke an HTTP-triggered Cloud Function from another service (like another Cloud Function, Cloud Run, GCE, etc.), the recommended practice is to use IAM-based authentication. Make the function private (require authentication). Create a dedicated service account for the calling service. Grant this service account the `roles/cloudfunctions.invoker` role on the target function. The calling service then authenticates using its service account credentials (e.g., via ADC) and includes an OIDC identity token in the request header, which Cloud Functions verifies (C). IAP (A) is not typically used for function-to-function auth. Viewer role (B) is insufficient. OAuth Client IDs (D) are for user authentication flows.",
            "conditions": [
                "Secure Cloud Function invocation from authorized services only, sensitive data."
            ],
            "caseStudyContext": null
        },
        {
            "id": 127,
            "topic": "Compute",
            "question": "You are deploying your applications on Compute Engine. One of your Compute Engine instances failed to launch. What should you do? (Choose two.)",
            "options": {
                "A": "Determine whether your file system is corrupted.",
                "B": "Access Compute Engine as a different SSH user.",
                "C": "Troubleshoot firewall rules or routes on an instance.",
                "D": "Check whether your instance boot disk is completely full.",
                "E": "Check whether network traffic to or from your instance is being dropped."
            },
            "correctAnswer": [
                "A",
                "D"
            ],
            "explanation": "Instance launch failures are often related to the boot disk. Common causes include the boot disk being full (D), preventing the OS from starting essential services, or the file system on the boot disk being corrupted (A), making it unreadable by the bootloader or kernel. Network issues (C, E) typically prevent connectivity *after* boot, not the launch itself. SSH user issues (B) are also irrelevant if the instance doesn't even reach a state where SSH is running.",
            "conditions": [
                "GCE instance failed to launch/boot."
            ],
            "caseStudyContext": null
        },
        {
            "id": 128,
            "topic": "IAM & Security",
            "question": "Your web application is deployed to the corporate intranet. You need to migrate the web application to Google Cloud. The web application must be available only to company employees and accessible to employees as they travel. You need to ensure the security and accessibility of the web application while minimizing application changes. What should you do?",
            "options": {
                "A": "Configure the application to check authentication credentials for each HTTP(S) request to the application.",
                "B": "Configure Identity-Aware Proxy to allow employees to access the application through its public IP address.",
                "C": "Configure a Compute Engine instance that requests users to log in to their corporate account. Change the web application DNS to point to the proxy Compute Engine instance. After authenticating, the Compute Engine instance forwards requests to and from the web application.",
                "D": "Configure a Compute Engine instance that requests users to log in to their corporate account. Change the web application DNS to point to the proxy Compute Engine instance. After authenticating, the Compute Engine issues an HTTP redirect to a public IP address hosting the web application."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "The requirements are: migrate to GCP, accessible only to employees, accessible while traveling, minimize app changes. Identity-Aware Proxy (IAP) meets these. By placing the application (e.g., on GCE or GKE) behind a Google Cloud HTTP(S) Load Balancer and enabling IAP, access is controlled based on user identity (e.g., Google Workspace accounts) and IAM permissions. Users can access it securely from anywhere after authenticating, without needing VPNs or complex application changes (B). Implementing app-level auth (A) requires code changes. A custom proxy (C, D) adds complexity and management overhead compared to managed IAP.",
            "conditions": [
                "Migrate internal web app to GCP, employees only access, accessible while traveling, minimize app changes."
            ],
            "caseStudyContext": null
        },
        {
            "id": 129,
            "topic": "Networking",
            "question": "You have an application that uses an HTTP Cloud Function to process user activity from both desktop browser and mobile application clients. This function will serve as the endpoint for all metric submissions using HTTP POST.\nDue to legacy restrictions, the function must be mapped to a domain that is separate from the domain requested by users on web or mobile sessions. The domain for the Cloud Function is https://fn.example.com. Desktop and mobile clients use the domain https://www.example.com.\nYou need to add a header to the function's HTTP response so that only those browser and mobile sessions can submit metrics to the Cloud Function. Which response header should you add?",
            "options": {
                "A": "Access-Control-Allow-Origin: *",
                "B": "Access-Control-Allow-Origin: https://*.example.com",
                "C": "Access-Control-Allow-Origin: https://fn.example.com",
                "D": "Access-Control-Allow-origin: https://www.example.com"
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "This scenario describes a Cross-Origin Resource Sharing (CORS) requirement. Browsers enforce the same-origin policy, preventing JavaScript running on `https://www.example.com` from making requests to `https://fn.example.com` unless the server at `fn.example.com` explicitly allows it. The `Access-Control-Allow-Origin` response header from the Cloud Function (`fn.example.com`) must specify the origin (`https://www.example.com`) that is permitted to make the request (D). `*` (A) allows any origin (less secure). `*.example.com` (B) uses a wildcard which is sometimes supported but specific origin is better. Allowing its own origin (C) doesn't help the client.",
            "conditions": [
                "HTTP Cloud Function at fn.example.com called by clients from www.example.com, need to allow cross-origin requests."
            ],
            "caseStudyContext": null
        },
        {
            "id": 130,
            "topic": "Databases",
            "question": "You have an HTTP Cloud Function that is called via POST. Each submission's request body has a flat, unnested JSON structure containing numeric and text data. After the Cloud Function completes, the collected data should be immediately available for ongoing and complex analytics by many users in parallel. How should you persist the submissions?",
            "options": {
                "A": "Directly persist each POST request's JSON data into Datastore.",
                "B": "Transform the POST request's JSON data, and stream it into BigQuery.",
                "C": "Transform the POST request's JSON data, and store it in a regional Cloud SQL cluster.",
                "D": "Persist each POST request's JSON data as an individual file within Cloud Storage, with the file name containing the request identifier."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "The requirement for data to be immediately available for complex analytics by many users in parallel strongly points to BigQuery. BigQuery is designed for large-scale, SQL-based analytics. Data can be streamed directly into BigQuery tables from sources like Cloud Functions, making it available for querying almost instantly (B). Datastore (A) and Cloud SQL (C) are primarily transactional databases and less suited for complex, parallel analytics on large volumes. Storing individual files in Cloud Storage (D) requires an additional ETL step before data can be analyzed efficiently in bulk.",
            "conditions": [
                "Cloud Function receives JSON POST data, persist for immediate, complex, parallel analytics."
            ],
            "caseStudyContext": null
        },
        {
            "id": 131,
            "topic": "IAM & Security",
            "question": "Your security team is auditing all deployed applications running in Google Kubernetes Engine. After completing the audit, your team discovers that some of the applications send traffic within the cluster in clear text. You need to ensure that all application traffic is encrypted as quickly as possible while minimizing changes to your applications and maintaining support from Google. What should you do?",
            "options": {
                "A": "Use Network Policies to block traffic between applications.",
                "B": "Install Istio, enable proxy injection on your application namespace, and then enable mTLS.",
                "C": "Define Trusted Network ranges within the application, and configure the applications to allow traffic only from those networks.",
                "D": "Use an automated process to request SSL Certificates for your applications from Let's Encrypt and add them to your applications."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "Istio (or Anthos Service Mesh, which is managed Istio) provides automatic mutual TLS (mTLS) encryption for traffic *between* services within the mesh. By installing Istio, enabling automatic sidecar proxy injection for the relevant namespaces, and configuring mTLS (often enabled by default or with simple policy), all service-to-service traffic handled by the proxies will be encrypted without requiring application code changes (B). Network Policies (A) control *whether* traffic is allowed, not encryption. Application-level changes (C, D) require modifying applications.",
            "conditions": [
                "GKE applications send internal traffic in clear text, need to encrypt quickly, minimize app changes, use supported solution."
            ],
            "caseStudyContext": null
        },
        {
            "id": 132,
            "topic": "Monitoring & Logging",
            "question": "You migrated some of your applications to Google Cloud. You are using a legacy monitoring platform deployed on-premises for both on-premises and cloud- deployed applications. You discover that your notification system is responding slowly to time-critical problems in the cloud applications. What should you do?",
            "options": {
                "A": "Replace your monitoring platform with Cloud Monitoring.",
                "B": "Install the Cloud Monitoring agent on your Compute Engine instances.",
                "C": "Migrate some traffic back to your old platform. Perform A/B testing on the two platforms concurrently.",
                "D": "Use Cloud Logging and Cloud Monitoring to capture logs, monitor, and send alerts. Send them to your existing platform."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "This is very similar to Q2. The problem is slow notifications for cloud apps from the on-prem system. Since only *some* apps were migrated, replacing the entire system (A) might be premature. Installing agents (B) helps get data into Cloud Monitoring but doesn't fix notifications alone. Migrating traffic (C) is irrelevant. The best approach is to leverage Cloud Monitoring for its fast, cloud-native alerting capabilities for the cloud applications, while potentially still integrating with the existing platform by forwarding logs/metrics/alerts to maintain a unified view (D). This addresses the critical latency issue where it occurs.",
            "conditions": [
                "Some apps migrated to GCP, hybrid monitoring (on-prem + GCP), slow notifications for cloud apps from on-prem system."
            ],
            "caseStudyContext": null
        },
        {
            "id": 133,
            "topic": "Compute",
            "question": "You recently deployed your application in Google Kubernetes Engine, and now need to release a new version of your application. You need the ability to instantly roll back to the previous version in case there are issues with the new version. Which deployment model should you use?",
            "options": {
                "A": "Perform a rolling deployment, and test your new application after the deployment is complete.",
                "B": "Perform A/B testing, and test your application periodically after the new tests are implemented.",
                "C": "Perform a blue/green deployment, and test your new application after the deployment is. complete.",
                "D": "Perform a canary deployment, and test your new application periodically after the new version is deployed."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "Blue/green deployment maintains two identical environments. Traffic is directed to one (blue). The new version is deployed to the other (green). After testing green, traffic is switched instantly. If issues arise, traffic can be instantly switched back to the stable blue environment. This provides the required instant rollback capability (C). Rolling updates (A) gradually replace pods, making rollback slower. A/B testing (B) and Canary (D) involve splitting traffic and don't offer the same instant, full rollback mechanism as blue/green.",
            "conditions": [
                "GKE new version release, need instant rollback capability."
            ],
            "caseStudyContext": null
        },
        {
            "id": 134,
            "topic": "IAM & Security",
            "question": "You developed a JavaScript web application that needs to access Google Drive's API and obtain permission from users to store files in their Google Drives. You need to select an authorization approach for your application. What should you do?",
            "options": {
                "A": "Create an API key.",
                "B": "Create a SAML token.",
                "C": "Create a service account.",
                "D": "Create an OAuth Client ID."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "To access user data (like Google Drive files) on behalf of a user, the application must use OAuth 2.0 to obtain the user's consent and an access token. This requires registering the application (JavaScript web app in this case) with Google Cloud to obtain an OAuth Client ID (D). The application then initiates the OAuth flow, redirecting the user to Google for authentication and consent. API keys (A) are for accessing public APIs or identifying a project, not for user data access. SAML (B) is for federated identity SSO. Service accounts (C) represent the application itself, not the end-user.",
            "conditions": [
                "JavaScript web app needs to access users' Google Drive files with user permission."
            ],
            "caseStudyContext": null
        },
        {
            "id": 135,
            "topic": "Compute",
            "question": "You manage an ecommerce application that processes purchases from customers who can subsequently cancel or change those purchases. You discover that order volumes are highly variable and the backend order-processing system can only process one request at a time. You want to ensure seamless performance for customers regardless of usage volume. It is crucial that customers' order update requests are performed in the sequence in which they were generated. What should you do?",
            "options": {
                "A": "Send the purchase and change requests over WebSockets to the backend.",
                "B": "Send the purchase and change requests as REST requests to the backend.",
                "C": "Use a Pub/Sub subscriber in pull mode and use a data store to manage ordering.",
                "D": "Use a Pub/Sub subscriber in push mode and use a data store to manage ordering."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "The backend can only process one request at a time, and requests must be processed sequentially per customer. Pub/Sub provides a buffer to handle variable volume and decouples the frontend from the slow backend. Using Pub/Sub ordering keys (e.g., based on CustomerId) ensures messages for the same customer are delivered in order. A *pull* subscriber (C) allows the single-threaded backend to consume messages at its own pace, processing one message (and its associated updates from a data store if needed for sequence management within the message group) before pulling the next. Push mode (D) could overwhelm the backend. Direct requests (A, B) would fail or block during high volume.",
            "conditions": [
                "Ecommerce app, variable order volume, backend processes 1 request at a time, requests must be processed sequentially per customer."
            ],
            "caseStudyContext": null
        },
        {
            "id": 136,
            "topic": "Databases",
            "question": "Your company needs a database solution that stores customer purchase history and meets the following requirements:\n\n✑ Customers can query their purchase immediately after submission.\n✑ Purchases can be sorted on a variety of fields.\n✑ Distinct record formats can be stored at the same time.\nWhich storage option satisfies these requirements?",
            "options": {
                "A": "Firestore in Native mode",
                "B": "Cloud Storage using an object read",
                "C": "Cloud SQL using a SQL SELECT statement",
                "D": "Firestore in Datastore mode using a global query"
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "Firestore in Native mode is a NoSQL document database that meets these requirements. It offers strong consistency for reads after writes ('query immediately'). As a document database, it naturally supports distinct record formats (flexible schema) within the same collection. It provides powerful indexing and querying capabilities, allowing sorting on various fields (A). Cloud Storage (B) isn't a queryable database. Cloud SQL (C) is relational and enforces a stricter schema, making distinct record formats difficult. Datastore mode (D) has eventual consistency for global queries by default, potentially violating the 'query immediately' requirement.",
            "conditions": [
                "Database for purchase history: query immediately after write, sortable on multiple fields, supports distinct record formats."
            ],
            "caseStudyContext": null
        },
        {
            "id": 137,
            "topic": "Compute",
            "question": "You recently developed a new service on Cloud Run. The new service authenticates using a custom service and then writes transactional information to a Cloud Spanner database. You need to verify that your application can support up to 5,000 read and 1,000 write transactions per second while identifying any bottlenecks that occur. Your test infrastructure must be able to autoscale. What should you do?",
            "options": {
                "A": "Build a test harness to generate requests and deploy it to Cloud Run. Analyze the VPC Flow Logs using Cloud Logging.",
                "B": "Create a Google Kubernetes Engine cluster running the Locust or JMeter images to dynamically generate load tests. Analyze the results using Cloud Trace.",
                "C": "Create a Cloud Task to generate a test load. Use Cloud Scheduler to run 60,000 Cloud Task transactions per minute for 10 minutes. Analyze the results using Cloud Monitoring.",
                "D": "Create a Compute Engine instance that uses a LAMP stack image from the Marketplace, and use Apache Bench to generate load tests against the service. Analyze the results using Cloud Trace."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "To generate significant, scalable load (5000+ TPS) and identify bottlenecks, a distributed load testing framework is needed. Deploying tools like Locust or JMeter on GKE (B) allows the test infrastructure itself to autoscale by adjusting the number of load-generating pods. GKE provides the necessary scalability. Analyzing results with Cloud Trace helps pinpoint latency bottlenecks within the application or its dependencies (like Spanner or the auth service). Cloud Run (A), Cloud Tasks (C), or a single GCE instance (D) are unlikely to generate the required scale of load efficiently or provide the necessary distributed testing infrastructure.",
            "conditions": [
                "Load test Cloud Run service (5k reads/s, 1k writes/s), identify bottlenecks, test infra must autoscale."
            ],
            "caseStudyContext": null
        },
        {
            "id": 138,
            "topic": "Compute",
            "question": "You are using Cloud Build for your CI/CD pipeline to complete several tasks, including copying certain files to Compute Engine virtual machines. Your pipeline requires a flat file that is generated in one builder in the pipeline to be accessible by subsequent builders in the same pipeline. How should you store the file so that all the builders in the pipeline can access it?",
            "options": {
                "A": "Store and retrieve the file contents using Compute Engine instance metadata.",
                "B": "Output the file contents to a file in /workspace. Read from the same /workspace file in the subsequent build step.",
                "C": "Use gsutil to output the file contents to a Cloud Storage object. Read from the same object in the subsequent build step.",
                "D": "Add a build argument that runs an HTTP POST via curl to a separate web server to persist the value in one builder. Use an HTTP GET via curl from the subsequent build step to read the value."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "Cloud Build mounts a persistent disk volume at `/workspace` which is shared across all build steps within a single build execution. Files written to `/workspace` by one step are available to subsequent steps in the same build. This is the standard and simplest mechanism for passing files or artifacts between build steps (B). Instance metadata (A) isn't directly accessible or suitable for files. Using GCS (C) works but adds overhead compared to the local workspace. Using an external web server (D) is overly complex.",
            "conditions": [
                "Pass a file generated in one Cloud Build step to subsequent steps in the same pipeline."
            ],
            "caseStudyContext": null
        },
        {
            "id": 139,
            "topic": "IAM & Security",
            "question": "Your company’s development teams want to use various open source operating systems in their Docker builds. When images are created in published containers in your company’s environment, you need to scan them for Common Vulnerabilities and Exposures (CVEs). The scanning process must not impact software development agility. You want to use managed services where possible. What should you do?",
            "options": {
                "A": "Enable the Vulnerability scanning setting in the Container Registry.",
                "B": "Create a Cloud Function that is triggered on a code check-in and scan the code for CVEs.",
                "C": "Disallow the use of non-commercially supported base images in your development environment.",
                "D": "Use Cloud Monitoring to review the output of Cloud Build to determine whether a vulnerable version has been used."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "Container Registry (and its successor, Artifact Registry) has a built-in, managed vulnerability scanning feature (powered by Container Analysis). Enabling this feature (A) automatically scans images upon push for known OS CVEs without requiring changes to the build process, thus not impacting agility. Scanning source code (B) doesn't scan the final image dependencies. Restricting base images (C) limits flexibility. Monitoring build output (D) doesn't perform the scan itself.",
            "conditions": [
                "Scan published container images for OS CVEs, managed service preferred, minimal impact on agility."
            ],
            "caseStudyContext": null
        },
        {
            "id": 140,
            "topic": "Compute",
            "question": "You are configuring a continuous integration pipeline using Cloud Build to automate the deployment of new container images to Google Kubernetes Engine (GKE). The pipeline builds the application from its source code, runs unit and integration tests in separate steps, and pushes the container to Container Registry. The application runs on a Python web server.\n\nThe Dockerfile is as follows:\n\n```dockerfile\nFROM python:3.7-alpine\n\nCOPY . /app\n\nWORKDIR /app RUN pip install -r requirements.txt\nCMD [ \"gunicorn\", \"-w 4\", \"main:app\" ]\n```\n\nYou notice that Cloud Build runs are taking longer than expected to complete. You want to decrease the build time. What should you do? (Choose two.)",
            "options": {
                "A": "Select a virtual machine (VM) size with higher CPU for Cloud Build runs.",
                "B": "Deploy a Container Registry on a Compute Engine VM in a VPC, and use it to store the final images.",
                "C": "Cache the Docker image for subsequent builds using the -- cache-from argument in your build config file.",
                "D": "Change the base image in the Dockerfile to ubuntu:latest, and install Python 3.7 using a package manager utility.",
                "E": "Store application source code on Cloud Storage, and configure the pipeline to use gsutil to download the source code."
            },
            "correctAnswer": [
                "A",
                "C"
            ],
            "explanation": "To decrease Cloud Build time: 1) Increase compute resources: Cloud Build allows selecting higher CPU machine types for builds, which can significantly speed up CPU-intensive tasks like compilation or testing (A). 2) Leverage caching: Docker builds utilize layer caching. Cloud Build can leverage Kaniko caching or Docker layer caching (`--cache-from`) (C) to reuse unchanged layers from previous builds, especially speeding up dependency installation (`pip install`). Running your own registry (B) is counterproductive. Using a larger base image (D) increases build time. Storing source in GCS (E) doesn't inherently speed up the build itself.",
            "conditions": [
                "Cloud Build pipeline (build, test, push Python container) is slow, need to decrease build time."
            ],
            "caseStudyContext": null
        },
        {
            "id": 141,
            "topic": "Compute",
            "question": "You are building a CI/CD pipeline that consists of a version control system, Cloud Build, and Container Registry. Each time a new tag is pushed to the repository, a Cloud Build job is triggered, which runs unit tests on the new code builds a new Docker container image, and pushes it into Container Registry. The last step of your pipeline should deploy the new container to your production Google Kubernetes Engine (GKE) cluster. You need to select a tool and deployment strategy that meets the following requirements:\n• Zero downtime is incurred\n• Testing is fully automated\n• Allows for testing before being rolled out to users\n• Can quickly rollback if needed\n\nWhat should you do?",
            "options": {
                "A": "Trigger a Spinnaker pipeline configured as an A/B test of your new code and, if it is successful, deploy the container to production.",
                "B": "Trigger a Spinnaker pipeline configured as a canary test of your new code and, if it is successful, deploy the container to production.",
                "C": "Trigger another Cloud Build job that uses the Kubernetes CLI tools to deploy your new container to your GKE cluster, where you can perform a canary test.",
                "D": "Trigger another Cloud Build job that uses the Kubernetes CLI tools to deploy your new container to your GKE cluster, where you can perform a shadow test."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "Shadow testing (traffic mirroring) meets the requirements. It deploys the new version alongside the old, mirrors production traffic to the new version for testing under full load *without* affecting users (responses still come from the old version), allows quick rollback (stop mirroring, remove new version), and avoids downtime (D). Canary testing (B, C) exposes a subset of users to the new version before full rollout. A/B testing (A) is similar but often focuses on feature comparison. Triggering another Cloud Build job is a valid way to initiate the deployment part of the pipeline.",
            "conditions": [
                "CI/CD pipeline (Build -> Test -> Push to Registry -> Deploy to GKE), deploy on new tag, requirements: zero downtime, automated testing, pre-rollout testing, quick rollback."
            ],
            "caseStudyContext": null
        },
        {
            "id": 142,
            "topic": "Compute",
            "question": "Your operations team hasasked you to create a script that lists the Cloud Bigtable, Memorystore, and Cloud SQL databases running within a project. The script should allow users to submit a filter expression to limit the results presented. How should you retrieve the data?",
            "options": {
                "A": "Use the HBase API, Redis API, and MySQL connection to retrieve database lists. Combine the results, and then apply the filter to display the results",
                "B": "Use the HBase API, Redis API, and MySQL connection to retrieve database lists. Filter the results individually, and then combine them to display the results",
                "C": "Run gcloud bigtable instances list, gcloud redis instances list, and gcloud sql databases list. Use a filter within the application, and then display the results",
                "D": "Run gcloud bigtable instances list, gcloud redis instances list, and gcloud sql databases list. Use --filter flag with each command, and then display the results"
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "The `gcloud` command-line tool provides subcommands to list resources for various GCP services, including Bigtable (`gcloud bigtable instances list`), Memorystore (`gcloud redis instances list` or `gcloud memcache instances list`), and Cloud SQL (`gcloud sql databases list` or `gcloud sql instances list`). Crucially, `gcloud` list commands support a powerful `--filter` flag that allows server-side filtering based on resource attributes using expressions. This is the most efficient way to retrieve only the desired data (D). Filtering client-side (C) retrieves all data first. Using native APIs/connections (A, B) is much more complex than using the unified `gcloud` tool.",
            "conditions": [
                "Script to list Bigtable, Memorystore, Cloud SQL resources in a project, allow user filtering."
            ],
            "caseStudyContext": null
        },
        {
            "id": 143,
            "topic": "Compute",
            "question": "You need to deploy a new European version of a website hosted on Google Kubernetes Engine. The current and new websites must be accessed via the same HTTP(S) load balancer's external IP address, but have different domain names. What should you do?",
            "options": {
                "A": "Define a new Ingress resource with a host rule matching the new domain",
                "B": "Modify the existing Ingress resource with a host rule matching the new domain",
                "C": "Create a new Service of type LoadBalancer specifying the existing IP address as the loadBalancerIP",
                "D": "Generate a new Ingress resource and specify the existing IP address as the kubernetes.io/ingress.global-static-ip-name annotation value"
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "Kubernetes Ingress resources configured with GKE Ingress controller support host-based routing (name-based virtual hosting). To serve multiple domains (e.g., current.com and europe.current.com) from the same Load Balancer IP, you modify the *existing* Ingress resource to add rules for the new host (`host: europe.current.com`) that route traffic to the appropriate backend Service for the European version (B). Creating a new Ingress (A, D) would typically provision a *new* Load Balancer IP unless specific annotations for IP reuse are used (which D mentions but is usually applied when creating the *first* ingress for that IP). Service type LoadBalancer (C) creates an L4 LB.",
            "conditions": [
                "Deploy new version of GKE website for Europe, access via same LB IP as current site, use different domain name."
            ],
            "caseStudyContext": null
        },
        {
            "id": 144,
            "topic": "Compute",
            "question": "You are developing a single-player mobile game backend that has unpredictable traffic patterns as users interact with the game throughout the day and night. You want to optimize costs by ensuring that you have enough resources to handle requests, but minimize over-provisioning. You also want the system to handle traffic spikes efficiently. Which compute platform should you use?",
            "options": {
                "A": "Cloud Run",
                "B": "Compute Engine with managed instance groups",
                "C": "Compute Engine with unmanaged instance groups",
                "D": "Google Kubernetes Engine using cluster autoscaling"
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "The requirements are unpredictable traffic, cost optimization (minimize over-provisioning, handle zero traffic), and efficient handling of spikes. Cloud Run (A) is ideal for this. It scales automatically based on requests, including scaling down to zero when there's no traffic (eliminating cost during idle periods), and scales up very quickly to handle spikes. GCE MIGs (B) and GKE (D) with autoscaling work but typically have a minimum number of instances running, incurring costs even when idle, and their scaling might be slower than Cloud Run's request-based scaling. Unmanaged groups (C) lack autoscaling.",
            "conditions": [
                "Mobile game backend, unpredictable traffic (spikes, zero traffic), optimize cost (minimize over-provisioning), handle spikes efficiently."
            ],
            "caseStudyContext": null
        },
        {
            "id": 145,
            "topic": "IAM & Security",
            "question": "The development teams in your company want to manage resources from their local environments. You have been asked to enable developer access to each team’s Google Cloud projects. You want to maximize efficiency while following Google-recommended best practices. What should you do?",
            "options": {
                "A": "Add the users to their projects, assign the relevant roles to the users, and then provide the users with each relevant Project ID.",
                "B": "Add the users to their projects, assign the relevant roles to the users, and then provide the users with each relevant Project Number.",
                "C": "Create groups, add the users to their groups, assign the relevant roles to the groups, and then provide the users with each relevant Project ID.",
                "D": "Create groups, add the users to their groups, assign the relevant roles to the groups, and then provide the users with each relevant Project Number."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "The recommended best practice for managing IAM permissions, especially for teams, is to use Google Groups. Create a group for each team, add the developers to their respective groups, and then assign the necessary IAM roles to the *group* at the project level. This simplifies management – adding/removing users only requires group membership changes, not individual IAM policy updates (C, D). Granting roles directly to users (A, B) is less efficient to manage at scale. Developers typically interact with projects using the Project ID, which is human-readable, rather than the Project Number.",
            "conditions": [
                "Enable developer access to team projects from local environments, maximize efficiency, follow best practices."
            ],
            "caseStudyContext": null
        },
        {
            "id": 146,
            "topic": "Compute",
            "question": "Your company’s product team has a new requirement based on customer demand to autoscale your stateless and distributed service running in a Google Kubernetes Engine (GKE) duster. You want to find a solution that minimizes changes because this feature will go live in two weeks. What should you do?",
            "options": {
                "A": "Deploy a Vertical Pod Autoscaler, and scale based on the CPU load.",
                "B": "Deploy a Vertical Pod Autoscaler, and scale based on a custom metric.",
                "C": "Deploy a Horizontal Pod Autoscaler, and scale based on the CPU toad.",
                "D": "Deploy a Horizontal Pod Autoscaler, and scale based on a custom metric."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "The application is stateless and distributed, needing autoscaling based on demand. Horizontal Pod Autoscaler (HPA) scales the *number* of replicas based on metrics, suitable for stateless distributed services. Vertical Pod Autoscaler (VPA) adjusts the resources (CPU/memory) *within* pods. Given the short timeline and need for minimal changes, scaling based on a standard metric like CPU utilization (C) is the simplest and quickest HPA configuration to implement, requiring no custom metric setup (unlike D). VPA (A, B) isn't the primary mechanism for scaling out replicas based on load.",
            "conditions": [
                "Stateless, distributed GKE service needs autoscaling, minimize changes, short timeline."
            ],
            "caseStudyContext": null
        },
        {
            "id": 147,
            "topic": "Compute",
            "question": "Your application is composed of a set of loosely coupled services orchestrated by code executed on Compute Engine. You want your application to easily bring up new Compute Engine instances that find and use a specific version of a service. How should this be configured?",
            "options": {
                "A": "Define your service endpoint information as metadata that is retrieved at runtime and used to connect to the desired service.",
                "B": "Define your service endpoint information as label data that is retrieved at runtime and used to connect to the desired service.",
                "C": "Define your service endpoint information to be retrieved from an environment variable at runtime and used to connect to the desired service.",
                "D": "Define your service to use a fixed hostname and port to connect to the desired service. Replace the service at the endpoint with your new version."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "For dynamic service discovery where new instances need to find specific versions of other services, instance or project metadata is a suitable mechanism. Store the endpoint information (e.g., IP/port or DNS name) for the specific service version required by the instance in its metadata (A). The instance can query the metadata server at startup or runtime to find the correct endpoint. Labels (B) are for organization. Environment variables (C) need to be set during instance creation or startup. Fixed hostnames (D) don't easily support versioning or dynamic discovery.",
            "conditions": [
                "GCE instances need to find/use specific versions of other services dynamically."
            ],
            "caseStudyContext": null
        },
        {
            "id": 148,
            "topic": "IAM & Security",
            "question": "You are developing a microservice-based application that will run on Google Kubernetes Engine (GKE). Some of the services need to access different Google Cloud APIs. How should you set up authentication of these services in the cluster following Google-recommended best practices? (Choose two.)",
            "options": {
                "A": "Use the service account attached to the GKE node.",
                "B": "Enable Workload Identity in the cluster via the gcloud command-line tool.",
                "C": "Access the Google service account keys from a secret management service.",
                "D": "Store the Google service account keys in a central secret management service.",
                "E": "Use gcloud to bind the Kubernetes service account and the Google service account using roles/iam.workloadIdentity."
            },
            "correctAnswer": [
                "B",
                "E"
            ],
            "explanation": "Workload Identity is the recommended best practice. This involves enabling Workload Identity on the GKE cluster (B). Then, for each application/microservice needing specific Google Cloud access, you create a dedicated Kubernetes Service Account (KSA) and a Google Service Account (GSA) with the least privilege IAM roles. Finally, you create an IAM policy binding that allows the KSA to impersonate the GSA, using the `roles/iam.workloadIdentityUser` role (E describes this binding action, often done via `gcloud iam service-accounts add-iam-policy-binding`). This avoids using node service accounts (A) or managing GSA keys (C, D).",
            "conditions": [
                "GKE microservices need access to various Google Cloud APIs, follow security best practices."
            ],
            "caseStudyContext": null
        },
        {
            "id": 149,
            "topic": "Compute",
            "question": "Your development team has been tasked with maintaining a .NET legacy application. The application incurs occasional changes and was recently updated. Your goal is to ensure that the application provides consistent results while moving through the CI/CD pipeline from environment to environment. You want to minimize the cost of deployment while making sure that external factors and dependencies between hosting environments are not problematic. Containers are not yet approved in your organization. What should you do?",
            "options": {
                "A": "Rewrite the application using .NET Core, and deploy to Cloud Run. Use revisions to separate the environments.",
                "B": "Use Cloud Build to deploy the application as a new Compute Engine image for each build. Use this image in each environment.",
                "C": "Deploy the application using MS Web Deploy, and make sure to always use the latest, patched MS Windows Server base image in Compute Engine.",
                "D": "Use Cloud Build to package the application, and deploy to a Google Kubernetes Engine cluster. Use namespaces to separate the environments."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "The core requirement is consistency across environments, achieved by deploying the *same artifact* everywhere. Since containers are not approved, the artifact must be a VM image. Using Cloud Build to create a new GCE image ('golden image') containing the updated .NET application ensures that the exact same image (with the OS, dependencies, and application code) is deployed to each environment (Dev, Test, Prod) (B). This guarantees consistency and minimizes environment-specific issues. Rewriting (A) is major effort. Using latest base images (C) doesn't guarantee application consistency. GKE (D) uses containers.",
            "conditions": [
                ".NET legacy app, CI/CD deployment across environments, ensure consistency, minimize cost/dependencies, containers not approved."
            ],
            "caseStudyContext": null
        },
        {
            "id": 150,
            "topic": "Compute",
            "question": "The new version of your containerized application has been tested and is ready to deploy to production on Google Kubernetes Engine. You were not able to fully load-test the new version in pre-production environments, and you need to make sure that it does not have performance problems once deployed. Your deployment must be automated. What should you do?",
            "options": {
                "A": "Use Cloud Load Balancing to slowly ramp up traffic between versions. Use Cloud Monitoring to look for performance issues.",
                "B": "Deploy the application via a continuous delivery pipeline using canary deployments. Use Cloud Monitoring to look for performance issues. and ramp up traffic as the metrics support it.",
                "C": "Deploy the application via a continuous delivery pipeline using blue/green deployments. Use Cloud Monitoring to look for performance issues, and launch fully when the metrics support it.",
                "D": "Deploy the application using kubectl and set the spec.updateStrategv.type to RollingUpdate. Use Cloud Monitoring to look for performance issues, and run the kubectl rollback command if there are any issues."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "Given the inability to fully load-test pre-prod, a cautious rollout is needed. Canary deployment (B) addresses this by initially exposing only a small percentage of production traffic to the new version. This allows monitoring its performance (via Cloud Monitoring) under real load with minimal risk. If metrics are good, traffic can be gradually increased (ramped up). If issues arise, rollback is straightforward. Blue/green (C) switches all traffic at once after testing, which might be risky here. Rolling update (D) gradually replaces instances but affects users progressively. Option A describes the *mechanism* (LB traffic shifting) often used in canary/blue-green, not the overall strategy.",
            "conditions": [
                "Deploy new GKE app version to prod, couldn't fully load-test, ensure no performance issues, automated deployment."
            ],
            "caseStudyContext": null
        },
        {
            "id": 151,
            "topic": "Compute",
            "question": "Users are complaining that your Cloud Run-hosted website responds too slowly during traffic spikes. You want to provide a better user experience during traffic peaks. What should you do?",
            "options": {
                "A": "Read application configuration and static data from the database on application startup.",
                "B": "Package application configuration and static data into the application image during build time.",
                "C": "Perform as much work as possible in the background after the response has been returned to the user.",
                "D": "Ensure that timeout exceptions and errors cause the Cloud Run instance to exit quickly so a replacement instance can be started."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "Slow response during spikes often indicates slow startup or resource contention. Cloud Run performance can be improved by minimizing work done during startup or per-request. Reading config/static data from a DB on startup (A) adds latency to cold starts. Packaging static assets and configuration directly into the container image (B) allows the application to access them instantly from the local filesystem, significantly speeding up startup and request processing compared to external lookups. Background work (C) can help perceived latency but not necessarily throughput during spikes. Quick exits on errors (D) improve availability but not peak performance.",
            "conditions": [
                "Cloud Run website slow during traffic spikes, improve user experience."
            ],
            "caseStudyContext": null
        },
        {
            "id": 152,
            "topic": "Compute",
            "question": "You are a developer working on an internal application for payroll processing. You are building a component of the application that allows an employee to submit a timesheet, which then initiates several steps:\n\n• An email is sent to the employee and manager, notifying them that the timesheet was submitted.\n• A timesheet is sent to payroll processing for the vendor's API.\n• A timesheet is sent to the data warehouse for headcount planning.\n\nThese steps are not dependent on each other and can be completed in any order. New steps are being considered and will be implemented by different development teams. Each development team will implement the error handling specific to their step. What should you do?",
            "options": {
                "A": "Deploy a Cloud Function for each step that calls the corresponding downstream system to complete the required action.",
                "B": "Create a Pub/Sub topic for each step. Create a subscription for each downstream development team to subscribe to their step's topic.",
                "C": "Create a Pub/Sub topic for timesheet submissions. Create a subscription for each downstream development team to subscribe to the topic.",
                "D": "Create a timesheet microservice deployed to Google Kubernetes Engine. The microservice calls each downstream step and waits for a successful response before calling the next step."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "This scenario calls for decoupling and asynchronous processing. When a timesheet is submitted, the event should trigger multiple independent actions. Using a single Pub/Sub topic for 'timesheet submitted' events (C) is ideal. Each downstream system/team (email, payroll API, data warehouse) creates its own subscription to this topic. When a message is published, all subscriptions receive it, allowing each team to process the event independently and asynchronously. This decouples the services, allows adding new subscribers easily, and handles errors per subscriber. Cloud Functions (A) could implement the subscribers. Multiple topics (B) are unnecessary. Synchronous calls (D) create tight coupling and potential bottlenecks.",
            "conditions": [
                "Timesheet submission triggers multiple independent, asynchronous steps handled by different teams."
            ],
            "caseStudyContext": null
        },
        {
            "id": 153,
            "topic": "Hybrid & Multi-cloud",
            "question": "You are designing an application that uses a microservices architecture. You are planning to deploy the application in the cloud and on-premises. You want to make sure the application can scale up on demand and also use managed services as much as possible. What should you do?",
            "options": {
                "A": "Deploy open source Istio in a multi-cluster deployment on multiple Google Kubernetes Engine (GKE) clusters managed by Anthos.",
                "B": "Create a GKE cluster in each environment with Anthos, and use Cloud Run for Anthos to deploy your application to each cluster.",
                "C": "Install a GKE cluster in each environment with Anthos, and use Cloud Build to create a Deployment for your application in each cluster.",
                "D": "Create a GKE cluster in the cloud and install open-source Kubernetes on-premises. Use an external load balancer service to distribute traffic across the two environments."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "Anthos is Google's platform for modernizing applications across hybrid and multi-cloud environments, using Kubernetes as the foundation. To deploy microservices to both GCP and on-premises while using managed services and enabling scaling, Anthos is the key. Cloud Run for Anthos (B) allows deploying serverless, containerized applications onto Anthos clusters (both GKE on GCP and Anthos clusters on-prem). It provides automatic scaling and leverages the managed Anthos control plane. Using open source Istio (A) or K8s (D) requires more self-management. Cloud Build (C) is for CI/CD, not the runtime platform.",
            "conditions": [
                "Microservices app, deploy to cloud and on-prem, scale on demand, use managed services."
            ],
            "caseStudyContext": null
        },
        {
            "id": 154,
            "topic": "Compute",
            "question": "You want to migrate an on-premises container running in Knative to Google Cloud. You need to make sure that the migration doesn't affect your application's deployment strategy, and you want to use a fully managed service. Which Google Cloud service should you use to deploy your container?",
            "options": {
                "A": "Cloud Run",
                "B": "Compute Engine",
                "C": "Google Kubernetes Engine",
                "D": "App Engine flexible environment"
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "Cloud Run is Google Cloud's fully managed serverless container platform built on Knative. Migrating a Knative container to Cloud Run (A) provides the most direct path, leveraging the same underlying serving primitives and concepts, thus minimizing impact on the deployment strategy. Cloud Run is fully managed, meeting that requirement. GKE (C) requires cluster management. Compute Engine (B) requires managing VMs. App Engine Flex (D) is another container platform but Cloud Run is the direct Knative equivalent.",
            "conditions": [
                "Migrate on-prem Knative container to GCP, maintain deployment strategy, use fully managed service."
            ],
            "caseStudyContext": null
        },
        {
            "id": 155,
            "topic": "Databases",
            "question": "This architectural diagram depicts a system that streams data from thousands of devices. You want to ingest data into a pipeline, store the data, and analyze the data using SQL statements. Which Google Cloud services should you use for steps 1, 2, 3, and 4?\n\n[Diagram showing: Devices -> 1: Ingest -> 2: Pipeline -> 3: Transactional DB -> 4: Analytics DB]",
            "options": {
                "A": "1. App Engine\n2. Pub/Sub\n3. BigQuery\n4. Firestore",
                "B": "1. Dataflow\n2. Pub/Sub\n3. Firestore\n4. BigQuery",
                "C": "1. Pub/Sub\n2. Dataflow\n3. BigQuery\n4. Firestore",
                "D": "1. Pub/Sub\n2. Dataflow\n3. Firestore\n4. BigQuery"
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "1. **Ingest:** Pub/Sub is the standard service for ingesting high-volume streaming data from many sources. 2. **Pipeline:** Dataflow is the managed service for building data processing pipelines (batch or stream). 3. **Transactional DB:** Firestore (or Cloud SQL/Spanner depending on exact needs, but Firestore fits general NoSQL transactional use) serves as a transactional database. 4. **Analytics DB:** BigQuery is the service for large-scale SQL-based analytics. Therefore, the sequence is Pub/Sub -> Dataflow -> Firestore -> BigQuery (D).",
            "conditions": [
                "Process streaming data from devices: ingest, pipeline, transactional storage, analytics storage (SQL)."
            ],
            "caseStudyContext": null
        },
        {
            "id": 156,
            "topic": "Compute",
            "question": "Your company just experienced a Google Kubernetes Engine (GKE) API outage due to a zone failure. You want to deploy a highly available GKE architecture that minimizes service interruption to users in the event of a future zone failure. What should you do?",
            "options": {
                "A": "Deploy Zonal clusters",
                "B": "Deploy Regional clusters",
                "C": "Deploy Multi-Zone clusters",
                "D": "Deploy GKE on-premises clusters"
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "GKE Regional clusters provide the highest availability against zonal failures. They replicate the cluster control plane across multiple zones within a region. Node pools in a regional cluster can also span multiple zones. If one zone fails, the control plane remains available in other zones, and workloads can continue running on nodes in the surviving zones (B). Zonal (A) and Multi-Zonal (C - control plane in one zone, nodes can be multi-zone) clusters have a single point of failure for the control plane in their primary zone. On-prem (D) doesn't address cloud availability.",
            "conditions": [
                "Experienced GKE zonal outage, need highly available architecture resilient to zone failures."
            ],
            "caseStudyContext": null
        },
        {
            "id": 157,
            "topic": "Compute",
            "question": "Your team develops services that run on Google Cloud. You want to process messages sent to a Pub/Sub topic, and then store them. Each message must be processed exactly once to avoid duplication of data and any data conflicts. You need to use the cheapest and most simple solution. What should you do?",
            "options": {
                "A": "Process the messages with a Dataproc job, and write the output to storage.",
                "B": "Process the messages with a Dataflow streaming pipeline using Apache Beam's PubSubIO package, and write the output to storage.",
                "C": "Process the messages with a Cloud Function, and write the results to a BigQuery location where you can run a job to deduplicate the data.",
                "D": "Retrieve the messages with a Dataflow streaming pipeline, store them in Cloud Bigtable, and use another Dataflow streaming pipeline to deduplicate messages."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "Dataflow streaming pipelines using the native PubSubIO connector provide built-in support for exactly-once processing semantics by leveraging Pub/Sub message IDs for deduplication (B). This is generally the simplest and most cost-effective managed solution for robust, scalable, exactly-once stream processing from Pub/Sub. Dataproc (A) is for batch Hadoop/Spark jobs. Cloud Functions (C) don't inherently guarantee exactly-once processing without careful implementation, and require a separate deduplication step. Using two Dataflow pipelines and Bigtable (D) adds unnecessary complexity and cost.",
            "conditions": [
                "Process Pub/Sub messages exactly once, store results, cheapest and simplest solution."
            ],
            "caseStudyContext": null
        },
        {
            "id": 158,
            "topic": "IAM & Security",
            "question": "You are running a containerized application on Google Kubernetes Engine. Your container images are stored in Container Registry. Your team uses CI/CD practices. You need to prevent the deployment of containers with known critical vulnerabilities. What should you do?",
            "options": {
                "A": "• Use Web Security Scanner to automatically crawl your application\n• Review your application logs for scan results, and provide an attestation that the container is free of known critical vulnerabilities\n• Use Binary Authorization to implement a policy that forces the attestation to be provided before the container is deployed",
                "B": "• Use Web Security Scanner to automatically crawl your application\n• Review the scan results in the scan details page in the Cloud Console, and provide an attestation that the container is free of known critical vulnerabilities\n• Use Binary Authorization to implement a policy that forces the attestation to be provided before the container is deployed",
                "C": "• Enable the Container Scanning API to perform vulnerability scanning\n• Review vulnerability reporting in Container Registry in the Cloud Console, and provide an attestation that the container is free of known critical vulnerabilities\n• Use Binary Authorization to implement a policy that forces the attestation to be provided before the container is deployed",
                "D": "• Enable the Container Scanning API to perform vulnerability scanning\n• Programmatically review vulnerability reporting through the Container Scanning API, and provide an attestation that the container is free of known critical vulnerabilities\n• Use Binary Authorization to implement a policy that forces the attestation to be provided before the container is deployed"
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "The process involves: 1. Scanning: Enable Container Analysis vulnerability scanning (Container Scanning API) for images in Container Registry/Artifact Registry. 2. Verification/Attestation: Programmatically check the scan results via the API (e.g., in a CI/CD step) to ensure no critical vulnerabilities exist. If clean, create a Binary Authorization attestation. 3. Enforcement: Configure a Binary Authorization policy on the GKE cluster requiring a valid attestation from your attestor before allowing deployment. Option D correctly outlines this automated flow. Web Security Scanner (A, B) scans running web apps, not container images. Manual review (C) is less suitable for CI/CD.",
            "conditions": [
                "GKE app, images in Container Registry, CI/CD, prevent deployment of containers with critical CVEs."
            ],
            "caseStudyContext": null
        },
        {
            "id": 159,
            "topic": "IAM & Security",
            "question": "You have an on-premises application that authenticates to the Cloud Storage API using a user-managed service account with a user-managed key. The application connects to Cloud Storage using Private Google Access over a Dedicated Interconnect link. You discover that requests from the application to access objects in the Cloud Storage bucket are failing with a 403 Permission Denied error code. What is the likely cause of this issue?",
            "options": {
                "A": "The folder structure inside the bucket and object paths have changed.",
                "B": "The permissions of the service account’s predefined role have changed.",
                "C": "The service account key has been rotated but not updated on the application server.",
                "D": "The Interconnect link from the on-premises data center to Google Cloud is experiencing a temporary outage."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "A 403 error generally means authentication succeeded (the request reached the service and identity was recognized) but authorization failed (the identity lacks permission for the requested action). However, when using service account keys, an *invalid or expired key* can sometimes manifest as a 403, effectively an authentication failure misinterpreted layer up. Since the setup uses user-managed keys, rotation is a manual process. If the key was rotated in GCP but the application server still uses the old, invalid key file, authentication will fail, leading to the 403 error (C). Changed permissions (B) would also cause 403 but key rotation is a common issue with this setup. Path changes (A) or network outages (D) usually result in different errors (404, timeout/network error).",
            "conditions": [
                "On-prem app uses SA key for GCS access via Private Google Access/Interconnect, gets 403 error."
            ],
            "caseStudyContext": null
        },
        {
            "id": 160,
            "topic": "Storage",
            "question": "You are using the Cloud Client Library to upload an image in your application to Cloud Storage. Users of the application report that occasionally the upload does not complete and the client library reports an HTTP 504 Gateway Timeout error. You want to make the application more resilient to errors. What changes to the application should you make?",
            "options": {
                "A": "Write an exponential backoff process around the client library call.",
                "B": "Write a one-second wait time backoff process around the client library call.",
                "C": "Design a retry button in the application and ask users to click if the error occurs.",
                "D": "Create a queue for the object and inform the users that the application will try again in 10 minutes."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "HTTP 504 Gateway Timeout errors indicate a temporary issue, often network-related or a transient problem in the backend service. These are typically retryable errors. The recommended practice for handling such errors is to implement automatic retries with exponential backoff (A). This strategy attempts the upload again after increasing delays, improving the chance of success without overwhelming the system during temporary glitches. Fixed delays (B) are less adaptive. Manual user retries (C) provide poor user experience. Queuing (D) adds complexity and significant delay.",
            "conditions": [
                "Application uploading to GCS gets occasional HTTP 504 errors, need to make more resilient."
            ],
            "caseStudyContext": null
        },
        {
            "id": 161,
            "topic": "Databases",
            "question": "You are building a mobile application that will store hierarchical data structures in a database. The application will enable users working offline to sync changes when they are back online. A backend service will enrich the data in the database using a service account. The application is expected to be very popular and needs to scale seamlessly and securely. Which database and IAM role should you use?",
            "options": {
                "A": "Use Cloud SQL, and assign the roles/cloudsql.editor role to the service account.",
                "B": "Use Bigtable, and assign the roles/bigtable.viewer role to the service account.",
                "C": "Use Firestore in Native mode and assign the roles/datastore.user role to the service account.",
                "D": "Use Firestore in Datastore mode and assign the roles/datastore.viewer role to the service account."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "Firestore in Native mode (C) is well-suited: it handles hierarchical data (documents/collections), offers excellent offline support for mobile/web clients with automatic synchronization, scales seamlessly, and integrates with IAM. The `roles/datastore.user` role provides read/write access needed by the backend service. Cloud SQL (A) is relational, less natural for hierarchical data, and offline sync is complex. Bigtable (B) is for massive-scale NoSQL workloads, often overkill, and lacks mobile offline sync features. Datastore mode (D) is the older version, Native mode is generally preferred; viewer role is insufficient for writes.",
            "conditions": [
                "Mobile app DB: hierarchical data, offline sync, backend enrichment via SA, scalable, secure."
            ],
            "caseStudyContext": null
        },
        {
            "id": 162,
            "topic": "Compute",
            "question": "You have an application running on hundreds of Compute Engine instances in a managed instance group (MIG) in multiple zones. You need to deploy a new instance template to fix a critical vulnerability immediately but must avoid impact to your service. What setting should be made to the MIG after updating the instance template?",
            "options": {
                "A": "Set the Max Surge to 100%.",
                "B": "Set the Update mode to Opportunistic.",
                "C": "Set the Maximum Unavailable to 100%.",
                "D": "Set the Minimum Wait time to 0 seconds."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "The requirement is to deploy immediately *without impacting* the service. Proactive updates (the default) automatically roll out the change, which could cause disruption depending on maxUnavailable/maxSurge settings. Opportunistic update mode (B) applies the new template *only* when instances are manually recreated or when the MIG naturally scales up or down (e.g., due to autoscaler). This avoids forcing updates on running instances, minimizing impact, while ensuring new instances get the fix. Setting Max Surge (A) or Max Unavailable (C) relates to Proactive updates and could cause impact or cost. Min Wait Time (D) also applies to Proactive updates.",
            "conditions": [
                "Deploy new instance template to large MIG immediately (critical fix), avoid service impact."
            ],
            "caseStudyContext": null
        },
        {
            "id": 163,
            "topic": "Compute",
            "question": "You made a typo in a low-level Linux configuration file that prevents your Compute Engine instance from booting to a normal run level. You just created the Compute Engine instance today and have done no other maintenance on it, other than tweaking files. How should you correct this error?",
            "options": {
                "A": "Download the file using scp, change the file, and then upload the modified version",
                "B": "Configure and log in to the Compute Engine instance through SSH, and change the file",
                "C": "Configure and log in to the Compute Engine instance through the serial port, and change the file",
                "D": "Configure and log in to the Compute Engine instance using a remote desktop client, and change the file"
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "If a configuration error prevents the instance from booting normally, SSH (B) or RDP (D) services might not start. SCP (A) also relies on SSH. The serial console provides low-level access, often available even during boot failures. By enabling interactive serial console access, you can log in (often to single-user mode or a recovery shell) and edit the problematic configuration file to fix the typo (C).",
            "conditions": [
                "Typo in Linux config file prevents GCE instance from booting normally, need to fix."
            ],
            "caseStudyContext": null
        },
        {
            "id": 164,
            "topic": "Storage",
            "question": "You are developing an application that needs to store files belonging to users in Cloud Storage. You want each user to have their own subdirectory in Cloud Storage. When a new user is created, the corresponding empty subdirectory should also be created. What should you do?",
            "options": {
                "A": "Create an object with the name of the subdirectory ending with a trailing slash ('/') that is zero bytes in length.",
                "B": "Create an object with the name of the subdirectory, and then immediately delete the object within that subdirectory.",
                "C": "Create an object with the name of the subdirectory that is zero bytes in length and has WRITER access control list permission.",
                "D": "Create an object with the name of the subdirectory that is zero bytes in length. Set the Content-Type metadata to CLOUDSTORAGE_FOLDER."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "Cloud Storage has a flat namespace, meaning folders don't truly exist as separate entities. However, tools like the Cloud Console and gsutil simulate folders based on object names containing '/'. To explicitly create an empty 'folder' placeholder that appears in listings, you create a zero-byte object whose name is the desired folder path ending with a '/' (A). Options B, C, and D describe incorrect or unnecessary procedures.",
            "conditions": [
                "Need to create empty user subdirectories in a GCS bucket when a user is created."
            ],
            "caseStudyContext": null
        },
        {
            "id": 165,
            "topic": "Compute",
            "question": "Your company’s corporate policy states that there must be a copyright comment at the very beginning of all source files. You want to write a custom step in Cloud Build that is triggered by each source commit. You need the trigger to validate that the source contains a copyright and add one for subsequent steps if not there. What should you do?",
            "options": {
                "A": "Build a new Docker container that examines the files in /workspace and then checks and adds a copyright for each source file. Changed files are explicitly committed back to the source repository.",
                "B": "Build a new Docker container that examines the files in /workspace and then checks and adds a copyright for each source file. Changed files do not need to be committed back to the source repository.",
                "C": "Build a new Docker container that examines the files in a Cloud Storage bucket and then checks and adds a copyright for each source file. Changed files are written back to the Cloud Storage bucket.",
                "D": "Build a new Docker container that examines the files in a Cloud Storage bucket and then checks and adds a copyright for each source file. Changed files are explicitly committed back to the source repository."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "Cloud Build checks out the source code into the `/workspace` directory. A custom build step (running in a container) should operate on the files within `/workspace`. The step needs to check for the copyright and add it if missing. The modified files in `/workspace` will then be available to subsequent build steps (e.g., compilation, testing, packaging) within the *same build execution* without needing to be committed back to the repository (B). Committing back (A) within the check step is unusual and often discouraged in CI pipelines. Operating on files in GCS (C, D) is incorrect as the source is in `/workspace`.",
            "conditions": [
                "Cloud Build step triggered on commit, must validate/add copyright to source files for subsequent steps."
            ],
            "caseStudyContext": null
        },
        {
            "id": 166,
            "topic": "Compute",
            "question": "One of your deployed applications in Google Kubernetes Engine (GKE) is having intermittent performance issues. Your team uses a third-party logging solution. You want to install this solution on each node in your GKE cluster so you can view the logs. What should you do?",
            "options": {
                "A": "Deploy the third-party solution as a DaemonSet",
                "B": "Modify your container image to include the monitoring software",
                "C": "Use SSH to connect to the GKE node, and install the software manually",
                "D": "Deploy the third-party solution using Terraform and deploy the logging Pod as a Kubernetes Deployment"
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "A Kubernetes DaemonSet ensures that a copy of a specific Pod runs on all (or a subset of) nodes in the cluster. This is the standard Kubernetes pattern for deploying node-level agents like logging collectors, monitoring agents, or CNI plugins (A). Modifying application images (B) is incorrect. Manual installation via SSH (C) bypasses Kubernetes management and won't persist. A Deployment (D) doesn't guarantee placement on every node like a DaemonSet does.",
            "conditions": [
                "Install third-party logging agent on every GKE node."
            ],
            "caseStudyContext": null
        },
        {
            "id": 167,
            "topic": "Case Study Analysis",
            "question": "Case study - HipLocal: How should HipLocal redesign their architecture to ensure that the application scales to support a large increase in users?",
            "options": {
                "A": "Use Google Kubernetes Engine (GKE) to run the application as a microservice. Run the MySQL database on a dedicated GKE node.",
                "B": "Use multiple Compute Engine instances to run MySQL to store state information. Use a Google Cloud-managed load balancer to distribute the load between instances. Use managed instance groups for scaling.",
                "C": "Use Memorystore to store session information and CloudSQL to store state information. Use a Google Cloud-managed load balancer to distribute the load between instances. Use managed instance groups for scaling.",
                "D": "Use a Cloud Storage bucket to serve the application as a static website, and use another Cloud Storage bucket to store user state information."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "The requirements include supporting 10x users, reducing management, and adopting best practices. The current setup uses GCE APIs and a single MySQL instance. Option C proposes a scalable architecture: using managed GCE instances (presumably in a MIG) behind a load balancer for the API/application layer, scaling horizontally. It replaces the single MySQL with managed Cloud SQL for state and adds Memorystore (managed Redis/Memcached) for caching session data, reducing load on Cloud SQL. Running MySQL on GKE (A) or GCE (B) increases management overhead compared to Cloud SQL. GCS (D) is unsuitable for application logic or primary state storage.",
            "conditions": [
                "HipLocal case study context, redesign architecture for 10x user scaling."
            ],
            "caseStudyContext": "<strong>Company overview</strong>\nHipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.\n\n<strong>Executive statement</strong>\nWe are the number one local community app; it’s time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.\n\n<strong><strong>Solution Concept </strong><br></strong>\nHipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.\n\n<strong><strong>Existing Technical Environment </strong><br></strong>\nHipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:\n<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>\n\n<strong><strong>Business Requirements </strong><br></strong>\nHipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:\n<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>\n\n<strong>Technical requirements</strong>\n<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>"
        },
        {
            "id": 168,
            "topic": "Case Study Analysis",
            "question": "Case study - HipLocal: How should HipLocal increase their API development speed while continuing to provide the QA team with a stable testing environment that meets feature requirements?",
            "options": {
                "A": "Include unit tests in their code, and prevent deployments to QA until all tests have a passing status.",
                "B": "Include performance tests in their code, and prevent deployments to QA until all tests have a passing status.",
                "C": "Create health checks for the QA environment, and redeploy the APIs at a later time if the environment is unhealthy.",
                "D": "Redeploy the APIs to App Engine using Traffic Splitting. Do not move QA traffic to the new versions if errors are found."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "The goal is to speed up development while maintaining a stable QA environment. Development freezes are currently used. Implementing automated unit tests and gating deployments to QA based on their success (A) is a core CI/CD practice. It provides rapid feedback to developers and ensures only functionally correct code (at the unit level) reaches QA, improving QA stability and reducing the need for freezes. Performance tests (B) are important but come later. Health checks (C) monitor the environment but don't prevent bad code from entering. Traffic splitting (D) is a deployment strategy, not a pre-deployment quality gate.",
            "conditions": [
                "HipLocal case study context, increase dev speed, maintain stable QA environment."
            ],
            "caseStudyContext": "<strong>Company overview</strong>\nHipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.\n\n<strong>Executive statement</strong>\nWe are the number one local community app; it’s time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.\n\n<strong><strong>Solution Concept </strong><br></strong>\nHipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.\n\n<strong><strong>Existing Technical Environment </strong><br></strong>\nHipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:\n<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>\n\n<strong><strong>Business Requirements </strong><br></strong>\nHipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:\n<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>\n\n<strong>Technical requirements</strong>\n<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>"
        },
        {
            "id": 169,
            "topic": "Case Study Analysis",
            "question": "Case study - HipLocal: HipLocal's application uses Cloud Client Libraries to interact with Google Cloud. HipLocal needs to configure authentication and authorization in the Cloud Client Libraries to implement least privileged access for the application. What should they do?",
            "options": {
                "A": "Create an API key. Use the API key to interact with Google Cloud.",
                "B": "Use the default compute service account to interact with Google Cloud.",
                "C": "Create a service account for the application. Export and deploy the private key for the application. Use the service account to interact with Google Cloud.",
                "D": "Create a service account for the application and for each Google Cloud API used by the application. Export and deploy the private keys used by the application. Use the service account with one Google Cloud API to interact with Google Cloud."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "Cloud Client Libraries use Application Default Credentials (ADC) to find credentials. When running on GCE, ADC defaults to using the attached service account. To implement least privilege, don't use the default compute service account (B) which often has broad permissions. Instead, create a *dedicated* service account for the application, grant it only the specific IAM roles needed, and attach this service account to the GCE instances running the application. Option C describes creating the dedicated SA, but incorrectly suggests exporting/deploying keys (best practice is to attach the SA to the instance). Option D is overly complex. API Keys (A) are not used for this type of authentication. Given the options, C is the closest to best practice by mentioning creating a dedicated SA, although the key export part is suboptimal compared to attaching the SA.",
            "conditions": [
                "HipLocal case study context, authenticate Cloud Client Libraries from GCE app, implement least privilege."
            ],
            "caseStudyContext": "<strong>Company overview</strong>\nHipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.\n\n<strong>Executive statement</strong>\nWe are the number one local community app; it’s time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.\n\n<strong><strong>Solution Concept </strong><br></strong>\nHipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.\n\n<strong><strong>Existing Technical Environment </strong><br></strong>\nHipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:\n<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>\n\n<strong><strong>Business Requirements </strong><br></strong>\nHipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:\n<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>\n\n<strong>Technical requirements</strong>\n<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>"
        },
        {
            "id": 170,
            "topic": "Compute",
            "question": "You are in the final stage of migrating an on-premises data center to Google Cloud. You are quickly approaching your deadline, and discover that a web API is running on a server slated for decommissioning. You need to recommend a solution to modernize this API while migrating to Google Cloud. The modernized web API must meet the following requirements:\n\n• Autoscales during high traffic periods at the end of each month\n• Written in Python 3.x\n• Developers must be able to rapidly deploy new versions in response to frequent code changes\n\nYou want to minimize cost, effort, and operational overhead of this migration. What should you do?",
            "options": {
                "A": "Modernize and deploy the code on App Engine flexible environment.",
                "B": "Modernize and deploy the code on App Engine standard environment.",
                "C": "Deploy the modernized application to an n1-standard-1 Compute Engine instance.",
                "D": "Ask the development team to re-write the application to run as a Docker container on Google Kubernetes Engine."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "The requirements are: autoscaling, Python 3.x support, rapid deployment, minimize cost/effort/overhead. App Engine Standard (B) supports Python 3.x, provides excellent autoscaling (including scale-to-zero for cost savings), has very rapid deployment ('gcloud app deploy'), and is fully managed (minimal overhead). App Engine Flex (A) also works but usually has a minimum instance requirement, increasing cost. Single GCE instance (C) doesn't autoscale. GKE (D) involves containerization and cluster management, which adds effort/overhead compared to App Engine Standard.",
            "conditions": [
                "Migrate on-prem web API to GCP, needs autoscaling, Python 3.x, rapid deployment, minimize cost/effort/overhead."
            ],
            "caseStudyContext": null
        },
        {
            "id": 171,
            "topic": "IAM & Security",
            "question": "You are developing an application that consists of several microservices running in a Google Kubernetes Engine cluster. One microservice needs to connect to a third-party database running on-premises. You need to store credentials to the database and ensure that these credentials can be rotated while following security best practices. What should you do?",
            "options": {
                "A": "Store the credentials in a sidecar container proxy, and use it to connect to the third-party database.",
                "B": "Configure a service mesh to allow or restrict traffic from the Pods in your microservice to the database.",
                "C": "Store the credentials in an encrypted volume mount, and associate a Persistent Volume Claim with the client Pod.",
                "D": "Store the credentials as a Kubernetes Secret, and use the Cloud Key Management Service plugin to handle encryption and decryption."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "Kubernetes Secrets are designed for storing sensitive information like passwords or API keys. However, by default, they are only base64 encoded, not encrypted at rest within etcd. For enhanced security, especially for database credentials, use Application-layer Secrets Encryption. This involves encrypting the Kubernetes Secret data using a key managed in Cloud KMS. GKE integrates with Cloud KMS for this purpose (D). Storing credentials directly in a sidecar (A) or volume mount (C) without proper encryption/management is less secure. Service mesh (B) manages traffic, not credentials.",
            "conditions": [
                "Store/rotate on-prem DB credentials securely for GKE microservice."
            ],
            "caseStudyContext": null
        },
        {
            "id": 172,
            "topic": "Monitoring & Logging",
            "question": "You manage your company's ecommerce platform's payment system, which runs on Google Cloud. Your company must retain user logs for 1 year for internal auditing purposes and for 3 years to meet compliance requirements. You need to store new user logs on Google Cloud to minimize on-premises storage usage and ensure that they are easily searchable. You want to minimize effort while ensuring that the logs are stored correctly. What should you do?",
            "options": {
                "A": "Store the logs in a Cloud Storage bucket with bucket lock turned on.",
                "B": "Store the logs in a Cloud Storage bucket with a 3-year retention period.",
                "C": "Store the logs in Cloud Logging as custom logs with a custom retention period.",
                "D": "Store the logs in a Cloud Storage bucket with a 1-year retention period. After 1 year, move the logs to another bucket with a 2-year retention period."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "The requirements are: retain logs for 3 years (compliance), easily searchable for 1 year (auditing), minimize effort. Cloud Logging provides native log ingestion, storage, and powerful search capabilities. You can configure custom log retention periods for log buckets, setting it to 3 years (or more) to meet compliance (C). Logs are easily searchable within Cloud Logging for the entire retention period, satisfying the audit requirement. Storing in GCS (A, B, D) makes searching much harder and requires extra steps or tools. Bucket Lock (A) is for immutability, not just retention.",
            "conditions": [
                "Store user logs for 1yr (searchable audit) and 3yrs (compliance), minimize effort."
            ],
            "caseStudyContext": null
        },
        {
            "id": 173,
            "topic": "IAM & Security",
            "question": "Your company has a new security initiative that requires all data stored in Google Cloud to be encrypted by customer-managed encryption keys. You plan to use Cloud Key Management Service (KMS) to configure access to the keys. You need to follow the \"separation of duties\" principle and Google-recommended best practices. What should you do? (Choose two.)",
            "options": {
                "A": "Provision Cloud KMS in its own project.",
                "B": "Do not assign an owner to the Cloud KMS project.",
                "C": "Provision Cloud KMS in the project where the keys are being used.",
                "D": "Grant the roles/cloudkms.admin role to the owner of the project where the keys from Cloud KMS are being used.",
                "E": "Grant an owner role for the Cloud KMS project to a different user than the owner of the project where the keys from Cloud KMS are being used."
            },
            "correctAnswer": [
                "A",
                "E"
            ],
            "explanation": "Separation of duties best practices for KMS recommend: 1. Hosting KMS resources (key rings, keys) in a separate, dedicated GCP project (A). This isolates key management from resource management. 2. Managing permissions carefully. Granting the owner role of the KMS project to a *different* individual or group than the owner(s) of the projects using the keys (E) ensures that those who manage resources cannot also manage the keys protecting them, and vice versa. Not assigning an owner (B) is possible but using an Org Admin might be better. C violates separation. D grants admin rights to the resource owner, also violating separation.",
            "conditions": [
                "Use KMS with CMEK, follow separation of duties best practices."
            ],
            "caseStudyContext": null
        },
        {
            "id": 174,
            "topic": "Migration",
            "question": "You need to migrate a standalone Java application running in an on-premises Linux virtual machine (VM) to Google Cloud in a cost-effective manner. You decide not to take the lift-and-shift approach, and instead you plan to modernize the application by converting it to a container. How should you accomplish this task?",
            "options": {
                "A": "Use Migrate for Anthos to migrate the VM to your Google Kubernetes Engine (GKE) cluster as a container.",
                "B": "Export the VM as a raw disk and import it as an image. Create a Compute Engine instance from the Imported image.",
                "C": "Use Migrate for Compute Engine to migrate the VM to a Compute Engine instance, and use Cloud Build to convert it to a container.",
                "D": "Use Jib to build a Docker image from your source code, and upload it to Artifact Registry. Deploy the application in a GKE cluster, and test the application."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "The goal is modernization by containerization, not lift-and-shift. Jib is a tool specifically designed to containerize Java applications directly from source code (Maven/Gradle projects) without needing a Dockerfile or Docker daemon, producing optimized images (D). This is often the most direct path for containerizing existing Java codebases. Migrate for Anthos (A) can containerize VMs but might be more complex. Options B and C involve lift-and-shift to GCE first, adding steps.",
            "conditions": [
                "Migrate on-prem Java app (Linux VM) to GCP container, modernize (no lift-and-shift), cost-effective."
            ],
            "caseStudyContext": null
        },
        {
            "id": 175,
            "topic": "Compute",
            "question": "Your organization has recently begun an initiative to replatform their legacy applications onto Google Kubernetes Engine. You need to decompose a monolithic application into microservices. Multiple instances have read and write access to a configuration file, which is stored on a shared file system. You want to minimize the effort required to manage this transition, and you want to avoid rewriting the application code. What should you do?",
            "options": {
                "A": "Create a new Cloud Storage bucket, and mount it via FUSE in the container.",
                "B": "Create a new persistent disk, and mount the volume as a shared PersistentVolume.",
                "C": "Create a new Filestore instance, and mount the volume as an NFS PersistentVolume.",
                "D": "Create a new ConfigMap and volumeMount to store the contents of the configuration file."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "The application requires multiple instances (Pods) to have simultaneous read and write access to the *same* configuration file, mimicking a shared file system. Kubernetes PersistentVolumes with ReadWriteMany (RWX) access mode are needed. Google Cloud Filestore provides managed NFS volumes that support RWX and can be mounted into GKE Pods via PersistentVolumes/PersistentVolumeClaims (C). GCE Persistent Disks (B) usually only support ReadWriteOnce. Cloud Storage FUSE (A) has limitations with concurrent writes and consistency. ConfigMaps (D) are read-only once mounted.",
            "conditions": [
                "Decompose monolith for GKE, multiple instances need read/write access to shared config file (currently on shared FS), minimize effort, avoid code rewrite."
            ],
            "caseStudyContext": null
        },
        {
            "id": 176,
            "topic": "Compute",
            "question": "Your team is developing unit tests for Cloud Function code. The code is stored in a Cloud Source Repositories repository. You are responsible for implementing the tests. Only a specific service account has the necessary permissions to deploy the code to Cloud Functions. You want to ensure that the code cannot be deployed without first passing the tests. How should you configure the unit testing process?",
            "options": {
                "A": "Configure Cloud Build to deploy the Cloud Function. If the code passes the tests, a deployment approval is sent to you.",
                "B": "Configure Cloud Build to deploy the Cloud Function, using the specific service account as the build agent. Run the unit tests after successful deployment.",
                "C": "Configure Cloud Build to run the unit tests. If the code passes the tests, the developer deploys the Cloud Function.",
                "D": "Configure Cloud Build to run the unit tests, using the specific service account as the build agent. If the code passes the tests, Cloud Build deploys the Cloud Function."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "The goal is an automated pipeline where deployment happens only after tests pass, using a specific service account for deployment. Cloud Build can orchestrate this. The pipeline should first run the unit tests. If the tests succeed, the next step should deploy the Cloud Function. Since a specific service account is required for deployment, Cloud Build needs to execute the deployment step using (or acting as) that service account (D). Running tests *after* deployment (B) defeats the purpose of gating. Manual deployment (C) or manual approval (A) doesn't meet the automation goal.",
            "conditions": [
                "Unit test Cloud Function code, deploy only if tests pass, deployment requires specific SA, automate."
            ],
            "caseStudyContext": null
        },
        {
            "id": 177,
            "topic": "Monitoring & Logging",
            "question": "Your team detected a spike of errors in an application running on Cloud Run in your production project. The application is configured to read messages from Pub/Sub topic A, process the messages, and write the messages to topic B. You want to conduct tests to identify the cause of the errors. You can use a set of mock messages for testing. What should you do?",
            "options": {
                "A": "Deploy the Pub/Sub and Cloud Run emulators on your local machine. Deploy the application locally, and change the logging level in the application to DEBUG or INFO. Write mock messages to topic A, and then analyze the logs.",
                "B": "Use the gcloud CLI to write mock messages to topic A. Change the logging level in the application to DEBUG or INFO, and then analyze the logs.",
                "C": "Deploy the Pub/Sub emulator on your local machine. Point the production application to your local Pub/Sub topics. Write mock messages to topic A, and then analyze the logs.",
                "D": "Use the Google Cloud console to write mock messages to topic A. Change the logging level in the application to DEBUG or INFO, and then analyze the logs."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "To safely diagnose production errors without impacting the live system, local testing with emulators is the best practice. Deploying the Pub/Sub emulator allows you to simulate topic A locally. Running the Cloud Run application container locally (e.g., via Docker) allows you to connect it to the local emulator. Increasing the logging level provides detailed diagnostics. Publishing mock messages to the local emulator topic A lets you observe the application's processing and identify the error's cause by analyzing the detailed local logs (A). Injecting mock messages into production (B, D) or pointing production to local emulators (C) is risky and inappropriate.",
            "conditions": [
                "Cloud Run app processing Pub/Sub messages (A->B) has errors in prod, need to test/debug with mock messages."
            ],
            "caseStudyContext": null
        },
        {
            "id": 178,
            "topic": "IAM & Security",
            "question": "You are developing a Java Web Server that needs to interact with Google Cloud services via the Google Cloud API on the user's behalf. Users should be able to authenticate to the Google Cloud API using their Google Cloud identities. Which workflow should you implement in your web application?",
            "options": {
                "A": "1. When a user arrives at your application, prompt them for their Google username and password.\n2. Store an SHA password hash in your application's database along with the user's username.\n3. The application authenticates to the Google Cloud API using HTTPs requests with the user's username and password hash in the Authorization request header.",
                "B": "1. When a user arrives at your application, prompt them for their Google username and password.\n2. Forward the user's username and password in an HTTPS request to the Google Cloud authorization server, and request an access token.\n3. The Google server validates the user's credentials and returns an access token to the application.\n4. The application uses the access token to call the Google Cloud API.",
                "C": "1. When a user arrives at your application, route them to a Google Cloud consent screen with a list of requested permissions that prompts the user to sign in with SSO to their Google Account.\n2. After the user signs in and provides consent, your application receives an authorization code from a Google server.\n3. The Google server returns the authorization code to the user, which is stored in the browser's cookies.\n4. The user authenticates to the Google Cloud API using the authorization code in the cookie.",
                "D": "1. When a user arrives at your application, route them to a Google Cloud consent screen with a list of requested permissions that prompts the user to sign in with SSO to their Google Account.\n2. After the user signs in and provides consent, your application receives an authorization code from a Google server.\n3. The application requests a Google Server to exchange the authorization code with an access token.\n4. The Google server responds with the access token that is used by the application to call the Google Cloud API."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "This describes the standard OAuth 2.0 Authorization Code Grant Flow, which is the recommended method for web server applications to obtain access tokens to call Google APIs on behalf of users. The application redirects the user to Google for authentication and consent (Step 1). Google redirects back to the application with an authorization code (Step 2). The application backend then securely exchanges this code with Google for an access token (and optionally a refresh token) (Step 3). The application uses the access token to make API calls on the user's behalf (Step 4). Options A and B involve handling user passwords directly, which is insecure and violates OAuth principles. Option C incorrectly describes how the authorization code is handled and used.",
            "conditions": [
                "Java web server needs to call Google Cloud APIs on behalf of users using their Google identities."
            ],
            "caseStudyContext": null
        },
        {
            "id": 179,
            "topic": "Compute",
            "question": "You recently developed a new application. You want to deploy the application on Cloud Run without a Dockerfile. Your organization requires that all container images are pushed to a centrally managed container repository. How should you build your container using Google Cloud services? (Choose two.)",
            "options": {
                "A": "Push your source code to Artifact Registry.",
                "B": "Submit a Cloud Build job to push the image.",
                "C": "Use the pack build command with pack CLI.",
                "D": "Include the --source flag with the gcloud run deploy CLI command.",
                "E": "Include the --platform=kubernetes flag with the gcloud run deploy CLI command."
            },
            "correctAnswer": [
                "C",
                "D"
            ],
            "explanation": "To build a container image without a Dockerfile for Cloud Run, Google Cloud Buildpacks can be used. This can be done implicitly with `gcloud run deploy --source .` (D), which uses Cloud Build and Buildpacks behind the scenes, builds the image, pushes it to Artifact Registry (creating a repo if needed), and deploys. Alternatively, you can use the `pack` CLI (C) (part of the Buildpacks project) locally or within a CI/CD system like Cloud Build to explicitly build the image using buildpacks, then manually push it to the central Artifact Registry, and finally deploy to Cloud Run. Both methods achieve building without a Dockerfile. Pushing source to AR (A) is incorrect. Submitting a standard Cloud Build job (B) typically assumes a Dockerfile or explicit build steps. --platform=kubernetes (E) is for Cloud Run for Anthos.",
            "conditions": [
                "Deploy app to Cloud Run without Dockerfile, use Google Cloud build services, push image to central repo."
            ],
            "caseStudyContext": null
        },
        {
            "id": 180,
            "topic": "Databases",
            "question": "You work for an organization that manages an online ecommerce website. Your company plans to expand across the world; however, the estore currently serves one specific region. You need to select a SQL database and configure a schema that will scale as your organization grows. You want to create a table that stores all customer transactions and ensure that the customer (CustomerId) and the transaction (TransactionId) are unique. What should you do?",
            "options": {
                "A": "Create a Cloud SQL table that has TransactionId and CustomerId configured as primary keys. Use an incremental number for the TransactionId.",
                "B": "Create a Cloud SQL table that has TransactionId and CustomerId configured as primary keys. Use a random string (UUID) for the Transactionid.",
                "C": "Create a Cloud Spanner table that has TransactionId and CustomerId configured as primary keys. Use a random string (UUID) for the TransactionId.",
                "D": "Create a Cloud Spanner table that has TransactionId and CustomerId configured as primary keys. Use an incremental number for the TransactionId."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "The requirement for global expansion and scaling points towards Cloud Spanner, a globally distributed, horizontally scalable SQL database. Cloud SQL is regional. To ensure uniqueness of (CustomerId, TransactionId) and avoid potential hotspotting issues with globally distributed writes, using a randomly generated UUID for TransactionId as part of the composite primary key is the best practice for Spanner (C). Incremental numbers (A, D) can lead to write hotspots in distributed systems.",
            "conditions": [
                "Ecommerce app expanding globally, need scalable SQL DB for transactions, ensure unique (CustomerId, TransactionId)."
            ],
            "caseStudyContext": null
        },
        {
            "id": 181,
            "topic": "IAM & Security",
            "question": "You are developing an application that will store and access sensitive unstructured data objects in a Cloud Storage bucket. To comply with regulatory requirements, you need to ensure that all data objects are available for at least 7 years after their initial creation. Objects created more than 3 years ago are accessed very infrequently (less than once a year). You need to configure object storage while ensuring that storage cost is optimized. What should you do? (Choose two.)",
            "options": {
                "A": "Set a retention policy on the bucket with a period of 7 years.",
                "B": "Use IAM Conditions to provide access to objects 7 years after the object creation date.",
                "C": "Enable Object Versioning to prevent objects from being accidentally deleted for 7 years after object creation.",
                "D": "Create an object lifecycle policy on the bucket that moves objects from Standard Storage to Archive Storage after 3 years.",
                "E": "Implement a Cloud Function that checks the age of each object in the bucket and moves the objects older than 3 years to a second bucket with the Archive Storage class. Use Cloud Scheduler to trigger the Cloud Function on a daily schedule."
            },
            "correctAnswer": [
                "A",
                "D"
            ],
            "explanation": "Requirement 1 (Retain >= 7 years): Bucket Retention Policy (A) is the direct way to enforce this, preventing deletion/modification for the specified duration. Requirement 2 (Cost optimization): Data accessed less than once a year after 3 years fits the Archive Storage class. An Object Lifecycle Management policy (D) can automatically transition objects from Standard to Archive storage based on age (e.g., after 3 years), optimizing costs. Object Versioning (C) adds cost and doesn't enforce retention time. IAM (B) controls access, not retention. Cloud Function (E) is overly complex for standard lifecycle management.",
            "conditions": [
                "Store sensitive GCS objects, retain >= 7 years, optimize cost for objects > 3 years accessed < 1/year."
            ],
            "caseStudyContext": null
        },
        {
            "id": 182,
            "topic": "Compute",
            "question": "You are developing an application using different microservices that must remain internal to the cluster. You want the ability to configure each microservice with a specific number of replicas. You also want the ability to address a specific microservice from any other microservice in a uniform way, regardless of the number of replicas the microservice scales to. You plan to implement this solution on Google Kubernetes Engine. What should you do?",
            "options": {
                "A": "Deploy each microservice as a Deployment. Expose the Deployment in the cluster using a Service, and use the Service DNS name to address it from other microservices within the cluster.",
                "B": "Deploy each microservice as a Deployment. Expose the Deployment in the cluster using an Ingress, and use the Ingress IP address to address the Deployment from other microservices within the cluster.",
                "C": "Deploy each microservice as a Pod. Expose the Pod in the cluster using a Service, and use the Service DNS name to address the microservice from other microservices within the cluster.",
                "D": "Deploy each microservice as a Pod. Expose the Pod in the cluster using an Ingress, and use the Ingress IP address to address the Pod from other microservices within the cluster."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "Kubernetes Deployments manage ReplicaSets to ensure a desired number of Pod replicas for a microservice. Kubernetes Services provide a stable abstraction (IP address and DNS name) to access the pods managed by a Deployment. For internal-only communication, a Service of type ClusterIP is used. Other microservices connect using the Service's DNS name, and the Service load balances requests across the available Pod replicas (A). Ingress (B, D) is for external access. Using raw Pods (C, D) doesn't provide replica management or scaling.",
            "conditions": [
                "Internal GKE microservices, configure replicas, uniform addressing independent of scaling."
            ],
            "caseStudyContext": null
        },
        {
            "id": 183,
            "topic": "Monitoring & Logging",
            "question": "You are developing an application that will handle requests from end users. You need to secure a Cloud Function called by the application to allow authorized end users to authenticate to the function via the application while restricting access to unauthorized users. You will integrate Google Sign-In as part of the solution and want to follow Google-recommended best practices. What should you do?",
            "options": {
                "A": "Deploy from a source code repository and grant users the roles/cloudfunctions.viewer role.",
                "B": "Deploy from a source code repository and grant users the roles/cloudfunctions.invoker role",
                "C": "Deploy from your local machine using gcloud and grant users the roles/cloudfunctions.admin role",
                "D": "Deploy from your local machine using gcloud and grant users the roles/cloudfunctions.developer role"
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "To secure an HTTP Cloud Function so only authenticated users can call it, you should configure it to require authentication. When using Google Sign-In, the client application obtains an ID token after the user signs in. This ID token should be included in the `Authorization: Bearer <ID_TOKEN>` header when calling the Cloud Function. The function (or an API Gateway in front of it) validates the token. To authorize the *user* to call the function, their identity needs the `roles/cloudfunctions.invoker` permission on the function. Granting this role directly to users or, preferably, to a group they belong to, enforces authorization (B). Viewer, Admin, Developer roles are incorrect for invocation permission.",
            "conditions": [
                "Secure Cloud Function called by app, authenticate end users via Google Sign-In, restrict access."
            ],
            "caseStudyContext": null
        },
        {
            "id": 184,
            "topic": "IAM & Security",
            "question": "You are running a web application on Google Kubernetes Engine that you inherited. You want to determine whether the application is using libraries with known vulnerabilities or is vulnerable to XSS attacks. Which service should you use?",
            "options": {
                "A": "Google Cloud Armor",
                "B": "Debugger",
                "C": "Web Security Scanner",
                "D": "Error Reporting"
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "Web Security Scanner (part of Security Command Center) is designed to scan running web applications (hosted on GKE, App Engine, GCE) for common vulnerabilities like Cross-Site Scripting (XSS) and usage of outdated/vulnerable libraries (C). Cloud Armor (A) is for WAF/DDoS. Debugger (B) is for inspecting code execution. Error Reporting (D) aggregates application errors.",
            "conditions": [
                "Inherited GKE web app, check for vulnerable libraries and XSS."
            ],
            "caseStudyContext": null
        },
        {
            "id": 185,
            "topic": "Storage",
            "question": "You are building a highly available and globally accessible application that will serve static content to users. You need to configure the storage and serving components. You want to minimize management overhead and latency while maximizing reliability for users. What should you do?",
            "options": {
                "A": "1. Create a managed instance group. Replicate the static content across the virtual machines (VMs)\n2. Create an external HTTP(S) load balancer.\n3. Enable Cloud CDN, and send traffic to the managed instance group.",
                "B": "1. Create an unmanaged instance group. Replicate the static content across the VMs.\n2. Create an external HTTP(S) load balancer\n3. Enable Cloud CDN, and send traffic to the unmanaged instance group.",
                "C": "1. Create a Standard storage class, regional Cloud Storage bucket. Put the static content in the bucket\n2. Reserve an external IP address, and create an external HTTP(S) load balancer\n3. Enable Cloud CDN, and send traffic to your backend bucket",
                "D": "1. Create a Standard storage class, multi-regional Cloud Storage bucket. Put the static content in the bucket.\n2. Reserve an external IP address, and create an external HTTP(S) load balancer.\n3. Enable Cloud CDN, and send traffic to your backend bucket."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "For globally serving static content with high availability, low latency, and minimal management: 1. Store content in a multi-regional Cloud Storage bucket (D) for high durability and availability across regions. 2. Use an external HTTP(S) Load Balancer with Cloud CDN enabled. Configure the load balancer to use the GCS bucket as a backend. Cloud CDN caches content at edge locations globally, providing low latency. This setup is fully managed and highly reliable. Using GCE instances (A, B) requires managing VMs and replicating content, increasing overhead. A regional bucket (C) doesn't offer the same global availability as multi-regional.",
            "conditions": [
                "Serve static content globally, highly available, low latency, minimize management overhead."
            ],
            "caseStudyContext": null
        },
        {
            "id": 186,
            "topic": "Case Study Analysis",
            "question": "Case study - HipLocal: HipLocal wants to reduce the latency of their services for users in global locations. They have created read replicas of their database in locations where their users reside and configured their service to read traffic using those replicas. How should they further reduce latency for all database interactions with the least amount of effort?",
            "options": {
                "A": "Migrate the database to Bigtable and use it to serve all global user traffic.",
                "B": "Migrate the database to Cloud Spanner and use it to serve all global user traffic.",
                "C": "Migrate the database to Firestore in Datastore mode and use it to serve all global user traffic.",
                "D": "Migrate the services to Google Kubernetes Engine and use a load balancer service to better scale the application."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "HipLocal uses MySQL and needs global scale with low latency and consistency (implied by 'consistent experience' requirement). Read replicas help read latency but not write latency or global consistency. Migrating to Cloud Spanner (B) provides a globally distributed, strongly consistent database with low latency reads (via follower replicas) and manageable write latency globally. Bigtable (A) and Firestore (C) are NoSQL and represent a larger migration effort from MySQL. Migrating services to GKE (D) doesn't solve the database latency/distribution problem.",
            "conditions": [
                "HipLocal case study context, reduce global DB latency (already using read replicas), least effort."
            ],
            "caseStudyContext": "<strong>Company overview</strong>\nHipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.\n\n<strong>Executive statement</strong>\nWe are the number one local community app; it’s time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.\n\n<strong><strong>Solution Concept </strong><br></strong>\nHipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.\n\n<strong><strong>Existing Technical Environment </strong><br></strong>\nHipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:\n<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>\n\n<strong><strong>Business Requirements </strong><br></strong>\nHipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:\n<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>\n\n<strong>Technical requirements</strong>\n<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>"
        },
        {
            "id": 187,
            "topic": "Case Study Analysis",
            "question": "Case study - HipLocal: Which Google Cloud product addresses HipLocal’s <strong>Business Requirements </strong><br> for service level indicators and objectives?",
            "options": {
                "A": "Cloud Profiler",
                "B": "Cloud Monitoring",
                "C": "Cloud Trace",
                "D": "Cloud Logging"
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "The business requirement is to 'Define service level indicators (SLIs) and service level objectives (SLOs).' Cloud Monitoring (B) provides the functionality to define SLIs based on metrics (e.g., availability, latency), set SLOs (target levels for SLIs), track error budgets, and create alerts based on SLO compliance. Profiler (A), Trace (C), and Logging (D) are for performance analysis, latency tracing, and log management, respectively, not directly for defining and tracking SLOs.",
            "conditions": [
                "HipLocal case study context, need product to define/track SLIs/SLOs."
            ],
            "caseStudyContext": "<strong>Company overview</strong>\nHipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.\n\n<strong>Executive statement</strong>\nWe are the number one local community app; it’s time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.\n\n<strong><strong>Solution Concept </strong><br></strong>\nHipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.\n\n<strong><strong>Existing Technical Environment </strong><br></strong>\nHipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:\n<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>\n\n<strong><strong>Business Requirements </strong><br></strong>\nHipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:\n<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>\n\n<strong>Technical requirements</strong>\n<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>"
        },
        {
            "id": 188,
            "topic": "Case Study Analysis",
            "question": "Case study - HipLocal: A recent security audit discovers that HipLocal’s database credentials for their Compute Engine-hosted MySQL databases are stored in plain text on persistent disks. HipLocal needs to reduce the risk of these credentials being stolen. What should they do?",
            "options": {
                "A": "Create a service account and download its key. Use the key to authenticate to Cloud Key Management Service (KMS) to obtain the database credentials.",
                "B": "Create a service account and download its key. Use the key to authenticate to Cloud Key Management Service (KMS) to obtain a key used to decrypt the database credentials.",
                "C": "Create a service account and grant it the roles/iam.serviceAccountUser role. Impersonate as this account and authenticate using the Cloud SQL Proxy.",
                "D": "Grant the roles/secretmanager.secretAccessor role to the Compute Engine service account. Store and access the database credentials with the Secret Manager API."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "Storing credentials in plain text is insecure. Secret Manager is Google Cloud's managed service for securely storing and accessing secrets like database credentials. The application running on GCE should use its attached service account identity to authenticate to the Secret Manager API. Grant the GCE service account the `roles/secretmanager.secretAccessor` role to allow it to retrieve the stored database credentials at runtime (D). Using KMS (A, B) is for key management, not directly storing arbitrary secrets like passwords. Option C relates to impersonation, not secret retrieval, and assumes Cloud SQL Proxy is used (the scenario mentions MySQL on GCE).",
            "conditions": [
                "HipLocal case study context, MySQL DB creds stored insecurely on GCE PD, need secure solution."
            ],
            "caseStudyContext": "<strong>Company overview</strong>\nHipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.\n\n<strong>Executive statement</strong>\nWe are the number one local community app; it’s time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.\n\n<strong><strong>Solution Concept </strong><br></strong>\nHipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.\n\n<strong><strong>Existing Technical Environment </strong><br></strong>\nHipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:\n<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>\n\n<strong><strong>Business Requirements </strong><br></strong>\nHipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:\n<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>\n\n<strong>Technical requirements</strong>\n<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>"
        },
        {
            "id": 189,
            "topic": "Case Study Analysis",
            "question": "Case study - HipLocal: HipLocal is expanding into new locations. They must capture additional data each time the application is launched in a new European country. This is causing delays in the development process due to constant schema changes and a lack of environments for conducting testing on the application changes. How should they resolve the issue while meeting the <strong>Business Requirements </strong><br>?",
            "options": {
                "A": "Create new Cloud SQL instances in Europe and North America for testing and deployment. Provide developers with local MySQL instances to conduct testing on the application changes.",
                "B": "Migrate data to Bigtable. Instruct the development teams to use the Cloud SDK to emulate a local Bigtable development environment.",
                "C": "Move from Cloud SQL to MySQL hosted on Compute Engine. Replicate hosts across regions in the Americas and Europe. Provide developers with local MySQL instances to conduct testing on the application changes.",
                "D": "Migrate data to Firestore in Native mode and set up instances in Europe and North America. Instruct the development teams to use the Cloud SDK to emulate a local Firestore in Native mode development environment."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "The core issues are development delays due to 'constant schema changes' and lack of testing environments. This points towards needing a database with a flexible schema and good local development/testing support. Firestore (D) is a NoSQL document database with a flexible schema, accommodating evolving data requirements easily. It supports regional instances (Europe/NA) for data locality (GDPR requirement). Crucially, Firestore has excellent local emulator support via the SDK, allowing developers to test locally without needing dedicated cloud instances, addressing the testing environment bottleneck. Cloud SQL (A, C) has a rigid schema. Bigtable (B) is less suited for general application data and local emulation might be more complex.",
            "conditions": [
                "HipLocal case study context, need to capture varying data per country (schema changes), dev delays due to lack of test environments."
            ],
            "caseStudyContext": "<strong>Company overview</strong>\nHipLocal is a community application designed to facilitate communication between people in close proximity. It is used for event planning and organizing sporting events, and for businesses to connect with their local communities. HipLocal launched recently in a few neighborhoods in Dallas and is rapidly growing into a global phenomenon. Its unique style of hyper-local community communication and business outreach is in demand around the world.\n\n<strong>Executive statement</strong>\nWe are the number one local community app; it’s time to take our local community services global. Our venture capital investors want to see rapid growth and the same great experience for new local and virtual communities that come online, whether their members are 10 or 10,000 miles away from each other.\n\n<strong><strong>Solution Concept </strong><br></strong>\nHipLocal wants to expand their existing service with updated functionality in new locations to better serve their global customers. They want to hire and train a new team to support these locations in their time zones. They will need to ensure that the application scales smoothly and provides clear uptime data, and that they analyze and respond to any issues that occur.\n\n<strong><strong>Existing Technical Environment </strong><br></strong>\nHipLocal's environment is a mixture of on-premises hardware and infrastructure running in Google Cloud. The HipLocal team understands their application well, but has limited experience in globally scaled applications. Their <strong>Existing Technical Environment </strong><br> is as follows:\n<li>Existing APIs run on Compute Engine virtual machine instances hosted in Google Cloud.</li><li>State is stored in a single instance MySQL database in Google Cloud.</li><li>Release cycles include development freezes to allow for QA testing.</li><li>The application has no consistent logging.</li><li>Applications are manually deployed by infrastructure engineers during periods of slow traffic on weekday evenings.</li><li>There are basic indicators of uptime; alerts are frequently fired when the APIs are unresponsive.</li>\n\n<strong><strong>Business Requirements </strong><br></strong>\nHipLocal's investors want to expand their footprint and support the increase in demand they are experiencing. Their requirements are:\n<li>Expand availability of the application to new locations.</li><li>Support 10x as many concurrent users.</li><li>Ensure a consistent experience for users when they travel to different locations.</li><li>Obtain user activity metrics to better understand how to monetize their product.</li><li>Ensure compliance with regulations in the new regions (for example, GDPR).</li><li>Reduce infrastructure management time and cost.</li><li>Adopt the Google-recommended practices for cloud computing:<ul><li>Develop standardized workflows and processes around application lifecycle management.</li><li>Define service level indicators (SLIs) and service level objectives (SLOs).</li></ul></li>\n\n<strong>Technical requirements</strong>\n<li>Provide secure communications between the on-premises data center and cloud-hosted applications and infrastructure.</li><li>The application must provide usage metrics and monitoring.</li><li>APIs require authentication and authorization.</li><li>Implement faster and more accurate validation of new features.</li><li>Logging and performance metrics must provide actionable information to be able to provide debugging information and alerts.</li><li>Must scale to meet user demand.</li>"
        },
        {
            "id": 190,
            "topic": "Databases",
            "question": "You are writing from a Go application to a Cloud Spanner database. You want to optimize your application’s performance using Google-recommended best practices. What should you do?",
            "options": {
                "A": "Write to Cloud Spanner using Cloud Client Libraries.",
                "B": "Write to Cloud Spanner using Google API Client Libraries",
                "C": "Write to Cloud Spanner using a custom gRPC client library.",
                "D": "Write to Cloud Spanner using a third-party HTTP client library."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "Google Cloud Client Libraries (A) are the idiomatic and recommended way to interact with Google Cloud services, including Spanner, from applications. They provide high-level abstractions, handle authentication (using ADC), retries, and other complexities, simplifying development and often incorporating performance best practices (like session pooling for Spanner). Google API Client Libraries (B) are lower-level, auto-generated libraries. Building custom gRPC (C) or HTTP (D) clients is unnecessary and complex when official client libraries exist.",
            "conditions": [
                "Write to Spanner from Go app, optimize performance using best practices."
            ],
            "caseStudyContext": null
        },
        {
            "id": 191,
            "topic": "IAM & Security",
            "question": "You have an application deployed in Google Kubernetes Engine (GKE). You need to update the application to make authorized requests to Google Cloud managed services. You want this to be a one-time setup, and you need to follow security best practices of auto-rotating your security keys and storing them in an encrypted store. You already created a service account with appropriate access to the Google Cloud service. What should you do next?",
            "options": {
                "A": "Assign the Google Cloud service account to your GKE Pod using Workload Identity.",
                "B": "Export the Google Cloud service account, and share it with the Pod as a Kubernetes Secret.",
                "C": "Export the Google Cloud service account, and embed it in the source code of the application.",
                "D": "Export the Google Cloud service account, and upload it to HashiCorp Vault to generate a dynamic service account for your application."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "The requirements emphasize security best practices: auto-rotating keys and encrypted storage. Manually exporting and managing service account keys (B, C, D) violates these principles, as keys need manual rotation and secure storage becomes the application's responsibility. Workload Identity (A) is the recommended solution. It binds a GKE service account (used by the Pod) to a Google service account (GSA). The Pod uses its automatically mounted, auto-rotated KSA token to get short-lived GSA credentials from the metadata server, eliminating the need to handle GSA keys directly.",
            "conditions": [
                "GKE app needs to access GCP services, one-time setup, follow best practices (auto-rotated keys, encrypted store), GSA already created."
            ],
            "caseStudyContext": null
        },
        {
            "id": 192,
            "topic": "IAM & Security",
            "question": "You are planning to deploy hundreds of microservices in your Google Kubernetes Engine (GKE) cluster. How should you secure communication between the microservices on GKE using a managed service?",
            "options": {
                "A": "Use global HTTP(S) Load Balancing with managed SSL certificates to protect your services",
                "B": "Deploy open source Istio in your GKE cluster, and enable mTLS in your Service Mesh",
                "C": "Install cert-manager on GKE to automatically renew the SSL certificates.",
                "D": "Install Anthos Service Mesh, and enable mTLS in your Service Mesh."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "To secure inter-service communication within a GKE cluster using a *managed* service, Anthos Service Mesh (ASM) is the recommended Google Cloud offering. ASM is a managed Istio distribution that provides features like automatic mutual TLS (mTLS) encryption and authentication between services in the mesh (D). Deploying open source Istio (B) requires self-management. Global Load Balancing (A) secures traffic *into* the cluster, not between microservices. Cert-manager (C) manages TLS certificates, often for Ingress, but doesn't inherently provide mTLS for inter-service traffic like a service mesh does.",
            "conditions": [
                "Secure communication between microservices within GKE cluster, use managed service."
            ],
            "caseStudyContext": null
        },
        {
            "id": 193,
            "topic": "Storage",
            "question": "You are developing an application that will store and access sensitive unstructured data objects in a Cloud Storage bucket. To comply with regulatory requirements, you need to ensure that all data objects are available for at least 7 years after their initial creation. Objects created more than 3 years ago are accessed very infrequently (less than once a year). You need to configure object storage while ensuring that storage cost is optimized. What should you do? (Choose two.)",
            "options": {
                "A": "Set a retention policy on the bucket with a period of 7 years.",
                "B": "Use IAM Conditions to provide access to objects 7 years after the object creation date.",
                "C": "Enable Object Versioning to prevent objects from being accidentally deleted for 7 years after object creation.",
                "D": "Create an object lifecycle policy on the bucket that moves objects from Standard Storage to Archive Storage after 3 years.",
                "E": "Implement a Cloud Function that checks the age of each object in the bucket and moves the objects older than 3 years to a second bucket with the Archive Storage class. Use Cloud Scheduler to trigger the Cloud Function on a daily schedule."
            },
            "correctAnswer": [
                "A",
                "D"
            ],
            "explanation": "Requirement 1 (Retain >= 7 years): Bucket Retention Policy (A) is the direct way to enforce this, preventing deletion/modification for the specified duration. Requirement 2 (Cost optimization): Data accessed less than once a year after 3 years fits the Archive Storage class. An Object Lifecycle Management policy (D) can automatically transition objects from Standard to Archive storage based on age (e.g., after 3 years), optimizing costs. Object Versioning (C) adds cost and doesn't enforce retention time. IAM (B) controls access, not retention. Cloud Function (E) is overly complex for standard lifecycle management.",
            "conditions": [
                "Store sensitive GCS objects, retain >= 7 years, optimize cost for objects > 3 years accessed < 1/year."
            ],
            "caseStudyContext": null
        },
        {
            "id": 194,
            "topic": "Compute",
            "question": "You are developing an application using different microservices that must remain internal to the cluster. You want the ability to configure each microservice with a specific number of replicas. You also want the ability to address a specific microservice from any other microservice in a uniform way, regardless of the number of replicas the microservice scales to. You plan to implement this solution on Google Kubernetes Engine. What should you do?",
            "options": {
                "A": "Deploy each microservice as a Deployment. Expose the Deployment in the cluster using a Service, and use the Service DNS name to address it from other microservices within the cluster.",
                "B": "Deploy each microservice as a Deployment. Expose the Deployment in the cluster using an Ingress, and use the Ingress IP address to address the Deployment from other microservices within the cluster.",
                "C": "Deploy each microservice as a Pod. Expose the Pod in the cluster using a Service, and use the Service DNS name to address the microservice from other microservices within the cluster.",
                "D": "Deploy each microservice as a Pod. Expose the Pod in the cluster using an Ingress, and use the Ingress IP address to address the Pod from other microservices within the cluster."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "Kubernetes Deployments manage ReplicaSets to ensure a desired number of Pod replicas for a microservice. Kubernetes Services provide a stable abstraction (IP address and DNS name) to access the pods managed by a Deployment. For internal-only communication, a Service of type ClusterIP is used. Other microservices connect using the Service's DNS name, and the Service load balances requests across the available Pod replicas (A). Ingress (B, D) is for external access. Using raw Pods (C, D) doesn't provide replica management or scaling.",
            "conditions": [
                "Internal GKE microservices, configure replicas, uniform addressing independent of scaling."
            ],
            "caseStudyContext": null
        },
        {
            "id": 195,
            "topic": "Monitoring & Logging",
            "question": "You are building an application that uses a distributed microservices architecture. You want to measure the performance and system resource utilization in one of the microservices written in Java. What should you do?",
            "options": {
                "A": "Instrument the service with Cloud Profiler to measure CPU utilization and method-level execution times in the service.",
                "B": "Instrument the service with Debugger to investigate service errors.",
                "C": "Instrument the service with Cloud Trace to measure request latency.",
                "D": "Instrument the service with OpenCensus to measure service latency, and write custom metrics to Cloud Monitoring."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "Cloud Profiler is specifically designed to continuously analyze CPU usage and memory allocation of production applications with low overhead, attributing it back to the source code (e.g., Java methods). This directly addresses the need to measure performance (method execution times) and resource utilization (CPU, memory) (A). Debugger (B) is for inspecting state. Cloud Trace (C) measures end-to-end latency across services but not detailed internal resource usage per function. OpenCensus (D) is a library for collecting metrics/traces, but Profiler provides the specific CPU/memory analysis needed.",
            "conditions": [
                "Measure performance (method times) and resource utilization (CPU/memory) of a Java microservice."
            ],
            "caseStudyContext": null
        },
        {
            "id": 196,
            "topic": "Networking",
            "question": "Your team is responsible for maintaining an application that aggregates news articles from many different sources. Your monitoring dashboard contains publicly accessible real-time reports and runs on a Compute Engine instance as a web application. External stakeholders and analysts need to access these reports via a secure channel without authentication. How should you configure this secure channel?",
            "options": {
                "A": "Add a public IP address to the instance. Use the service account key of the instance to encrypt the traffic.",
                "B": "Use Cloud Scheduler to trigger Cloud Build every hour to create an export from the reports. Store the reports in a public Cloud Storage bucket.",
                "C": "Add an HTTP(S) load balancer in front of the monitoring dashboard. Configure Identity-Aware Proxy to secure the communication channel.",
                "D": "Add an HTTP(S) load balancer in front of the monitoring dashboard. Set up a Google-managed SSL certificate on the load balancer for traffic encryption."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "The requirements are: public access, secure channel (HTTPS), no authentication needed. Placing an external HTTP(S) Load Balancer in front of the GCE instance allows terminating SSL/TLS encryption at the load balancer. Using a Google-managed SSL certificate simplifies certificate provisioning and renewal (D). This provides a secure HTTPS channel without requiring application-level changes or user authentication. Service account keys (A) aren't used for traffic encryption. Batch exports (B) aren't real-time. IAP (C) enforces authentication, which is explicitly not required.",
            "conditions": [
                "Publicly accessible GCE web dashboard, secure channel (HTTPS) needed, no user authentication required."
            ],
            "caseStudyContext": null
        },
        {
            "id": 197,
            "topic": "Compute",
            "question": "You are planning to add unit tests to your application. You need to be able to assert that published Pub/Sub messages are processed by your subscriber in order. You want the unit tests to be cost-effective and reliable. What should you do?",
            "options": {
                "A": "Implement a mocking framework.",
                "B": "Create a topic and subscription for each tester.",
                "C": "Add a filter by tester to the subscription.",
                "D": "Use the Pub/Sub emulator."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "To reliably test Pub/Sub interactions locally, including message ordering, without incurring costs or depending on the live service, the Pub/Sub emulator (D) is the best practice. It runs locally, mimicking the Pub/Sub API. Mocking frameworks (A) can test the application's interaction logic but not the actual Pub/Sub behavior like ordering guarantees. Creating real topics/subscriptions per tester (B) is costly and less reliable for unit tests. Filters (C) don't apply to this testing scenario.",
            "conditions": [
                "Unit test Pub/Sub subscriber, assert message ordering, cost-effective, reliable."
            ],
            "caseStudyContext": null
        },
        {
            "id": 198,
            "topic": "Compute",
            "question": "You have an application deployed in Google Kubernetes Engine (GKE) that reads and processes Pub/Sub messages. Each Pod handles a fixed number of messages per minute. The rate at which messages are published to the Pub/Sub topic varies considerably throughout the day and week, including occasional large batches of messages published at a single moment.\n\nYou want to scale your GKE Deployment to be able to process messages in a timely manner. What GKE feature should you use to automatically adapt your workload?",
            "options": {
                "A": "Vertical Pod Autoscaler in Auto mode",
                "B": "Vertical Pod Autoscaler in Recommendation mode",
                "C": "Horizontal Pod Autoscaler based on an external metric",
                "D": "Horizontal Pod Autoscaler based on resources utilization"
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "The workload needs to scale based on the number of messages waiting in Pub/Sub, not directly on the Pods' CPU/memory usage (which might be low if there are no messages). This backlog size is an external metric relative to the GKE cluster. The Horizontal Pod Autoscaler (HPA) can be configured to scale based on external metrics, such as the number of unacknowledged messages in a Pub/Sub subscription (C). Scaling based on resource utilization (D) wouldn't react correctly to the backlog size. VPA (A, B) adjusts pod resources, not replica count.",
            "conditions": [
                "GKE app processes Pub/Sub messages, variable message rate, scale Deployment based on backlog."
            ],
            "caseStudyContext": null
        },
        {
            "id": 199,
            "topic": "Compute",
            "question": "You are using Cloud Run to host a web application. You need to securely obtain the application project ID and region where the application is running and display this information to users. You want to use the most performant approach. What should you do?",
            "options": {
                "A": "Use HTTP requests to query the available metadata server at the http://metadata.google.internal/ endpoint with the Metadata-Flavor: Google header.",
                "B": "In the Google Cloud console, navigate to the Project Dashboard and gather configuration details. Navigate to the Cloud Run “Variables & Secrets” tab, and add the desired environment variables in Key:Value format.",
                "C": "In the Google Cloud console, navigate to the Project Dashboard and gather configuration details. Write the application configuration information to Cloud Run's in-memory container filesystem.",
                "D": "Make an API call to the Cloud Asset Inventory API from the application and format the request to include instance metadata."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "Cloud Run provides a metadata server accessible from within the container instance at `http://metadata.google.internal/`. Querying this server via HTTP requests (with the `Metadata-Flavor: Google` header) is the standard, secure, and most performant way to retrieve instance/project metadata like project ID and region directly from within the running application (A). Manually configuring environment variables (B) or writing to the filesystem (C) requires external steps and isn't dynamic. Calling external APIs like Cloud Asset Inventory (D) adds unnecessary latency and complexity.",
            "conditions": [
                "Cloud Run app needs to get its own Project ID and Region, secure and performant approach."
            ],
            "caseStudyContext": null
        },
        {
            "id": 200,
            "topic": "IAM & Security",
            "question": "You need to deploy resources from your laptop to Google Cloud using Terraform. Resources in your Google Cloud environment must be created using a service account. Your Cloud Identity has the roles/iam.serviceAccountTokenCreator Identity and Access Management (IAM) role and the necessary permissions to deploy the resources using Terraform. You want to set up your development environment to deploy the desired resources following Google-recommended best practices. What should you do?",
            "options": {
                "A": "1. Download the service account’s key file in JSON format, and store it locally on your laptop.\n2. Set the GOOGLE_APPLICATION_CREDENTIALS environment variable to the path of your downloaded key file.",
                "B": "1. Run the following command from a command line: gcloud config set auth/impersonate_service_account service-account-name@project.iam.gserviceacccount.com.\n2. Set the GOOGLE_OAUTH_ACCESS_TOKEN environment variable to the value that is returned by the gcloud auth print-access-token command.",
                "C": "1. Run the following command from a command line: gcloud auth application-default login.\n2. In the browser window that opens, authenticate using your personal credentials.",
                "D": "1. Store the service account's key file in JSON format in Hashicorp Vault.\n2. Integrate Terraform with Vault to retrieve the key file dynamically, and authenticate to Vault using a short-lived access token."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "The best practice is to avoid downloading and storing service account keys locally (eliminates A, D). Since your user identity has permission to impersonate the target service account (`roles/iam.serviceAccountTokenCreator`), you can use impersonation. The `gcloud config set auth/impersonate_service_account` command configures gcloud itself for impersonation, but Terraform needs credentials. A common pattern is to use `gcloud auth print-access-token --impersonate-service-account=<SA_EMAIL>` to get a short-lived access token for the service account and set this token in the `GOOGLE_OAUTH_ACCESS_TOKEN` environment variable for Terraform to use (closest match is B, although step 1 isn't directly needed for Terraform). Option C uses your user credentials, not the required service account.",
            "conditions": [
                "Deploy GCP resources via Terraform from laptop, must use specific SA, user has impersonation rights, follow best practices."
            ],
            "caseStudyContext": null
        },
        {
            "id": 201,
            "topic": "Monitoring & Logging",
            "question": "Your company uses Cloud Logging to manage large volumes of log data. You need to build a real-time log analysis architecture that pushes logs to a third-party application for processing. What should you do?",
            "options": {
                "A": "Create a Cloud Logging log export to Pub/Sub.",
                "B": "Create a Cloud Logging log export to BigQuery.",
                "C": "Create a Cloud Logging log export to Cloud Storage.",
                "D": "Create a Cloud Function to read Cloud Logging log entries and send them to the third-party application."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "To push logs in real-time from Cloud Logging to an external system (like a third-party application), the standard and recommended method is to create a log sink (export) configured with Pub/Sub as the destination (A). Cloud Logging forwards matching log entries to the specified Pub/Sub topic almost instantly. The third-party application can then subscribe to this topic to receive the logs. Exporting to BigQuery (B) or Cloud Storage (C) is for batch analysis or archival, not real-time pushing. A Cloud Function (D) polling logs adds latency and complexity compared to the direct Pub/Sub sink.",
            "conditions": [
                "Real-time push of Cloud Logging logs to a third-party application."
            ],
            "caseStudyContext": null
        },
        {
            "id": 202,
            "topic": "Storage",
            "question": "You are developing a new public-facing application that needs to retrieve specific properties in the metadata of users’ objects in their respective Cloud Storage buckets. Due to privacy and data residency requirements, you must retrieve only the metadata and not the object data. You want to maximize the performance of the retrieval process. How should you retrieve the metadata?",
            "options": {
                "A": "Use the patch method.",
                "B": "Use the compose method.",
                "C": "Use the copy method.",
                "D": "Use the fields request parameter."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "Most Google Cloud REST APIs, including the Cloud Storage JSON API, support partial responses using the `fields` query parameter. To retrieve only specific metadata properties (and not the object content) for performance and privacy, you should use the `objects.get` method and specify the desired metadata fields in the `fields` parameter (e.g., `fields=name,contentType,timeCreated`). This minimizes the data transferred (D). Patch (A) is for partial updates. Compose (B) combines objects. Copy (C) copies objects.",
            "conditions": [
                "Retrieve specific metadata properties (not data) of GCS objects for users, maximize performance, respect privacy."
            ],
            "caseStudyContext": null
        },
        {
            "id": 203,
            "topic": "Compute",
            "question": "You are deploying a microservices application to Google Kubernetes Engine (GKE) that will broadcast livestreams. You expect unpredictable traffic patterns and large variations in the number of concurrent users. Your application must meet the following requirements:\n\n• Scales automatically during popular events and maintains high availability\n• Is resilient in the event of hardware failures\n\nHow should you configure the deployment parameters? (Choose two.)",
            "options": {
                "A": "Distribute your workload evenly using a multi-zonal node pool.",
                "B": "Distribute your workload evenly using multiple zonal node pools.",
                "C": "Use cluster autoscaler to resize the number of nodes in the node pool, and use a Horizontal Pod Autoscaler to scale the workload.",
                "D": "Create a managed instance group for Compute Engine with the cluster nodes. Configure autoscaling rules for the managed instance group.",
                "E": "Create alerting policies in Cloud Monitoring based on GKE CPU and memory utilization. Ask an on-duty engineer to scale the workload by executing a script when CPU and memory usage exceed predefined thresholds."
            },
            "correctAnswer": [
                "A",
                "C"
            ],
            "explanation": "Requirement 1 (Resilience): Use a multi-zonal node pool (A) to spread nodes across multiple zones within a region, ensuring workload survival during a zone failure. Multiple single-zone pools (B) don't offer the same automatic balancing. Requirement 2 (Scaling): Use the GKE cluster autoscaler to automatically add/remove nodes based on Pod scheduling needs, and use the Horizontal Pod Autoscaler (HPA) to automatically adjust the number of application Pod replicas based on metrics like CPU or custom metrics (C). GKE nodes *are* MIGs, but you configure scaling via cluster autoscaler/HPA, not directly on the MIG (D). Manual scaling (E) is explicitly against the requirements.",
            "conditions": [
                "Deploy GKE microservices (livestreaming), unpredictable traffic, need auto-scaling, high availability, resilience to hardware failure."
            ],
            "caseStudyContext": null
        },
        {
            "id": 204,
            "topic": "Storage",
            "question": "You work for a rapidly growing financial technology startup. You manage the payment processing application written in Go and hosted on Cloud Run in the Singapore region (asia-southeast1). The payment processing application processes data stored in a Cloud Storage bucket that is also located in the Singapore region.\n\nThe startup plans to expand further into the Asia Pacific region. You plan to deploy the Payment Gateway in Jakarta, Hong Kong, and Taiwan over the next six months. Each location has data residency requirements that require customer data to reside in the country where the transaction was made. You want to minimize the cost of these deployments. What should you do?",
            "options": {
                "A": "Create a Cloud Storage bucket in each region, and create a Cloud Run service of the payment processing application in each region.",
                "B": "Create a Cloud Storage bucket in each region, and create three Cloud Run services of the payment processing application in the Singapore region.",
                "C": "Create three Cloud Storage buckets in the Asia multi-region, and create three Cloud Run services of the payment processing application in the Singapore region.",
                "D": "Create three Cloud Storage buckets in the Asia multi-region, and create three Cloud Run revisions of the payment processing application in the Singapore region."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "Data residency requirements mandate storing data within specific countries/regions (Jakarta, HK, Taiwan). This requires separate Cloud Storage buckets located in the respective regions (or appropriate multi-regions covering *only* those locations, but regional is more precise). To minimize latency and potential data egress costs, the Cloud Run service processing the data should also be deployed in the same region as the data it processes. Therefore, deploying both a bucket and a Cloud Run service in *each* required region (A) is the correct approach to meet residency, performance, and cost requirements. Processing data in Singapore from other regions (B, C, D) violates residency and incurs latency/egress costs.",
            "conditions": [
                "Cloud Run app processes data from GCS bucket (both currently Singapore). Expand to Jakarta, HK, Taiwan. Strict data residency per location. Minimize cost."
            ],
            "caseStudyContext": null
        },
        {
            "id": 205,
            "topic": "Databases",
            "question": "You recently joined a new team that has a Cloud Spanner database instance running in production. Your manager has asked you to optimize the Spanner instance to reduce cost while maintaining high reliability and availability of the database. What should you do?",
            "options": {
                "A": "Use Cloud Logging to check for error logs, and reduce Spanner processing units by small increments until you find the minimum capacity required.",
                "B": "Use Cloud Trace to monitor the requests per sec of incoming requests to Spanner, and reduce Spanner processing units by small increments until you find the minimum capacity required.",
                "C": "Use Cloud Monitoring to monitor the CPU utilization, and reduce Spanner processing units by small increments until you find the minimum capacity required.",
                "D": "Use Snapshot Debugger to check for application errors, and reduce Spanner processing units by small increments until you find the minimum capacity required."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "Cloud Spanner cost is primarily driven by the amount of compute capacity (nodes or processing units) provisioned and storage used. To optimize cost by reducing compute capacity safely, you need to monitor resource utilization. Cloud Monitoring provides key Spanner metrics, including CPU utilization. The recommended practice is to monitor CPU utilization (specifically high-priority CPU) and ensure it stays below recommended thresholds (e.g., 65% for regional, 45% for multi-region) to maintain performance and availability. By monitoring this metric (C), you can incrementally reduce processing units while ensuring utilization stays within safe limits. Error logs (A), request rates (B), or Debugger (D) don't directly indicate if compute capacity can be safely reduced.",
            "conditions": [
                "Optimize existing Cloud Spanner instance cost, maintain reliability/availability."
            ],
            "caseStudyContext": null
        },
        {
            "id": 206,
            "topic": "Monitoring & Logging",
            "question": "You recently deployed a Go application on Google Kubernetes Engine (GKE). The operations team has noticed that the application's CPU usage is high even when there is low production traffic. The operations team has asked you to optimize your application's CPU resource consumption. You want to determine which Go functions consume the largest amount of CPU. What should you do?",
            "options": {
                "A": "Deploy a Fluent Bit daemonset on the GKE cluster to log data in Cloud Logging. Analyze the logs to get insights into your application code’s performance.",
                "B": "Create a custom dashboard in Cloud Monitoring to evaluate the CPU performance metrics of your application.",
                "C": "Connect to your GKE nodes using SSH. Run the top command on the shell to extract the CPU utilization of your application.",
                "D": "Modify your Go application to capture profiling data. Analyze the CPU metrics of your application in flame graphs in Profiler."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "To identify which specific *functions* within the Go application are consuming the most CPU, you need code-level profiling. Cloud Profiler collects CPU and memory profiles from running applications (including Go apps on GKE) with low overhead. It requires instrumenting the application (D) to include the Profiler agent. The collected data can then be visualized as flame graphs in the Cloud Console, clearly showing CPU time spent in different functions (D). Logging (A), high-level monitoring metrics (B), or node-level `top` (C) do not provide the necessary function-level granularity.",
            "conditions": [
                "Go app on GKE has high CPU usage at low traffic, need to find CPU-intensive functions."
            ],
            "caseStudyContext": null
        },
        {
            "id": 207,
            "topic": "IAM & Security",
            "question": "Your team manages a Google Kubernetes Engine (GKE) cluster where an application is running. A different team is planning to integrate with this application. Before they start the integration, you need to ensure that the other team cannot make changes to your application, but they can deploy the integration on GKE. What should you do?",
            "options": {
                "A": "Using Identity and Access Management (IAM), grant the Viewer IAM role on the cluster project to the other team.",
                "B": "Create a new GKE cluster. Using Identity and Access Management (IAM), grant the Editor role on the cluster project to the other team.",
                "C": "Create a new namespace in the existing cluster. Using Identity and Access Management (IAM), grant the Editor role on the cluster project to the other team.",
                "D": "Create a new namespace in the existing cluster. Using Kubernetes role-based access control (RBAC), grant the Admin role on the new namespace to the other team."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "To allow another team to deploy their *own* applications (the integration) into your GKE cluster without letting them modify *your* existing application, use Kubernetes namespaces and RBAC. Create a new namespace specifically for the other team. Then, create a Kubernetes Role (or ClusterRole bound to the namespace via RoleBinding) that grants permissions needed for deployment (like creating Deployments, Services, etc.) *only within that new namespace*. Grant this Role to the other team's users or service accounts (D). Project-level IAM roles (A, B, C) are too broad and grant permissions across the entire cluster or project, not namespace-specific deployment rights.",
            "conditions": [
                "Allow another team to deploy their integration app to your GKE cluster, prevent them from modifying your existing app."
            ],
            "caseStudyContext": null
        },
        {
            "id": 208,
            "topic": "Monitoring & Logging",
            "question": "You have recently instrumented a new application with OpenTelemetry, and you want to check the latency of your application requests in Trace. You want to ensure that a specific request is always traced. What should you do?",
            "options": {
                "A": "Wait 10 minutes, then verify that Trace captures those types of requests automatically.",
                "B": "Write a custom script that sends this type of request repeatedly from your dev project.",
                "C": "Use the Trace API to apply custom attributes to the trace.",
                "D": "Add the X-Cloud-Trace-Context header to the request with the appropriate parameters."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "Cloud Trace uses sampling to decide which requests to trace to manage volume and cost. To *force* a specific request to be traced, regardless of the sampling decision, you need to add the `X-Cloud-Trace-Context` HTTP header to the incoming request. Setting the trace options flag within this header to `1` (trace=true) signals to the tracing libraries (like OpenTelemetry configured with a Cloud Trace exporter) and downstream services that this request must be traced (D). Waiting (A), sending repeated requests (B), or adding attributes (C) doesn't override the sampling mechanism.",
            "conditions": [
                "Application instrumented with OpenTelemetry sending data to Cloud Trace, need to force tracing for a specific request."
            ],
            "caseStudyContext": null
        },
        {
            "id": 209,
            "topic": "Compute",
            "question": "You are trying to connect to your Google Kubernetes Engine (GKE) cluster using kubectl from Cloud Shell. You have deployed your GKE cluster with a public endpoint. From Cloud Shell, you run the following command:\n\n`gcloud container clusters get-credentials CLUSTER_NAME --region=REGION`\n\nYou notice that the kubectl commands time out without returning an error message. What is the most likely cause of this issue?",
            "options": {
                "A": "Your user account does not have privileges to interact with the cluster using kubectl.",
                "B": "Your Cloud Shell external IP address is not part of the authorized networks of the cluster.",
                "C": "The Cloud Shell is not part of the same VPC as the GKE cluster.",
                "D": "A VPC firewall is blocking access to the cluster’s endpoint."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "GKE clusters with public endpoints can optionally restrict access to the control plane API server using 'authorized networks'. If this feature is enabled, only connections from specified IP CIDR blocks are allowed. Cloud Shell instances have ephemeral external IP addresses. If authorized networks are enabled on the GKE cluster and the Cloud Shell IP is not included in the allowed list, attempts to connect via `kubectl` (which talks to the API server endpoint) will time out (B). Lack of privileges (A) usually results in a 401/403 error, not a timeout. Cloud Shell runs outside your VPC (C). VPC firewalls (D) don't typically block outbound access to GKE public endpoints.",
            "conditions": [
                "Connect to GKE public endpoint via kubectl from Cloud Shell, commands time out."
            ],
            "caseStudyContext": null
        },
        {
            "id": 210,
            "topic": "Storage",
            "question": "You are developing a web application that contains private images and videos stored in a Cloud Storage bucket. Your users are anonymous and do not have Google Accounts. You want to use your application-specific logic to control access to the images and videos. How should you configure access?",
            "options": {
                "A": "Cache each web application user's IP address to create a named IP table using Google Cloud Armor. Create a Google Cloud Armor security policy that allows users to access the backend bucket.",
                "B": "Grant the Storage Object Viewer IAM role to allUsers. Allow users to access the bucket after authenticating through your web application.",
                "C": "Configure Identity-Aware Proxy (IAP) to authenticate users into the web application. Allow users to access the bucket after authenticating through IAP.",
                "D": "Generate a signed URL that grants read access to the bucket. Allow users to access the URL after authenticating through your web application."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "The bucket contains private objects, and access needs to be controlled by application logic for anonymous users. Signed URLs (D) are the standard solution. The web application, after performing its own authentication/authorization checks, generates a short-lived Signed URL granting temporary read access to a specific object. This URL is given to the client (browser), allowing direct download from GCS without the user needing Google credentials or the object being public. Granting `allUsers` (B) makes objects public. IAP (C) requires Google accounts. Cloud Armor (A) controls access at the network edge, not fine-grained object access based on application logic.",
            "conditions": [
                "Serve private GCS objects (images/videos) to anonymous users based on application logic."
            ],
            "caseStudyContext": null
        },
        {
            "id": 211,
            "topic": "Compute",
            "question": "You have deployed an application in Google Kubernetes Engine (GKE). You need to update the application to make authorized requests to Google Cloud managed services. You want this to be a one-time setup, and you need to follow security best practices of auto-rotating your security keys and storing them in an encrypted store. You already created a service account with appropriate access to the Google Cloud service. What should you do next?",
            "options": {
                "A": "Assign the Google Cloud service account to your GKE Pod using Workload Identity.",
                "B": "Export the Google Cloud service account, and share it with the Pod as a Kubernetes Secret.",
                "C": "Export the Google Cloud service account, and embed it in the source code of the application.",
                "D": "Export the Google Cloud service account, and upload it to HashiCorp Vault to generate a dynamic service account for your application."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "This is identical to Q191. Workload Identity (A) is the best practice. It avoids exporting/managing GSA keys (B, C, D) by binding a KSA (used by the Pod) to the pre-created GSA. The Pod then uses its automatically managed KSA token to obtain short-lived GSA credentials, adhering to auto-rotation and secure storage principles.",
            "conditions": [
                "GKE app needs to access GCP services, one-time setup, follow best practices (auto-rotated keys, encrypted store), GSA already created."
            ],
            "caseStudyContext": null
        },
        {
            "id": 212,
            "topic": "Compute",
            "question": "You are a developer at a large organization. You are deploying a web application to Google Kubernetes Engine (GKE). The DevOps team has built a CI/CD pipeline that uses Cloud Deploy to deploy the application to Dev, Test, and Prod clusters in GKE. After Cloud Deploy successfully deploys the application to the Dev cluster, you want to automatically promote it to the Test cluster. How should you configure this process following Google-recommended best practices?",
            "options": {
                "A": "1. Create a Cloud Build trigger that listens for SUCCEEDED Pub/Sub messages from the clouddeploy-operations topic.\n2. Configure Cloud Build to include a step that promotes the application to the Test cluster.",
                "B": "1. Create a Cloud Function that calls the Google Cloud Deploy API to promote the application to the Test cluster.\n2. Configure this function to be triggered by SUCCEEDED Pub/Sub messages from the cloud-builds topic.",
                "C": "1. Create a Cloud Function that calls the Google Cloud Deploy API to promote the application to the Test cluster.\n2. Configure this function to be triggered by SUCCEEDED Pub/Sub messages from the clouddeploy-operations topic.",
                "D": "1. Create a Cloud Build pipeline that uses the gke-deploy builder.\n2. Create a Cloud Build trigger that listens for SUCCEEDED Pub/Sub messages from the cloud-builds topic.\n3. Configure this pipeline to run a deployment step to the Test cluster."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "Cloud Deploy publishes notifications about its operations (like successful rollouts) to a specific Pub/Sub topic (`clouddeploy-operations`). To automate promotion upon successful Dev deployment, listen for the `SUCCEEDED` event for the Dev target on this topic. A Cloud Function is a suitable lightweight, event-driven mechanism to react to this Pub/Sub message (C). The function's logic would call the Cloud Deploy API (or `gcloud deploy releases promote`) to promote the successful release to the Test target. Using Cloud Build (A) works but is potentially heavier than a simple function for this task. Listening to the wrong topic (`cloud-builds`) (B, D) is incorrect.",
            "conditions": [
                "Using Cloud Deploy for GKE deployments (Dev, Test, Prod). Automate promotion Dev->Test after successful Dev deploy."
            ],
            "caseStudyContext": null
        },
        {
            "id": 213,
            "topic": "IAM & Security",
            "question": "Your application is running as a container in a Google Kubernetes Engine cluster. You need to add a secret to your application using a secure approach. What should you do?",
            "options": {
                "A": "Create a Kubernetes Secret, and pass the Secret as an environment variable to the container.",
                "B": "Enable Application-layer Secret Encryption on the cluster using a Cloud Key Management Service (KMS) key.",
                "C": "Store the credential in Cloud KMS. Create a Google service account (GSA) to read the credential from Cloud KMS. Export the GSA as a .json file, and pass the .json file to the container as a volume which can read the credential from Cloud KMS.",
                "D": "Store the credential in Secret Manager. Create a Google service account (GSA) to read the credential from Secret Manager. Create a Kubernetes service account (KSA) to run the container. Use Workload Identity to configure your KSA to act as a GSA."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "Storing secrets directly in Kubernetes Secrets (A) is not the most secure method as they are only base64 encoded by default. Application-layer encryption (B) secures K8s secrets at rest in etcd but doesn't change how they are accessed. Using KMS directly (C) is for key management, not arbitrary secrets, and involves insecure key handling. The most secure Google-recommended approach is to store the secret in Secret Manager, grant a GSA access, and use Workload Identity to allow the Pod (via its KSA) to authenticate as the GSA and retrieve the secret directly from the Secret Manager API at runtime (D).",
            "conditions": [
                "Add secret to GKE application securely."
            ],
            "caseStudyContext": null
        },
        {
            "id": 214,
            "topic": "IAM & Security",
            "question": "You are a developer at a financial institution. You use Cloud Shell to interact with Google Cloud services. User data is currently stored on an ephemeral disk; however, a recently passed regulation mandates that you can no longer store sensitive information on an ephemeral disk. You need to implement a new storage solution for your user data. You want to minimize code changes. Where should you store your user data?",
            "options": {
                "A": "Store user data on a Cloud Shell home disk, and log in at least every 120 days to prevent its deletion.",
                "B": "Store user data on a persistent disk in a Compute Engine instance.",
                "C": "Store user data in a Cloud Storage bucket.",
                "D": "Store user data in BigQuery tables."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "The constraint is minimizing code changes, implying the current interaction might be file-based on an ephemeral disk (perhaps associated with a GCE instance accessed via Cloud Shell, or Cloud Shell's own temporary disk beyond $HOME). A GCE Persistent Disk (B) offers durable block storage mountable as a standard filesystem, providing the most direct replacement for file-based operations with minimal code changes. Cloud Shell home disk (A) is persistent but small and tied to Cloud Shell activity. Cloud Storage (C) is object storage (API changes needed). BigQuery (D) is a data warehouse (schema/query changes needed).",
            "conditions": [
                "Interact via Cloud Shell, currently store user data on ephemeral disk, must move to persistent storage, minimize code changes."
            ],
            "caseStudyContext": null
        },
        {
            "id": 215,
            "topic": "Compute",
            "question": "You are a lead developer working on a new retail system that runs on Cloud Run and Firestore in Datastore mode. A web UI requirement is for the system to display a list of available products when users access the system and for the user to be able to browse through all products. You have implemented this requirement in the minimum viable product (MVP) phase by returning a list of all available products stored in Firestore.",
            "options": {
                "A": "Create a user-managed service account with a custom Identity and Access Management (IAM) role.",
                "B": "Create a user-managed service account with the Storage Admin Identity and Access Management (IAM) role.",
                "C": "Create a user-managed service account with the Project Editor Identity and Access Management (IAM) role.",
                "D": "Use the default service account linked to the Cloud Run revision in production."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "To follow the principle of least privilege when granting Cloud Run access to other GCP services (like Cloud Storage in this implied context, although Firestore is mentioned earlier), you should create a dedicated, user-managed service account. Assign a custom IAM role to this service account that contains *only* the permissions required by the application (e.g., `storage.objects.get` if reading from GCS) (A). Using broad roles like Storage Admin (B) or Project Editor (C), or the default service account (D - which is often Editor by default), grants excessive permissions.",
            "conditions": [
                "Cloud Run application needs access to other GCP resources (e.g., Cloud Storage), follow least privilege."
            ],
            "caseStudyContext": null
        },
        {
            "id": 216,
            "topic": "Compute",
            "question": "Your team is developing unit tests for Cloud Function code. The code is stored in a Cloud Source Repositories repository. You are responsible for implementing the tests. Only a specific service account has the necessary permissions to deploy the code to Cloud Functions. You want to ensure that the code cannot be deployed without first passing the tests. How should you configure the unit testing process?",
            "options": {
                "A": "Configure Cloud Build to deploy the Cloud Function. If the code passes the tests, a deployment approval is sent to you.",
                "B": "Configure Cloud Build to deploy the Cloud Function, using the specific service account as the build agent. Run the unit tests after successful deployment.",
                "C": "Configure Cloud Build to run the unit tests. If the code passes the tests, the developer deploys the Cloud Function.",
                "D": "Configure Cloud Build to run the unit tests, using the specific service account as the build agent. If the code passes the tests, Cloud Build deploys the Cloud Function."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "The goal is an automated pipeline where deployment happens only after tests pass, using a specific service account for deployment. Cloud Build can orchestrate this. The pipeline should first run the unit tests. If the tests succeed, the next step should deploy the Cloud Function. Since a specific service account is required for deployment, Cloud Build needs to execute the deployment step using (or acting as) that service account (D). Running tests *after* deployment (B) defeats the purpose of gating. Manual deployment (C) or manual approval (A) doesn't meet the automation goal.",
            "conditions": [
                "Unit test Cloud Function code, deploy only if tests pass, deployment requires specific SA, automate."
            ],
            "caseStudyContext": null
        },
        {
            "id": 217,
            "topic": "Monitoring & Logging",
            "question": "Your team detected a spike of errors in an application running on Cloud Run in your production project. The application is configured to read messages from Pub/Sub topic A, process the messages, and write the messages to topic B. You want to conduct tests to identify the cause of the errors. You can use a set of mock messages for testing. What should you do?",
            "options": {
                "A": "Deploy the Pub/Sub and Cloud Run emulators on your local machine. Deploy the application locally, and change the logging level in the application to DEBUG or INFO. Write mock messages to topic A, and then analyze the logs.",
                "B": "Use the gcloud CLI to write mock messages to topic A. Change the logging level in the application to DEBUG or INFO, and then analyze the logs.",
                "C": "Deploy the Pub/Sub emulator on your local machine. Point the production application to your local Pub/Sub topics. Write mock messages to topic A, and then analyze the logs.",
                "D": "Use the Google Cloud console to write mock messages to topic A. Change the logging level in the application to DEBUG or INFO, and then analyze the logs."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "To safely diagnose production errors without impacting the live system, local testing with emulators is the best practice. Deploying the Pub/Sub emulator allows you to simulate topic A locally. Running the Cloud Run application container locally (e.g., via Docker) allows you to connect it to the local emulator. Increasing the logging level provides detailed diagnostics. Publishing mock messages to the local emulator topic A lets you observe the application's processing and identify the error's cause by analyzing the detailed local logs (A). Injecting mock messages into production (B, D) or pointing production to local emulators (C) is risky and inappropriate.",
            "conditions": [
                "Cloud Run app processing Pub/Sub messages (A->B) has errors in prod, need to test/debug with mock messages."
            ],
            "caseStudyContext": null
        },
        {
            "id": 218,
            "topic": "IAM & Security",
            "question": "You are developing a Java Web Server that needs to interact with Google Cloud services via the Google Cloud API on the user's behalf. Users should be able to authenticate to the Google Cloud API using their Google Cloud identities. Which workflow should you implement in your web application?",
            "options": {
                "A": "1. When a user arrives at your application, prompt them for their Google username and password.\n2. Store an SHA password hash in your application's database along with the user's username.\n3. The application authenticates to the Google Cloud API using HTTPs requests with the user's username and password hash in the Authorization request header.",
                "B": "1. When a user arrives at your application, prompt them for their Google username and password.\n2. Forward the user's username and password in an HTTPS request to the Google Cloud authorization server, and request an access token.\n3. The Google server validates the user's credentials and returns an access token to the application.\n4. The application uses the access token to call the Google Cloud API.",
                "C": "1. When a user arrives at your application, route them to a Google Cloud consent screen with a list of requested permissions that prompts the user to sign in with SSO to their Google Account.\n2. After the user signs in and provides consent, your application receives an authorization code from a Google server.\n3. The Google server returns the authorization code to the user, which is stored in the browser's cookies.\n4. The user authenticates to the Google Cloud API using the authorization code in the cookie.",
                "D": "1. When a user arrives at your application, route them to a Google Cloud consent screen with a list of requested permissions that prompts the user to sign in with SSO to their Google Account.\n2. After the user signs in and provides consent, your application receives an authorization code from a Google server.\n3. The application requests a Google Server to exchange the authorization code with an access token.\n4. The Google server responds with the access token that is used by the application to call the Google Cloud API."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "This describes the standard OAuth 2.0 Authorization Code Grant Flow, which is the recommended method for web server applications to obtain access tokens to call Google APIs on behalf of users. The application redirects the user to Google for authentication and consent (Step 1). Google redirects back to the application with an authorization code (Step 2). The application backend then securely exchanges this code with Google for an access token (and optionally a refresh token) (Step 3). The application uses the access token to make API calls on the user's behalf (Step 4). Options A and B involve handling user passwords directly, which is insecure and violates OAuth principles. Option C incorrectly describes how the authorization code is handled and used.",
            "conditions": [
                "Java web server needs to call Google Cloud APIs on behalf of users using their Google identities."
            ],
            "caseStudyContext": null
        },
        {
            "id": 219,
            "topic": "Compute",
            "question": "You recently developed a new application. You want to deploy the application on Cloud Run without a Dockerfile. Your organization requires that all container images are pushed to a centrally managed container repository. How should you build your container using Google Cloud services? (Choose two.)",
            "options": {
                "A": "Push your source code to Artifact Registry.",
                "B": "Submit a Cloud Build job to push the image.",
                "C": "Use the pack build command with pack CLI.",
                "D": "Include the --source flag with the gcloud run deploy CLI command.",
                "E": "Include the --platform=kubernetes flag with the gcloud run deploy CLI command."
            },
            "correctAnswer": [
                "C",
                "D"
            ],
            "explanation": "To build a container image without a Dockerfile for Cloud Run, Google Cloud Buildpacks can be used. This can be done implicitly with `gcloud run deploy --source .` (D), which uses Cloud Build and Buildpacks behind the scenes, builds the image, pushes it to Artifact Registry (creating a repo if needed), and deploys. Alternatively, you can use the `pack` CLI (C) (part of the Buildpacks project) locally or within a CI/CD system like Cloud Build to explicitly build the image using buildpacks, then manually push it to the central Artifact Registry, and finally deploy to Cloud Run. Both methods achieve building without a Dockerfile. Pushing source to AR (A) is incorrect. Submitting a standard Cloud Build job (B) typically assumes a Dockerfile or explicit build steps. --platform=kubernetes (E) is for Cloud Run for Anthos.",
            "conditions": [
                "Deploy app to Cloud Run without Dockerfile, use Google Cloud build services, push image to central repo."
            ],
            "caseStudyContext": null
        },
        {
            "id": 220,
            "topic": "Databases",
            "question": "You work for an organization that manages an online ecommerce website. Your company plans to expand across the world; however, the estore currently serves one specific region. You need to select a SQL database and configure a schema that will scale as your organization grows. You want to create a table that stores all customer transactions and ensure that the customer (CustomerId) and the transaction (TransactionId) are unique. What should you do?",
            "options": {
                "A": "Create a Cloud SQL table that has TransactionId and CustomerId configured as primary keys. Use an incremental number for the TransactionId.",
                "B": "Create a Cloud SQL table that has TransactionId and CustomerId configured as primary keys. Use a random string (UUID) for the Transactionid.",
                "C": "Create a Cloud Spanner table that has TransactionId and CustomerId configured as primary keys. Use a random string (UUID) for the TransactionId.",
                "D": "Create a Cloud Spanner table that has TransactionId and CustomerId configured as primary keys. Use an incremental number for the TransactionId."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "The requirement for global expansion and scaling points towards Cloud Spanner, a globally distributed, horizontally scalable SQL database. Cloud SQL is regional. To ensure uniqueness of (CustomerId, TransactionId) and avoid potential hotspotting issues with globally distributed writes, using a randomly generated UUID for TransactionId as part of the composite primary key is the best practice for Spanner (C). Incremental numbers (A, D) can lead to write hotspots in distributed systems.",
            "conditions": [
                "Ecommerce app expanding globally, need scalable SQL DB for transactions, ensure unique (CustomerId, TransactionId)."
            ],
            "caseStudyContext": null
        },
        {
            "id": 221,
            "topic": "Monitoring & Logging",
            "question": "You are monitoring a web application that is written in Go and deployed in Google Kubernetes Engine. You notice an increase in CPU and memory utilization. You need to determine which source code is consuming the most CPU and memory resources. What should you do?",
            "options": {
                "A": "Download, install, and start the Snapshot Debugger agent in your VM. Take debug snapshots of the functions that take the longest time. Review the call stack frame, and identify the local variables at that level in the stack.",
                "B": "Import the Cloud Profiler package into your application, and initialize the Profiler agent. Review the generated flame graph in the Google Cloud console to identify time-intensive functions.",
                "C": "Import OpenTelemetry and Trace export packages into your application, and create the trace provider. Review the latency data for your application on the Trace overview page, and identify where bottlenecks are occurring.",
                "D": "Create a Cloud Logging query that gathers the web application's logs. Write a Python script that calculates the difference between the timestamps from the beginning and the end of the application's longest functions to identify time-intensive functions."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "Cloud Profiler is designed to analyze CPU and memory usage attributed to specific functions or lines of code in production applications with low overhead. Instrumenting the Go application with the Cloud Profiler agent allows continuous collection of this data. The results can be viewed as flame graphs in the Cloud Console, directly showing which parts of the code consume the most CPU or allocate the most memory (B). Debugger snapshots (A) capture state at a point in time. Trace (C) focuses on request latency across services. Analyzing logs (D) provides limited insight into internal resource consumption.",
            "conditions": [
                "Go app on GKE, increased CPU/memory usage, need to find resource-intensive source code."
            ],
            "caseStudyContext": null
        },
        {
            "id": 222,
            "topic": "Compute",
            "question": "You have a container deployed on Google Kubernetes Engine. The container can sometimes be slow to launch, so you have implemented a liveness probe. You notice that the liveness probe occasionally fails on launch. What should you do?",
            "options": {
                "A": "Add a startup probe.",
                "B": "Increase the initial delay for the liveness probe.",
                "C": "Increase the CPU limit for the container.",
                "D": "Add a readiness probe."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "Liveness probes check if a container is running; if it fails, the container is restarted. Readiness probes check if a container is ready to serve traffic. If a container is slow to start, a liveness probe might fail before the application is fully initialized, causing unnecessary restarts. A startup probe (A) is specifically designed for this scenario. It runs first, and Kubernetes disables liveness/readiness checks until the startup probe succeeds (or times out). This prevents the liveness probe from killing a slow-starting container prematurely. Increasing initial delay (B) can work but might be hard to tune perfectly. CPU limits (C) might be relevant but aren't the direct fix. Readiness probes (D) don't prevent liveness failures during startup.",
            "conditions": [
                "GKE container slow to launch, liveness probe fails occasionally on launch."
            ],
            "caseStudyContext": null
        },
        {
            "id": 223,
            "topic": "Networking",
            "question": "You manage your company's ecommerce platform's payment system, which runs on Google Cloud. Your company must retain user logs for 1 year for internal auditing purposes and for 3 years to meet compliance requirements. You need to store new user logs on Google Cloud to minimize on-premises storage usage and ensure that they are easily searchable. You want to minimize effort while ensuring that the logs are stored correctly. What should you do?",
            "options": {
                "A": "Use an external HTTP(S) load balancer to route a predetermined percentage of traffic to two different color schemes of your application. Analyze the results to determine whether there is a statistically significant difference in sales.",
                "B": "Use an external HTTP(S) load balancer to route traffic to the original color scheme while the new deployment is created and tested. After testing is complete, reroute all traffic to the new color scheme. Analyze the results to determine whether there is a statistically significant difference in sales.",
                "C": "Use an external HTTP(S) load balancer to mirror traffic to the new version of your application. Analyze the results to determine whether there is a statistically significant difference in sales.",
                "D": "Enable a feature flag that displays the new color scheme to half of all users. Monitor sales to see whether they increase for this group of users."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "A/B testing involves comparing two (or more) versions of something by exposing them to different segments of users simultaneously and measuring the results. To test the effect of a new color scheme on sales, you need to deploy both the old and new schemes and direct a portion of live traffic to each. Using a load balancer to split traffic based on weights (e.g., 50% to version A, 50% to version B) is a standard way to implement this randomization (A). Option B describes blue/green deployment. Option C describes traffic mirroring (shadow testing). Option D uses feature flags, which is another valid A/B testing technique, but load balancer traffic splitting is also a direct and common method.",
            "conditions": [
                "Test new website color scheme effect on sales using A/B testing on live production traffic."
            ],
            "caseStudyContext": null
        },
        {
            "id": 224,
            "topic": "Compute",
            "question": "You are a developer at a large corporation. You manage three Google Kubernetes Engine clusters on Google Cloud. Your team’s developers need to switch from one cluster to another regularly without losing access to their preferred development tools. You want to configure access to these multiple clusters while following Google-recommended best practices. What should you do?",
            "options": {
                "A": "Ask the developers to use Cloud Shell and run gcloud container clusters get-credential to switch to another cluster.",
                "B": "In a configuration file, define the clusters, users, and contexts. Share the file with the developers and ask them to use kubect1 contig to add cluster, user, and context details.",
                "C": "Ask the developers to install the gcloud CLI on their workstation and run gcloud container clusters get-credentials to switch to another cluster.",
                "D": "Ask the developers to open three terminals on their workstation and use kubect1 config to configure access to each cluster."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "To allow developers to use their local tools (IDEs, etc.) while easily switching between GKE cluster contexts, installing the `gcloud` CLI locally is the standard approach. The `gcloud container clusters get-credentials CLUSTER_NAME --zone/region REGION_NAME` command automatically fetches cluster details and authentication info (using the developer's gcloud login) and automatically configures the local `kubectl` (kubeconfig file) with the necessary contexts. Developers can then switch context using `kubectl config use-context` (C). Cloud Shell (A) requires working within the browser. Manually managing kubeconfig files (B, D) is cumbersome and error-prone compared to the `gcloud` integration.",
            "conditions": [
                "Developers need to switch between multiple GKE clusters regularly from their workstations, use preferred tools, follow best practices."
            ],
            "caseStudyContext": null
        },
        {
            "id": 225,
            "topic": "Compute",
            "question": "You are developing an online gaming platform as a microservices application on Google Kubernetes Engine (GKE). Users on social media are complaining about long loading times for certain URL requests to the application. You need to investigate performance bottlenecks in the application and identify which HTTP requests have a significantly high latency span in user requests. What should you do?",
            "options": {
                "A": "Use an external HTTP(S) load balancer to route traffic to the original color scheme while the new deployment is created and tested. After testing is complete, reroute all traffic to the new color scheme. Analyze the results to determine whether there is a statistically significant difference in sales.",
                "B": "Deploy your application on Google Kubernetes Engine with Anthos Service Mesh. Use traffic splitting to direct a subset of user traffic to the new version based on the user-agent header.",
                "C": "Deploy your application on App Engine. Use traffic splitting to direct a subset of user traffic to the new version based on the IP address.",
                "D": "Deploy your application on Compute Engine. Use Traffic Director to direct a subset of user traffic to the new version based on predefined weights."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "The requirement is to test with production users based on their *operating system* and allow quick rollback. Anthos Service Mesh (ASM) provides fine-grained traffic control, including routing based on HTTP request headers like `User-Agent`, which contains OS information. You can configure ASM to route users with specific OSes (e.g., iOS, Android) to the new version while others see the old version (B). ASM also facilitates quick rollback by adjusting routing rules. Cloud Run (A) traffic splitting is typically based on percentage or tags. App Engine (C) splits by IP, cookie, or random weight. Traffic Director (D) can route based on headers but ASM provides a more integrated service mesh solution for GKE.",
            "conditions": [
                "A/B test new application version, route subset of production users based on OS, quick rollback needed."
            ],
            "caseStudyContext": null
        },
        {
            "id": 226,
            "topic": "Compute",
            "question": "Your team is writing a backend application to implement the business logic for an interactive voice response (IVR) system that will support a payroll application. The IVR system has the following technical characteristics:\n\n• Each customer phone call is associated with a unique IVR session.\n• The IVR system creates a separate persistent gRPC connection to the backend for each session.\n• If the connection is interrupted, the IVR system establishes a new connection, causing a slight latency for that call.\n\nYou need to determine which compute environment should be used to deploy the backend application. Using current call data, you determine that:\n\n• Call duration ranges from 1 to 30 minutes.\n• Calls are typically made during business hours.\n• There are significant spikes of calls around certain known dates (e.g., pay days), or when large payroll changes occur.\n\nYou want to minimize cost, effort, and operational overhead. Where should you deploy the backend application?",
            "options": {
                "A": "Compute Engine",
                "B": "Google Kubernetes Engine cluster in Standard mode",
                "C": "Cloud Functions",
                "D": "Cloud Run"
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "The application needs to handle persistent gRPC connections (up to 30 mins), scale efficiently for unpredictable spikes, and minimize cost/overhead. Cloud Run (D) supports gRPC (including streaming) and scales automatically based on requests/connections, down to zero when idle, minimizing cost. While Cloud Functions (C) are serverless, they have shorter execution time limits (max 9 mins for HTTP, 60 mins for Gen2 event-driven, but less ideal for persistent connections). GKE (B) and Compute Engine (A) provide more control but require more management overhead for infrastructure and scaling compared to fully managed Cloud Run.",
            "conditions": [
                "Backend for IVR system, persistent gRPC connections (1-30 min), unpredictable traffic spikes, minimize cost/effort/overhead."
            ],
            "caseStudyContext": null
        },
        {
            "id": 227,
            "topic": "Databases",
            "question": "You are developing an application hosted on Google Cloud that uses a MySQL relational database schema. The application will have a large volume of reads and writes to the database and will require backups and ongoing capacity planning. Your team does not have time to fully manage the database but can take on small administrative tasks. How should you host the database?",
            "options": {
                "A": "Configure Cloud SQL to host the database, and import the schema into Cloud SQL.",
                "B": "Deploy MySQL from the Google Cloud Marketplace to the database using a client, and import the schema.",
                "C": "Configure Bigtable to host the database, and import the data into Bigtable.",
                "D": "Configure Cloud Spanner to host the database, and import the schema into Cloud Spanner.",
                "E": "Configure Firestore to host the database, and import the data into Firestore."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "The requirements are: MySQL relational schema, large read/write volume, backups needed, capacity planning needed, minimal full management but small admin tasks OK. Cloud SQL for MySQL (A) is the fully managed relational database service for MySQL on Google Cloud. It handles backups, patching, replication, and offers scaling options (addressing capacity planning), significantly reducing management overhead compared to self-hosting MySQL on GCE (implied by B). Bigtable (C) and Firestore (E) are NoSQL. Cloud Spanner (D) is relational but typically for larger, globally distributed needs and might be overkill/more complex than managed MySQL if regional scope is sufficient.",
            "conditions": [
                "Host MySQL DB on GCP, large R/W volume, need backups/capacity planning, minimize full management."
            ],
            "caseStudyContext": null
        },
        {
            "id": 228,
            "topic": "Compute",
            "question": "You are developing a new web application using Cloud Run and committing code to Cloud Source Repositories. You want to deploy new code in the most efficient way possible. You have already created a Cloud Build YAML file that builds a container and runs the following command: gcloud run deploy. What should you do next?",
            "options": {
                "A": "Create a Pub/Sub topic to be notified when code is pushed to the repository. Create a Pub/Sub trigger that runs the build file when an event is published to the topic.",
                "B": "Create a build trigger that runs the build file in response to a repository code being pushed to the development branch.",
                "C": "Create a webhook build trigger that runs the build file in response to HTTP POST calls to the webhook URL.",
                "D": "Create a Cron job that runs the following command every 24 hours: gcloud builds submit."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "To automate deployment on code commits, Cloud Build triggers are used. Since the code is in Cloud Source Repositories, you can create a build trigger directly linked to the repository. Configuring the trigger to activate when code is pushed to a specific branch (like the development or main branch) (B) is the standard and most efficient way to integrate CSR and Cloud Build for CI/CD. Pub/Sub triggers (A) or webhooks (C) are for other event sources. Cron jobs (D) are for scheduled tasks, not commit-driven builds.",
            "conditions": [
                "Automate Cloud Run deployment from CSR using existing Cloud Build config (build + deploy steps), trigger on code push."
            ],
            "caseStudyContext": null
        },
        {
            "id": 229,
            "topic": "Compute",
            "question": "You are a developer at a large organization. You are deploying a web application to Google Kubernetes Engine (GKE). The DevOps team has built a CI/CD pipeline that uses Cloud Deploy to deploy the application to Dev, Test, and Prod clusters in GKE. After Cloud Deploy successfully deploys the application to the Dev cluster, you want to automatically promote it to the Test cluster. How should you configure this process following Google-recommended best practices?",
            "options": {
                "A": "1. Create a Cloud Build trigger that listens for SUCCEEDED Pub/Sub messages from the clouddeploy-operations topic.\n2. Configure Cloud Build to include a step that promotes the application to the Test cluster.",
                "B": "1. Create a Cloud Function that calls the Google Cloud Deploy API to promote the application to the Test cluster.\n2. Configure this function to be triggered by SUCCEEDED Pub/Sub messages from the cloud-builds topic.",
                "C": "1. Create a Cloud Function that calls the Google Cloud Deploy API to promote the application to the Test cluster.\n2. Configure this function to be triggered by SUCCEEDED Pub/Sub messages from the clouddeploy-operations topic.",
                "D": "1. Create a Cloud Build pipeline that uses the gke-deploy builder.\n2. Create a Cloud Build trigger that listens for SUCCEEDED Pub/Sub messages from the cloud-builds topic.\n3. Configure this pipeline to run a deployment step to the Test cluster."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "Cloud Deploy emits events to Pub/Sub on the `clouddeploy-operations` topic. To automate promotion after a successful Dev deployment, listen for the `SUCCEEDED` event for the Dev target on this topic. A Cloud Function is a suitable lightweight, event-driven mechanism to react to this Pub/Sub message (C). The function's logic would call the Cloud Deploy API (or `gcloud deploy releases promote`) to promote the successful release to the Test target. Using Cloud Build (A) works but is potentially heavier than a simple function for this task. Listening to the wrong topic (`cloud-builds`) (B, D) is incorrect.",
            "conditions": [
                "Using Cloud Deploy for GKE (Dev, Test, Prod), automate promotion Dev->Test after successful Dev deploy."
            ],
            "caseStudyContext": null
        },
        {
            "id": 230,
            "topic": "IAM & Security",
            "question": "Your application is running as a container in a Google Kubernetes Engine cluster. You need to add a secret to your application using a secure approach. What should you do?",
            "options": {
                "A": "Create a Kubernetes Secret, and pass the Secret as an environment variable to the container.",
                "B": "Enable Application-layer Secret Encryption on the cluster using a Cloud Key Management Service (KMS) key.",
                "C": "Store the credential in Cloud KMS. Create a Google service account (GSA) to read the credential from Cloud KMS. Export the GSA as a .json file, and pass the .json file to the container as a volume which can read the credential from Cloud KMS.",
                "D": "Store the credential in Secret Manager. Create a Google service account (GSA) to read the credential from Secret Manager. Create a Kubernetes service account (KSA) to run the container. Use Workload Identity to configure your KSA to act as a GSA."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "Storing secrets directly in Kubernetes Secrets (A) is not the most secure method as they are only base64 encoded by default. Application-layer encryption (B) secures K8s secrets at rest in etcd but doesn't change how they are accessed. Using KMS directly (C) is for key management, not arbitrary secrets, and involves insecure key export. The most secure Google-recommended approach is to store the secret in Secret Manager, grant a GSA access, and use Workload Identity to allow the Pod (via its KSA) to authenticate as the GSA and retrieve the secret directly from the Secret Manager API at runtime (D).",
            "conditions": [
                "Add secret to GKE application securely."
            ],
            "caseStudyContext": null
        },
        {
            "id": 231,
            "topic": "Storage",
            "question": "You are a developer at a financial institution. You use Cloud Shell to interact with Google Cloud services. User data is currently stored on an ephemeral disk; however, a recently passed regulation mandates that you can no longer store sensitive information on an ephemeral disk. You need to implement a new storage solution for your user data. You want to minimize code changes. Where should you store your user data?",
            "options": {
                "A": "Store user data on a Cloud Shell home disk, and log in at least every 120 days to prevent its deletion.",
                "B": "Store user data on a persistent disk in a Compute Engine instance.",
                "C": "Store user data in a Cloud Storage bucket.",
                "D": "Store user data in BigQuery tables."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "The constraint is minimizing code changes, implying the current interaction might be file-based on an ephemeral disk (perhaps associated with a GCE instance accessed via Cloud Shell, or Cloud Shell's own temporary disk beyond $HOME). A GCE Persistent Disk (B) offers durable block storage mountable as a standard filesystem, providing the most direct replacement for file-based operations with minimal code changes. Cloud Shell home disk (A) is persistent but small and tied to Cloud Shell activity. Cloud Storage (C) is object storage (API changes needed). BigQuery (D) is a data warehouse (schema/query changes needed).",
            "conditions": [
                "Interact via Cloud Shell, currently store user data on ephemeral disk, must move to persistent storage, minimize code changes."
            ],
            "caseStudyContext": null
        },
        {
            "id": 232,
            "topic": "Storage",
            "question": "You recently developed a web application to transfer log data to a Cloud Storage bucket daily. Authenticated users will regularly review logs from the prior two weeks for critical events. After that, logs will be reviewed once annually by an external auditor. Data must be stored for a period of no less than 7 years. You want to propose a storage solution that meets these requirements and minimizes costs. What should you do? (Choose two.)",
            "options": {
                "A": "Use the Bucket Lock feature to set the retention policy on the data.",
                "B": "Run a scheduled job to set the storage class to Coldline for objects older than 14 days.",
                "C": "Create a JSON Web Token (JWT) for users needing access to the Coldline storage buckets.",
                "D": "Create a lifecycle management policy to set the storage class to Coldline for objects older than 14 days.",
                "E": "Create a lifecycle management policy to set the storage class to Nearline for objects older than 14 days."
            },
            "correctAnswer": [
                "A",
                "D"
            ],
            "explanation": "Requirement 1 (Retain >= 7 years): Use Bucket Lock (or a standard retention policy) to enforce the 7-year retention period (A). Requirement 2 (Cost/Access Pattern): Logs are reviewed regularly for 14 days (implying Standard storage initially), then only annually. Coldline storage is suitable for data accessed about once per quarter. Archive is for less than once a year. A lifecycle policy should transition the data after the initial 14-day review period to a colder, cheaper class. Coldline (D) fits the 'once annually' access pattern better than Nearline (E - for monthly access). JWTs (C) are for authentication. Scheduled jobs (B) are less efficient than lifecycle policies.",
            "conditions": [
                "Store logs in GCS, review regularly for 14 days, then annually for audits, retain >= 7 years, minimize cost."
            ],
            "caseStudyContext": null
        },
        {
            "id": 233,
            "topic": "Compute",
            "question": "Your team is developing a Cloud Function triggered by Cloud Storage events. You want to accelerate testing and development of your Cloud Function while following Google-recommended best practices. What should you do?",
            "options": {
                "A": "Create a new Cloud Function that is triggered when Cloud Audit Logs detects the cloudfunctions.functions.sourceCodeSet operation in the original Cloud Function. Send mock requests to the new function to evaluate the functionality.",
                "B": "Make a copy of the Cloud Function, and rewrite the code to be HTTP-triggered. Edit and test the new version by triggering the HTTP endpoint. Send mock requests to the new function to evaluate the functionality.",
                "C": "Install the Functions Frameworks library, and configure the Cloud Function on localhost. Make a copy of the function, and make edits to the new version. Test the new version using curl.",
                "D": "Make a copy of the Cloud Function in the Google Cloud console. Use the Cloud console's in-line editor to make source code changes to the new function. Modify your web application to call the new function, and test the new version in production"
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "Functions Frameworks provide a standard way to write Cloud Functions code that can be run locally for testing and development, mimicking the Cloud Functions runtime environment. By installing the framework for the function's language (C), developers can run the function locally, invoke it (e.g., using `curl` with a payload simulating the GCS event), set breakpoints, and iterate much faster than deploying to the cloud for each test. This accelerates development significantly. Rewriting as HTTP (B) changes the trigger type. Using Audit Logs (A) or testing in production (D) are not suitable for rapid local development.",
            "conditions": [
                "Develop/test GCS-triggered Cloud Function, accelerate dev/test cycle, follow best practices."
            ],
            "caseStudyContext": null
        },
        {
            "id": 234,
            "topic": "Compute",
            "question": "Your team is setting up a build pipeline for an application that will run in Google Kubernetes Engine (GKE). For security reasons, you only want images produced by the pipeline to be deployed to your GKE cluster. Which combination of Google Cloud services should you use?",
            "options": {
                "A": "Cloud Build, Cloud Storage, and Binary Authorization",
                "B": "Google Cloud Deploy, Cloud Storage, and Google Cloud Armor",
                "C": "Google Cloud Deploy, Artifact Registry, and Google Cloud Armor",
                "D": "Cloud Build, Artifact Registry, and Binary Authorization"
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "The pipeline needs: 1. Build service: Cloud Build is the managed build service. 2. Image storage: Artifact Registry is the recommended managed registry for container images (and other artifacts). 3. Deployment enforcement: Binary Authorization enforces that only container images meeting specific criteria (e.g., signed by trusted authorities, passed vulnerability scans) can be deployed to GKE. This combination (D) directly addresses the requirements. Cloud Storage (A, B) isn't a container registry. Cloud Deploy (B, C) is for deployment orchestration, not build or enforcement. Cloud Armor (B, C) is for network security.",
            "conditions": [
                "Build pipeline for GKE app, deploy only images produced by the pipeline."
            ],
            "caseStudyContext": null
        },
        {
            "id": 235,
            "topic": "Monitoring & Logging",
            "question": "You are supporting a business-critical application in production deployed on Cloud Run. The application is reporting HTTP 500 errors that are affecting the usability of the application. You want to be alerted when the number of errors exceeds 15% of the requests within a specific time window. What should you do?",
            "options": {
                "A": "Create a Cloud Function that consumes the Cloud Monitoring API. Use Cloud Scheduler to trigger the Cloud Function daily and alert you if the number of errors is above the defined threshold.",
                "B": "Navigate to the Cloud Run page in the Google Cloud console, and select the service from the services list. Use the Metrics tab to visualize the number of errors for that revision, and refresh the page daily.",
                "C": "Create an alerting policy in Cloud Monitoring that alerts you if the number of errors is above the defined threshold.",
                "D": "Create a Cloud Function that consumes the Cloud Monitoring API. Use Cloud Composer to trigger the Cloud Function daily and alert you if the number of errors is above the defined threshold."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "Cloud Monitoring provides native alerting capabilities based on metrics. Cloud Run automatically sends metrics like request count and response codes (including 5xx errors) to Cloud Monitoring. You can create an alerting policy directly in Cloud Monitoring (C) that calculates the ratio of 5xx responses to total requests over a specified time window and triggers an alert if this ratio exceeds 15%. This is the standard, most efficient way. Using Cloud Functions (A, D) or manual checking (B) is unnecessarily complex or manual.",
            "conditions": [
                "Cloud Run app reporting HTTP 500 errors, need alert when error rate > 15% in a time window."
            ],
            "caseStudyContext": null
        },
        {
            "id": 236,
            "topic": "Compute",
            "question": "You need to build a public API that authenticates, enforces quotas, and reports metrics for API callers. Which tool should you use to complete this architecture?",
            "options": {
                "A": "App Engine",
                "B": "Cloud Endpoints",
                "C": "Identity-Aware Proxy",
                "D": "GKE Ingress for HTTP(S) Load Balancing"
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "Cloud Endpoints (and its more comprehensive counterpart, Apigee) is Google Cloud's managed API gateway solution. It's specifically designed to handle common API management tasks like authentication (API keys, JWT, OAuth), quota enforcement, monitoring, logging, and providing a developer portal (B). App Engine (A) can host APIs but lacks built-in management features. IAP (C) secures access based on user identity, not typically for public API management. GKE Ingress (D) exposes services but doesn't provide the full suite of API management features.",
            "conditions": [
                "Build public API, requires authentication, quotas, metrics."
            ],
            "caseStudyContext": null
        },
        {
            "id": 237,
            "topic": "Compute",
            "question": "You noticed that your application was forcefully shut down during a Deployment update in Google Kubernetes Engine. Your application didn’t close the database connection before it was terminated. You want to update your application to make sure that it completes a graceful shutdown. What should you do?",
            "options": {
                "A": "Update your code to process a received SIGTERM signal to gracefully disconnect from the database.",
                "B": "Configure a PodDisruptionBudget to prevent the Pod from being forcefully shut down.",
                "C": "Increase the terminationGracePeriodSeconds for your application.",
                "D": "Configure a PreStop hook to shut down your application."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "When Kubernetes terminates a Pod (e.g., during deployment updates), it sends a SIGTERM signal to the containers. Applications should trap this signal and initiate a graceful shutdown, including closing database connections, finishing requests, etc. (A). If the application doesn't exit within the `terminationGracePeriodSeconds`, Kubernetes sends SIGKILL. PodDisruptionBudgets (B) limit *voluntary* disruptions, not the shutdown process itself. Increasing the grace period (C) only helps if the application *already* handles SIGTERM but needs more time. A PreStop hook (D) can run commands before SIGTERM but handling the signal directly in the application is the most common and robust approach.",
            "conditions": [
                "GKE application shut down forcefully during update, didn't close DB connection, need graceful shutdown."
            ],
            "caseStudyContext": null
        },
        {
            "id": 238,
            "topic": "Databases",
            "question": "You are a lead developer working on a new retail system that runs on Cloud Run and Firestore in Datastore mode. A web UI requirement is for the system to display a list of available products when users access the system and for the user to be able to browse through all products. You have implemented this requirement in the minimum viable product (MVP) phase by returning a list of all available products stored in Firestore.\n\nA few months after go-live, you notice that Cloud Run instances are terminated with HTTP 500: Container instances are exceeding memory limits errors during busy times. This error coincides with spikes in the number of Datastore entity reads. You need to prevent Cloud Run from crashing and decrease the number of Datastore entity reads. You want to use a solution that optimizes system performance. What should you do?",
            "options": {
                "A": "Modify the query that returns the product list using integer offsets.",
                "B": "Modify the query that returns the product list using limits.",
                "C": "Modify the Cloud Run configuration to increase the memory limits.",
                "D": "Modify the query that returns the product list using cursors."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "The issue is that fetching *all* products overwhelms Cloud Run's memory, likely because the list is very large. Increasing memory (C) is a temporary fix that doesn't scale. Efficiently handling large result sets requires pagination. Firestore (in Datastore mode) recommends using query cursors (D) for pagination. Cursors mark a position in the result set, allowing you to fetch the next 'page' of results efficiently. Using limits (B) alone retrieves only the first N results. Using integer offsets (A) is inefficient in Datastore mode as skipped entities are still read internally.",
            "conditions": [
                "Cloud Run app lists all products from Firestore (Datastore mode), crashes with memory errors during high load/reads, need to fix."
            ],
            "caseStudyContext": null
        },
        {
            "id": 239,
            "topic": "Compute",
            "question": "You need to deploy an internet-facing microservices application to Google Kubernetes Engine (GKE). You want to validate new features using the A/B testing method. You have the following requirements for deploying new container image releases:\n• There is no downtime when new container images are deployed.\n• New production releases are tested and verified using a subset of production users.\n\nWhat should you do?",
            "options": {
                "A": "1. Configure your CI/CD pipeline to update the Deployment manifest file by replacing the container version with the latest version.\n2. Recreate the Pods in your cluster by applying the Deployment manifest file.\n3. Validate the application's performance by comparing its functionality with the previous release version, and roll back if an issue arises.",
                "B": "1. Create a second namespace on GKE for the new release version.\n2. Create a Deployment configuration for the second namespace with the desired number of Pods.\n3. Deploy new container versions in the second namespace.\n4. Update the Ingress configuration to route traffic to the namespace with the new container versions.",
                "C": "1. Install the Anthos Service Mesh on your GKE cluster.\n2. Create two Deployments on the GKE cluster, and label them with different version names.\n3. Implement an Istio routing rule to send a small percentage of traffic to the Deployment that references the new version of the application.",
                "D": "1. Implement a rolling update pattern by replacing the Pods gradually with the new release version.\n2. Validate the application's performance for the new subset of users during the rollout, and roll back if an issue arises."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "A/B testing requires routing a subset of users to a new version while the rest see the old version. Option C describes a canary deployment suitable for A/B testing using a service mesh (Anthos Service Mesh/Istio). You deploy both versions concurrently (Step 2) and use service mesh routing rules to direct a specific percentage (or characteristic subset) of traffic to the new version (Step 3), allowing testing with production users. This achieves zero downtime. Recreate (A) causes downtime. Option B describes blue/green, not A/B testing with a subset. Rolling updates (D) affect users gradually but don't easily target a specific subset for testing.",
            "conditions": [
                "Deploy GKE microservices, validate features via A/B testing, zero downtime, test with subset of prod users."
            ],
            "caseStudyContext": null
        },
        {
            "id": 240,
            "topic": "Compute",
            "question": "Your team manages a large Google Kubernetes Engine (GKE) cluster. Several application teams currently use the same namespace to develop microservices for the cluster. Your organization plans to onboard additional teams to create microservices. You need to configure multiple environments while ensuring the security and optimal performance of each team’s work. You want to minimize cost and follow Google-recommended best practices. What should you do?",
            "options": {
                "A": "Create new role-based access controls (RBAC) for each team in the existing cluster, and define resource quotas.",
                "B": "Create a new namespace for each environment in the existing cluster, and define resource quotas.",
                "C": "Create a new GKE cluster for each team.",
                "D": "Create a new namespace for each team in the existing cluster, and define resource quotas."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "Best practice for multi-tenancy within a single GKE cluster (to minimize cost vs. multiple clusters C) involves using namespaces for isolation. Create a separate namespace for *each team* (D) to isolate their resources and apply specific RBAC rules and ResourceQuotas. ResourceQuotas limit the total resources a namespace can consume, ensuring fair sharing and preventing one team from impacting others. RBAC alone (A) doesn't isolate resources. Namespaces per environment (B) doesn't isolate teams if multiple teams share an environment.",
            "conditions": [
                "Multiple teams developing microservices in large GKE cluster, onboard more teams, need multi-environment config, ensure security/performance per team, minimize cost, follow best practices."
            ],
            "caseStudyContext": null
        },
        {
            "id": 241,
            "topic": "Compute",
            "question": "You have a Java application deployed on Cloud Run. Your application requires access to a database hosted on Cloud SQL. Due to regulatory requirements, your connection to the Cloud SQL instance must use its internal IP address. How should you configure the connectivity while following Google-recommended best practices?",
            "options": {
                "A": "Configure your Cloud Run service with a Cloud SQL connection.",
                "B": "Configure your Cloud Run service to use a Serverless VPC Access connector.",
                "C": "Configure your application to use the Cloud SQL Java connector.",
                "D": "Configure your application to connect to an instance of the Cloud SQL Auth proxy."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "Cloud Run services run in a Google-managed environment separate from your VPC. To connect to a Cloud SQL instance using its private/internal IP address, Cloud Run needs a network path into your VPC. The Serverless VPC Access connector (B) provides this bridge, allowing Cloud Run services to reach resources within your VPC, including Cloud SQL instances with private IPs. Cloud SQL connections (A) usually refer to public IP or proxy methods. The Java connector (C) and Auth Proxy (D) primarily handle authentication and connection management, often over public IPs or requiring a network path which the VPC Access connector provides for private IPs.",
            "conditions": [
                "Cloud Run Java app needs to connect to Cloud SQL using internal IP address."
            ],
            "caseStudyContext": null
        },
        {
            "id": 242,
            "topic": "Storage",
            "question": "Your application stores customers’ content in a Cloud Storage bucket, with each object being encrypted with the customer's encryption key. The key for each object in Cloud Storage is entered into your application by the customer. You discover that your application is receiving an HTTP 4xx error when reading the object from Cloud Storage. What is a possible cause of this error?",
            "options": {
                "A": "You attempted the read operation on the object with the customer's base64-encoded key.",
                "B": "You attempted the read operation without the base64-encoded SHA256 hash of the encryption key.",
                "C": "You entered the same encryption algorithm specified by the customer when attempting the read operation.",
                "D": "You attempted the read operation on the object with the base64-encoded SHA256 hash of the customer's key."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "When using Customer-Supplied Encryption Keys (CSEK) with the Cloud Storage JSON API, requests (like GET for reading) must include *both* the base64-encoded AES-256 key itself (in headers like `x-goog-encryption-key`) *and* the base64-encoded SHA256 hash of that key (in headers like `x-goog-encryption-key-sha256`). A 4xx error (like 400 Bad Request) will occur if these headers are missing, malformed, or if the provided key/hash doesn't match the one used during upload. Option B correctly identifies that omitting the required hash header would cause an error. Providing only the key (A) or only the hash (D) is insufficient. Algorithm (C) is typically fixed (AES256).",
            "conditions": [
                "Reading GCS object encrypted with CSEK, receiving HTTP 4xx error."
            ],
            "caseStudyContext": null
        },
        {
            "id": 243,
            "topic": "IAM & Security",
            "question": "You have two Google Cloud projects, named Project A and Project B. You need to create a Cloud Function in Project A that saves the output in a Cloud Storage bucket in Project B. You want to follow the principle of least privilege. What should you do?",
            "options": {
                "A": "1. Create a Google service account in Project B.\n2. Deploy the Cloud Function with the service account in Project A.\n3. Assign this service account the roles/storage.objectCreator role on the storage bucket residing in Project B.",
                "B": "1. Create a Google service account in Project A\n2. Deploy the Cloud Function with the service account in Project A.\n3. Assign this service account the roles/storage.objectCreator role on the storage bucket residing in Project B.",
                "C": "1. Determine the default App Engine service account (PROJECT_ID@appspot.gserviceaccount.com) in Project A.\n2. Deploy the Cloud Function with the default App Engine service account in Project A.\n3. Assign the default App Engine service account the roles/storage.objectCreator role on the storage bucket residing in Project B.",
                "D": "1. Determine the default App Engine service account (PROJECT_ID@appspot.gserviceaccount.com) in Project B.\n2. Deploy the Cloud Function with the default App Engine service account in Project A.\n3. Assign the default App Engine service account the roles/storage.objectCreator role on the storage bucket residing in Project B."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "The Cloud Function runs in Project A, so its runtime identity (service account) must also belong to Project A. Following least privilege, create a dedicated service account in Project A specifically for this function. Deploy the function specifying this service account as its identity. Then, grant this service account (from Project A) the necessary permission (`roles/storage.objectCreator`) on the target resource (the bucket in Project B) (B). Using an SA from Project B (A, D) is incorrect for a function running in A. Using default service accounts (C, D) often grants broader permissions than necessary.",
            "conditions": [
                "Cloud Function in Project A writes to GCS bucket in Project B, follow least privilege."
            ],
            "caseStudyContext": null
        },
        {
            "id": 244,
            "topic": "Monitoring & Logging",
            "question": "A governmental regulation was recently passed that affects your application. For compliance purposes, you are now required to send a duplicate of specific application logs from your application’s project to a project that is restricted to the security team. What should you do?",
            "options": {
                "A": "Create user-defined log buckets in the security team’s project. Configure a Cloud Logging sink to route your application’s logs to log buckets in the security team’s project.",
                "B": "Create a job that copies the logs from the _Required log bucket into the security team’s log bucket in their project.",
                "C": "Modify the _Default log bucket sink rules to reroute the logs into the security team’s log bucket.",
                "D": "Create a job that copies the System Event logs from the _Required log bucket into the security team’s log bucket in their project."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "Cloud Logging sinks allow routing log entries matching a filter to supported destinations, including log buckets in *other* projects. To send specific application logs to a secure project, create a sink in the application's project with a filter matching those logs. Set the destination as a log bucket created in the security team's project (A). The sink needs appropriate permissions granted via its writer identity in the destination project. Copying logs manually or via jobs (B, D) is inefficient. Modifying the `_Default` sink (C) affects all logs, not just specific ones. `_Required` bucket is for specific audit logs.",
            "conditions": [
                "Send duplicate of specific application logs from Project A to a secure Log Bucket in Project B for compliance."
            ],
            "caseStudyContext": null
        },
        {
            "id": 245,
            "topic": "Compute",
            "question": "You plan to deploy a new Go application to Cloud Run. The source code is stored in Cloud Source Repositories. You need to configure a fully managed, automated, continuous deployment pipeline that runs when a source code commit is made. You want to use the simplest deployment solution. What should you do?",
            "options": {
                "A": "Configure a cron job on your workstations to periodically run gcloud run deploy --source in the working directory.",
                "B": "Configure a Jenkins trigger to run the container build and deploy process for each source code commit to Cloud Source Repositories.",
                "C": "Configure continuous deployment of new revisions from a source repository for Cloud Run using buildpacks.",
                "D": "Use Cloud Build with a trigger configured to run the container build and deploy process for each source code commit to Cloud Source Repositories."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "For a fully managed, automated CD pipeline triggered by commits to CSR, Cloud Build is the standard GCP solution. Create a Cloud Build trigger linked to the CSR repository, configured to activate on commits to a specific branch. Define a `cloudbuild.yaml` file (or use inferred build steps) that builds the Go application container and deploys it to Cloud Run using `gcloud run deploy` (D). This is simpler and more integrated than managing Jenkins (B) or relying on cron jobs (A). Option C describes the buildpack mechanism but not the full automated pipeline setup with triggers.",
            "conditions": [
                "Automated CD pipeline for Go app (CSR -> Cloud Run), trigger on commit, fully managed, simplest solution."
            ],
            "caseStudyContext": null
        },
        {
            "id": 246,
            "topic": "Networking",
            "question": "Your team has created an application that is hosted on a Google Kubernetes Engine (GKE) cluster. You need to connect the application to a legacy REST service that is deployed in two GKE clusters in two different regions. You want to connect your application to the target service in a way that is resilient. You also want to be able to run health checks on the legacy service on a separate port. How should you set up the connection? (Choose two.)",
            "options": {
                "A": "Use Traffic Director with a sidecar proxy to connect the application to the service.",
                "B": "Use a proxyless Traffic Director configuration to connect the application to the service.",
                "C": "Configure the legacy service's firewall to allow health checks originating from the sidecar proxy.",
                "D": "Configure the legacy service's firewall to allow health checks originating from the application.",
                "E": "Configure the legacy service's firewall to allow health checks originating from the Traffic Director control plane."
            },
            "correctAnswer": [
                "A",
                "C"
            ],
            "explanation": "Traffic Director provides global load balancing and service discovery across multiple GKE clusters/regions, enhancing resilience. Using sidecar proxies (Envoy deployed via Traffic Director) (A) intercepts traffic from the calling application and routes it intelligently to healthy backends in either region. Traffic Director configures health checks for the backends. These health checks originate from the Envoy proxies co-located with the *calling* application (or dedicated Google health check systems integrated with TD, often represented by the proxy). Therefore, the legacy service's firewall must allow ingress for these health checks from the proxies (C). Proxyless gRPC doesn't apply to REST. Health checks don't originate directly from the app (D) or the TD control plane (E).",
            "conditions": [
                "Connect GKE app to legacy REST service in 2 GKE clusters (diff regions), resilient connection, health checks on separate port."
            ],
            "caseStudyContext": null
        },
        {
            "id": 247,
            "topic": "Compute",
            "question": "You have an application running in a production Google Kubernetes Engine (GKE) cluster. You use Cloud Deploy to automatically deploy your application to your production GKE cluster. As part of your development process, you are planning to make frequent changes to the application’s source code and need to select the tools to test the changes before pushing them to your remote source code repository. Your toolset must meet the following requirements:\n• Test frequent local changes automatically.\n• Local deployment emulates production deployment.\n\nWhich tools should you use to test building and running a container on your laptop using minimal resources?",
            "options": {
                "A": "Docker Compose and dockerd",
                "B": "Terraform and kubeadm",
                "C": "Minikube and Skaffold",
                "D": "kaniko and Tekton"
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "To emulate a GKE production deployment locally for testing, you need a local Kubernetes environment and a tool to streamline the build/deploy cycle. Minikube provides a lightweight, single-node Kubernetes cluster running locally. Skaffold automates the inner development loop: it watches source code, automatically builds container images (using Docker, Jib, Buildpacks, etc.), pushes them (if needed), and deploys manifests to a Kubernetes cluster (like Minikube). This combination (C) closely emulates the K8s deployment environment locally with automation for frequent changes. Docker Compose (A) isn't Kubernetes. Terraform/kubeadm (B) are for infrastructure setup. Kaniko/Tekton (D) are build/pipeline tools often used in CI/CD, not primarily for local dev loops.",
            "conditions": [
                "Test local code changes for GKE app automatically before pushing, local deployment must emulate production, minimal local resources."
            ],
            "caseStudyContext": null
        },
        {
            "id": 248,
            "topic": "Compute",
            "question": "You are deploying a Python application to Cloud Run using Cloud Source Repositories and Cloud Build. The Cloud Build pipeline is shown below:\n\n```yaml\nsteps:\n- name: python:3.9\n  entrypoint: pip\n  args: ['install', '-r', 'requirements.txt', '--user']\n- name: python:3.9\n  entrypoint: python\n  args: ['main.py']\n- name: gcr.io/cloud-builders/docker\n  args: ['build', '-t', 'gcr.io/myproject/myimage', '.']\n- name: gcr.io/cloud-builders/docker\n  args: ['push', 'gcr.io/myproject/myimage']\n- name: gcr.io/google.com/cloudsdktool/cloud-sdk\n  entrypoint: gcloud\n  args: ['run', 'deploy', 'myservice', '--image=gcr.io/myproject/myimage', '--region=us-central1']\n```\n\nYou want to optimize deployment times and avoid unnecessary steps. What should you do?",
            "options": {
                "A": "Remove the step that pushes the container to Artifact Registry.",
                "B": "Deploy a new Docker registry in a VPC, and use Cloud Build worker pools inside the VPC to run the build pipeline.",
                "C": "Store image artifacts in a Cloud Storage bucket in the same region as the Cloud Run instance.",
                "D": "Add the --cache-from argument to the Docker build step in your build config file."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "The provided pipeline installs dependencies and runs the app outside Docker, then builds and pushes a Docker image, and finally deploys it. To optimize build times, especially the `docker build` step, leveraging Docker layer caching is crucial. Adding the `--cache-from` argument to the `docker build` step, pointing to a previously built image (e.g., the latest successful build of `gcr.io/myproject/myimage`), allows Docker to reuse unchanged layers, significantly speeding up the build, especially dependency installation (D). Removing the push (A) prevents deployment. A custom registry (B) adds complexity. Storing artifacts in GCS (C) isn't standard for Docker builds.",
            "conditions": [
                "Optimize Cloud Build pipeline (Python app build/push/deploy to Cloud Run)."
            ],
            "caseStudyContext": null
        },
        {
            "id": 249,
            "topic": "Compute",
            "question": "You are developing an event-driven application. You have created a topic to receive messages sent to Pub/Sub. You want those messages to be processed in real time. You need the application to be independent from any other system and only incur costs when new messages arrive. How should you configure the architecture?",
            "options": {
                "A": "Deploy the application on Compute Engine. Use a Pub/Sub push subscription to process new messages in the topic.",
                "B": "Deploy your code on Cloud Functions. Use a Pub/Sub trigger to invoke the Cloud Function. Use the Pub/Sub API to create a pull subscription to the Pub/Sub topic and read messages from it.",
                "C": "Deploy the application on Google Kubernetes Engine. Use the Pub/Sub API to create a pull subscription to the Pub/Sub topic and read messages from it.",
                "D": "Deploy your code on Cloud Functions. Use a Pub/Sub trigger to handle new messages in the topic."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "The requirements are: event-driven (Pub/Sub trigger), real-time processing, independence, pay-per-use (only when messages arrive). Cloud Functions (D) perfectly match this. They are serverless, event-driven, and you only pay when they execute. Configuring a Pub/Sub trigger automatically invokes the function in near real-time whenever a message arrives on the topic. Compute Engine (A) and GKE (C) incur costs even when idle and require managing infrastructure. Option B unnecessarily adds a manual pull subscription within the triggered function.",
            "conditions": [
                "Event-driven app, process Pub/Sub messages in real time, independent system, pay only on message arrival."
            ],
            "caseStudyContext": null
        },
        {
            "id": 250,
            "topic": "Monitoring & Logging",
            "question": "You have an application running on Google Kubernetes Engine (GKE). The application is currently using a logging library and is outputting to standard output. You need to export the logs to Cloud Logging, and you need the logs to include metadata about each request. You want to use the simplest method to accomplish this. What should you do?",
            "options": {
                "A": "Change your application’s logging library to the Cloud Logging library, and configure your application to export logs to Cloud Logging.",
                "B": "Update your application to output logs in JSON format, and add the necessary metadata to the JSON.",
                "C": "Update your application to output logs in CSV format, and add the necessary metadata to the CSV.",
                "D": "Install the Fluent Bit agent on each of your GKE nodes, and have the agent export all logs from /var/log."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "GKE clusters with Cloud Operations for GKE enabled (default) automatically collect logs written to standard output (stdout) and standard error (stderr) by containers and send them to Cloud Logging. To include custom metadata and ensure logs are parsed correctly by Cloud Logging, the simplest method is to structure the log output as single-line JSON objects written to stdout (B). Cloud Logging can automatically detect and parse JSON, making the fields (including your custom metadata) searchable. Changing libraries (A) requires more code changes. CSV (C) isn't automatically parsed like JSON. The agent (Fluent Bit/fluentd) is already running by default (D).",
            "conditions": [
                "GKE app logs to stdout, need logs in Cloud Logging with request metadata, simplest method."
            ],
            "caseStudyContext": null
        },
        {
            "id": 251,
            "topic": "Networking",
            "question": "You are working on a new application that is deployed on Cloud Run and uses Cloud Functions. Each time new features are added, new Cloud Functions and Cloud Run services are deployed. You use ENV variables to keep track of the services and enable interservice communication, but the maintenance of the ENV variables has become difficult. You want to implement dynamic discovery in a scalable way. What should you do?",
            "options": {
                "A": "Configure your microservices to use the Cloud Run Admin and Cloud Functions APIs to query for deployed Cloud Run services and Cloud Functions in the Google Cloud project.",
                "B": "Create a Service Directory namespace. Use API calls to register the services during deployment, and query during runtime.",
                "C": "Rename the Cloud Functions and Cloud Run services endpoint is using a well-documented naming convention.",
                "D": "Deploy Hashicorp Consul on a single Compute Engine instance. Register the services with Consul during deployment, and query during runtime."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "Manually managing endpoints via environment variables becomes difficult as services change. Service Directory is Google Cloud's managed service discovery platform. You can register your Cloud Run and Cloud Function services (and other services) with Service Directory during deployment (e.g., via API calls in CI/CD). At runtime, other services can query Service Directory (via API or DNS) to dynamically discover the current endpoints of the services they need to call (B). Querying Admin APIs directly (A) is less efficient. Naming conventions (C) don't provide dynamic resolution. Self-managing Consul (D) adds operational overhead compared to managed Service Directory.",
            "conditions": [
                "Cloud Run/Cloud Function microservices, ENV vars for service endpoints hard to maintain, need scalable dynamic discovery."
            ],
            "caseStudyContext": null
        },
        {
            "id": 252,
            "topic": "Compute",
            "question": "You work for a financial services company that has a container-first approach. Your team develops microservices applications. A Cloud Build pipeline creates the container image, runs regression tests, and publishes the image to Artifact Registry. You need to ensure that only containers that have passed the regression tests are deployed to Google Kubernetes Engine (GKE) clusters. You have already enabled Binary Authorization on the GKE clusters. What should you do next?",
            "options": {
                "A": "Create an attestor and a policy. After a container image has successfully passed the regression tests, use Cloud Build to run Kritis Signer to create an attestation for the container image.",
                "B": "Deploy Voucher Server and Voucher Client components. After a container image has successfully passed the regression tests, run Voucher Client as a step in the Cloud Build pipeline.",
                "C": "Set the Pod Security Standard level to Restricted for the relevant namespaces. Use Cloud Build to digitally sign the container images that have passed the regression tests.",
                "D": "Create an attestor and a policy. Create an attestation for the container images that have passed the regression tests as a step in the Cloud Build pipeline."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "Binary Authorization requires attestations to verify image compliance. The process is: 1. Define an attestor (representing the testing process) and a policy requiring attestation from this attestor. 2. In the Cloud Build pipeline, *after* regression tests pass, add a step to create an attestation for the specific image digest, referencing the attestor. This can be done using `gcloud beta container binauthz attestations create` (D). Kritis Signer (A) or Voucher (B) are specific tools/frameworks that can *implement* attestation creation, but the core step is creating the attestation itself. Pod Security Standards (C) relate to pod runtime security, not deployment authorization based on prior checks.",
            "conditions": [
                "GKE deployment, images built/tested/pushed via Cloud Build, Binary Authorization enabled, need to ensure only tested images are deployed."
            ],
            "caseStudyContext": null
        },
        {
            "id": 253,
            "topic": "IAM & Security",
            "question": "You are reviewing and updating your Cloud Build steps to adhere to best practices. Currently, your build steps include:\n\n1. Pull the source code from a source repository.\n2. Build a container image\n3. Upload the built image to Artifact Registry.\n\nYou need to add a step to perform a vulnerability scan of the built container image, and you want the results of the scan to be available to your deployment pipeline running in Google Cloud. You want to minimize changes that could disrupt other teams’ processes. What should you do?",
            "options": {
                "A": "Enable Binary Authorization, and configure it to attest that no vulnerabilities exist in a container image.",
                "B": "Upload the built container images to your Docker Hub instance, and scan them for vulnerabilities.",
                "C": "Enable the Container Scanning API in Artifact Registry, and scan the built container images for vulnerabilities.",
                "D": "Add Artifact Registry to your Aqua Security instance, and scan the built container images for vulnerabilities."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "Artifact Registry integrates with Container Analysis to provide automated vulnerability scanning. By enabling the Container Scanning API (C), images pushed to Artifact Registry will be automatically scanned for known OS and language pack vulnerabilities. The results are stored in Container Analysis and accessible via API or the Cloud Console. This is a managed solution, requires minimal change (just enabling the API), and makes results available within GCP for downstream pipeline steps (like Binary Authorization attestors or custom checks). Binary Authorization (A) *enforces* policies based on scan results/attestations, but doesn't perform the scan. External registries (B) or tools (D) add complexity.",
            "conditions": [
                "Add vulnerability scanning step to Cloud Build pipeline (after image push to AR), results available to deployment pipeline, minimize disruption."
            ],
            "caseStudyContext": null
        },
        {
            "id": 254,
            "topic": "Monitoring & Logging",
            "question": "You are developing an online gaming platform as a microservices application on Google Kubernetes Engine (GKE). Users on social media are complaining about long loading times for certain URL requests to the application. You need to investigate performance bottlenecks in the application and identify which HTTP requests have a significantly high latency span in user requests. What should you do?",
            "options": {
                "A": "Configure GKE workload metrics using kubectl. Select all Pods to send their metrics to Cloud Monitoring. Create a custom dashboard of application metrics in Cloud Monitoring to determine performance bottlenecks of your GKE cluster.",
                "B": "Update your microservices to log HTTP request methods and URL paths to STDOUT. Use the logs router to send container logs to Cloud Logging. Create filters in Cloud Logging to evaluate the latency of user requests across different methods and URL paths.",
                "C": "Instrument your microservices by installing the OpenTelemetry tracing package. Update your application code to send traces to Trace for inspection and analysis. Create an analysis report on Trace to analyze user requests.",
                "D": "Install tcpdump on your GKE nodes. Run tcpdump to capture network traffic over an extended period of time to collect data. Analyze the data files using Wireshark to determine the cause of high latency."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "To understand latency within and across microservices for specific user requests, distributed tracing is the appropriate tool. Instrumenting the microservices with OpenTelemetry (or OpenCensus) allows capturing trace data (spans showing time spent in different parts of the request path). Sending this data to Cloud Trace enables visualization and analysis of request latency, identifying which services or operations contribute most to the overall delay for specific URLs (C). Metrics (A) provide aggregated data, not per-request traces. Logs (B) can show timing but tracing gives a clearer picture of distributed calls. Network dumps (D) are very low-level and hard to analyze for application-level latency.",
            "conditions": [
                "GKE microservices app, users report slow loading times for specific URLs, need to identify high-latency requests/bottlenecks."
            ],
            "caseStudyContext": null
        },
        {
            "id": 255,
            "topic": "Compute",
            "question": "You need to load-test a set of REST API endpoints that are deployed to Cloud Run. The API responds to HTTP POST requests. Your load tests must meet the following requirements:\n• Load is initiated from multiple parallel threads.\n• User traffic to the API originates from multiple source IP addresses.\n• Load can be scaled up using additional test instances.\n\nYou want to follow Google-recommended best practices. How should you configure the load testing?",
            "options": {
                "A": "Create an image that has cURL installed, and configure cURL to run a test plan. Deploy the image in a managed instance group, and run one instance of the image for each VM.",
                "B": "Create an image that has cURL installed, and configure cURL to run a test plan. Deploy the image in an unmanaged instance group, and run one instance of the image for each VM.",
                "C": "Deploy a distributed load testing framework on a private Google Kubernetes Engine cluster. Deploy additional Pods as needed to initiate more traffic and support the number of concurrent users.",
                "D": "Download the container image of a distributed load testing framework on Cloud Shell. Sequentially start several instances of the container on Cloud Shell to increase the load on the API."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "The requirements point to needing a distributed, scalable load generation system. Deploying a distributed load testing framework (like Locust, k6, JMeter) on GKE (C) allows you to easily scale the number of load-generating Pods (workers) to achieve high parallelism and simulate many users/threads. GKE can distribute these pods across multiple nodes, potentially providing different source IPs (especially if using multiple node pools or external IPs). Using simple cURL on GCE (A, B) is less sophisticated and harder to scale/coordinate. Cloud Shell (D) is not suitable for scalable load generation.",
            "conditions": [
                "Load test Cloud Run REST API (POST requests), requirements: parallel threads, multiple source IPs, scalable load."
            ],
            "caseStudyContext": null
        },
        {
            "id": 256,
            "topic": "IAM & Security",
            "question": "Your team is creating a serverless web application on Cloud Run. The application needs to access images stored in a private Cloud Storage bucket. You want to give the application Identity and Access Management (IAM) permission to access the images in the bucket, while also securing the services using Google-recommended best practices. What should you do?",
            "options": {
                "A": "Enforce signed URLs for the desired bucket. Grant the Storage Object Viewer IAM role on the bucket to the Compute Engine default service account.",
                "B": "Enforce public access prevention for the desired bucket. Grant the Storage Object Viewer IAM role on the bucket to the Compute Engine default service account.",
                "C": "Enforce signed URLs for the desired bucket. Create and update the Cloud Run service to use a user-managed service account. Grant the Storage Object Viewer IAM role on the bucket to the service account.",
                "D": "Enforce public access prevention for the desired bucket. Create and update the Cloud Run service to use a user-managed service account. Grant the Storage Object Viewer IAM role on the bucket to the service account."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "Best practices: 1. Keep buckets private: Enforce public access prevention. 2. Least privilege: Create a dedicated user-managed service account for the Cloud Run service. Grant this service account only the necessary permissions (e.g., `roles/storage.objectViewer`) on the specific bucket. 3. Configure Cloud Run to run *as* this service account. This allows the application to authenticate using its service account identity (via ADC) without needing keys (D). Signed URLs (A, C) are for granting access *to end users*, not for service-to-service access. Using the default Compute Engine SA (A, B) grants overly broad permissions.",
            "conditions": [
                "Cloud Run app needs access to private GCS bucket, follow security best practices (least privilege, private bucket)."
            ],
            "caseStudyContext": null
        },
        {
            "id": 257,
            "topic": "Compute",
            "question": "You are using Cloud Run to host a global ecommerce web application. Your company’s design team is creating a new color scheme for the web app. You have been tasked with determining whether the new color scheme will increase sales. You want to conduct testing on live production traffic. How should you design the study?",
            "options": {
                "A": "Use an external HTTP(S) load balancer to route a predetermined percentage of traffic to two different color schemes of your application. Analyze the results to determine whether there is a statistically significant difference in sales.",
                "B": "Use an external HTTP(S) load balancer to route traffic to the original color scheme while the new deployment is created and tested. After testing is complete, reroute all traffic to the new color scheme. Analyze the results to determine whether there is a statistically significant difference in sales.",
                "C": "Use an external HTTP(S) load balancer to mirror traffic to the new version of your application. Analyze the results to determine whether there is a statistically significant difference in sales.",
                "D": "Enable a feature flag that displays the new color scheme to half of all users. Monitor sales to see whether they increase for this group of users."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "This is a classic A/B testing scenario aimed at comparing user behavior (sales) between two versions (color schemes). Deploying both versions (e.g., as separate Cloud Run revisions or services) and using a mechanism to split live traffic randomly between them is required. An external HTTP(S) Load Balancer configured for weighted traffic splitting (A) can route, for example, 50% of users to the old scheme and 50% to the new scheme, allowing for direct comparison of sales metrics. Option B is blue/green. Option C is traffic mirroring. Option D (feature flags) is another valid A/B technique, but LB splitting is also common and directly supported.",
            "conditions": [
                "A/B test new website color scheme on Cloud Run using live production traffic, measure impact on sales."
            ],
            "caseStudyContext": null
        },
        {
            "id": 258,
            "topic": "Compute",
            "question": "You are a developer at a large corporation. You manage three Google Kubernetes Engine clusters on Google Cloud. Your team’s developers need to switch from one cluster to another regularly without losing access to their preferred development tools. You want to configure access to these multiple clusters while following Google-recommended best practices. What should you do?",
            "options": {
                "A": "Ask the developers to use Cloud Shell and run gcloud container clusters get-credential to switch to another cluster.",
                "B": "In a configuration file, define the clusters, users, and contexts. Share the file with the developers and ask them to use kubect1 contig to add cluster, user, and context details.",
                "C": "Ask the developers to install the gcloud CLI on their workstation and run gcloud container clusters get-credentials to switch to another cluster.",
                "D": "Ask the developers to open three terminals on their workstation and use kubect1 config to configure access to each cluster."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "To allow developers to use their local tools (IDEs, etc.) while easily switching between GKE cluster contexts, installing the `gcloud` CLI locally is the standard approach. The `gcloud container clusters get-credentials CLUSTER_NAME --zone/region REGION_NAME` command automatically fetches cluster details and authentication info (using the developer's gcloud login) and automatically configures the local `kubectl` (kubeconfig file) with the necessary contexts. Developers can then switch context using `kubectl config use-context` (C). Cloud Shell (A) requires working within the browser. Manually managing kubeconfig files (B, D) is cumbersome and error-prone compared to the `gcloud` integration.",
            "conditions": [
                "Developers need to switch between multiple GKE clusters regularly from their workstations, use preferred tools, follow best practices."
            ],
            "caseStudyContext": null
        },
        {
            "id": 259,
            "topic": "Databases",
            "question": "You are a lead developer working on a new retail system that runs on Cloud Run and Firestore. A web UI requirement is for the user to be able to browse through all products. A few months after go-live, you notice that Cloud Run instances are terminated with HTTP 500: Container instances are exceeding memory limits errors during busy times. This error coincides with spikes in the number of Firestore queries.\n\nYou need to prevent Cloud Run from crashing and decrease the number of Firestore queries. You want to use a solution that optimizes system performance. What should you do?",
            "options": {
                "A": "Modify the query that returns the product list using cursors with limits.",
                "B": "Create a custom index over the products.",
                "C": "Modify the query that returns the product list using integer offsets.",
                "D": "Modify the Cloud Run configuration to increase the memory limits."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "Similar to Q238, the root cause is fetching too many product documents at once, overwhelming Cloud Run's memory. The solution is pagination. Firestore's recommended pagination method uses query cursors combined with limits. A limit restricts the number of documents returned per page, and a cursor marks the position to start the next page fetch (A). This drastically reduces the number of entities read per request and the memory needed by Cloud Run. Offsets (C) are inefficient. Custom indexes (B) improve query speed but don't solve fetching too much data. Increasing memory (D) is a temporary fix.",
            "conditions": [
                "Cloud Run app lists all products from Firestore, crashes with memory errors during high load/queries, need to fix."
            ],
            "caseStudyContext": null
        },
        {
            "id": 260,
            "topic": "Compute",
            "question": "You are a developer at a large organization. Your team uses Git for source code management (SCM). You want to ensure that your team follows Google-recommended best practices to manage code to drive higher rates of software delivery. Which SCM process should your team use?",
            "options": {
                "A": "Each developer commits their code to the main branch before each product release, conducts testing, and rolls back if integration issues are detected.",
                "B": "Each group of developers copies the repository, commits their changes to their repository, and merges their code into the main repository before each product release.",
                "C": "Each developer creates a branch for their own work, commits their changes to their branch, and merges their code into the main branch daily.",
                "D": "Each group of developers creates a feature branch from the main branch for their work, commits their changes to their branch, and merges their code into the main branch after the change advisory board approves it."
            },
            "correctAnswer": [
                "C"
            ],
            "explanation": "Google (and DevOps best practices like DORA) recommend trunk-based development or short-lived feature branches with frequent integration into the main branch (daily or more often) to enable high software delivery rates. Option C describes this: developers work on short-lived branches and merge back daily, minimizing merge conflicts and enabling continuous integration. Committing directly to main before release (A) is risky. Copying repos (B) is not standard Git workflow. Long-lived feature branches with delayed merges pending CAB approval (D) slow down delivery and increase merge complexity.",
            "conditions": [
                "Select SCM process, follow Google best practices, drive high software delivery rates."
            ],
            "caseStudyContext": null
        },
        {
            "id": 261,
            "topic": "Compute",
            "question": "You have a web application that publishes messages to Pub/Sub. You plan to build new versions of the application locally and want to quickly test Pub/Sub integration for each new build. How should you configure local testing?",
            "options": {
                "A": "Install Cloud Code on the integrated development environment (IDE). Navigate to Cloud APIs, and enable Pub/Sub against a valid Google Project ID. When developing locally, configure your application to call pubsub.googleapis.com.",
                "B": "Install the Pub/Sub emulator using gcloud, and start the emulator with a valid Google Project ID. When developing locally, configure your application to use the local emulator with ${gcloud beta emulators pubsub env-init}.",
                "C": "In the Google Cloud console, navigate to the API Library, and enable the Pub/Sub API. When developing locally, configure your application to call pubsub.googleapis.com.",
                "D": "Install the Pub/Sub emulator using gcloud, and start the emulator with a valid Google Project IWhen developing locally, configure your application to use the local emulator by exporting the PUBSUB_EMULATOR_HOST variable."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "For local testing of Pub/Sub integration, the emulator is the recommended approach. You install it via `gcloud components install pubsub-emulator`. Start the emulator. To configure your application (running on the same machine) to use the emulator, the `gcloud beta emulators pubsub env-init` command prints the necessary environment variable (`PUBSUB_EMULATOR_HOST`) settings. You can either export these manually (like in D) or, more conveniently, use shell evaluation like `eval $(gcloud beta emulators pubsub env-init)` to set them automatically in the current shell (implied by B's syntax). Calling the real API (A, C) isn't local testing.",
            "conditions": [
                "Test Pub/Sub publishing integration locally for new app builds."
            ],
            "caseStudyContext": null
        },
        {
            "id": 262,
            "topic": "Monitoring & Logging",
            "question": "Your ecommerce application receives external requests and forwards them to third-party API services for credit card processing, shipping, and inventory management as shown in the diagram.\n\n[Diagram: User -> Ecommerce App -> [Credit Card API, Shipping API, Inventory API]]\n\nYour customers are reporting that your application is running slowly at unpredictable times. The application doesn’t report any metrics. You need to determine the cause of the inconsistent performance. What should you do?",
            "options": {
                "A": "Install the OpenTelemetry library for your respective language, and instrument your application.",
                "B": "Install the Ops Agent inside your container and configure it to gather application metrics.",
                "C": "Modify your application to read and forward the X-Cloud-Trace-Context header when it calls the downstream services.",
                "D": "Enable Managed Service for Prometheus on the Google Kubernetes Engine cluster to gather application metrics."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "The problem involves unpredictable slowness likely related to dependencies (third-party APIs). Distributed tracing is needed to pinpoint where latency occurs. OpenTelemetry (A) is the industry standard for instrumenting applications to generate traces (and metrics/logs). By adding OpenTelemetry libraries and instrumenting the calls within the ecommerce app (including calls to the third-party APIs), you can capture end-to-end traces and send them to a backend like Cloud Trace for analysis, identifying which service calls are slow. Ops Agent (B) collects system/runtime metrics. Forwarding trace context (C) is part of tracing but requires instrumentation first. Prometheus (D) is for metrics, not tracing.",
            "conditions": [
                "Ecommerce app calls 3rd-party APIs, slow/inconsistent performance, no metrics currently, need to find cause."
            ],
            "caseStudyContext": null
        },
        {
            "id": 263,
            "topic": "Compute",
            "question": "You are developing a new application. You want the application to be triggered only when a given file is updated in your Cloud Storage bucket. Your trigger might change, so your process must support different types of triggers. You want the configuration to be simple so that multiple team members can update the triggers in the future. What should you do?",
            "options": {
                "A": "Configure Cloud Storage events to be sent to Pub/Sub, and use Pub/Sub events to trigger a Cloud Build job that executes your application.",
                "B": "Create an Eventarc trigger that monitors your Cloud Storage bucket for a specific filename, and set the target as Cloud Run.",
                "C": "Configure a Cloud Function that executes your application and is triggered when an object is updated in Cloud Storage.",
                "D": "Configure a Firebase function that executes your application and is triggered when an object is updated in Cloud Storage."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "Eventarc is Google Cloud's unified eventing platform. It allows routing events from various sources (like Cloud Storage, Pub/Sub, Audit Logs) to different targets (like Cloud Run, Cloud Functions, Workflows). It supports flexible filtering (e.g., specific filename patterns) and decouples the event source from the target, making it easy to change triggers or targets later (B). Direct Cloud Function triggers (C) are simpler for basic GCS events but less flexible if trigger types change. Pub/Sub+Cloud Build (A) adds extra steps. Firebase functions (D) are typically for Firebase projects.",
            "conditions": [
                "Trigger application when specific GCS file updated, support changing trigger types, simple config for team updates."
            ],
            "caseStudyContext": null
        },
        {
            "id": 264,
            "topic": "Compute",
            "question": "You are defining your system tests for an application running in Cloud Run in a Google Cloud project. You need to create a testing environment that is isolated from the production environment. You want to fully automate the creation of the testing environment with the least amount of effort and execute automated tests. What should you do?",
            "options": {
                "A": "Using Cloud Build, execute Terraform scripts to create a new Google Cloud project and a Cloud Run instance of your application in the Google Cloud project.",
                "B": "Using Cloud Build, execute a Terraform script to deploy a new Cloud Run revision in the existing Google Cloud project. Use traffic splitting to send traffic to your test environment.",
                "C": "Using Cloud Build, execute gcloud commands to create a new Google Cloud project and a Cloud Run instance of your application in the Google Cloud project.",
                "D": "Using Cloud Build, execute gcloud commands to deploy a new Cloud Run revision in the existing Google Cloud project. Use traffic splitting to send traffic to your test environment."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "The requirement is a *fully isolated* testing environment, automated creation with least effort. Creating a completely new, ephemeral Google Cloud project for each test run (A or C) provides the strongest isolation. Using Infrastructure as Code (IaC) like Terraform (A) is the standard way to fully automate the creation and teardown of cloud resources, including projects and Cloud Run services, making it repeatable and manageable. Using gcloud commands (C) works but Terraform is generally preferred for complex infrastructure automation. Deploying revisions within the *same* project (B, D) does not provide full isolation from the production environment.",
            "conditions": [
                "Create isolated test environment for Cloud Run app, fully automated creation, least effort, run automated tests."
            ],
            "caseStudyContext": null
        },
        {
            "id": 265,
            "topic": "Compute",
            "question": "You are a cluster administrator for Google Kubernetes Engine (GKE). Your organization’s clusters are enrolled in a release channel. You need to be informed of relevant events that affect your GKE clusters, such as available upgrades and security bulletins. What should you do?",
            "options": {
                "A": "Configure cluster notifications to be sent to a Pub/Sub topic.",
                "B": "Execute a scheduled query against the google_cloud_release_notes BigQuery dataset.",
                "C": "Query the GKE API for available versions.",
                "D": "Create an RSS subscription to receive a daily summary of the GKE release notes."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "GKE provides a Cluster Notifications feature that allows administrators to subscribe to specific types of cluster-related events, including available upgrades and security bulletins. These notifications can be published to a Pub/Sub topic (A). You can then integrate this topic with other systems (e.g., alerting, ticketing) to ensure relevant teams are informed. Querying BigQuery datasets (B) or the GKE API (C) requires active polling. RSS feeds (D) provide general release notes but not specific notifications tailored to your cluster's channel and version.",
            "conditions": [
                "Need notifications for GKE cluster events (upgrades, security bulletins) for clusters in release channels."
            ],
            "caseStudyContext": null
        },
        {
            "id": 266,
            "topic": "Compute",
            "question": "You are tasked with using C++ to build and deploy a microservice for an application hosted on Google Cloud. The code needs to be containerized and use several custom software libraries that your team has built. You do not want to maintain the underlying infrastructure of the application. How should you deploy the microservice?",
            "options": {
                "A": "Use Cloud Functions to deploy the microservice.",
                "B": "Use Cloud Build to create the container, and deploy it on Cloud Run.",
                "C": "Use Cloud Shell to containerize your microservice, and deploy it on a Container-Optimized OS Compute Engine instance.",
                "D": "Use Cloud Shell to containerize your microservice, and deploy it on standard Google Kubernetes Engine."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "The requirements are: C++ microservice, containerized, uses custom libraries, minimize infrastructure maintenance (prefer serverless/managed). Cloud Run (B) is a fully managed platform for running stateless containers. It supports custom containers built using any language (including C++) and libraries. You can use Cloud Build to automate the container build process (including compiling C++ code and linking custom libraries) and then deploy the resulting container to Cloud Run. Cloud Functions (A) doesn't natively support C++. GCE (C) and GKE (D) require managing underlying VMs or clusters, violating the minimal infrastructure maintenance requirement.",
            "conditions": [
                "Deploy C++ containerized microservice using custom libraries, minimize infra maintenance."
            ],
            "caseStudyContext": null
        },
        {
            "id": 267,
            "topic": "IAM & Security",
            "question": "You need to containerize a web application that will be hosted on Google Cloud behind a global load balancer with SSL certificates. You don’t have the time to develop authentication at the application level, and you want to offload SSL encryption and management from your application. You want to configure the architecture using managed services where possible. What should you do?",
            "options": {
                "A": "Host the application on Google Kubernetes Engine, and deploy an NGINX Ingress Controller to handle authentication.",
                "B": "Host the application on Google Kubernetes Engine, and deploy cert-manager to manage SSL certificates.",
                "C": "Host the application on Compute Engine, and configure Cloud Endpoints for your application.",
                "D": "Host the application on Google Kubernetes Engine, and use Identity-Aware Proxy (IAP) with Cloud Load Balancing and Google-managed certificates."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "Requirements: Containerized app, global LB, managed SSL, offload auth, managed services. Hosting on GKE meets the container requirement. Google Cloud Load Balancing (GCLB) provides global load balancing and can use Google-managed SSL certificates, offloading SSL management. Identity-Aware Proxy (IAP) integrates with GCLB to provide managed authentication based on Google identities, eliminating the need for application-level auth development (D). Nginx Ingress (A) requires self-management. Cert-manager (B) manages certs but doesn't handle auth. Cloud Endpoints (C) is an API gateway, not typically used for securing a standard web app this way, and GCE isn't container-native.",
            "conditions": [
                "Containerized web app, global LB, managed SSL, offload authentication, managed services."
            ],
            "caseStudyContext": null
        },
        {
            "id": 268,
            "topic": "Compute",
            "question": "You manage a system that runs on stateless Compute Engine VMs and Cloud Run instances. Cloud Run is connected to a VPC, and the ingress setting is set to Internal. You want to schedule tasks on Cloud Run. You create a service account and grant it the roles/run.invoker Identity and Access Management (IAM) role. When you create a schedule and test it, a 403 Permission Denied error is returned in Cloud Logging. What should you do?",
            "options": {
                "A": "Grant the service account the roles/run.developer IAM role.",
                "B": "Configure a cron job on the Compute Engine VMs to trigger Cloud Run on schedule.",
                "C": "Change the Cloud Run ingress setting to 'Internal and Cloud Load Balancing.'",
                "D": "Use Cloud Scheduler with Pub/Sub to invoke Cloud Run."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "Cloud Run services with 'Internal' ingress can only be invoked by resources within the same VPC network or by specific authorized Google Cloud services like Cloud Tasks or Pub/Sub via push subscriptions configured with appropriate authentication. Cloud Scheduler cannot directly invoke an internal Cloud Run service. The standard pattern for scheduling internal Cloud Run services is to have Cloud Scheduler publish a message to a Pub/Sub topic. Create a Pub/Sub push subscription targeting the Cloud Run service's HTTPS endpoint, configured to use the service account with the `run.invoker` role for authentication (D). Granting broader roles (A) or changing ingress (C) doesn't solve the invocation path issue. Triggering from GCE (B) works but uses GCE instead of a managed scheduler.",
            "conditions": [
                "Schedule tasks on Cloud Run service with Internal ingress, getting 403 error from scheduler."
            ],
            "caseStudyContext": null
        },
        {
            "id": 269,
            "topic": "Databases",
            "question": "You work on an application that relies on Cloud Spanner as its main datastore. New application features have occasionally caused performance regressions. You want to prevent performance issues by running an automated performance test with Cloud Build for each commit made. If multiple commits are made at the same time, the tests might run concurrently. What should you do?",
            "options": {
                "A": "Create a new project with a random name for every build. Load the required data. Delete the project after the test is run.",
                "B": "Create a new Cloud Spanner instance for every build. Load the required data. Delete the Cloud Spanner instance after the test is run.",
                "C": "Create a project with a Cloud Spanner instance and the required data. Adjust the Cloud Build build file to automatically restore the data to its previous state after the test is run.",
                "D": "Start the Cloud Spanner emulator locally. Load the required data. Shut down the emulator after the test is run."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "Performance testing requires an environment that closely resembles production, ruling out the emulator (D) which has limitations. Tests need isolation, especially when run concurrently. Creating a completely new project (A) is excessive overhead. Reusing a single instance and restoring data (C) creates contention and makes concurrent runs unreliable. Creating a dedicated, ephemeral Cloud Spanner *instance* for each build (B) provides the necessary isolation for concurrent performance tests against a real Spanner environment. While potentially costly, it's the most reliable way to get accurate, isolated performance results.",
            "conditions": [
                "Automated performance testing for Spanner app in Cloud Build per commit, handle concurrent tests."
            ],
            "caseStudyContext": null
        },
        {
            "id": 270,
            "topic": "Compute",
            "question": "Your company's security team uses Identity and Access Management (IAM) to track which users have access to which resources. You need to create a version control system that can integrate with your security team's processes. You want your solution to support fast release cycles and frequent merges to your main branch to minimize merge conflicts. What should you do?",
            "options": {
                "A": "Create a Cloud Source Repositories repository, and use trunk-based development.",
                "B": "Create a Cloud Source Repositories repository, and use feature-based development.",
                "C": "Create a GitHub repository, mirror it to a Cloud Source Repositories repository, and use trunk-based development.",
                "D": "Create a GitHub repository, mirror it to a Cloud Source Repositories repository, and use feature-based development."
            },
            "correctAnswer": [
                "A"
            ],
            "explanation": "The requirements are IAM integration and support for fast cycles/frequent merges (trunk-based development). Cloud Source Repositories (CSR) integrates directly with Google Cloud IAM for access control (A, B). Trunk-based development involves developers merging small changes frequently into the main ('trunk') branch, minimizing merge conflicts and enabling fast cycles (A, C). Combining CSR for IAM integration and trunk-based development (A) directly meets the requirements. Using GitHub and mirroring (C, D) adds complexity and potential delays, although IAM can still control CSR access. Feature-based development (B, D) typically involves longer-lived branches, counter to the frequent merge requirement.",
            "conditions": [
                "Create VCS integrated with GCP IAM, support fast release cycles/frequent merges."
            ],
            "caseStudyContext": null
        },
        {
            "id": 271,
            "topic": "Compute",
            "question": "You recently developed an application that monitors a large number of stock prices. You need to configure Pub/Sub to receive messages and update the current stock price in an in-memory database. A downstream service needs the most up-to-date prices in the in-memory database to perform stock trading transactions. Each message contains three pieces or information:\n\n• Stock symbol\n• Stock price\n• Timestamp for the update\n\nHow should you set up your Pub/Sub subscription?",
            "options": {
                "A": "Create a push subscription with exactly-once delivery enabled.",
                "B": "Create a pull subscription with both ordering and exactly-once delivery turned off.",
                "C": "Create a pull subscription with ordering enabled, using the stock symbol as the ordering key.",
                "D": "Create a push subscription with both ordering and exactly-once delivery turned off."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "The downstream service needs the *most up-to-date* price. This implies that processing older messages for the same stock symbol is unnecessary or even harmful. Therefore, message ordering (C) is not required and might even introduce delays if an older message blocks newer ones for the same key. Exactly-once delivery (A) adds overhead and is only available for pull subscriptions; since only the latest price matters, at-least-once delivery is acceptable. A push subscription (D) delivers messages with low latency as they arrive, fitting the need for up-to-date prices, without the overhead of ordering or exactly-once guarantees.",
            "conditions": [
                "Pub/Sub subscription for stock price updates, downstream needs most up-to-date price for trading."
            ],
            "caseStudyContext": null
        },
        {
            "id": 272,
            "topic": "Databases",
            "question": "You are a developer at a social media company. The company runs their social media website on-premises and uses MySQL as a backend to store user profiles and user posts. Your company plans to migrate to Google Cloud, and your learn will migrate user profile information to Firestore. You are tasked with designing the Firestore collections. What should you do?",
            "options": {
                "A": "Create one root collection for user profiles, and create one root collection for user posts.",
                "B": "Create one root collection for user profiles, and create one subcollection for each user's posts.",
                "C": "Create one root collection for user profiles, and store each user's post as a nested list in the user profile document.",
                "D": "Create one root collection for user posts, and create one subcollection for each user's profile."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "Firestore uses a collection/document model. User profiles fit naturally into a root collection (`users`) where each document represents a user (`users/{userId}`). User posts have a one-to-many relationship with users. Storing posts in a subcollection under the corresponding user document (`users/{userId}/posts/{postId}`) (B) is the standard Firestore pattern for this. It allows efficient retrieval of all posts for a specific user. Separate root collections (A) make relating posts to users harder. Nested lists (C) are limited by document size. Storing profiles under posts (D) is illogical.",
            "conditions": [
                "Migrate MySQL user profiles/posts to Firestore, design collections."
            ],
            "caseStudyContext": null
        },
        {
            "id": 273,
            "topic": "Monitoring & Logging",
            "question": "Your team has created an application that is hosted on a Google Kubemetes Engine (GKE) cluster. You need to connect the application to a legacy REST service that is deployed in two GKE clusters in two different regions. You want to connect your application to the legacy service in a way that is resilient and requires the fewest number of steps. You also want to be able to run probe-based health checks on the legacy service on a separate port. How should you set up the connection? (Choose two.)",
            "options": {
                "A": "Create a Cloud Function that consumes the Monitoring API. Create a schedule to trigger the Cloud Function hourly and alert you if the average memory consumption is outside the defined range.",
                "B": "In Cloud Monitoring, create an alerting policy to notify you if the average memory consumption is outside the defined range.",
                "C": "Create a Cloud Function that runs on a schedule, executes kubectl top on all the workloads on the cluster, and sends an email alert if the average memory consumption is outside the defined range.",
                "D": "Write a script that pulls the memory consumption of the instance at the OS level and sends an email alert if the average memory consumption is outside the defined range."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "GKE clusters integrated with Cloud Operations automatically send container and pod metrics, including memory consumption, to Cloud Monitoring. The simplest way to get alerts based on these metrics is to create an alerting policy directly within Cloud Monitoring (B). You can define conditions based on the average memory consumption metric (e.g., `kubernetes.io/container/memory/used_bytes`), set the thresholds (20% and 80%), and configure notification channels. Using Cloud Functions (A, C) or custom scripts (D) adds unnecessary complexity for standard metric-based alerting.",
            "conditions": [
                "Alert when average GKE container memory consumption is <20% or >80%."
            ],
            "caseStudyContext": null
        },
        {
            "id": 274,
            "topic": "Compute",
            "question": "You manage a microservice-based ecommerce platform on Google Cloud that sends confirmation emails to a third-party email service provider using a Cloud Function. Your company just launched a marketing campaign, and some customers are reporting that they have not received order confirmation emails. You discover that the services triggering the Cloud Function are receiving HTTP 500 errors. You need to change the way emails are handled to minimize email loss. What should you do?",
            "options": {
                "A": "Increase the Cloud Function's timeout to nine minutes.",
                "B": "Configure the sender application to publish the outgoing emails in a message to a Pub/Sub topic. Update the Cloud Function configuration to consume the Pub/Sub queue.",
                "C": "Configure the sender application to write emails to Memorystore and then trigger the Cloud Function. When the function is triggered, it reads the email details from Memorystore and sends them to the email service.",
                "D": "Configure the sender application to retry the execution of the Cloud Function every one second if a request fails."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "Receiving HTTP 500 errors from the Cloud Function suggests it might be failing intermittently, possibly due to issues calling the third-party email service or internal problems, especially under load from the marketing campaign. Directly calling the function synchronously means failures can lead to lost emails. Decoupling the sending service from the email function using Pub/Sub (B) provides resilience. The sender publishes the email request to a topic. The Cloud Function becomes a Pub/Sub-triggered function, consuming messages from the topic's subscription. Pub/Sub automatically handles retries if the function fails temporarily, minimizing email loss. Increasing timeout (A), using Memorystore (C), or simple retries (D) don't provide the same level of decoupling and resilience.",
            "conditions": [
                "Cloud Function sends emails via 3rd-party API, triggering services get 500 errors from function, need to minimize email loss."
            ],
            "caseStudyContext": null
        },
        {
            "id": 275,
            "topic": "Compute",
            "question": "You have a web application that publishes messages to Pub/Sub. You plan to build new versions of the application locally and need to quickly test Pub/Sub integration for each new build. How should you configure local testing?",
            "options": {
                "A": "In the Google Cloud console, navigate to the API Library, and enable the Pub/Sub API. When developing locally configure your application to call pubsub.googleapis.com.",
                "B": "Install the Pub/Sub emulator using gcloud, and start the emulator with a valid Google Project ID. When developing locally, configure your applicat.cn to use the local emulator by exporting the PUBSUB_EMULATOR_HOST variable.",
                "C": "Run the gcloud config set api_endpoint_overrides/pubsub https://pubsubemulator.googleapis.com.com/ command to change the Pub/Sub endpoint prior to starting the application.",
                "D": "Install Cloud Code on the integrated development environment (IDE). Navigate to Cloud APIs, and enable Pub/Sub against a valid Google Project IWhen developing locally, configure your application to call pubsub.googleapis.com."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "This is similar to Q261. To test Pub/Sub integration locally quickly and reliably, use the Pub/Sub emulator. Install it via gcloud. Start the emulator. Configure your local application to connect to the emulator endpoint, typically by setting the `PUBSUB_EMULATOR_HOST` environment variable (B). This directs the Pub/Sub client library calls to the local emulator instead of the live service. Calling the real API (A, D) is not local testing. Setting gcloud config overrides (C) affects gcloud itself, not necessarily the application's client library.",
            "conditions": [
                "Test Pub/Sub publishing integration locally for new app builds."
            ],
            "caseStudyContext": null
        },
        {
            "id": 276,
            "topic": "Databases",
            "question": "You recently developed an application that monitors a large number of stock prices. You need to configure Pub/Sub to receive a high volume messages and update the current stock price in a single large in-memory database. A downstream service needs the most up-to-date prices in the in-memory database to perform stock trading transactions. Each message contains three pieces or information:\n• Stock symbol\n• Stock price\n• Timestamp for the update\n\nHow should you set up your Pub/Sub subscription?",
            "options": {
                "A": "Create a pull subscription with exactly-once delivery enabled.",
                "B": "Create a push subscription with both ordering and exactly-once delivery turned off.",
                "C": "Create a push subscription with exactly-once delivery enabled.",
                "D": "Create a pull subscription with both ordering and exactly-once delivery turned off."
            },
            "correctAnswer": [
                "B"
            ],
            "explanation": "The critical requirement is updating an in-memory DB with the *most up-to-date* price for low-latency downstream access. Ordering messages per stock (C) can delay updates if an old message blocks newer ones. Exactly-once delivery (A, C) adds overhead and is only available for pull; it's unnecessary if overwriting with the latest price is acceptable. A push subscription (B, C) minimizes latency by sending messages directly as they arrive. Since ordering and exactly-once aren't strictly needed and low latency is key, a simple push subscription with default at-least-once delivery and no ordering (B) is the most appropriate fit.",
            "conditions": [
                "Pub/Sub for high-volume stock prices, update in-memory DB, downstream needs latest price, low latency important."
            ],
            "caseStudyContext": null
        },
        {
            "id": 277,
            "topic": "Networking",
            "question": "Your team has created an application that is hosted on a Google Kubemetes Engine (GKE) cluster. You need to connect the application to a legacy REST service that is deployed in two GKE clusters in two different regions. You want to connect your application to the legacy service in a way that is resilient and requires the fewest number of steps. You also want to be able to run probe-based health checks on the legacy service on a separate port. How should you set up the connection? (Choose two.)",
            "options": {
                "A": "Use Traffic Director with a sidecar proxy to connect the application to the service.",
                "B": "Set up a proxyless Traffic Director configuration for the application.",
                "C": "Configure the legacy service's firewall to allow health checks originating from the sidecar proxy.",
                "D": "Configure the legacy service's firewall to allow health checks originating from the application.",
                "E": "Configure the legacy service's firewall to allow health checks originating from the Traffic Director control plane."
            },
            "correctAnswer": [
                "A",
                "C"
            ],
            "explanation": "This is identical to Q246. Traffic Director provides resilient, multi-region routing. Using sidecar proxies (A) allows Traffic Director to manage traffic to the legacy REST services. Traffic Director configures health checks, which are executed by the sidecar proxies (or associated Google health checkers). Therefore, the firewalls for the legacy service clusters must allow ingress traffic for these health checks on the specified port, originating from the proxies/health checkers (C).",
            "conditions": [
                "Connect GKE app to legacy REST service in 2 GKE clusters (diff regions), resilient connection, health checks on separate port, fewest steps."
            ],
            "caseStudyContext": null
        },
        {
            "id": 278,
            "topic": "Monitoring & Logging",
            "question": "You are monitoring a web application that is written in Go and deployed in Google Kubernetes Engine. You notice an increase in CPU and memory utilization. You need to determine which function is consuming the most CPU and memory resources. What should you do?",
            "options": {
                "A": "Add print commands to the application source code to log when each function is called, and redeploy the application.",
                "B": "Create a Cloud Logging query that gathers the web application s logs. Write a Python script that calculates the difference between the timestamps from the beginning and the end of the application's longest functions to identify time-intensive functions.",
                "C": "Import OpenTelemetry and Trace export packages into your application, and create the trace provider. Review the latency data for your application on the Trace overview page, and identify which functions cause the most latency.",
                "D": "Import the Cloud Profiler package into your application, and initialize the Profiler agent. Review the generated flame graph in the Google Cloud console to identify time-intensive functions."
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "This is identical to Q206. Cloud Profiler is the tool for identifying which specific functions within an application (including Go) are consuming CPU and memory. Instrumenting the app with the Profiler agent and analyzing the resulting flame graphs (D) provides this function-level insight.",
            "conditions": [
                "Go app on GKE, increased CPU/memory usage, need to find resource-intensive functions."
            ],
            "caseStudyContext": null
        },
        {
            "id": 279,
            "topic": "Compute",
            "question": "You are developing a flower ordering application. Currently you have three microservices:\n• Order Service (receives the orders)\n• Order Fulfillment Service (processes the orders)\n• Notification Service (notifies the customer when the order is filled)\n\nYou need to determine how the services will communicate with each other. You want incoming orders to be processed quickly and you need to collect order information for fulfillment. You also want to make sure orders are not lost between your services and are able to communicate asynchronously. How should the requests be processed?",
            "options": {
                "A": "[Diagram: Order request -> Order Service -> Order Fulfillment Service -> Notification Service]",
                "B": "[Diagram: Order request -> Order Service -> Pub/Sub -> Order Fulfillment Service -> Notification Service]",
                "C": "[Diagram: Order request -> Order Service -> Order Fulfillment Service -> Pub/Sub -> Notification Service]",
                "D": "[Diagram: Order request -> Order Service -> Pub/Sub queue -> Order Fulfillment Service -> Firestore Database -> Pub/Sub queue -> Notification Service]"
            },
            "correctAnswer": [
                "D"
            ],
            "explanation": "The requirements are: process orders quickly (implies decoupling Order Service), collect info for fulfillment, ensure orders aren't lost, asynchronous communication. Option D provides the best decoupling and resilience: Order Service receives request, quickly publishes an 'order received' event to Pub/Sub. Fulfillment Service subscribes, processes the order (persisting details likely needed, e.g., to Firestore), and then publishes an 'order fulfilled' event to another Pub/Sub topic. Notification Service subscribes to the 'fulfilled' topic to send the notification. This makes the flow asynchronous, resilient (Pub/Sub retains messages), and decoupled. Options A, B, C show less robust or incomplete flows.",
            "conditions": [
                "Microservices communication (Order, Fulfillment, Notification), process quickly, no lost orders, asynchronous."
            ],
            "caseStudyContext": null
        }
    ]
    }